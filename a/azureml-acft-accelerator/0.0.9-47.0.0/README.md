# Comparing `tmp/azureml_acft_accelerator-0.0.9-py3-none-any.whl.zip` & `tmp/azureml_acft_accelerator-47.0.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,48 +1,55 @@
-Zip file size: 178811 bytes, number of entries: 46
--rw-rw-rw-  2.0 fat      267 b- defN 23-May-17 17:31 azureml/__init__.py
--rw-rw-rw-  2.0 fat      267 b- defN 23-May-17 17:31 azureml/acft/__init__.py
--rw-rw-rw-  2.0 fat   753597 b- defN 23-May-17 17:31 azureml/acft/accelerator/NOTICE
--rw-rw-rw-  2.0 fat      367 b- defN 23-May-17 17:31 azureml/acft/accelerator/__init__.py
--rw-rw-rw-  2.0 fat     5548 b- defN 23-May-17 17:31 azureml/acft/accelerator/constants.py
--rw-rw-rw-  2.0 fat    15809 b- defN 23-May-17 17:31 azureml/acft/accelerator/finetune.py
--rw-rw-rw-  2.0 fat      305 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/__init__.py
--rw-rw-rw-  2.0 fat    14790 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_layers.py
--rw-rw-rw-  2.0 fat     9729 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_wrapper.py
--rw-rw-rw-  2.0 fat      747 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/__init__.py
--rw-rw-rw-  2.0 fat     1126 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_bart.py
--rw-rw-rw-  2.0 fat     1406 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_base.py
--rw-rw-rw-  2.0 fat      960 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_bert.py
--rw-rw-rw-  2.0 fat     1140 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_bert_base_uncased.py
--rw-rw-rw-  2.0 fat      965 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_camembert.py
--rw-rw-rw-  2.0 fat     1206 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_deberta.py
--rw-rw-rw-  2.0 fat     1217 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_deberta_base.py
--rw-rw-rw-  2.0 fat      975 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_distilbert.py
--rw-rw-rw-  2.0 fat     1153 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_distilbert_base_uncased.py
--rw-rw-rw-  2.0 fat     1250 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_gpt2.py
--rw-rw-rw-  2.0 fat     1270 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_gpt_neox.py
--rw-rw-rw-  2.0 fat      972 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_llama.py
--rw-rw-rw-  2.0 fat     1139 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_mbart.py
--rw-rw-rw-  2.0 fat      966 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_roberta.py
--rw-rw-rw-  2.0 fat     1125 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_roberta_base.py
--rw-rw-rw-  2.0 fat     1114 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_t5.py
--rw-rw-rw-  2.0 fat     1140 b- defN 23-May-17 17:31 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_turing_nlr_v3_cased.py
--rw-rw-rw-  2.0 fat      298 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/__init__.py
--rw-rw-rw-  2.0 fat      837 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/config.py
--rw-rw-rw-  2.0 fat     3690 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/decorators.py
--rw-rw-rw-  2.0 fat    19656 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/hf_argparser.py
--rw-rw-rw-  2.0 fat     2152 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/license_utils.py
--rw-rw-rw-  2.0 fat     8750 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/logging_utils.py
--rw-rw-rw-  2.0 fat     3715 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/model_utils.py
--rw-rw-rw-  2.0 fat     3267 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/run_utils.py
--rw-rw-rw-  2.0 fat    17885 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/trainer_utils.py
--rw-rw-rw-  2.0 fat      307 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/error_handling/__init__.py
--rw-rw-rw-  2.0 fat     5015 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/error_handling/error_definitions.py
--rw-rw-rw-  2.0 fat     1763 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/error_handling/error_strings.py
--rw-rw-rw-  2.0 fat     3772 b- defN 23-May-17 17:31 azureml/acft/accelerator/utils/error_handling/exceptions.py
--rw-rw-rw-  2.0 fat   753597 b- defN 23-May-17 17:31 azureml_acft_accelerator-0.0.9.data/data/azureml-acft-accelerator/NOTICE
--rw-rw-rw-  2.0 fat      859 b- defN 23-May-17 17:36 azureml_acft_accelerator-0.0.9.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     1566 b- defN 23-May-17 17:36 azureml_acft_accelerator-0.0.9.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-May-17 17:36 azureml_acft_accelerator-0.0.9.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 23-May-17 17:36 azureml_acft_accelerator-0.0.9.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     5076 b- defN 23-May-17 17:36 azureml_acft_accelerator-0.0.9.dist-info/RECORD
-46 files, 1652860 bytes uncompressed, 170255 bytes compressed:  89.7%
+Zip file size: 195114 bytes, number of entries: 53
+-rw-rw-rw-  2.0 fat      267 b- defN 24-Mar-22 10:04 azureml/__init__.py
+-rw-rw-rw-  2.0 fat      267 b- defN 24-Mar-22 10:04 azureml/acft/__init__.py
+-rw-rw-rw-  2.0 fat   753597 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/NOTICE
+-rw-rw-rw-  2.0 fat      888 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/__init__.py
+-rw-rw-rw-  2.0 fat       36 b- defN 24-Mar-22 10:09 azureml/acft/accelerator/_version.py
+-rw-rw-rw-  2.0 fat    10075 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/constants.py
+-rw-rw-rw-  2.0 fat    19692 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/finetune.py
+-rw-rw-rw-  2.0 fat      305 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/__init__.py
+-rw-rw-rw-  2.0 fat    14789 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_layers.py
+-rw-rw-rw-  2.0 fat    10493 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_wrapper.py
+-rw-rw-rw-  2.0 fat    12775 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/peft_lora_wrapper.py
+-rw-rw-rw-  2.0 fat      784 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/__init__.py
+-rw-rw-rw-  2.0 fat     1126 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_bart.py
+-rw-rw-rw-  2.0 fat     1406 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_base.py
+-rw-rw-rw-  2.0 fat      960 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_bert.py
+-rw-rw-rw-  2.0 fat     1140 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_bert_base_uncased.py
+-rw-rw-rw-  2.0 fat      965 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_camembert.py
+-rw-rw-rw-  2.0 fat     1206 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_deberta.py
+-rw-rw-rw-  2.0 fat     1217 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_deberta_base.py
+-rw-rw-rw-  2.0 fat      975 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_distilbert.py
+-rw-rw-rw-  2.0 fat     1153 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_distilbert_base_uncased.py
+-rw-rw-rw-  2.0 fat      978 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_falcon.py
+-rw-rw-rw-  2.0 fat     1250 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_gpt2.py
+-rw-rw-rw-  2.0 fat     1270 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_gpt_neox.py
+-rw-rw-rw-  2.0 fat      972 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_llama.py
+-rw-rw-rw-  2.0 fat     1139 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_mbart.py
+-rw-rw-rw-  2.0 fat      966 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_roberta.py
+-rw-rw-rw-  2.0 fat     1125 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_roberta_base.py
+-rw-rw-rw-  2.0 fat     1114 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_t5.py
+-rw-rw-rw-  2.0 fat     1140 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_turing_nlr_v3_cased.py
+-rw-rw-rw-  2.0 fat      298 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/__init__.py
+-rw-rw-rw-  2.0 fat    23869 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/callbacks.py
+-rw-rw-rw-  2.0 fat      664 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/checkpoint_utils.py
+-rw-rw-rw-  2.0 fat     2225 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/code_utils.py
+-rw-rw-rw-  2.0 fat      837 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/config.py
+-rw-rw-rw-  2.0 fat     4681 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/decorators.py
+-rw-rw-rw-  2.0 fat     1940 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/deepspeed_utils.py
+-rw-rw-rw-  2.0 fat    19656 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/hf_argparser.py
+-rw-rw-rw-  2.0 fat     2176 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/license_utils.py
+-rw-rw-rw-  2.0 fat     8753 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/logging_utils.py
+-rw-rw-rw-  2.0 fat     3766 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/model_utils.py
+-rw-rw-rw-  2.0 fat     5638 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/run_utils.py
+-rw-rw-rw-  2.0 fat    14352 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/trainer_utils.py
+-rw-rw-rw-  2.0 fat      307 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/error_handling/__init__.py
+-rw-rw-rw-  2.0 fat     6335 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/error_handling/error_definitions.py
+-rw-rw-rw-  2.0 fat     2909 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/error_handling/error_strings.py
+-rw-rw-rw-  2.0 fat     3772 b- defN 24-Mar-22 10:04 azureml/acft/accelerator/utils/error_handling/exceptions.py
+-rw-rw-rw-  2.0 fat   753597 b- defN 24-Mar-22 10:04 azureml_acft_accelerator-47.0.0.data/data/azureml-acft-accelerator/NOTICE
+-rw-rw-rw-  2.0 fat      859 b- defN 24-Mar-22 10:09 azureml_acft_accelerator-47.0.0.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     1719 b- defN 24-Mar-22 10:09 azureml_acft_accelerator-47.0.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-Mar-22 10:09 azureml_acft_accelerator-47.0.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 24-Mar-22 10:09 azureml_acft_accelerator-47.0.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     5826 b- defN 24-Mar-22 10:09 azureml_acft_accelerator-47.0.0.dist-info/RECORD
+53 files, 1708349 bytes uncompressed, 185324 bytes compressed:  89.2%
```

## zipnote {}

```diff
@@ -6,14 +6,17 @@
 
 Filename: azureml/acft/accelerator/NOTICE
 Comment: 
 
 Filename: azureml/acft/accelerator/__init__.py
 Comment: 
 
+Filename: azureml/acft/accelerator/_version.py
+Comment: 
+
 Filename: azureml/acft/accelerator/constants.py
 Comment: 
 
 Filename: azureml/acft/accelerator/finetune.py
 Comment: 
 
 Filename: azureml/acft/accelerator/lora_wrapper/__init__.py
@@ -21,14 +24,17 @@
 
 Filename: azureml/acft/accelerator/lora_wrapper/lora_layers.py
 Comment: 
 
 Filename: azureml/acft/accelerator/lora_wrapper/lora_wrapper.py
 Comment: 
 
+Filename: azureml/acft/accelerator/lora_wrapper/peft_lora_wrapper.py
+Comment: 
+
 Filename: azureml/acft/accelerator/lora_wrapper/lora_configs/__init__.py
 Comment: 
 
 Filename: azureml/acft/accelerator/lora_wrapper/lora_configs/lora_bart.py
 Comment: 
 
 Filename: azureml/acft/accelerator/lora_wrapper/lora_configs/lora_base.py
@@ -51,14 +57,17 @@
 
 Filename: azureml/acft/accelerator/lora_wrapper/lora_configs/lora_distilbert.py
 Comment: 
 
 Filename: azureml/acft/accelerator/lora_wrapper/lora_configs/lora_distilbert_base_uncased.py
 Comment: 
 
+Filename: azureml/acft/accelerator/lora_wrapper/lora_configs/lora_falcon.py
+Comment: 
+
 Filename: azureml/acft/accelerator/lora_wrapper/lora_configs/lora_gpt2.py
 Comment: 
 
 Filename: azureml/acft/accelerator/lora_wrapper/lora_configs/lora_gpt_neox.py
 Comment: 
 
 Filename: azureml/acft/accelerator/lora_wrapper/lora_configs/lora_llama.py
@@ -78,20 +87,32 @@
 
 Filename: azureml/acft/accelerator/lora_wrapper/lora_configs/lora_turing_nlr_v3_cased.py
 Comment: 
 
 Filename: azureml/acft/accelerator/utils/__init__.py
 Comment: 
 
+Filename: azureml/acft/accelerator/utils/callbacks.py
+Comment: 
+
+Filename: azureml/acft/accelerator/utils/checkpoint_utils.py
+Comment: 
+
+Filename: azureml/acft/accelerator/utils/code_utils.py
+Comment: 
+
 Filename: azureml/acft/accelerator/utils/config.py
 Comment: 
 
 Filename: azureml/acft/accelerator/utils/decorators.py
 Comment: 
 
+Filename: azureml/acft/accelerator/utils/deepspeed_utils.py
+Comment: 
+
 Filename: azureml/acft/accelerator/utils/hf_argparser.py
 Comment: 
 
 Filename: azureml/acft/accelerator/utils/license_utils.py
 Comment: 
 
 Filename: azureml/acft/accelerator/utils/logging_utils.py
@@ -114,26 +135,26 @@
 
 Filename: azureml/acft/accelerator/utils/error_handling/error_strings.py
 Comment: 
 
 Filename: azureml/acft/accelerator/utils/error_handling/exceptions.py
 Comment: 
 
-Filename: azureml_acft_accelerator-0.0.9.data/data/azureml-acft-accelerator/NOTICE
+Filename: azureml_acft_accelerator-47.0.0.data/data/azureml-acft-accelerator/NOTICE
 Comment: 
 
-Filename: azureml_acft_accelerator-0.0.9.dist-info/LICENSE.txt
+Filename: azureml_acft_accelerator-47.0.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_acft_accelerator-0.0.9.dist-info/METADATA
+Filename: azureml_acft_accelerator-47.0.0.dist-info/METADATA
 Comment: 
 
-Filename: azureml_acft_accelerator-0.0.9.dist-info/WHEEL
+Filename: azureml_acft_accelerator-47.0.0.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_acft_accelerator-0.0.9.dist-info/top_level.txt
+Filename: azureml_acft_accelerator-47.0.0.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_acft_accelerator-0.0.9.dist-info/RECORD
+Filename: azureml_acft_accelerator-47.0.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/acft/accelerator/__init__.py

```diff
@@ -1,10 +1,27 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 
 __path__ = __import__('pkgutil').extend_path(__path__, __name__)  # type: ignore
 
+import sys
 from . import finetune
 from . import constants
 from . import utils
 from . import lora_wrapper
+from azureml.automl.core.shared import logging_utilities, log_server
+
+try:
+    from ._version import ver as VERSION, selfver as SELFVERSION
+    __version__ = VERSION
+except ImportError:
+    VERSION = '0.0.0+dev'
+    SELFVERSION = VERSION
+    __version__ = VERSION
+
+PROJECT_NAME = __name__
+
+# Mark this package as being allowed to log certain built-in types
+module = sys.modules[__name__]
+logging_utilities.mark_package_exceptions_as_loggable(module)
+log_server.install_sockethandler(__name__)
```

## azureml/acft/accelerator/constants.py

```diff
@@ -1,37 +1,47 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 
 """File for adding all the constants"""
 
 from dataclasses import dataclass, field
-from typing import Optional
+from typing import Optional, List, Union
 
 from transformers.utils import WEIGHTS_NAME
 
 
 @dataclass
+class ErrorConstants:
+
+    CUDA_OUT_OF_MEMORY_ERROR = "CUDA out of memory"
+    AUTO_FIND_BATCH_SIZE_MEMORY_ERROR = "No executable batch size found, reached zero"
+    LOSS_SCALE_AT_MINIMUM = "Current loss scale already at minimum - cannot decrease scale anymore"
+
+
+@dataclass
 class SaveFileConstants:
     """
     A class to represent constants for metadata related to saving the model.
     """
     OPTIMIZATION_ARGS_SAVE_PATH = "Azureml_finetune_optimization_args.json"
     IO_ARGS_SAVE_PATH = "Azureml_io_args.json"
     LICENSE_SAVE_PATH = "LICENSE"
     ACFT_TRAINER_CHECKPOINTS_PATH = "/tmp/acft/trainer/"
+    CHECKPOINT_DONE_PATH = "checkpoint_done.txt"
 
 
 @dataclass
 class HfConstants:
     """
     A class to represent constants for hugging face files.
     """
     PT_WEIGHTS_FILE = "pytorch_model.bin"
     TOKENIZER_FILE = "tokenizer.json"
+    CONFIG_FILE = "config.json"
 
 
 @dataclass
 class HfTrainerType:
     SEQ2SEQ: str = "Seq2Seq"
     DEFAULT: str = "default"
 
@@ -73,36 +83,114 @@
 class AzuremlConstants:
     """
     General constants
     """
     LORA_LAYER_SEARCH_STRINGS = ["lora_A", "lora_B"]
     LORA_BASE_FOLDER = "Azureml_ft_lora_dir"
     LORA_WEIGHTS_NAME = WEIGHTS_NAME
+    BASE_WEIGHTS_FOLDER = "Azureml_base_weights"
+    LORA_SAFE_TENSORS_WEIGHTS_NAME = "pytorch_model.safetensors"
+
+
+@dataclass
+class PeftLoRAConstants:
+    """
+    Peft LoRA constants
+    """
+    PEFT_ADAPTER_WEIGHTS_FOLDER = "peft_adapter_weights"
+    PEFT_ADAPTER_CONFIG_FILE_NAME = "adapter_config.json"
+    PEFT_LORA_BASE_MODEL_PATH_KEY = "base_model_name_or_path"
+    PEFT_LORA_LAYER_PREFIX = "lora"
+    PEFT_MODEL_LAYER_PREFIX = "base_model.model."
+    ACFT_PEFT_CHECKPOINT_PATH = "/tmp/acft_peft_lora_adapter"
 
 
 @dataclass
 class HfModelTypes:
     GPT2 = "gpt2"
     ROBERTA = "roberta"
     DEBERTA = "deberta"
     DISTILBERT = "distilbert"
     BERT = "bert"
     BART = "bart"
     MBART = "mbart"
     T5 = "t5"
     CAMEMBERT = "camembert"
     GPT_NEOX = "gpt_neox"
-    LLAMA= "llama"
+    LLAMA = "llama"
+    FALCON = "falcon"
+    REFINEDWEBMODEL = "RefinedWebModel"
+
+
+@dataclass
+class LoraAlgo:
+    """
+    Lora algorithm to use
+        1. custom lora implementation defined under :folder utils/lora_wrapper
+        2. PEFT
+        3. AUTO
+    """
+    CUSTOM = "custom"
+    PEFT = "peft"
+    AUTO = "auto"
+
+
+@dataclass
+class LoraSaveFormat:
+    """
+    File format to save Lora layers
+        1. pytorch
+        2. safetensors
+    """
+    PYTORCH = "pytorch"
+    SAFETENSORS = "safetensors"
 
 
 @dataclass
 class _AzuremlOptimizationArgs:
     """Optimization args of azureml"""
 
+    # Quantization parameters
+    finetune_in_8bit: bool = field(
+        default=False,
+        metadata={
+            "help": "enable 8 bit training"
+        }
+    )
+    finetune_in_4bit: bool = field(
+        default=False,
+        metadata={
+            "help": "enable 4 bit training"
+        }
+    )
+
     # LoRA parameters
+    lora_algo: str = field(
+        default=LoraAlgo.AUTO,
+        metadata={
+            "help": "lora algorithm to use - `custom` or `peft`."
+            "When `apply_lora` is set to True and lora_algo is set to auto, the following configurations will enable peft."
+            "1.4bit finetuning\n2.8bit finetuning\n3.deepspeed stage3 finetuning"
+        }
+    )
+    lora_target_modules: Union[str, None, List[str]] = field(
+        default=None,
+        metadata={
+            "help": "List of module names or regex expression of the module names to replace with Lora."
+            "For example, ['q', 'v'] or '.*decoder.*(SelfAttention|EncDecAttention).*(q|v)$'. For few model families,"
+            "this information is already a part of mapping TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING in the "
+            "pkg"
+        }
+    )
+    peft_task_type: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "peft task type will help to identify which peft model to load"
+        }
+    )
     apply_lora: bool = field(
         default=False,
         metadata={
             "help": "If set to true, LoRA will be applied"
         },
     )
     lora_alpha: int = field(
@@ -113,17 +201,25 @@
         default=0.0,
         metadata={"help": "lora dropout value"},
     )
     lora_r: int = field(
         default=8,
         metadata={"help": "lora dimension"},
     )
+    lora_save_format: str = field(
+        default=LoraSaveFormat.SAFETENSORS,
+        metadata={"help": "format to use to save lora layers"},
+    )
+    lora_save_path: Optional[str] = field(
+        default=None,
+        metadata={"help": "additional output path to save only lora layers"},
+    )
     model_type: Optional[str] = field(
         default=None,
-        metadata={"help": "family of the model to which lora needs to be applied for"}
+        metadata={"help": "family of the model to which lora needs to be applied for"},
     )
 
     # model parameters
     model_name: Optional[str] = field(
         default=None,
         metadata={"help": "Model name"}
     )
@@ -146,14 +242,38 @@
     deepspeed_config: Optional[str] = field(
         default=None,
         metadata={
             "help": "Deepspeed config to be used for finetuning"
         },
     )
 
+    # Flash attention
+    apply_flash_attention: bool = field(
+        default=False,
+        metadata={
+            "help": "If set to true, will enable flash attention for training."
+        },
+    )
+    flash_attention_version: int = field(
+        default=-1,
+        metadata={
+            "help": "Flash attention version being used for finetuning. If -1 flash attention is disabled."
+        },
+    )
+
+    # Leaf Modules for MoE models
+    leaf_modules_of_moe_models: Optional[List[str]] = field(
+        default=None,
+        metadata={
+            "help": "List of Leaf modules of MoE models. These layers will have `_z3_leaf=True` in their module."
+            " For example, ['MixtralSparseMoeBlock'] is used as leaf_modules_of_moe_models"
+            " for mistralai/Mixtral-8x7B-v0.1 as it has MoE layers."
+        },
+    )
+
     # Evaluation interval
     evaluation_steps_interval: int = field(
         default=0,
         metadata={
             "help": "Steps between 2 evaluations"
         }
     )
@@ -200,10 +320,34 @@
 
     # Input
     model_selector_output: str = field(
         metadata={"help": "Output directory of model selector component"}
     )
 
 
+@dataclass
+class _AzuremlModelMetaArgs:
+    """Model Meta args of azureml"""
+
+    model_metadata: str = field(
+        metadata={"help": "model metadata info to be dumped in MLModel file and checkpoints"}
+    )
+
+
 class AzuremlRunType:
     PIPELINE_RUN = "azureml.PipelineRun"
     STEP_RUN = "azureml.StepRun"
+    HYPERDRIVE_RUN = "hyperdrive"
+    AUTOML_RUN = "automl"
+    FINETUNE_RUN = "finetuning"
+
+
+class RunPropertyConstants:
+    """Run property constants (keys and values)"""
+
+    # Keys
+    SCORE = "score"
+    RUN_ALGORITHM = "run_algorithm"
+    RUN_TEMPLATE = "runTemplate"
+
+    # Values
+    AUTOML_CHILD = "automl_child"
```

## azureml/acft/accelerator/finetune.py

```diff
@@ -9,51 +9,55 @@
 import os
 import json
 import time
 import math
 from dataclasses import asdict
 from typing import Any, Callable, Dict, List, Union, Optional
 from pathlib import Path
-import shutil
 
 from transformers import TrainerCallback
 from transformers import PreTrainedTokenizerBase, PreTrainedModel
 from transformers.trainer import Trainer
 from transformers.trainer_seq2seq import Seq2SeqTrainer
 from transformers.trainer_callback import EarlyStoppingCallback
 from transformers.trainer_utils import IntervalStrategy
+from transformers.deepspeed import is_deepspeed_zero3_enabled
 
 import torch
 import torch.nn as nn
 import torch.distributed as dist
 from torch.utils.data import Dataset as TorchDataset, IterableDataset as TorchIterableDataset
 
 from datasets.arrow_dataset import Dataset as DatasetsDataset
 from datasets.iterable_dataset import IterableDataset as DatasetsIterableDataset
 
-from .constants import SaveFileConstants, MetricConstants, AzuremlConstants
-from .constants import _AzuremlOptimizationArgs, _AzuremlIOArgs, HfTrainerType
-from .utils.logging_utils import get_logger_app
+from .constants import SaveFileConstants, AzuremlConstants
+from .constants import _AzuremlOptimizationArgs, _AzuremlIOArgs, _AzuremlModelMetaArgs, HfTrainerType, LoraAlgo
+from .lora_wrapper.peft_lora_wrapper import PeftLoraWrapper
+from .lora_wrapper.lora_wrapper import LoraWrapper
 from .utils.hf_argparser import HfArgumentParser
-from .utils.license_utils import download_license_file
+from .utils.callbacks import LoraPyTorchSaveCallback, FinetuneCallback, NebulaCallback, ShouldSaveCheckpointOnEvaluate
 
 from .utils.trainer_utils import (
     identify_training_args_cls,
     identify_trainer_cls,
     resolve_conflicts_trainer_deepspeed_args,
-    FinetuneCallback
+    is_nebula_enabled,
 )
-from .utils.decorators import swallow_all_exceptions
-from .utils.model_utils import (
-    print_model_summary,
-    add_lora_layers_to_model
+from .utils.model_utils import add_lora_layers_to_model
+from .utils.deepspeed_utils import set_z3_leaf_modules_for_moe_models
+
+from azureml.acft.common_components import get_logger_app
+from azureml.acft.common_components.utils.callbacks import SaveExtraFilesToCheckpoints
+from azureml.acft.common_components.utils.error_handling.swallow_all_exceptions_decorator import (
+    swallow_all_exceptions
 )
 
 
-logger = get_logger_app()
+logger = get_logger_app(__name__)
 
 
 class AzuremlFinetuneArgs:
     def __init__(
         self,
         finetune_args: Dict[str, Any],
         trainer_type: str = HfTrainerType.DEFAULT,
@@ -63,15 +67,16 @@
         if finetune_args.get("deepspeed", None) is not None:
             finetune_args = resolve_conflicts_trainer_deepspeed_args(finetune_args)
 
         if trainer_type not in HfTrainerType.get_fields():
             raise Exception(f"Trainer type not supported. It should be one of {HfTrainerType.get_fields()}")
         self.trainer_type = trainer_type
 
-        training_args_cls = identify_training_args_cls(trainer_type)
+        apply_ort = finetune_args.get("apply_ort", False)
+        training_args_cls = identify_training_args_cls(trainer_type, apply_ort)
         logger.info(f"Identified training args class: {training_args_cls}")
 
         # Set this flag to enable training in CPU computes
         if not torch.cuda.is_available():
             finetune_args["xpu_backend"] = "mpi"
             finetune_args["no_cuda"] = True
             logger.warning(
@@ -80,29 +85,55 @@
 
         if not finetune_args.pop("save_checkpoints_to_output", True):
             finetune_args["output_dir"] = SaveFileConstants.ACFT_TRAINER_CHECKPOINTS_PATH
             Path(finetune_args["output_dir"]).mkdir(exist_ok=True, parents=True)
             logger.info("Using ACFT_TRAINER_CHECKPOINTS_PATH to save checkpoints")
 
         # parse the data into training args and optimzation args
-        parser = HfArgumentParser([_AzuremlOptimizationArgs, _AzuremlIOArgs, training_args_cls])
-        (self.optimization_args, self.io_args, self.trainer_args), unsed_args = parser.parse_dict(
-            finetune_args, allow_extra_keys=True)
-        logger.info(f"Optimization args: {self.optimization_args}")
-        logger.info(f"IO args: {self.io_args}")
-        logger.info(f"The following args are unused by the trainer - {unsed_args}")
+        parser = HfArgumentParser([_AzuremlOptimizationArgs, _AzuremlIOArgs, _AzuremlModelMetaArgs, training_args_cls])
+        (self.optimization_args, self.io_args, self.modelmeta_args, self.trainer_args), unused_args = parser.parse_dict(
+            finetune_args,
+            allow_extra_keys=True
+        )
 
         self.__post_init__()
+
+        logger.info(f"Optimization args: {self.optimization_args}")
+        logger.info(f"IO args: {self.io_args}")
+        logger.info(f"The following args are unused by the trainer - {unused_args}")
         logger.info(f"Trainer args: {self.trainer_args}")
+        logger.info(f"ModelMeta args: {self.modelmeta_args}")
 
     def __post_init__(self):
         """Set some additional trainer args"""
         setattr(self.trainer_args, "report_to", [])
         # Loads the model at the end of training so that the best model will be saved in the end
+        logger.warn("Enforcing load_best_model_at_end=True so that finetuning output in the end is the best model checkpoint and not the last checkpoint")
         setattr(self.trainer_args, "load_best_model_at_end", True)
+        if self.optimization_args.apply_lora and self.optimization_args.lora_algo == LoraAlgo.AUTO:
+            # force peft algorithm for lora in case of 4/8 bit or deepspeed stage-3 training
+            if self._enable_peft():
+                logger.info(
+                    "Finetuning with quantization/deepspeed stage-3 is enabled with lora. "
+                    "Forcing the lora algorithm to peft."
+                )
+                setattr(self.optimization_args, "lora_algo", LoraAlgo.PEFT)
+            else:
+                logger.info("Enabling custom lora")
+                setattr(self.optimization_args, "lora_algo", LoraAlgo.CUSTOM)
+
+    def _enable_peft(self) -> bool:
+        """Force enable peft when quantization or stage3 is enabled with LoRA."""
+        if (
+            self.optimization_args.finetune_in_8bit or
+            self.optimization_args.finetune_in_4bit or
+            is_deepspeed_zero3_enabled()
+        ):
+            return True
+        return False
 
     def save(self):
         if self.trainer_args.should_save:  # save only on rank-0
             # saving only optimization and io args here
             # trainer args will be save as part of :func trainer _save method
             optimization_args_save_path = os.path.join(self.io_args.pytorch_model_folder, SaveFileConstants.OPTIMIZATION_ARGS_SAVE_PATH)
             with open(optimization_args_save_path, 'w') as fp:
@@ -129,166 +160,189 @@
     """Azureml trainer class to train/finetune the model"""
 
     def __init__(
         self,
         finetune_args: AzuremlFinetuneArgs,
         dataset_args: AzuremlDatasetArgs,
         model: Union[nn.Module, PreTrainedModel],
-        tokenizer: Optional[PreTrainedTokenizerBase]=None,
-        metric_func: Optional[Callable]=None,
-        custom_trainer_callbacks: Optional[List[TrainerCallback]]=None,
-        custom_trainer_functions: Optional[Dict[str, Callable]]=None,
-        new_initalized_layers: Optional[List[str]]=None,
-        hf_trainer: Optional[Union[Trainer, Seq2SeqTrainer]]=None
+        tokenizer: Optional[PreTrainedTokenizerBase] = None,
+        metric_func: Optional[Callable] = None,
+        preprocess_logits_for_metrics_callback: Optional[Callable] = None,
+        custom_trainer_callbacks: Optional[List[TrainerCallback]] = None,
+        custom_trainer_functions: Optional[Dict[str, Callable]] = None,
+        new_initalized_layers: Optional[List[str]] = None,
+        hf_trainer: Optional[Union[Trainer, Seq2SeqTrainer]] = None
     ):
         self.finetune_args = finetune_args
         self.optimization_args = finetune_args.optimization_args
         self.io_args = finetune_args.io_args
         self.trainer_args = finetune_args.trainer_args
+        self.modelmeta_args = finetune_args.modelmeta_args
         self.trainer_cls = identify_trainer_cls(finetune_args.trainer_type, self.optimization_args.apply_ort)
         logger.info(f"Identified trainer class: {self.trainer_cls}")
 
         self.dataset_args = dataset_args
         self.custom_trainer_functions = custom_trainer_functions or {}
         self.custom_trainer_callbacks = custom_trainer_callbacks or []
 
         # TODO add validations for interfaces
         self.model = model
         self.new_initalized_layers = new_initalized_layers
         self.tokenizer = tokenizer
         self.metric_func = metric_func
+        self.preprocess_logits_for_metrics_callback = preprocess_logits_for_metrics_callback
 
         self.__post_init__()
 
     def __post_init__(self):
         # TODO add validations for interfaces
         # set attributes to the trainer function
         setattr(self.trainer_cls, "CUSTOM_FUNCTIONS", self.custom_trainer_functions)
+        setattr(self.trainer_cls, "OPTIMIZATION_ARGS", self.optimization_args.__dict__)
+        setattr(self.trainer_cls, "IO_ARGS", self.io_args.__dict__)
+        setattr(self.trainer_cls, "MODELMETA_ARGS", self.modelmeta_args.__dict__)
 
-    @swallow_all_exceptions(logger)
+    @swallow_all_exceptions(time_delay=60)
     def train(self):
         """
         prepares necessary objects for finetuning and triggers finetuning and saves the best model
         """
 
         model, lora_wrapper_obj = self.model, None
+
+        # Set leaf modules for MoE models in case of deepspeed stage 3.
+        # This is to stop recursively setting hooks for modules in MoE models.
+        if is_deepspeed_zero3_enabled() and self.optimization_args.leaf_modules_of_moe_models:
+            set_z3_leaf_modules_for_moe_models(model, self.optimization_args.leaf_modules_of_moe_models)
+
         is_lora_weights_path_exist = False
         if self.optimization_args.model_name is not None:
             finetune_lora_weights_path = os.path.join(
-                self.io_args.model_selector_output, self.optimization_args.model_name, \
-                    AzuremlConstants.LORA_BASE_FOLDER, AzuremlConstants.LORA_WEIGHTS_NAME)
-            is_lora_weights_path_exist = os.path.isfile(finetune_lora_weights_path)
-        if self.optimization_args.apply_lora:
-            model, lora_wrapper_obj = add_lora_layers_to_model(
-                model=model,
-                unmerge_weights=is_lora_weights_path_exist,
-                optimizer_args=self.optimization_args,
-                new_initialized_layers=self.new_initalized_layers
+                self.io_args.model_selector_output,
+                self.optimization_args.model_name,
+                AzuremlConstants.LORA_BASE_FOLDER, AzuremlConstants.LORA_WEIGHTS_NAME
             )
+            is_lora_weights_path_exist = os.path.isfile(finetune_lora_weights_path)
 
-        print_model_summary(model, print_params=True)
+        # TODO move the entire lora wrapping code to :func model_init or lora callback
+        if self.optimization_args.apply_lora:
+            if self.optimization_args.lora_algo == LoraAlgo.PEFT:
+                peft_lora_wrapper = PeftLoraWrapper(
+                    model=model,
+                    model_name_or_path=os.path.join(
+                        self.io_args.model_selector_output, self.optimization_args.model_name
+                    ),
+                    finetune_in_4bit=self.optimization_args.finetune_in_4bit,
+                    finetune_in_8bit=self.optimization_args.finetune_in_8bit,
+                    lora_alpha=self.optimization_args.lora_alpha,
+                    lora_r=self.optimization_args.lora_r,
+                    lora_dropout=self.optimization_args.lora_dropout,
+                    newly_initialized_layers=self.new_initalized_layers,
+                    peft_task_type=self.optimization_args.peft_task_type,
+                    target_modules=self.optimization_args.lora_target_modules,
+                )
+                # not enabling gradient checkpointing as needs to be handled PeftLoraWrapper
+                # TODO: use trainer_args.gradient_checkpointing at prepare_model_for_kbit_training
+                if not (self.optimization_args.finetune_in_4bit or self.optimization_args.finetune_in_8bit) \
+                        and self.trainer_args.gradient_checkpointing:
+                    logger.info("Enabling gradient checkpointing for peft LoRA model")
+                    peft_lora_wrapper.model.gradient_checkpointing_enable()
+                peft_lora_wrapper.peft_model_init()
+                lora_wrapper_obj = peft_lora_wrapper
+                model = peft_lora_wrapper.model
+            else:   # custom LoRA
+                logger.info("Preparing lora model")
+                model, lora_wrapper_obj = add_lora_layers_to_model(
+                    model=model,
+                    unmerge_weights=is_lora_weights_path_exist,
+                    optimizer_args=self.optimization_args,
+                    new_initialized_layers=self.new_initalized_layers
+                )
+
+        # commenting below code for now as Flash attention V1 implementation will be taken up later
+        # if self.optimization_args.flash_attention_version == 1:
+        #     # convert model to half precision (16 bit)
+        #     logger.info("Converting model to 16 bit as using FlashAttention.")
+        #     model.half()
+        #     # convert model to bettertransformer
+        #     model = model.to_bettertransformer()    # type: ignore
+        #     logger.info("Model converted to bettertransformer")
 
         if (
             isinstance(self.dataset_args.train_dataset, (DatasetsDataset, TorchDataset)) and
             self.trainer_args.evaluation_strategy == IntervalStrategy.STEPS and
             self.optimization_args.evaluation_steps_interval > 0
         ):
             # resetting eval_steps only for fixed size datasets
             logger.info("Updating eval steps")
             # TODO Move this to post_init
             num_examples = len(self.dataset_args.train_dataset)  # type:ignore
             logger.info(f"number of trining examples: {num_examples}, world size: {self.trainer_args.world_size}")
             num_update_steps_per_epoch = num_examples // self.trainer_args.gradient_accumulation_steps
             num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)
             num_update_steps_per_epoch_per_world = max(num_update_steps_per_epoch // self.trainer_args.world_size, 1)
-            mod_steps = int(math.floor(num_update_steps_per_epoch_per_world * self.optimization_args.evaluation_steps_interval))
+            mod_steps = int(
+                math.floor(num_update_steps_per_epoch_per_world * self.optimization_args.evaluation_steps_interval))
             setattr(self.trainer_args, "eval_steps", mod_steps)
             setattr(self.trainer_args, "save_steps", mod_steps)
             # TODO Update evaluation_steps in scripts file to eval_steps
             logger.info(f"Updated evaluation_steps from {self.trainer_args.eval_steps} to {mod_steps}")
 
         # adding trainer callbacks
         trainer_callbacks = []
-        trainer_callbacks.append(
-            FinetuneCallback(log_metrics_at_root=self.optimization_args.log_metrics_at_root,
-                             set_log_prefix=self.optimization_args.set_log_prefix)
-        )
+        trainer_callbacks.append(FinetuneCallback(
+            log_metrics_at_root=self.optimization_args.log_metrics_at_root,
+            set_log_prefix=self.optimization_args.set_log_prefix,
+            model_name=self.optimization_args.model_name,
+        ))
+        trainer_callbacks.append(ShouldSaveCheckpointOnEvaluate())
         if self.optimization_args.apply_early_stopping:
             logger.info("Applying Early stopping as trainer callback")
             early_stopping = EarlyStoppingCallback(
                 early_stopping_patience=self.optimization_args.early_stopping_patience,
                 early_stopping_threshold=self.optimization_args.early_stopping_threshold,
             )
             trainer_callbacks.append(early_stopping)
-        # Add the additional trainbacks supplied by the user
-        trainer_callbacks.extend(self.custom_trainer_callbacks)
-        logger.info(trainer_callbacks)
+        if is_nebula_enabled(self.trainer_args.deepspeed):
+            logger.info("Applying Nebula as trainer callback")
+            trainer_callbacks.append(NebulaCallback())
+        # save extrafiles to checkpointfolders
+        trainer_callbacks.append(SaveExtraFilesToCheckpoints(
+            metadata=self.finetune_args.modelmeta_args.model_metadata,
+            model_selector_output=self.finetune_args.io_args.model_selector_output,
+            optimization_args = self.optimization_args,
+            io_args=self.io_args,
+        ))
+        logger.info(f"Trainer callbacks: {trainer_callbacks}")
+
+        self.hf_trainer = self.ft_with_trainer(
+            model,
+            trainer_callbacks=trainer_callbacks,
+            lora_wrapper_obj=lora_wrapper_obj,
+            load_lora_weights=is_lora_weights_path_exist,
+        )
 
-        self.hf_trainer = self.ft_with_trainer(model, trainer_callbacks=trainer_callbacks, load_lora_weights=is_lora_weights_path_exist)
-
-        # Saving the model (TODO move the saving of mlflow and lora based merge to trainer utils)
-        # no lora
-        #   - only base model (HF model) weights will be saved to pytorch_model.bin
-        # lora
-        #   - merge_lora_weights=True
-        #       - base weights will be saved to pytorch_model.bin
-        #       - lora weights will be saved to `AzuremlConstants.LoraBaseFolder`/pytorch_model.bin
-        logger.info(f"Saving the fine-tuned model to {self.io_args.pytorch_model_folder}")
-        # merging the lora layers on process 0
-        if lora_wrapper_obj is not None and self.hf_trainer.args.should_save and self.optimization_args.apply_lora:
-            lora_layer_search_strings = AzuremlConstants.LORA_LAYER_SEARCH_STRINGS
-            logger.info(f"Merging the lora weights! Lora layer search strings: {lora_layer_search_strings}")
-
-            self.hf_trainer.model = lora_wrapper_obj.merge_lora_layers(
-                self.hf_trainer.model, lora_layer_search_strings=lora_layer_search_strings)
-
-            # store the lora layers state dict separately
-            lora_layers_state_dict = lora_wrapper_obj.get_lora_layers_state_dict(
-                self.hf_trainer.model, lora_layer_search_strings=lora_layer_search_strings)
-            lora_weights_save_path = os.path.join(
-                self.io_args.pytorch_model_folder, AzuremlConstants.LORA_BASE_FOLDER, AzuremlConstants.LORA_WEIGHTS_NAME)
-            os.makedirs(os.path.dirname(lora_weights_save_path), exist_ok=True)
-            logger.info(f"Saving the lora weights to {lora_weights_save_path}")
-            torch.save(lora_layers_state_dict, lora_weights_save_path)  # save only lora weights
-
-            # set the ignore weights to lora layers so that only HF model weights will be saved
-            # TODO see if there is a way to not set the private variable
-            ignore_keys = list(lora_layers_state_dict.keys())
-            # TODO keys_to_ignore_on_save is not valid for nn.Module
-            self.hf_trainer.model._keys_to_ignore_on_save = ignore_keys
-            logger.info(f"Ignoring the following keys while saving the merged lora model: {ignore_keys}")
-
-        # In Trainer, model save happens only in main process
-        # https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.save_model
-        self.hf_trainer.save_model(self.io_args.pytorch_model_folder)
-
-        # saving input model LICENSE file to output
-        # TODO need to handle LICENSE file as part of model import compnent or contrib package
-        if self.hf_trainer.args.should_save and self.optimization_args.model_name:
-            license_file_path = Path(self.io_args.model_selector_output, \
-                                self.optimization_args.model_name, SaveFileConstants.LICENSE_SAVE_PATH)
-            if license_file_path.is_file():
-                shutil.copy(str(license_file_path), self.io_args.pytorch_model_folder)
-                logger.info("LICENSE file is copied to pytorch model folder")
-            else:
-                download_license_file(self.optimization_args.model_name, self.io_args.pytorch_model_folder)
+        # save model in case of lora disabled
+        if not self.optimization_args.apply_lora:
+            self.hf_trainer.save_model(self.io_args.pytorch_model_folder)
 
         # saving the args
         self.finetune_args.save()
 
         # Adding a barrier to wait for all the processes to finish
         if dist.is_initialized():
             logger.info("Waiting at barrier")
             dist.barrier()
 
     def ft_with_trainer(
         self,
         model: Union[nn.Module, PreTrainedModel],
         trainer_callbacks: List[TrainerCallback],
-        load_lora_weights: bool = False
+        lora_wrapper_obj: Optional[Union[PeftLoraWrapper, LoraWrapper]] = None,
+        load_lora_weights: bool = False,
     ) -> Trainer:
         """
         handles the finetuning of a pre-trained model
         """
 
         if dist.is_initialized():
             logger.info(f"local_rank = {dist.get_rank()}; world_size = {dist.get_world_size()}")
@@ -301,26 +355,48 @@
             train_dataset=self.dataset_args.train_dataset,  # type: ignore
             eval_dataset=self.dataset_args.validation_dataset,  # type: ignore
             compute_metrics=self.metric_func,
             args=self.trainer_args,
             tokenizer=self.tokenizer,
             data_collator=self.dataset_args.data_collator,
             callbacks=trainer_callbacks,
+            preprocess_logits_for_metrics=self.preprocess_logits_for_metrics_callback,
         )
 
+        # adding callback to save LoRA models
+        if self.optimization_args.apply_lora and lora_wrapper_obj is not None:
+            pytorch_save_callback = LoraPyTorchSaveCallback(
+                trainer=trainer,
+                lora_wrapper_obj=lora_wrapper_obj,
+                pytorch_save_folder=self.io_args.pytorch_model_folder,
+                optimization_args=self.optimization_args,
+                lora_algo=self.optimization_args.lora_algo,
+            )
+            trainer.add_callback(pytorch_save_callback)
+            logger.info("Added LoraPyTorchSaveCallback to Trainer")
+
+        # Add the additional trainbacks supplied by the user
+        for user_callback in self.custom_trainer_callbacks:
+            trainer.add_callback(user_callback)
+            user_callback_name = user_callback if isinstance(user_callback, type) else user_callback.__class__
+            logger.info(f"Added user callback {user_callback_name} to Trainer")
+
         logger.info("Training started!")
         start_time = time.time()
         # Continual Finetuning case
         if load_lora_weights and self.optimization_args.model_name:
             # load the lora weights for the case where model is saved using merge_lora_weights=True
-            lora_weights_folder = os.path.join(self.io_args.model_selector_output, \
-                                               self.optimization_args.model_name, AzuremlConstants.LORA_BASE_FOLDER)
+            lora_weights_folder = os.path.join(
+                self.io_args.model_selector_output,
+                self.optimization_args.model_name,
+                AzuremlConstants.LORA_BASE_FOLDER
+            )
             logger.info(f"Loading the lora weights from {lora_weights_folder}")
             trainer.load_model_finetuned_weights(resume_from_checkpoint=lora_weights_folder)
-        trainer.train()
+        trainer.train(resume_from_checkpoint=self.trainer_args.resume_from_checkpoint)
         end_time = time.time()
         logger.info("Training completed in {} sec".format(end_time - start_time))
 
         return trainer
 
     @property
     def should_save(self):
```

## azureml/acft/accelerator/lora_wrapper/lora_layers.py

```diff
@@ -362,12 +362,12 @@
                 _eval_helper()
         else:
             _eval_helper()
 
     def forward(self, x: torch.Tensor):
         if self.r > 0 and not self.merged:
             return F.conv2d(
-                x, 
+                x,
                 self.weight + (self.lora_B @ self.lora_A).view(self.weight.shape) * self.scaling,
                 self.bias, self.stride, self.padding, self.dilation, self.groups
             )
         return nn.Conv2d.forward(self, x)
```

## azureml/acft/accelerator/lora_wrapper/lora_wrapper.py

```diff
@@ -9,19 +9,22 @@
 from typing import List, Tuple, Union, Optional
 
 import traceback
 from collections import OrderedDict
 
 from . import lora_configs
 from . import lora_layers as lora
-from ..utils.logging_utils import get_logger_app
+from azureml.acft.common_components import get_logger_app
 from ..constants import AzuremlConstants, HfModelTypes
+from azureml.acft.common_components.utils.error_handling.exceptions import ACFTValidationException
+from azureml.acft.common_components.utils.error_handling.error_definitions import ACFTUserError
+from azureml._common._error_definition.azureml_error import AzureMLError  # type: ignore
 
 
-logger = get_logger_app()
+logger = get_logger_app(__name__)
 
 # TODO Move to file
 # Store `model_type` as the key
 LORA_CONFIG_MAP = OrderedDict(
     [
         [HfModelTypes.GPT2, "LoraGpt2"],
         [HfModelTypes.BERT, "LoraBert"],
@@ -30,32 +33,34 @@
         [HfModelTypes.DISTILBERT, "LoraDistilbert"],
         [HfModelTypes.T5, "LoraT5"],
         [HfModelTypes.BART, "LoraBart"],
         [HfModelTypes.MBART, "LoraMbart"],
         [HfModelTypes.CAMEMBERT, "LoraCamembert"],
         [HfModelTypes.GPT_NEOX, "LoraGptNeoX"],
         [HfModelTypes.LLAMA, "LoraLlama"],
+        [HfModelTypes.FALCON, "LoraFalcon"],
+        [HfModelTypes.REFINEDWEBMODEL, "LoraFalcon"],
     ]
 )
 
 
 class LoraWrapper:
     """
     LoRA Wrapper class used to integrarte lora in training
     """
 
     def __init__(
         self,
         hf_model_type: str,
-        newly_initialized_params: Optional[List[str]]=None,
-        lora_r: int=8,
-        lora_alpha: int=128,
-        lora_dropout: float=0.0,
-        merge_weights: bool=True,
-        lora_weights_merged_with_base_model: bool=True,
+        newly_initialized_params: Optional[List[str]] = None,
+        lora_r: int = 8,
+        lora_alpha: int = 128,
+        lora_dropout: float = 0.0,
+        merge_weights: bool = True,
+        lora_weights_merged_with_base_model: bool = True,
     ):
         """
         hf_model_type - bert_base_uncased and bert_base_cased both belong to model_type bert
         newly_initialized_parameters - set of parameters that are newly initialized, i.e. in case of lora
             only the newly initialized parameters will be set to trainable
         lora_r - the dimension to which the weight matrix is down projected to
             In case of Linear layer, say the weight (W) dimension is m x n, the lora decomposes W to matrices B and A
@@ -70,15 +75,23 @@
             During the start of training or whenever the .train() method is called with
             lora_weights_merged_with_base_model and merge_weights are set to True, the weights and lora layers will get
             unmerged so that the training resumes from the checkpoints last saved model weights
             unmerging => W = W - BA
         """
 
         if hf_model_type not in LORA_CONFIG_MAP:
-            raise ValueError(f"Lora is not supported for {hf_model_type}")
+            raise ACFTValidationException._with_error(
+            AzureMLError.create(
+                ACFTUserError,
+                pii_safe_message=(
+                    f"Lora is not supported for {hf_model_type}. "
+                    f"List of supported model_types with Lora: {list(LORA_CONFIG_MAP.keys())}"
+                )
+            )
+        )
 
         # get lora parameters
         lora_config_obj = getattr(lora_configs, LORA_CONFIG_MAP[hf_model_type])(
             lora_r, lora_alpha, lora_dropout, merge_weights)
         self.lora_params = lora_config_obj.get_lora_parameters()
         self.lora_weights_merged_with_base_model = lora_weights_merged_with_base_model
 
@@ -200,10 +213,10 @@
       self, model, lora_layer_search_strings: List[str] = AzuremlConstants.LORA_LAYER_SEARCH_STRINGS):
         """
         This function identifies the lora layers and returns the state_dict of the same
         """
         lora_layers_state_dict = {}
         for name, param in model.named_parameters():
             if any([name.endswith(search_str) for search_str in lora_layer_search_strings]):  # lora layer
-                lora_layers_state_dict[name] = param
+                lora_layers_state_dict[name] = param.detach()   # param.detach() won't work for CPU training
 
         return lora_layers_state_dict
```

## azureml/acft/accelerator/lora_wrapper/lora_configs/__init__.py

```diff
@@ -15,7 +15,8 @@
 from .lora_distilbert import LoraDistilbert
 from .lora_t5 import LoraT5
 from .lora_bart import LoraBart
 from .lora_mbart import LoraMbart
 from .lora_camembert import LoraCamembert
 from .lora_gpt_neox import LoraGptNeoX
 from .lora_llama import LoraLlama
+from .lora_falcon import LoraFalcon
```

## azureml/acft/accelerator/utils/decorators.py

```diff
@@ -2,26 +2,29 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 
 """
 This file defines the decorators used in the package
 """
 
-import time
-from email import message
-from inspect import trace
 import logging
 from functools import wraps
-import functools
-import traceback
+
+from ..constants import ErrorConstants
+
 from azureml._common.exceptions import AzureMLException
 from azureml._common._error_definition.azureml_error import AzureMLError  # type: ignore
-from .error_handling.error_definitions import LLMInternalError, ValidationError
+from .error_handling.error_definitions import (
+    LLMInternalError,
+    ValidationError,
+    InsufficientGPUMemory,
+    InsufficientGPUMemoryAutoFindBatchSize,
+    LossScaleAtMinimum
+)
 from .error_handling.exceptions import ValidationException
-from .config import Config
 
 
 def swallow_all_exceptions(logger: logging.Logger):
     """
     Swallow all exceptions
     1. Catch all the exceptions arising in the functions wherever used
     2. Raise the exception as an AzureML Exception so that it does not get scrubbed by PII scrubber
@@ -41,24 +44,44 @@
                 logger.info("exiting due to validation error")
 
                 for handler in logger.handlers:
                     handler.flush()
 
                 raise AzureMLException._with_error(AzureMLError.create(ValidationError, error=e))
             except Exception as e:
-                # This will be logged to exceptions table
-                az_e = AzureMLException._with_error(AzureMLError.create(LLMInternalError, error=e))
+
+                str_e = str(e)
+
+                if ErrorConstants.CUDA_OUT_OF_MEMORY_ERROR in str_e:
+                    azureml_error = AzureMLError.create(InsufficientGPUMemory, error=e)
+                elif ErrorConstants.AUTO_FIND_BATCH_SIZE_MEMORY_ERROR in str_e:
+                    azureml_error = AzureMLError.create(InsufficientGPUMemoryAutoFindBatchSize, error=e)
+                elif ErrorConstants.LOSS_SCALE_AT_MINIMUM in str_e:
+                    azureml_error = AzureMLError.create(LossScaleAtMinimum, error=e)
+                else:
+                    # This will be logged to exceptions table
+                    az_e = AzureMLException._with_error(AzureMLError.create(LLMInternalError, error=e))
+
+                    logger.error("Exception {} when calling {}".format(az_e, func.__name__))
+                    logger.info("exiting due to system error")
+
+                    for handler in logger.handlers:
+                        handler.flush()
+
+                    raise AzureMLException._with_error(AzureMLError.create(LLMInternalError, error=e))
+
+                az_e = AzureMLException._with_error(azureml_error)
 
                 logger.error("Exception {} when calling {}".format(az_e, func.__name__))
-                logger.info("exiting due to system error")
+                logger.info("exiting due to User error")
 
                 for handler in logger.handlers:
                     handler.flush()
 
-                raise AzureMLException._with_error(AzureMLError.create(LLMInternalError, error=e))
+                raise AzureMLException._with_error(azureml_error)
 
         return wrapper
 
     return wrap
 
 
 def retry(times, logger):
```

## azureml/acft/accelerator/utils/license_utils.py

```diff
@@ -19,18 +19,18 @@
 """This file defines the util functions used for downloading license file."""
 
 from huggingface_hub import hf_hub_download
 from pathlib import Path
 from shutil import copyfile
 
 from ..constants import SaveFileConstants
-from .logging_utils import get_logger_app
+from azureml.acft.common_components import get_logger_app
 
 
-logger = get_logger_app()
+logger = get_logger_app(__name__)
 
 
 def download_licence_file_from_huggingface_repo(model_id: str, download_path: str) -> str:
     """
     Download the LICENSE file from huggingface hub
     """
     license_file = hf_hub_download(
```

## azureml/acft/accelerator/utils/logging_utils.py

```diff
@@ -137,15 +137,15 @@
         if any([filter_str.lower() in data_message for filter_str in LOGS_TO_BE_FILTERED_APPINSIGHTS]):
             return False
 
     return True
 
 
 def get_logger_app(
-    logging_level: str = "INFO",
+    logging_level: str = "DEBUG",
     custom_dimensions: dict = None,
     name: str = Config.LOGGER_NAME,
 ):
     """
     Creates handlers and define formatter for emitting logs to AppInsights
     Also adds handlers to HF logs
     :returns logger which emits logs to stdOut and appInsights with PII Scrubbing
@@ -221,8 +221,8 @@
             # Config optimum logger
             if optimum_logging is not None:
                 optimum_logging.disable_default_handler()
                 optimum_logging.set_verbosity_info()
                 optimum_logging.add_handler(appinsights_handler)
                 optimum_logging.add_handler(stream_handler)
 
-    return logger
+    return logger
```

## azureml/acft/accelerator/utils/model_utils.py

```diff
@@ -9,32 +9,32 @@
 from typing import Union, Tuple, Optional, List
 
 import torch.nn as nn
 
 from transformers import PreTrainedModel
 
 from ..lora_wrapper.lora_wrapper import LoraWrapper
-from ..constants import _AzuremlOptimizationArgs
+from ..constants import _AzuremlOptimizationArgs, AzuremlConstants
 
 from azureml._common._error_definition.azureml_error import AzureMLError
-from .error_handling.exceptions import ResourceException, LLMException
-from .error_handling.error_definitions import ResourceNotFound, LLMInternalError
 
-from .logging_utils import get_logger_app
-from .decorators import swallow_all_exceptions
+from azureml.acft.common_components import get_logger_app
+from azureml.acft.common_components.utils.error_handling.swallow_all_exceptions_decorator import (
+    swallow_all_exceptions
+)
 
 
-logger = get_logger_app()
+logger = get_logger_app(__name__)
 
 
 def add_lora_layers_to_model(
     model: Union[nn.Module, PreTrainedModel],
     unmerge_weights: bool,
     optimizer_args: _AzuremlOptimizationArgs,
-    new_initialized_layers: Optional[List[str]]= None
+    new_initialized_layers: Optional[List[str]] = None,
 ) -> Tuple[Union[nn.Module, PreTrainedModel], Optional[LoraWrapper]]:
     """
     :param model
         Base model of any framework HuggingFace, MMAction
     :param unmerge_weights
         Unmerges the base weights and lora layers to resume training from last train state. In case of continual finetune,
         the base weights are subtracted from lora weights to arrive at the last train state
@@ -53,40 +53,42 @@
         lora_weights_merged_with_base_model=unmerge_weights,
     )
     model = lora_wrapper_obj.update_attention_block_with_lora_layers(model)
     model = lora_wrapper_obj.set_trainable_parameters(model)
 
     return model, lora_wrapper_obj
 
-@swallow_all_exceptions(logger)
+
+@swallow_all_exceptions(time_delay=60)
 def print_model_summary(model, print_params=False):
     """
     prints the model summary
     """
     model_param_train_info = []
     total_params, trainable_params = 0, 0
     prefix_for_grad, prefix_for_no_grad = "+" * 10, " " * 10
     for name, param in model.named_parameters():
         if print_params:
             total_params += param.numel()
             if param.requires_grad:
                 trainable_params += param.numel()
-                model_param_train_info.append(f"{prefix_for_grad}{name}")
+                model_param_train_info.append(f"{prefix_for_grad} {param.device} {param.dtype} {name}")
             else:
-                model_param_train_info.append(f"{prefix_for_no_grad}{name}")
+                model_param_train_info.append(f"{prefix_for_no_grad} {param.device} {param.dtype} {name}")
         else:
             model_param_train_info.append(name)
 
     model_param_train_info_str = "\n".join(model_param_train_info)
     logger.info(model_param_train_info_str)
     if print_params:
         logger.info(f"Total model parameters: {total_params}")
         logger.info(f"Total trainable parameters: {trainable_params}")
 
-@swallow_all_exceptions(logger)
+
+@swallow_all_exceptions(time_delay=60)
 def print_model_weights_top_n_layers(model, n=-1):
     """
     Prints model weights for top n layers.
     If n == -1 prints all layer weights
     """
     layer_count = 0
     check_layer = True
```

## azureml/acft/accelerator/utils/run_utils.py

```diff
@@ -1,20 +1,26 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 
 """
 This file contains functions for getting run parameters
 """
+import os
+from typing import Optional, Dict, Any
 
 from azureml.core import Run
+from azureml.core.run import _OfflineRun
 from azureml.core.compute import ComputeTarget
+from azureml.acft.common_components import get_logger_app
 
 from .config import Config
 
+
+logger = get_logger_app(__name__)
 run = Run.get_context()
 
 
 def _get_run_id():
     """
     Returns run ID of parent of current run
     """
@@ -71,55 +77,103 @@
     """
 
     if "OfflineRun" in run.id:
         return Config.OFFLINE_RUN_MESSAGE
     compute_name = _get_compute()
     if compute_name == "":
         return "No compute found."
-    
+
     try:
         cpu_cluster = ComputeTarget(workspace=run.experiment.workspace, name=compute_name)
     except Exception:
         # cannot log here, logger is not yet instantiated
         return f"could not retrieve vm size for compute {compute_name}"
 
     return cpu_cluster.vm_size
 
+
 def find_root(run):
     """
     Return the root run of current run.
     """
 
+    if isinstance(run, _OfflineRun):
+        logger.info("Found offline run. Skipping finding root.")
+        return run
+
     if not run.parent:
         return run
-    
+
     root_run = run.parent
     while root_run.parent:
         root_run = root_run.parent
 
     return root_run
 
-def add_run_properties(properties, logger, add_to_root=False):
-    """
-    Add properties to current run context.
-    For offline run _OfflineRun object handles it (azureml.core.run._OfflineRun)
-    """
-
-    if add_to_root and not "OfflineRun" in run.id:
 
-        root_run = find_root(run)
+def add_run_properties(
+    properties_to_add: Dict[str, Any],
+    add_to_root: bool = False,
+    custom_run: Optional[Run] = None,
+    skip_add_properties_if_any_already_exist: bool = False
+):
+    """Add properties to the current context run object / root run object / custom run object. The default behavior
+    to add properties is to ignore properties that already exists in current run.
+
+    :param properties_to_add: properties to add to the run object
+    :type Dict[str, Any]
+    :param add_to_root: If enabled, the properties will be added to the root run object
+    :type: boolean
+    :param custom_run: custom run object other than the current context run or root run to add the parameters. The
+    custom run object takes precedence when both add_to_root is True and custom_run is not None
+    :type: Optional[Run]
+    :param skip_add_properties_if_any_already_exist: Enabling this flag wil override the default behavior. Instead of
+    skipping the existing properties, the entire property dictionary would not be added if any of the keys in
+    :param properties_to_add are already part current properties.
+    """
+
+    # Identify run object to use
+    if custom_run is not None:
+        logger.info("Adding properties to custom run object.")
+        run_obj_to_use = custom_run
+    elif add_to_root:
+        logger.info("Adding properties to root run object.")
+        run_obj_to_use = find_root(run)
+    else:
+        logger.info("Adding properties to current run object.")
+        run_obj_to_use = run
 
-        properties_to_add = {}
-        root_run_properties = root_run.get_properties()
+    # Skip adding properties for offline run
+    if isinstance(run_obj_to_use, _OfflineRun):
+        logger.info("Found offline run. Skipping addition of properties.")
+        return
+
+    # Fetch already added properties for the run
+    existing_run_properties = run_obj_to_use.get_properties()
+    if skip_add_properties_if_any_already_exist:
+        any_property_exists = any([prop in existing_run_properties for prop in properties_to_add])
+        if any_property_exists:
+            logger.info(
+                "Atleast one of the existing property already exists."
+                f"Skipping addition of properties: {properties_to_add}"
+            )
+            return
+
+    # deleting the already existing properties from properties_to_add
+    new_properties_to_add = {}
+    for property, value in properties_to_add.items():
+        if property in existing_run_properties:
+            logger.info(f"The property {property} already exists. Not adding it.")
+            continue
+        new_properties_to_add[property] = value
+
+    # adding the remaining properties
+    logger.info(f"Adding the properties: {new_properties_to_add}")
+    run_obj_to_use.add_properties(new_properties_to_add)
 
-        # only add properties which are not already present
-        for property in properties:
-            if property not in root_run_properties:
-                properties_to_add[property] = properties[property]
-            else:
-                logger.info(f"skip adding property to root: {property}")
 
-        root_run.add_properties(properties=properties_to_add)
-        logger.info(f"added run properties to root: {properties_to_add}")
-    else:
-        run.add_properties(properties=properties)
-        logger.info(f"added run properties: {properties}")
+def is_main_process():
+    """
+    Function for determining whether the current process is master.
+    :return: Boolean for whether this process is master.
+    """
+    return os.environ.get('AZUREML_PROCESS_NAME', 'main') in {'main', 'rank_0'}
```

## azureml/acft/accelerator/utils/trainer_utils.py

```diff
@@ -1,54 +1,56 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 
 """
 This file contains utilty functions for Training
 """
-import os
 import json
 import argparse
-from math import isnan
-from typing import Optional, Dict, Any
+from typing import Optional, Dict, Any, Union
+from pathlib import Path
+import shutil
 
 import torch
 from datasets.arrow_dataset import Dataset
-from transformers import TrainingArguments, Seq2SeqTrainingArguments, Seq2SeqTrainer
-from transformers.trainer import Trainer, TRAINING_ARGS_NAME
-from transformers.trainer_callback import TrainerCallback
-from transformers.trainer_utils import IntervalStrategy
+from transformers.training_args import TrainingArguments
+from transformers.training_args_seq2seq import Seq2SeqTrainingArguments
+from transformers.trainer_seq2seq import Seq2SeqTrainer
+from transformers.trainer import Trainer
 
-from optimum.onnxruntime import ORTTrainer, ORTSeq2SeqTrainer
+from optimum.onnxruntime import ORTTrainer, ORTSeq2SeqTrainer, ORTTrainingArguments
 
-from ..constants import HfTrainerMethodsConstants, HfTrainerType, AzuremlRunType
-from .logging_utils import get_logger_app
+from ..constants import (
+    HfTrainerMethodsConstants,
+    HfTrainerType,
+    SaveFileConstants,
+)
+from ..utils.code_utils import get_model_custom_code_files, copy_code_files
+from ..utils.license_utils import download_license_file
 
+from azureml.acft.common_components import get_logger_app
 
-logger = get_logger_app()
+
+logger = get_logger_app(__name__)
 
 
 class TrainerMixin:
     """
     This is a mixin class that needs to used in conjunction with either of the below classes
         Trainer
         ORTTrainer
         Seq2SeqTrainer
         ORTSeq2SeqTrainer
     This class provides extra utility functions for trainer. Also it helps to customize some methods.
     """
 
     CUSTOM_FUNCTIONS = {}
-
-    def _save(self, output_dir: str, state_dict=None):
-        # NOTE updating the state dict for mmaction onboarding
-        self.model.save_pretrained(output_dir, state_dict=self.model.state_dict())
-        if self.tokenizer:
-            self.tokenizer.save_pretrained(output_dir)
-        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))
+    OPTIMIZATION_ARGS = {}
+    IO_ARGS = {}
 
     def load_model_finetuned_weights(self, resume_from_checkpoint: str):
         """
         load finetuned weights of a model
         applies lora weights + deepspeed init is handled internally
         """
         self.state.best_model_checkpoint = resume_from_checkpoint
@@ -86,74 +88,68 @@
         if HfTrainerMethodsConstants.AZUREML_COMPUTE_LOSS in self.__class__.CUSTOM_FUNCTIONS:
             compute_loss_func = self.__class__.CUSTOM_FUNCTIONS[HfTrainerMethodsConstants.AZUREML_COMPUTE_LOSS]
             logger.info(f"Using custom loss func: {compute_loss_func}")
             return compute_loss_func(model, inputs, return_outputs=return_outputs)
         else:
             return super().compute_loss(model, inputs, return_outputs=return_outputs)
 
+    def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):
+        """
+        Will save the model, so you can reload it using `from_pretrained()`.
+        Will only save from the main process.
+        """
+        super().save_model(output_dir, _internal_call)
+
+        # save additional files required by acft model
+        self.save_acft_pytorch_model_files(output_dir)
+
+    def save_acft_pytorch_model_files(self, output_dir):
+        """Will save additional files required for acft pytorch model from base model."""
+
+        # save files in the main process only
+        # else checkpoit-{{step_number}} file is getting created before directory being created
+        if self.args.should_save:
+            logger.info("Saving additional pytorch model files")
+
+            # check if root model has any custom code files and copy them to output model
+            if hasattr(self.model.config, "auto_map"):
+                logger.info("Saving code files to pytorch model")
+                # Check if any code files are present in the model folder
+                model_path = str(Path(
+                    self.__class__.IO_ARGS["model_selector_output"],
+                    self.__class__.OPTIMIZATION_ARGS["model_name"],
+                ))
+                py_code_files = get_model_custom_code_files(model_path, self.model)
+
+                # copying the py files
+                copy_code_files(py_code_files, [output_dir])
+
+            # saving input model LICENSE file to output
+            if self.__class__.OPTIMIZATION_ARGS["model_name"]:
+                license_file_path = Path(
+                    self.__class__.IO_ARGS["model_selector_output"],
+                    self.__class__.OPTIMIZATION_ARGS["model_name"],
+                    SaveFileConstants.LICENSE_SAVE_PATH
+                )
+                if license_file_path.is_file():
+                    shutil.copy(str(license_file_path), output_dir)
+                    logger.info("LICENSE file is copied to pytorch model folder")
+                else:
+                    download_license_file(
+                        self.__class__.OPTIMIZATION_ARGS["model_name"],
+                        output_dir,
+                    )
+
 
 class TrainerExtended(TrainerMixin, Trainer):
     """
     Subclassed Trainer class to customize behaviour
     """
     pass
 
-    # CUSTOM_FUNCTIONS = {}
-
-    # def _save(self, output_dir: str, state_dict=None):
-    #     # NOTE updating the state dict for mmaction onboarding
-    #     self.model.save_pretrained(output_dir, state_dict=self.model.state_dict())
-    #     if self.tokenizer:
-    #         self.tokenizer.save_pretrained(output_dir)
-    #     torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))
-
-    # def load_model_finetuned_weights(self, resume_from_checkpoint: str):
-    #     """
-    #     load finetuned weights of a model
-    #     applies lora weights + deepspeed init is handled internally
-    #     """
-    #     self.state.best_model_checkpoint = resume_from_checkpoint
-    #     self._load_best_model()
-    #     self.state.best_model_checkpoint = None
-
-    # def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:
-    #     if HfTrainerMethodsConstants.AzmlTrainSampler in TrainerExtended.CUSTOM_FUNCTIONS:
-    #         custom_train_sampler_func = TrainerExtended.CUSTOM_FUNCTIONS[HfTrainerMethodsConstants.AzmlTrainSampler]
-    #         logger.info(f"Using custom train sampler: {custom_train_sampler_func}")
-    #         return custom_train_sampler_func(self.train_dataset, self.args.world_size)
-    #     else:
-    #         logger.info("Calling the default train sampler")
-    #         return super()._get_train_sampler()
-
-    # def _get_eval_sampler(self, eval_dataset: Dataset) -> Optional[torch.utils.data.Sampler]:
-    #     if HfTrainerMethodsConstants.AzmlEvalSampler in TrainerExtended.CUSTOM_FUNCTIONS:
-    #         custom_eval_sampler_func = TrainerExtended.CUSTOM_FUNCTIONS[HfTrainerMethodsConstants.AzmlEvalSampler]
-    #         logger.info(f"Using custom eval sampler: {custom_eval_sampler_func}")
-    #         return custom_eval_sampler_func(eval_dataset, self.args.world_size)
-    #     else:
-    #         logger.info("Calling the default eval sampler")
-    #         return super()._get_eval_sampler(eval_dataset)
-
-    # def create_optimizer(self):
-    #     if HfTrainerMethodsConstants.AzmlOptimizer in TrainerExtended.CUSTOM_FUNCTIONS:
-    #         create_optimizer_func = TrainerExtended.CUSTOM_FUNCTIONS[HfTrainerMethodsConstants.AzmlOptimizer]
-    #         logger.info(f"Using custom optimizer: {create_optimizer_func}")
-    #         self.optimizer = create_optimizer_func(self.model, learning_rate=self.args.learning_rate)
-    #     else:
-    #         logger.info("Calling the default optimizer")
-    #         super().create_optimizer()
-
-    # def compute_loss(self, model, inputs, return_outputs=False):
-    #     if HfTrainerMethodsConstants.AzmlComputeLoss in TrainerExtended.CUSTOM_FUNCTIONS:
-    #         compute_loss_func = TrainerExtended.CUSTOM_FUNCTIONS[HfTrainerMethodsConstants.AzmlComputeLoss]
-    #         logger.info(f"Using custom loss func: {compute_loss_func}")
-    #         return compute_loss_func(model, inputs, return_outputs=return_outputs)
-    #     else:
-    #         return super().compute_loss(model, inputs, return_outputs=return_outputs)
-
 
 class Seq2SeqTrainerExtended(TrainerMixin, Seq2SeqTrainer):
     """
     Subclassed Trainer class to customize behaviour
     """
     pass
 
@@ -178,111 +174,45 @@
     """
     if trainer_type == HfTrainerType.SEQ2SEQ:
         return ORTSeq2SeqTrainerExtended if apply_ort else Seq2SeqTrainerExtended
     else:
         return ORTTrainerExtended if apply_ort else TrainerExtended
 
 
-def identify_training_args_cls(trainer_type: str):
+def identify_training_args_cls(trainer_type: str, apply_ort: bool):
     """
     Identify the trainer class and training arguments class
     """
     if trainer_type == HfTrainerType.SEQ2SEQ:
         return Seq2SeqTrainingArguments
-    return TrainingArguments
-
-
-# Trainer call back to log metrics
-# TODO move to mlflow logging
-class FinetuneCallback(TrainerCallback):
-    """
-    A [`TrainerCallback`] that sends the logs to [AzureML](https://pypi.org/project/azureml-sdk/).
-    """
-
-    def __init__(self, azureml_run=None, log_metrics_at_root=True, set_log_prefix=True):
-        """
-        init azureml_run which is azureml Run object
-        """
-        self.azureml_run = azureml_run
-        self.log_metrics_at_root = log_metrics_at_root
-        self.set_log_prefix = set_log_prefix
-
-    def _should_log_to_parent(self):
-        """
-        Check if we should log to parent pipeline run.
-
-        :return: Parent run if we should log else None.
-        :rtype: azureml.core.run
-        """
-        parent_run = self.azureml_run.parent
-        child_run = None
-        while parent_run is not None and (parent_run.type == AzuremlRunType.PIPELINE_RUN or parent_run.type == AzuremlRunType.STEP_RUN):
-            child_run = parent_run
-            parent_run = parent_run.parent
-        return child_run
-
-    def on_init_end(self, args, state, control, **kwargs):
-        """
-        executes after init and sets azureml_run
-        """
-        from azureml.core.run import Run
-
-        if self.azureml_run is None and state.is_world_process_zero:
-            self.azureml_run = Run.get_context()
-            logger.info("Initialized azureml run")
+    else:
+        return ORTTrainingArguments if apply_ort else TrainingArguments
 
-        if self.azureml_run is not None and "OfflineRun" in self.azureml_run.id:
-            logger.info("Failed to get context, run as Local run")
-            self.azureml_run = None
 
-    def on_log(self, args, state, control, logs=None, **kwargs):
-        """
-        logs metrics to azureml
-        """
-        if self.azureml_run and state.is_world_process_zero:
-            steps = None
-            if args.logging_strategy == IntervalStrategy.STEPS:
-                steps = state.global_step
-            for k, v in logs.items():
-                if isinstance(v, (int, float)) and not isnan(v):
-                    
-                    if not self.set_log_prefix:
-                        eval_prefix = 'eval_'
-                        train_prefix = 'train_'
-                        if k.startswith(eval_prefix):
-                            k = k[len(eval_prefix):]
-                        if k.startswith(train_prefix):
-                            k = k[len(train_prefix):]
-                            k = k + '_train'
-
-                    self.azureml_run.log(k, v, description=k, step=steps)
-
-                    if self.log_metrics_at_root:
-                        # Check if parent is a pipeline run.
-                        # If pipeline run, log all metrics to parent pipeline as well.
-                        parent_run = self._should_log_to_parent()
-                        if parent_run:
-                            logger.info(f"Logging metrics to {parent_run}")
-                            parent_run.log(k, v, description=k, step=steps)
-        else:
-            logger.info(f"Logging metrics for local run with step {state.global_step} - {logs}")
+def get_deepspeed_dict(deepspeed_config: Union[Dict, str]) -> Dict:
+    if isinstance(deepspeed_config, dict):
+        # deepspeed can also be a dict which has already been loaded from deepspeed json file.
+        # return it as is in such cases.
+        return deepspeed_config
+    with open(deepspeed_config) as fp:
+        deepspeed_config = json.load(fp)
+    return deepspeed_config
 
 
 def resolve_conflicts_trainer_deepspeed_args(finetune_args: Dict[str, Any]) -> Dict[str, Any]:
     """
     :param finetune_args: finetune component args loaded from component parameters
         If deepspeed is enabled, read the deepspeed config args and resolve conflicts with trainer_args
         NOTE deepspeed config parameters are given preference over component parameters
     """
 
     finetune_args_namespace = argparse.Namespace(**finetune_args)
 
     if finetune_args_namespace.deepspeed is not None:
-        with open(finetune_args_namespace.deepspeed) as fp:
-            ds_dict = json.load(fp)
+        ds_dict = get_deepspeed_dict(finetune_args_namespace.deepspeed)
 
         # per_device_train_batch_size
         # deepspeed - train_batch_size can not be handled currently, needs to be checked by user only
         # TODO: replicate for eval batch size
         if hasattr(finetune_args_namespace, "per_device_train_batch_size"):
             per_device_train_batch_size = ds_dict.get("train_micro_batch_size_per_gpu", finetune_args_namespace.per_device_train_batch_size)
             if per_device_train_batch_size != "auto":
@@ -351,21 +281,28 @@
         # fp16_full_eval, fp16_backend - not implemented by azmlft
         # fp16_backend is auto handled by HfTrainerDeepSpeedConfig and is always "amp" for azmlft
         setattr(finetune_args_namespace, "fp16_opt_level", "O1")      # default HFTrainer value
         fp16_opt_level = ds_dict.get("amp", {}).get("opt_level", finetune_args_namespace.fp16_opt_level)
         if fp16_opt_level != "auto":
             setattr(finetune_args_namespace, "fp16_opt_level", fp16_opt_level)
 
-        # bf-16
-        # bf16 - not implemented by azmlft
-        setattr(finetune_args_namespace, "bf16", False)           # default HFTrainer value
-        bf16 = ds_dict.get("bf16", {}).get("enabled", finetune_args_namespace.bf16)
-        if bf16 != "auto":
-            setattr(finetune_args_namespace, "bf16", bf16)
-        
-        logger.info(f"Resolved conflicts between finetune_args_namespace and deepspeed config: {finetune_args_namespace}")
+        # bf16
+        if hasattr(finetune_args_namespace, "bf16"):
+            bf16 = ds_dict.get("bf16", {}).get("enabled", finetune_args_namespace.bf16)
+            if bf16 != "auto":
+                setattr(finetune_args_namespace, "bf16", bf16)
+
+        logger.info(
+            f"Resolved conflicts between finetune_args_namespace and deepspeed config: {finetune_args_namespace}")
 
     else:
-        setattr(finetune_args_namespace, "fp16_opt_level","O1")  # default HFTrainer value
-        setattr(finetune_args_namespace, "bf16", False)          # default HFTrainer value
+        setattr(finetune_args_namespace, "fp16_opt_level", "O1")  # default HFTrainer value
 
     return vars(finetune_args_namespace)
+
+
+def is_nebula_enabled(deepspeed_config: Union[Dict, str]) -> bool:
+    if not deepspeed_config:
+        return False
+    ds_dict = get_deepspeed_dict(deepspeed_config)
+    nebula: Dict = ds_dict.get("nebula")
+    return bool(nebula and nebula.get("enabled"))
```

## azureml/acft/accelerator/utils/error_handling/error_definitions.py

```diff
@@ -41,14 +41,26 @@
     def message_format(self) -> str:
         """
         Message format
         """
 
         return LLMErrorStrings.LLM_GENERIC_ERROR
 
+class SKUNotSupported(NotSupported):
+    """
+    SKU Not Supported Error
+    """
+
+    @property
+    def message_format(self) -> str:
+        """
+        Message Format
+        """
+
+        return LLMErrorStrings.SKU_NOT_SUPPORTED
 
 @error_decorator(use_parent_error_code=True)
 class TaskNotSupported(NotSupported):
     """
     Task Not Supported Error
     """
 
@@ -234,7 +246,50 @@
     @property
     def message_format(self) -> str:
         """
         Message Format
         """
 
         return LLMErrorStrings.INVALID_LABEL
+
+
+@error_decorator(
+    use_parent_error_code=True, details_uri="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes-gpu"
+)
+class InsufficientGPUMemory(Memory):
+    """
+    Insufficient GPU memory error
+    """
+    @property
+    def message_format(self) -> str:
+        """
+        Message Format
+        """
+        return LLMErrorStrings.INSUFFICIENT_GPU_MEMORY
+
+
+@error_decorator(
+    use_parent_error_code=True, details_uri="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes-gpu"
+)
+class InsufficientGPUMemoryAutoFindBatchSize(Memory):
+    """
+    Insufficient GPU memory error
+    """
+    @property
+    def message_format(self) -> str:
+        """
+        Message Format
+        """
+        return LLMErrorStrings.INSUFFICIENT_GPU_MEMORY_AUTO_FIND_BATCH_SIZE
+
+
+class LossScaleAtMinimum(UserError):
+    """
+    Deepspeed Loss scale at minimum error
+    """
+
+    @property
+    def message_format(self) -> str:
+        """
+        Message Format
+        """
+        return LLMErrorStrings.LOSS_SCALE_AT_MINIMUM
```

## azureml/acft/accelerator/utils/error_handling/error_strings.py

```diff
@@ -17,19 +17,43 @@
     MODEL_NOT_SUPPORTED = "Given Model [{ModelName}] is not supported, Please check name or provide different model"
     MODEL_INCOMPATIBLE_WITH_TASK = (
         "The selected Model [{ModelName}] doesn't support the current Task [{TaskName}], "
         "Please select a different model")
     TOKENIZER_NOT_SUPPORTED = (
         "The selected Tokenizer [{Tokenizer}] doesn't support the current Task [{TaskName}], "
         "Please select a different tokenizer or tokenizer type")
+    SKU_NOT_SUPPORTED = (
+        "The selected compute does not meet minimum compute requirements, please use Standard_NC24s_v3 SKU."
+    )
     VALIDATION_ERROR = "Error while validating parameters [{error}]"
     RESOURCE_NOT_FOUND = "Resource [{ResourceName}] not found"
     INVALID_CHECKPOINT_DIRECTORY = "Provide a valid checkpoint directory. Got [{dir}]"
     PATH_NOT_FOUND = "Path [{path}] was not found"
     ML_CLIENT_NOT_CREATED = (
         "Failed to create ML Client. This is likely            because you didn't create a managed identity           "
         " and assign it to your compute cluster."
     )
     DEPLOYMENT_FAILED = "Failed to create deployment with error [{error}]"
     PREDICTION_FAILED = "Prediction Failed with error [{error}]"
     INVALID_LABEL = "Label {label} is not found in training/validation data"
     INVALID_DATASET = "Only one label found in training and validation data combined"
+    INSUFFICIENT_GPU_MEMORY = (
+        "There is not enough GPU memory on the machine to do the operation. Encountered error [{error}]. "
+        "You could try:\n"
+        "1. Running the experiment on Standard_NC24s_v3 SKU\n"
+        "2. Reducing the batch size\n"
+        "3. Enabling LoRA\n"
+        "4. Enabling deepspeed optimization\n"
+        "5. Enabling deepspeed and ORT optimization."
+    )
+    INSUFFICIENT_GPU_MEMORY_AUTO_FIND_BATCH_SIZE = (
+        "Auto find batch size couldn't find the right batch size to do the operation. Encountered error [{error}]. "
+        "You could try:\n"
+        "1. Running the experiment on Standard_NC24s_v3 SKU\n"
+        "2. Enabling LoRA\n"
+        "3. Enabling deepspeed optimization\n"
+        "4. Enabling deepspeed and ORT optimization."
+    )
+    LOSS_SCALE_AT_MINIMUM = (
+        "Encountered an error while training with Deepspeed [{error}]"
+        "Try with fp32 precision or provide a different Deepspeed config"
+    )
```

## Comparing `azureml_acft_accelerator-0.0.9.data/data/azureml-acft-accelerator/NOTICE` & `azureml_acft_accelerator-47.0.0.data/data/azureml-acft-accelerator/NOTICE`

 * *Files identical despite different names*

## Comparing `azureml_acft_accelerator-0.0.9.dist-info/LICENSE.txt` & `azureml_acft_accelerator-47.0.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_acft_accelerator-0.0.9.dist-info/METADATA` & `azureml_acft_accelerator-47.0.0.dist-info/METADATA`

 * *Files 16% similar despite different names*

```diff
@@ -1,36 +1,37 @@
-Metadata-Version: 2.1
-Name: azureml-acft-accelerator
-Version: 0.0.9
-Summary: Contains the acft accelerator package used in script to build the azureml components.
-Home-page: UNKNOWN
-Author: Microsoft Corp
-License: inline_license
-Platform: UNKNOWN
-Classifier: Development Status :: 3 - Alpha
-Classifier: Intended Audience :: Developers
-Classifier: Intended Audience :: Science/Research
-Classifier: License :: Other/Proprietary License
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
-Classifier: Operating System :: Microsoft :: Windows
-Classifier: Operating System :: MacOS
-Classifier: Operating System :: POSIX :: Linux
-Requires-Python: >=3.7,<3.10
-Description-Content-Type: text/x-rst
-Requires-Dist: azureml-core
-Requires-Dist: azureml-automl-core
-Requires-Dist: sentencepiece (>=0.1.96)
-Requires-Dist: transformers (>=4.26.1)
-Requires-Dist: optimum (>=1.6.4)
-Requires-Dist: accelerate (>=0.11.0)
-Provides-Extra: dev
-Requires-Dist: twine (==3.3.0) ; extra == 'dev'
-Requires-Dist: wheel (==0.37.0) ; extra == 'dev'
-Provides-Extra: test
-Requires-Dist: pytest (~=5.3.0) ; extra == 'test'
-
-The azureml-acft-accelerator package is a package containing functionality used by the the generic components for large language model for data preprocessing, fine-tuning, inferencing and deploying a managed endpoint. This package is not designed for end-user consumption and not to be released to pypi.
-
-
+Metadata-Version: 2.1
+Name: azureml-acft-accelerator
+Version: 47.0.0
+Summary: Contains the acft accelerator package used in script to build the azureml components.
+Home-page: 
+Author: Microsoft Corp
+License: inline_license
+Classifier: Development Status :: 3 - Alpha
+Classifier: Intended Audience :: Developers
+Classifier: Intended Audience :: Science/Research
+Classifier: License :: Other/Proprietary License
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
+Classifier: Operating System :: Microsoft :: Windows
+Classifier: Operating System :: MacOS
+Classifier: Operating System :: POSIX :: Linux
+Requires-Python: >=3.7,<3.12
+Description-Content-Type: text/x-rst
+License-File: LICENSE.txt
+Requires-Dist: azureml-core
+Requires-Dist: azureml-automl-core
+Requires-Dist: sentencepiece >=0.1.96
+Requires-Dist: transformers >=4.26.1
+Requires-Dist: optimum >=1.8.8
+Requires-Dist: accelerate >=0.21.0
+Requires-Dist: peft >=0.4.0
+Provides-Extra: dev
+Requires-Dist: twine ==3.3.0 ; extra == 'dev'
+Requires-Dist: wheel ==0.37.0 ; extra == 'dev'
+Provides-Extra: test
+Requires-Dist: pytest ~=5.3.0 ; extra == 'test'
+
+The azureml-acft-accelerator package is a package containing functionality used by the the generic components for large language model for data preprocessing, fine-tuning, inferencing and deploying a managed endpoint. This package is not designed for end-user consumption and not to be released to pypi.
```

## Comparing `azureml_acft_accelerator-0.0.9.dist-info/RECORD` & `azureml_acft_accelerator-47.0.0.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,46 +1,53 @@
 azureml/__init__.py,sha256=9yEu-iBWQHMmb7eapxBKtSs5hHWMDIZeGhXEntYSp4w,267
 azureml/acft/__init__.py,sha256=9yEu-iBWQHMmb7eapxBKtSs5hHWMDIZeGhXEntYSp4w,267
 azureml/acft/accelerator/NOTICE,sha256=r5tYujIfB30RONsS3E0XQeWMuGrcGqMIvg-fu8EHneQ,753597
-azureml/acft/accelerator/__init__.py,sha256=DF6Ba9T329L6mizV1Una7_iMh29PDKZlrysx86CEWJ8,367
-azureml/acft/accelerator/constants.py,sha256=QpXf-nxVsjL32XHQSXItfhVWjKuIhWmpLrP9LYtvl5Y,5548
-azureml/acft/accelerator/finetune.py,sha256=AF83FYm6NMXtWVLL39f9ZxWJD0wmI5NLhARCiQsdiyE,15809
+azureml/acft/accelerator/__init__.py,sha256=EJqSdrVKAG6_h8j70CZT8K_X20AlwQRtnF_s0Hij82k,888
+azureml/acft/accelerator/_version.py,sha256=6tbwTfcvkL7u8i_rkvzyeWb_6rwSOsoLFvfhWf89WZM,36
+azureml/acft/accelerator/constants.py,sha256=IDNfPMZTX1hloTqvDpWN8A6QJAu1f6cu0AlHUWFNDow,10075
+azureml/acft/accelerator/finetune.py,sha256=L-IL8pZF6x_Ebgh_X4bE0CUviZAEDjmjzdwAq1LFu3w,19692
 azureml/acft/accelerator/lora_wrapper/__init__.py,sha256=5VWOFLQvewyTULVS51nIPxYerF66ek-pa_yxNXd9-dg,305
-azureml/acft/accelerator/lora_wrapper/lora_layers.py,sha256=wtI0S_-wOOQ8hNUkQEyws6eCMEFb4xjhx_ng4f5oSW8,14790
-azureml/acft/accelerator/lora_wrapper/lora_wrapper.py,sha256=rycqU4X2n2LqvXMMZNcFq74Tz2GIzr2XuoekRIqFFS4,9729
-azureml/acft/accelerator/lora_wrapper/lora_configs/__init__.py,sha256=DDtdG5oSh9RW5NSFu8sSnG_YyrvNk4JpgDjRriOErS4,747
+azureml/acft/accelerator/lora_wrapper/lora_layers.py,sha256=as92OtyXSQ87HPW_AsNQQl7EHzPsdyiQaWvS5EkE5i0,14789
+azureml/acft/accelerator/lora_wrapper/lora_wrapper.py,sha256=nEdc5bROwuE37k_gR7qy9wKgCLpsBDHSi6wG7SuswpI,10493
+azureml/acft/accelerator/lora_wrapper/peft_lora_wrapper.py,sha256=iB6R25wSpT2yzV6SK2KPwmIJUoBM_cKyF2t51zzI0UQ,12775
+azureml/acft/accelerator/lora_wrapper/lora_configs/__init__.py,sha256=TEm54DE35PZ1Gb8C1FT7aLHNEM1b1CUGW0pcVXTRmbk,784
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_bart.py,sha256=x8yvoAVt8BJhYDlu2SecMCCQvZ6BBVqCIMRepEnKYCw,1126
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_base.py,sha256=9UB0QkW_wKPIb-73PTFPWgWwCDeWwf0sgYJ19y5l8Ms,1406
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_bert.py,sha256=oyVgFOeCyhZK1x6XGrGjAF5XuuNP7dCae4H2yeprt3I,960
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_bert_base_uncased.py,sha256=4mqjrrNDTiYzsstjenQ9X49kZEtARLsBXUNsQ9nzbZQ,1140
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_camembert.py,sha256=tIHmpGt4Jzmm1XohqpFTBm3IsCjJjfdQaRPi7JoQJKM,965
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_deberta.py,sha256=99W-6rSHvU4r-L8I_cTCyb_ATEUDAHZJt5evVaGHPFQ,1206
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_deberta_base.py,sha256=WGno0M0yIoKqiQFEEGNPXL-w5mDHIy12-5eSXUJcSI4,1217
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_distilbert.py,sha256=tfnfsdC6We-8sGrIgSCOWc3cODufy0Q9V1ay3UVWJ3g,975
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_distilbert_base_uncased.py,sha256=vDQEUsU2UtmSKS9CWHhYslkGvf6XP4HDQlCA6Od96Xc,1153
+azureml/acft/accelerator/lora_wrapper/lora_configs/lora_falcon.py,sha256=AQuz80IPiPwaKZYUDFCxjvXP2_YD2OCPrMj_O-AswXc,978
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_gpt2.py,sha256=XxWmpYjKwiLbrWgVcbsphRnUsYXGBArU0nvQrdcf9rw,1250
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_gpt_neox.py,sha256=q5oBee-7VM40nF_FMWea9rBiD14m1JYy6rg36QfZkS0,1270
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_llama.py,sha256=NgWg3aDAgtfi19OQ1r9gvnSaSUU8Oisr_UQrTS83UhM,972
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_mbart.py,sha256=cpmFzIpanivE2yqIfz0mdNBMC_vsMlOeTXGuMHcGqPU,1139
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_roberta.py,sha256=FnCJwCSo6ION8rKqV2mOIZoZPpNaASQH0WkF9Oi05Fo,966
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_roberta_base.py,sha256=yl1tRtKrCgOpqRw7OpdYjDo5BR749mlId3EYybuxwX8,1125
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_t5.py,sha256=aVeS77FIjfMGGvDFCdWrdfUli3OIVqiP9k6IT2BQO8U,1114
 azureml/acft/accelerator/lora_wrapper/lora_configs/lora_turing_nlr_v3_cased.py,sha256=z71BAyYXZyrAItXgHOyK8PaV2J8J3QlC3gNuQ0TUBb4,1140
 azureml/acft/accelerator/utils/__init__.py,sha256=ouf2rAWSHFJAg_qtQ9B9JwI7xakZ69hDiMGqpDuzL-c,298
+azureml/acft/accelerator/utils/callbacks.py,sha256=uGa2lwQuP_w6h97qw-n7iQhURvPalHWkMTEF11hj1zw,23869
+azureml/acft/accelerator/utils/checkpoint_utils.py,sha256=lzcran-OCr8tVq2QR7SiUnAq5pVXXMOTAGHZfkuj-8k,664
+azureml/acft/accelerator/utils/code_utils.py,sha256=TFBEgI_yb2I1dje3XQebz0PEYNk1194cO0Vyq_2gS8s,2225
 azureml/acft/accelerator/utils/config.py,sha256=cYx0863s28_hsjt3l8Rrf5I4gWq9pEw0W7RrrbewZOE,837
-azureml/acft/accelerator/utils/decorators.py,sha256=Qr8pMcsR7hXOvrpXLF-MkrwlOaSgatdY89UkAbgKC0g,3690
+azureml/acft/accelerator/utils/decorators.py,sha256=K1YX_JFPJ1RRUF5VdrTIUcV_XTHhs-qLG3_yHfohJoI,4681
+azureml/acft/accelerator/utils/deepspeed_utils.py,sha256=Nh_oMtS5XMa1P3smNZPV16hQVzZQLOHZomL2fP_7cts,1940
 azureml/acft/accelerator/utils/hf_argparser.py,sha256=9zLIC8bm16rOT5EcOHWb4m2ODFexRCtS06Wpc5UgG9M,19656
-azureml/acft/accelerator/utils/license_utils.py,sha256=SjSRp50m23u_4KWvCdyhNqNNgby-rrif8kdWzsGABNc,2152
-azureml/acft/accelerator/utils/logging_utils.py,sha256=MuBogV7xMrFg5n214DKSR33GgufZWNBbzqXRfixII-Q,8750
-azureml/acft/accelerator/utils/model_utils.py,sha256=ug4WrbaoBwrh64bXngk39fjREgFJgrEs8PWxJ0-D6pI,3715
-azureml/acft/accelerator/utils/run_utils.py,sha256=9D0V08rn5TeyF2YJpsstm9NyFLJPBerJEB0BvSt4IVA,3267
-azureml/acft/accelerator/utils/trainer_utils.py,sha256=EksGoujQhM0lbhpvHmwGd1qpx2Ng8YrnpwkOk-5btYU,17885
+azureml/acft/accelerator/utils/license_utils.py,sha256=EeTh_FXAQLtzQ6K-NxO5L-IwbH1S8YvVD6OgFIQXgww,2176
+azureml/acft/accelerator/utils/logging_utils.py,sha256=BRxrnhDXnOyRtK9Cqc6jnG0QskPmuv7bV55GduPzyaM,8753
+azureml/acft/accelerator/utils/model_utils.py,sha256=jI7oaLJRNTKnuS-iuf25PTCT2EDdHZCzLkzvvQCHffI,3766
+azureml/acft/accelerator/utils/run_utils.py,sha256=MnA9JEQmVfPupn_xniACTqK1qQMUl_9vSPayDNeuh1I,5638
+azureml/acft/accelerator/utils/trainer_utils.py,sha256=If5LjrZpAZffMdF5WyU18-9pzP2VxQ5o1YPWltq9Pa8,14352
 azureml/acft/accelerator/utils/error_handling/__init__.py,sha256=GrDriyrBquMnkOr9XqYg65XGeagZKp0Q3FRL-JlW1v8,307
-azureml/acft/accelerator/utils/error_handling/error_definitions.py,sha256=CxLkXeET5kCmyGelYyFO5UCBT05Bj-2PmDIMTEkxytE,5015
-azureml/acft/accelerator/utils/error_handling/error_strings.py,sha256=QL0AgwYtgOyihy8rCLWrUNdV184hzTPKLkJSfSUqlS0,1763
+azureml/acft/accelerator/utils/error_handling/error_definitions.py,sha256=RMwpT5EdfBzrIzX5A1wqXLOuYdq0Wmg0jsensxDCyRw,6335
+azureml/acft/accelerator/utils/error_handling/error_strings.py,sha256=yb-oBGQLPAW8N0gWXwPLTOulYbTVYxz55NxUCC6oXL4,2909
 azureml/acft/accelerator/utils/error_handling/exceptions.py,sha256=XeevqAapAO5IyVuKQtmHyAiispwSFeiJ6djbUVFs15o,3772
-azureml_acft_accelerator-0.0.9.data/data/azureml-acft-accelerator/NOTICE,sha256=r5tYujIfB30RONsS3E0XQeWMuGrcGqMIvg-fu8EHneQ,753597
-azureml_acft_accelerator-0.0.9.dist-info/LICENSE.txt,sha256=GBoIyZ-6vJ4xjRc8U3wTw4EfkuaEdVTm_gbr1Nm8uDI,859
-azureml_acft_accelerator-0.0.9.dist-info/METADATA,sha256=eXbZBbXXr2uBGwVo9IFJvL2Q8bPke-ofksbe031gS3s,1566
-azureml_acft_accelerator-0.0.9.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-azureml_acft_accelerator-0.0.9.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
-azureml_acft_accelerator-0.0.9.dist-info/RECORD,,
+azureml_acft_accelerator-47.0.0.data/data/azureml-acft-accelerator/NOTICE,sha256=r5tYujIfB30RONsS3E0XQeWMuGrcGqMIvg-fu8EHneQ,753597
+azureml_acft_accelerator-47.0.0.dist-info/LICENSE.txt,sha256=GBoIyZ-6vJ4xjRc8U3wTw4EfkuaEdVTm_gbr1Nm8uDI,859
+azureml_acft_accelerator-47.0.0.dist-info/METADATA,sha256=VJgOBLvI8zBF1cdKqnBWPjIySfXnpswFEsDR4Den-kc,1719
+azureml_acft_accelerator-47.0.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+azureml_acft_accelerator-47.0.0.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
+azureml_acft_accelerator-47.0.0.dist-info/RECORD,,
```

