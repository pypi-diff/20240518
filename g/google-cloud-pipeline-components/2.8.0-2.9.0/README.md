# Comparing `tmp/google_cloud_pipeline_components-2.8.0-py3-none-any.whl.zip` & `tmp/google_cloud_pipeline_components-2.9.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,516 +1,522 @@
-Zip file size: 1406959 bytes, number of entries: 514
--rw-r--r--  2.0 unx     1189 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/__init__.py
--rw-r--r--  2.0 unx      828 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_image.py
--rw-r--r--  2.0 unx      859 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_placeholders.py
--rw-r--r--  2.0 unx    11045 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/utils.py
--rw-r--r--  2.0 unx      677 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/version.py
--rw-r--r--  2.0 unx      606 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/__init__.py
--rw-r--r--  2.0 unx      606 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/__init__.py
--rw-r--r--  2.0 unx     6252 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/arbiter_preprocess.py
--rw-r--r--  2.0 unx     4432 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/autosxs_arbiter.py
--rw-r--r--  2.0 unx     2591 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/autosxs_metrics_computer.py
--rw-r--r--  2.0 unx     3598 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/bulk_inferrer.py
--rw-r--r--  2.0 unx     5045 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/deploy_llm_model.py
--rw-r--r--  2.0 unx     4601 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/deployment_graph.py
--rw-r--r--  2.0 unx     1521 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/env.py
--rw-r--r--  2.0 unx    20955 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/function_based.py
--rw-r--r--  2.0 unx    11746 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/preprocess_chat_dataset.py
--rw-r--r--  2.0 unx     3702 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/private_text_comparison_importer.py
--rw-r--r--  2.0 unx     4060 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/private_text_importer.py
--rw-r--r--  2.0 unx    10007 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/reinforcement_learning_graph.py
--rw-r--r--  2.0 unx     5634 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/reinforcer.py
--rw-r--r--  2.0 unx     9764 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/reward_model_graph.py
--rw-r--r--  2.0 unx     4853 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/reward_model_trainer.py
--rw-r--r--  2.0 unx     4931 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/supervised_fine_tuner.py
--rw-r--r--  2.0 unx     4175 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/task_preprocess.py
--rw-r--r--  2.0 unx     4734 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/upload_llm_model.py
--rw-r--r--  2.0 unx     4010 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/upload_tensorboard_metrics.py
--rw-r--r--  2.0 unx     4218 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/utils.py
--rw-r--r--  2.0 unx     1881 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/llm/utils_test.py
--rw-r--r--  2.0 unx      811 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model/__init__.py
--rw-r--r--  2.0 unx      662 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model/get_model/__init__.py
--rw-r--r--  2.0 unx     2307 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model/get_model/component.py
--rw-r--r--  2.0 unx     5645 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/__init__.py
--rw-r--r--  2.0 unx     3540 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/utils.py
--rw-r--r--  2.0 unx      963 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/version.py
--rw-r--r--  2.0 unx      665 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/chunking/__init__.py
--rw-r--r--  2.0 unx     4259 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/chunking/component.py
--rw-r--r--  2.0 unx      669 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/__init__.py
--rw-r--r--  2.0 unx     5452 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/component.py
--rw-r--r--  2.0 unx      677 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/__init__.py
--rw-r--r--  2.0 unx     7039 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/component.py
--rw-r--r--  2.0 unx      679 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/endpoint_batch_predict/__init__.py
--rw-r--r--  2.0 unx    11837 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/endpoint_batch_predict/component.py
--rw-r--r--  2.0 unx      671 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/error_analysis_annotation/__init__.py
--rw-r--r--  2.0 unx     4092 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/error_analysis_annotation/component.py
--rw-r--r--  2.0 unx      677 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/evaluated_annotation/__init__.py
--rw-r--r--  2.0 unx     5842 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/evaluated_annotation/component.py
--rw-r--r--  2.0 unx      674 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/feature_attribution/__init__.py
--rw-r--r--  2.0 unx     7664 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/feature_attribution/feature_attribution_component.py
--rw-r--r--  2.0 unx    14290 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/feature_attribution/feature_attribution_graph_component.py
--rw-r--r--  2.0 unx      674 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/feature_extractor/__init__.py
--rw-r--r--  2.0 unx     5539 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/feature_extractor/component.py
--rw-r--r--  2.0 unx      684 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluated_annotation/__init__.py
--rw-r--r--  2.0 unx     3053 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluated_annotation/component.py
--rw-r--r--  2.0 unx      674 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluation/__init__.py
--rw-r--r--  2.0 unx     8026 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluation/component.py
--rw-r--r--  2.0 unx      697 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_classification_postprocessor/__init__.py
--rw-r--r--  2.0 unx    11227 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_classification_postprocessor/component.py
--rw-r--r--  2.0 unx      680 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_embedding/__init__.py
--rw-r--r--  2.0 unx    14207 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_embedding/evaluation_llm_embedding_pipeline.py
--rw-r--r--  2.0 unx      691 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_embedding_retrieval/__init__.py
--rw-r--r--  2.0 unx     7265 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_embedding_retrieval/component.py
--rw-r--r--  2.0 unx      671 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/__init__.py
--rw-r--r--  2.0 unx     7485 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/component.py
--rw-r--r--  2.0 unx      684 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation_preprocessor/__init__.py
--rw-r--r--  2.0 unx     7913 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation_preprocessor/component.py
--rw-r--r--  2.0 unx      695 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_information_retrieval_preprocessor/__init__.py
--rw-r--r--  2.0 unx     8142 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_information_retrieval_preprocessor/component.py
--rw-r--r--  2.0 unx      701 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_retrieval_metrics/__init__.py
--rw-r--r--  2.0 unx     6483 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_retrieval_metrics/component.py
--rw-r--r--  2.0 unx      673 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_safety_bias/__init__.py
--rw-r--r--  2.0 unx     4932 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_safety_bias/component.py
--rw-r--r--  2.0 unx     7005 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/llm_safety_bias/evaluation_llm_safety_bias_pipeline.py
--rw-r--r--  2.0 unx      672 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/model_inference/__init__.py
--rw-r--r--  2.0 unx    14383 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/model_inference/component.py
--rw-r--r--  2.0 unx      682 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/__init__.py
--rw-r--r--  2.0 unx     5738 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/component.py
--rw-r--r--  2.0 unx      664 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql/__init__.py
--rw-r--r--  2.0 unx     8386 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql/evaluation_llm_text2sql_pipeline.py
--rw-r--r--  2.0 unx      680 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_evaluation/__init__.py
--rw-r--r--  2.0 unx     5646 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_evaluation/component.py
--rw-r--r--  2.0 unx      691 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_preprocess/__init__.py
--rw-r--r--  2.0 unx     5197 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_preprocess/component.py
--rw-r--r--  2.0 unx      701 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_validate_and_process/__init__.py
--rw-r--r--  2.0 unx     5636 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_validate_and_process/component.py
--rw-r--r--  2.0 unx      651 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/__init__.py
--rw-r--r--  2.0 unx      659 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/_implementation/__init__.py
--rw-r--r--  2.0 unx      665 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/_implementation/llm/__init__.py
--rw-r--r--  2.0 unx      954 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/_implementation/llm/templated_custom_job/__init__.py
--rw-r--r--  2.0 unx     5267 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/_implementation/llm/templated_custom_job/launcher.py
--rw-r--r--  2.0 unx     6728 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/_implementation/llm/templated_custom_job/remote_runner.py
--rw-r--r--  2.0 unx      668 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/_implementation/model/__init__.py
--rw-r--r--  2.0 unx      685 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/_implementation/model/get_model/__init__.py
--rw-r--r--  2.0 unx     4975 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/_implementation/model/get_model/get_model.py
--rw-r--r--  2.0 unx      678 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/_implementation/model_evaluation/__init__.py
--rw-r--r--  2.0 unx    10196 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/_implementation/model_evaluation/import_evaluated_annotation.py
--rw-r--r--  2.0 unx    17835 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/_implementation/model_evaluation/import_model_evaluation.py
--rw-r--r--  2.0 unx      651 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/preview/__init__.py
--rw-r--r--  2.0 unx      686 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/preview/custom_job/__init__.py
--rw-r--r--  2.0 unx     1922 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/preview/custom_job/launcher.py
--rw-r--r--  2.0 unx     3591 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/preview/custom_job/remote_runner.py
--rw-r--r--  2.0 unx      655 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/preview/dataflow/__init__.py
--rw-r--r--  2.0 unx      698 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/preview/dataflow/flex_template/__init__.py
--rw-r--r--  2.0 unx     1914 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/preview/dataflow/flex_template/launcher.py
--rw-r--r--  2.0 unx     9632 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/preview/dataflow/flex_template/remote_runner.py
--rw-r--r--  2.0 unx      662 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/preview/gcp_launcher/__init__.py
--rw-r--r--  2.0 unx     8447 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/preview/gcp_launcher/job_remote_runner.py
--rw-r--r--  2.0 unx      638 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/utils/__init__.py
--rw-r--r--  2.0 unx     3027 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/utils/artifact_utils.py
--rw-r--r--  2.0 unx     1841 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/utils/execution_context.py
--rw-r--r--  2.0 unx      651 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/__init__.py
--rw-r--r--  2.0 unx      651 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/aiplatform/__init__.py
--rw-r--r--  2.0 unx    10719 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/aiplatform/remote_runner.py
--rw-r--r--  2.0 unx     5257 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/aiplatform/utils.py
--rw-r--r--  2.0 unx      689 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/automl_training_job/__init__.py
--rw-r--r--  2.0 unx      701 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/automl_training_job/image/__init__.py
--rw-r--r--  2.0 unx     6257 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/automl_training_job/image/launcher.py
--rw-r--r--  2.0 unx     9061 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/automl_training_job/image/remote_runner.py
--rw-r--r--  2.0 unx      696 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/batch_prediction_job/__init__.py
--rw-r--r--  2.0 unx     2364 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/batch_prediction_job/launcher.py
--rw-r--r--  2.0 unx     9920 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/batch_prediction_job/remote_runner.py
--rw-r--r--  2.0 unx      655 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/__init__.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/create_model/__init__.py
--rw-r--r--  2.0 unx     2502 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/create_model/launcher.py
--rw-r--r--  2.0 unx     3825 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/create_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/__init__.py
--rw-r--r--  2.0 unx     3284 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/launcher.py
--rw-r--r--  2.0 unx     5770 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/drop_model/__init__.py
--rw-r--r--  2.0 unx     2627 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/drop_model/launcher.py
--rw-r--r--  2.0 unx     3306 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/drop_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/__init__.py
--rw-r--r--  2.0 unx     3307 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/launcher.py
--rw-r--r--  2.0 unx     5574 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/__init__.py
--rw-r--r--  2.0 unx     2958 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/launcher.py
--rw-r--r--  2.0 unx     3432 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/__init__.py
--rw-r--r--  2.0 unx     3639 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/launcher.py
--rw-r--r--  2.0 unx     5843 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/export_model/__init__.py
--rw-r--r--  2.0 unx     2775 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/export_model/launcher.py
--rw-r--r--  2.0 unx     4706 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/export_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/__init__.py
--rw-r--r--  2.0 unx     2656 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/launcher.py
--rw-r--r--  2.0 unx     3562 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/__init__.py
--rw-r--r--  2.0 unx     2932 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/launcher.py
--rw-r--r--  2.0 unx     3384 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/global_explain/__init__.py
--rw-r--r--  2.0 unx     2807 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/global_explain/launcher.py
--rw-r--r--  2.0 unx     2676 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/global_explain/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/__init__.py
--rw-r--r--  2.0 unx     2653 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/launcher.py
--rw-r--r--  2.0 unx     4046 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/__init__.py
--rw-r--r--  2.0 unx     2655 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/launcher.py
--rw-r--r--  2.0 unx     3441 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/__init__.py
--rw-r--r--  2.0 unx     2822 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/launcher.py
--rw-r--r--  2.0 unx     4662 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/__init__.py
--rw-r--r--  2.0 unx     2780 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/launcher.py
--rw-r--r--  2.0 unx     4637 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/__init__.py
--rw-r--r--  2.0 unx     3319 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/launcher.py
--rw-r--r--  2.0 unx     4731 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/__init__.py
--rw-r--r--  2.0 unx     2641 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/launcher.py
--rw-r--r--  2.0 unx     4018 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/__init__.py
--rw-r--r--  2.0 unx     2676 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/launcher.py
--rw-r--r--  2.0 unx     3194 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/__init__.py
--rw-r--r--  2.0 unx     2665 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/launcher.py
--rw-r--r--  2.0 unx     3166 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/__init__.py
--rw-r--r--  2.0 unx     3086 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/launcher.py
--rw-r--r--  2.0 unx     4050 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/__init__.py
--rw-r--r--  2.0 unx     3115 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/launcher.py
--rw-r--r--  2.0 unx     4337 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/__init__.py
--rw-r--r--  2.0 unx     3302 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/launcher.py
--rw-r--r--  2.0 unx     4613 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/__init__.py
--rw-r--r--  2.0 unx     2644 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/launcher.py
--rw-r--r--  2.0 unx     4032 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/__init__.py
--rw-r--r--  2.0 unx     2635 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/launcher.py
--rw-r--r--  2.0 unx     4211 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/__init__.py
--rw-r--r--  2.0 unx     2627 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/launcher.py
--rw-r--r--  2.0 unx     4001 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/predict_model/__init__.py
--rw-r--r--  2.0 unx     3302 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/predict_model/launcher.py
--rw-r--r--  2.0 unx     4578 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/predict_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/query_job/__init__.py
--rw-r--r--  2.0 unx     2473 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/query_job/launcher.py
--rw-r--r--  2.0 unx     2539 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/query_job/remote_runner.py
--rw-r--r--  2.0 unx      673 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/utils/__init__.py
--rw-r--r--  2.0 unx    12236 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/bigquery/utils/bigquery_util.py
--rw-r--r--  2.0 unx      686 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/custom_job/__init__.py
--rw-r--r--  2.0 unx     1917 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/custom_job/launcher.py
--rw-r--r--  2.0 unx     3586 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/custom_job/remote_runner.py
--rw-r--r--  2.0 unx      662 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataflow/__init__.py
--rw-r--r--  2.0 unx     3303 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataflow/dataflow_launcher.py
--rw-r--r--  2.0 unx     7415 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataflow/dataflow_python_job_remote_runner.py
--rw-r--r--  2.0 unx      699 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/__init__.py
--rw-r--r--  2.0 unx      705 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/create_pyspark_batch/__init__.py
--rw-r--r--  2.0 unx     2196 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/create_pyspark_batch/launcher.py
--rw-r--r--  2.0 unx      804 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/create_pyspark_batch/remote_runner.py
--rw-r--r--  2.0 unx      703 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/create_spark_batch/__init__.py
--rw-r--r--  2.0 unx     2188 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/create_spark_batch/launcher.py
--rw-r--r--  2.0 unx      800 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/create_spark_batch/remote_runner.py
--rw-r--r--  2.0 unx      705 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/create_spark_r_batch/__init__.py
--rw-r--r--  2.0 unx     2195 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/create_spark_r_batch/launcher.py
--rw-r--r--  2.0 unx      804 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/create_spark_r_batch/remote_runner.py
--rw-r--r--  2.0 unx      707 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/__init__.py
--rw-r--r--  2.0 unx     2203 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/launcher.py
--rw-r--r--  2.0 unx      808 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/remote_runner.py
--rw-r--r--  2.0 unx      663 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/utils/__init__.py
--rw-r--r--  2.0 unx    12872 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/dataproc/utils/dataproc_util.py
--rw-r--r--  2.0 unx      655 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/__init__.py
--rw-r--r--  2.0 unx      691 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/__init__.py
--rw-r--r--  2.0 unx     2288 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/launcher.py
--rw-r--r--  2.0 unx     2363 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/remote_runner.py
--rw-r--r--  2.0 unx      691 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/__init__.py
--rw-r--r--  2.0 unx     1938 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/launcher.py
--rw-r--r--  2.0 unx     2308 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/__init__.py
--rw-r--r--  2.0 unx     1926 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/launcher.py
--rw-r--r--  2.0 unx     2375 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/remote_runner.py
--rw-r--r--  2.0 unx      690 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/__init__.py
--rw-r--r--  2.0 unx     1934 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/launcher.py
--rw-r--r--  2.0 unx     4711 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/remote_runner.py
--rw-r--r--  2.0 unx      662 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/gcp_launcher/__init__.py
--rw-r--r--  2.0 unx     8350 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/gcp_launcher/job_remote_runner.py
--rw-r--r--  2.0 unx     4945 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/gcp_launcher/lro_remote_runner.py
--rw-r--r--  2.0 unx    10456 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/gcp_launcher/pipeline_remote_runner.py
--rw-r--r--  2.0 unx      660 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/__init__.py
--rw-r--r--  2.0 unx      876 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/error_util.py
--rw-r--r--  2.0 unx     2193 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/gcp_labels_util.py
--rw-r--r--  2.0 unx     2951 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/json_util.py
--rw-r--r--  2.0 unx     1882 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/parser_util.py
--rw-r--r--  2.0 unx      701 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/__init__.py
--rw-r--r--  2.0 unx     2240 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/launcher.py
--rw-r--r--  2.0 unx     4402 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/remote_runner.py
--rw-r--r--  2.0 unx      696 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/infra_validation_job/__init__.py
--rw-r--r--  2.0 unx     2280 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/infra_validation_job/launcher.py
--rw-r--r--  2.0 unx     4068 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/infra_validation_job/remote_runner.py
--rw-r--r--  2.0 unx      652 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/model/__init__.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/model/delete_model/__init__.py
--rw-r--r--  2.0 unx     1923 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/model/delete_model/launcher.py
--rw-r--r--  2.0 unx     2464 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/model/delete_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/model/export_model/__init__.py
--rw-r--r--  2.0 unx     2201 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/model/export_model/launcher.py
--rw-r--r--  2.0 unx     2479 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/model/export_model/remote_runner.py
--rw-r--r--  2.0 unx      688 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/model/upload_model/__init__.py
--rw-r--r--  2.0 unx     2331 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/model/upload_model/launcher.py
--rw-r--r--  2.0 unx     4480 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/model/upload_model/remote_runner.py
--rw-r--r--  2.0 unx      662 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/vertex_notification_email/__init__.py
--rw-r--r--  2.0 unx     1074 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/vertex_notification_email/executor.py
--rw-r--r--  2.0 unx      694 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/wait_gcp_resources/__init__.py
--rw-r--r--  2.0 unx     1928 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/wait_gcp_resources/launcher.py
--rw-r--r--  2.0 unx     5968 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/container/v1/wait_gcp_resources/remote_runner.py
--rw-r--r--  2.0 unx      606 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/__init__.py
--rw-r--r--  2.0 unx      606 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/__init__.py
--rw-r--r--  2.0 unx     2691 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/forecasting/__init__.py
--rw-r--r--  2.0 unx     5655 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/forecasting/forecasting_ensemble.py
--rw-r--r--  2.0 unx     6564 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_1_tuner.py
--rw-r--r--  2.0 unx     6376 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_2_tuner.py
--rw-r--r--  2.0 unx   367100 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/forecasting/learn_to_learn_forecasting_pipeline.yaml
--rw-r--r--  2.0 unx   365279 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/forecasting/sequence_to_sequence_forecasting_pipeline.yaml
--rw-r--r--  2.0 unx   364385 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/forecasting/temporal_fusion_transformer_forecasting_pipeline.yaml
--rw-r--r--  2.0 unx   367175 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/forecasting/time_series_dense_encoder_forecasting_pipeline.yaml
--rw-r--r--  2.0 unx    53844 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/forecasting/utils.py
--rw-r--r--  2.0 unx     3105 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/__init__.py
--rw-r--r--  2.0 unx     3643 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/auto_feature_engineering.py
--rw-r--r--  2.0 unx   516309 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_feature_selection_pipeline.yaml
--rw-r--r--  2.0 unx   589607 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_v2_pipeline.yaml
--rw-r--r--  2.0 unx     9602 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/distillation_stage_feature_transform_engine.py
--rw-r--r--  2.0 unx     7546 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/feature_selection.py
--rw-r--r--  2.0 unx    80892 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/feature_selection_pipeline.yaml
--rw-r--r--  2.0 unx    47372 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/feature_transform_engine.py
--rw-r--r--  2.0 unx     9980 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job.py
--rw-r--r--  2.0 unx   248549 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job_pipeline.yaml
--rw-r--r--  2.0 unx    12748 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer.py
--rw-r--r--  2.0 unx   207374 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer_pipeline.yaml
--rw-r--r--  2.0 unx   169668 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/utils.py
--rw-r--r--  2.0 unx     9975 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job.py
--rw-r--r--  2.0 unx   200559 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job_pipeline.yaml
--rw-r--r--  2.0 unx    11922 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer.py
--rw-r--r--  2.0 unx   196045 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer_pipeline.yaml
--rw-r--r--  2.0 unx     4787 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/xgboost_hyperparameter_tuning_job.py
--rw-r--r--  2.0 unx   229291 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/xgboost_hyperparameter_tuning_job_pipeline.yaml
--rw-r--r--  2.0 unx     2333 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/xgboost_trainer.py
--rw-r--r--  2.0 unx   213303 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/xgboost_trainer_pipeline.yaml
--rw-r--r--  2.0 unx     3292 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_large_search_space.json
--rw-r--r--  2.0 unx     3261 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_medium_search_space.json
--rw-r--r--  2.0 unx     2975 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_small_search_space.json
--rw-r--r--  2.0 unx     3287 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_large_search_space.json
--rw-r--r--  2.0 unx     3266 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_medium_search_space.json
--rw-r--r--  2.0 unx     2974 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_small_search_space.json
--rw-r--r--  2.0 unx     3269 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_large_search_space.json
--rw-r--r--  2.0 unx     3254 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_medium_search_space.json
--rw-r--r--  2.0 unx     2966 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_small_search_space.json
--rw-r--r--  2.0 unx     2647 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/configs/wide_and_deep_params.json
--rw-r--r--  2.0 unx     7748 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/tabular/configs/xgboost_params.json
--rw-r--r--  2.0 unx      758 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/vision/__init__.py
--rw-r--r--  2.0 unx     6310 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/vision/data_converter.py
--rw-r--r--  2.0 unx     6001 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/automl/vision/json_utils.py
--rw-r--r--  2.0 unx     1295 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/custom_job/__init__.py
--rw-r--r--  2.0 unx     7098 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/custom_job/component.py
--rw-r--r--  2.0 unx    16062 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/custom_job/utils.py
--rw-r--r--  2.0 unx      773 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/dataflow/__init__.py
--rw-r--r--  2.0 unx      886 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/llm/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/llm/infer/__init__.py
--rw-r--r--  2.0 unx     7384 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/llm/infer/component.py
--rw-r--r--  2.0 unx      606 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/llm/rlhf/__init__.py
--rw-r--r--  2.0 unx    10044 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/llm/rlhf/component.py
--rw-r--r--  2.0 unx     1949 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/model_evaluation/__init__.py
--rw-r--r--  2.0 unx     5791 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/model_evaluation/data_bias_component.py
--rw-r--r--  2.0 unx    11212 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/model_evaluation/evaluation_llm_classification_pipeline.py
--rw-r--r--  2.0 unx     9560 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/model_evaluation/evaluation_llm_text_generation_pipeline.py
--rw-r--r--  2.0 unx     7450 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_component.py
--rw-r--r--  2.0 unx    13624 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_graph_component.py
--rw-r--r--  2.0 unx     6674 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/model_evaluation/model_bias_component.py
--rw-r--r--  2.0 unx     7539 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/model_evaluation/utils.py
--rw-r--r--  2.0 unx      838 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/model_evaluation/model_based_llm_evaluation/__init__.py
--rw-r--r--  2.0 unx      606 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/model_evaluation/model_based_llm_evaluation/autosxs/__init__.py
--rw-r--r--  2.0 unx     9975 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/preview/model_evaluation/model_based_llm_evaluation/autosxs/autosxs_pipeline.py
--rw-r--r--  2.0 unx      661 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/proto/__init__.py
--rw-r--r--  2.0 unx     7687 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/proto/gcp_resources_pb2.py
--rw-r--r--  2.0 unx      606 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/types/__init__.py
--rw-r--r--  2.0 unx    23725 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/types/artifact_types.py
--rw-r--r--  2.0 unx      765 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/__init__.py
--rw-r--r--  2.0 unx      631 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/__init__.py
--rw-r--r--  2.0 unx      806 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/forecasting/__init__.py
--rw-r--r--  2.0 unx    52484 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_predict_pipeline.yaml
--rw-r--r--  2.0 unx   252190 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_train_pipeline.yaml
--rw-r--r--  2.0 unx    95718 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/forecasting/prophet_predict_pipeline.yaml
--rw-r--r--  2.0 unx     8363 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer.py
--rw-r--r--  2.0 unx   149452 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer_pipeline.yaml
--rw-r--r--  2.0 unx    15028 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/forecasting/utils.py
--rw-r--r--  2.0 unx     2318 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/__init__.py
--rw-r--r--  2.0 unx   503740 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/automl_tabular_pipeline.yaml
--rw-r--r--  2.0 unx     6708 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/cv_trainer.py
--rw-r--r--  2.0 unx     6935 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/ensemble.py
--rw-r--r--  2.0 unx     3108 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/finalizer.py
--rw-r--r--  2.0 unx     1333 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/infra_validator.py
--rw-r--r--  2.0 unx     5571 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/split_materialized_data.py
--rw-r--r--  2.0 unx     7769 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/stage_1_tuner.py
--rw-r--r--  2.0 unx    13389 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/stats_and_example_gen.py
--rw-r--r--  2.0 unx    12214 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/training_configurator_and_validator.py
--rw-r--r--  2.0 unx     8249 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/transform.py
--rw-r--r--  2.0 unx    74895 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/utils.py
--rw-r--r--  2.0 unx   321170 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/tabular/deprecated/default_pipeline.json
--rw-r--r--  2.0 unx     1803 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/training_job/__init__.py
--rw-r--r--  2.0 unx      655 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/training_job/automl_forecasting_training_job/__init__.py
--rw-r--r--  2.0 unx    27062 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/training_job/automl_forecasting_training_job/component.py
--rw-r--r--  2.0 unx      649 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/training_job/automl_image_training_job/__init__.py
--rw-r--r--  2.0 unx    18382 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/training_job/automl_image_training_job/component.py
--rw-r--r--  2.0 unx      651 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/training_job/automl_tabular_training_job/__init__.py
--rw-r--r--  2.0 unx    20801 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/training_job/automl_tabular_training_job/component.py
--rw-r--r--  2.0 unx      648 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/training_job/automl_text_training_job/__init__.py
--rw-r--r--  2.0 unx     9406 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/training_job/automl_text_training_job/component.py
--rw-r--r--  2.0 unx      649 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/training_job/automl_video_training_job/__init__.py
--rw-r--r--  2.0 unx     9237 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/automl/training_job/automl_video_training_job/component.py
--rw-r--r--  2.0 unx      992 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/batch_predict_job/__init__.py
--rw-r--r--  2.0 unx    20638 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/batch_predict_job/component.py
--rw-r--r--  2.0 unx     5500 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/__init__.py
--rw-r--r--  2.0 unx      670 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/create_model/__init__.py
--rw-r--r--  2.0 unx     4164 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/create_model/component.py
--rw-r--r--  2.0 unx      680 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/detect_anomalies_model/__init__.py
--rw-r--r--  2.0 unx     6965 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/detect_anomalies_model/component.py
--rw-r--r--  2.0 unx      668 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/drop_model/__init__.py
--rw-r--r--  2.0 unx     4055 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/drop_model/component.py
--rw-r--r--  2.0 unx      672 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/evaluate_model/__init__.py
--rw-r--r--  2.0 unx     6320 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/evaluate_model/component.py
--rw-r--r--  2.0 unx      680 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/explain_forecast_model/__init__.py
--rw-r--r--  2.0 unx     5890 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/explain_forecast_model/component.py
--rw-r--r--  2.0 unx      679 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/explain_predict_model/__init__.py
--rw-r--r--  2.0 unx     7174 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/explain_predict_model/component.py
--rw-r--r--  2.0 unx      670 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/export_model/__init__.py
--rw-r--r--  2.0 unx     4044 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/export_model/component.py
--rw-r--r--  2.0 unx      676 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/feature_importance/__init__.py
--rw-r--r--  2.0 unx     5158 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/feature_importance/component.py
--rw-r--r--  2.0 unx      672 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/forecast_model/__init__.py
--rw-r--r--  2.0 unx     5764 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/forecast_model/component.py
--rw-r--r--  2.0 unx      672 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/global_explain/__init__.py
--rw-r--r--  2.0 unx     4372 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/global_explain/component.py
--rw-r--r--  2.0 unx      677 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_advanced_weights/__init__.py
--rw-r--r--  2.0 unx     4578 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_advanced_weights/component.py
--rw-r--r--  2.0 unx      679 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_arima_coefficients/__init__.py
--rw-r--r--  2.0 unx     4772 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_arima_coefficients/component.py
--rw-r--r--  2.0 unx      675 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_arima_evaluate/__init__.py
--rw-r--r--  2.0 unx     5666 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_arima_evaluate/component.py
--rw-r--r--  2.0 unx      670 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_centroids/__init__.py
--rw-r--r--  2.0 unx     5477 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_centroids/component.py
--rw-r--r--  2.0 unx      677 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_confusion_matrix/__init__.py
--rw-r--r--  2.0 unx     5514 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_confusion_matrix/component.py
--rw-r--r--  2.0 unx      673 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_feature_info/__init__.py
--rw-r--r--  2.0 unx     4539 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_feature_info/component.py
--rw-r--r--  2.0 unx      685 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_principal_component_info/__init__.py
--rw-r--r--  2.0 unx     5353 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_principal_component_info/component.py
--rw-r--r--  2.0 unx      681 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_principal_components/__init__.py
--rw-r--r--  2.0 unx     5318 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_principal_components/component.py
--rw-r--r--  2.0 unx      670 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_recommend/__init__.py
--rw-r--r--  2.0 unx     5849 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_recommend/component.py
--rw-r--r--  2.0 unx      680 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_reconstruction_loss/__init__.py
--rw-r--r--  2.0 unx     5942 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_reconstruction_loss/component.py
--rw-r--r--  2.0 unx      670 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_roc_curve/__init__.py
--rw-r--r--  2.0 unx     5398 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_roc_curve/component.py
--rw-r--r--  2.0 unx      674 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_training_info/__init__.py
--rw-r--r--  2.0 unx     4571 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_training_info/component.py
--rw-r--r--  2.0 unx      671 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_trial_info/__init__.py
--rw-r--r--  2.0 unx     5083 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_trial_info/component.py
--rw-r--r--  2.0 unx      668 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_weights/__init__.py
--rw-r--r--  2.0 unx     4502 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/ml_weights/component.py
--rw-r--r--  2.0 unx      671 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/predict_model/__init__.py
--rw-r--r--  2.0 unx     6279 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/predict_model/component.py
--rw-r--r--  2.0 unx      667 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/query_job/__init__.py
--rw-r--r--  2.0 unx     4912 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/bigquery/query_job/component.py
--rw-r--r--  2.0 unx     1278 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/custom_job/__init__.py
--rw-r--r--  2.0 unx     6070 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/custom_job/component.py
--rw-r--r--  2.0 unx    15509 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/custom_job/utils.py
--rw-r--r--  2.0 unx     1063 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataflow/__init__.py
--rw-r--r--  2.0 unx      669 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataflow/flex_template/__init__.py
--rw-r--r--  2.0 unx    11701 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataflow/flex_template/component.py
--rw-r--r--  2.0 unx      664 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataflow/python_job/__init__.py
--rw-r--r--  2.0 unx     2692 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataflow/python_job/component.py
--rw-r--r--  2.0 unx     1445 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataproc/__init__.py
--rw-r--r--  2.0 unx      670 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataproc/create_pyspark_batch/__init__.py
--rw-r--r--  2.0 unx     6776 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataproc/create_pyspark_batch/component.py
--rw-r--r--  2.0 unx      668 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataproc/create_spark_batch/__init__.py
--rw-r--r--  2.0 unx     6735 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataproc/create_spark_batch/component.py
--rw-r--r--  2.0 unx      669 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataproc/create_spark_r_batch/__init__.py
--rw-r--r--  2.0 unx     6307 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataproc/create_spark_r_batch/component.py
--rw-r--r--  2.0 unx      673 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataproc/create_spark_sql_batch/__init__.py
--rw-r--r--  2.0 unx     6111 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataproc/create_spark_sql_batch/component.py
--rw-r--r--  2.0 unx     3108 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/__init__.py
--rw-r--r--  2.0 unx      644 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/create_image_dataset/__init__.py
--rw-r--r--  2.0 unx     5608 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/create_image_dataset/component.py
--rw-r--r--  2.0 unx      646 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/create_tabular_dataset/__init__.py
--rw-r--r--  2.0 unx     4368 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/create_tabular_dataset/component.py
--rw-r--r--  2.0 unx      643 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/create_text_dataset/__init__.py
--rw-r--r--  2.0 unx     5496 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/create_text_dataset/component.py
--rw-r--r--  2.0 unx      642 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/create_time_series_dataset/__init__.py
--rw-r--r--  2.0 unx     4381 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/create_time_series_dataset/component.py
--rw-r--r--  2.0 unx      644 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/create_video_dataset/__init__.py
--rw-r--r--  2.0 unx     5478 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/create_video_dataset/component.py
--rw-r--r--  2.0 unx      644 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/export_image_dataset/__init__.py
--rw-r--r--  2.0 unx     2982 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/export_image_dataset/component.py
--rw-r--r--  2.0 unx      646 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/export_tabular_dataset/__init__.py
--rw-r--r--  2.0 unx     2986 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/export_tabular_dataset/component.py
--rw-r--r--  2.0 unx      643 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/export_text_dataset/__init__.py
--rw-r--r--  2.0 unx     2980 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/export_text_dataset/component.py
--rw-r--r--  2.0 unx      650 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/export_time_series_dataset/__init__.py
--rw-r--r--  2.0 unx     2991 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/export_time_series_dataset/component.py
--rw-r--r--  2.0 unx      644 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/export_video_dataset/__init__.py
--rw-r--r--  2.0 unx     2982 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/export_video_dataset/component.py
--rw-r--r--  2.0 unx      642 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/get_vertex_dataset/__init__.py
--rw-r--r--  2.0 unx     3878 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/get_vertex_dataset/component.py
--rw-r--r--  2.0 unx      644 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/import_image_dataset/__init__.py
--rw-r--r--  2.0 unx     4114 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/import_image_dataset/component.py
--rw-r--r--  2.0 unx      643 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/import_text_dataset/__init__.py
--rw-r--r--  2.0 unx     4108 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/import_text_dataset/component.py
--rw-r--r--  2.0 unx      644 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/import_video_dataset/__init__.py
--rw-r--r--  2.0 unx     4114 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/dataset/import_video_dataset/component.py
--rw-r--r--  2.0 unx     1381 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/endpoint/__init__.py
--rw-r--r--  2.0 unx      664 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/endpoint/create_endpoint/__init__.py
--rw-r--r--  2.0 unx     4751 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/endpoint/create_endpoint/component.py
--rw-r--r--  2.0 unx      639 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/endpoint/delete_endpoint/__init__.py
--rw-r--r--  2.0 unx     2311 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/endpoint/delete_endpoint/component.py
--rw-r--r--  2.0 unx      661 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/endpoint/deploy_model/__init__.py
--rw-r--r--  2.0 unx    10066 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/endpoint/deploy_model/component.py
--rw-r--r--  2.0 unx      638 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/endpoint/undeploy_model/__init__.py
--rw-r--r--  2.0 unx     3284 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/endpoint/undeploy_model/component.py
--rw-r--r--  2.0 unx     1293 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/forecasting/__init__.py
--rw-r--r--  2.0 unx      658 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/forecasting/prepare_data_for_train/__init__.py
--rw-r--r--  2.0 unx     7404 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/forecasting/prepare_data_for_train/component.py
--rw-r--r--  2.0 unx      646 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/forecasting/preprocess/__init__.py
--rw-r--r--  2.0 unx     2241 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/forecasting/preprocess/component.py
--rw-r--r--  2.0 unx      644 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/forecasting/validate/__init__.py
--rw-r--r--  2.0 unx     1891 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/forecasting/validate/component.py
--rw-r--r--  2.0 unx     1300 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/__init__.py
--rw-r--r--  2.0 unx     9479 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/component.py
--rw-r--r--  2.0 unx     3118 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/utils.py
--rw-r--r--  2.0 unx     1148 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model/__init__.py
--rw-r--r--  2.0 unx      638 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model/delete_model/__init__.py
--rw-r--r--  2.0 unx     2490 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model/delete_model/component.py
--rw-r--r--  2.0 unx      661 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model/export_model/__init__.py
--rw-r--r--  2.0 unx     4908 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model/export_model/component.py
--rw-r--r--  2.0 unx      661 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model/upload_model/__init__.py
--rw-r--r--  2.0 unx     6646 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model/upload_model/component.py
--rw-r--r--  2.0 unx     2358 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model_evaluation/__init__.py
--rw-r--r--  2.0 unx    12122 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model_evaluation/classification_component.py
--rw-r--r--  2.0 unx    20122 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model_evaluation/error_analysis_pipeline.py
--rw-r--r--  2.0 unx    12142 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model_evaluation/evaluated_annotation_pipeline.py
--rw-r--r--  2.0 unx    46187 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_feature_attribution_pipeline.py
--rw-r--r--  2.0 unx    37108 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_pipeline.py
--rw-r--r--  2.0 unx    42404 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_unstructure_data_pipeline.py
--rw-r--r--  2.0 unx    51173 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model_evaluation/evaluation_feature_attribution_pipeline.py
--rw-r--r--  2.0 unx    10017 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model_evaluation/forecasting_component.py
--rw-r--r--  2.0 unx     9117 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/model_evaluation/regression_component.py
--rw-r--r--  2.0 unx      863 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/vertex_notification_email/__init__.py
--rw-r--r--  2.0 unx     2163 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/vertex_notification_email/component.py
--rw-r--r--  2.0 unx      878 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/wait_gcp_resources/__init__.py
--rw-r--r--  2.0 unx     2468 b- defN 23-Dec-14 01:13 google_cloud_pipeline_components/v1/wait_gcp_resources/component.py
--rw-r--r--  2.0 unx    11357 b- defN 23-Dec-14 01:21 google_cloud_pipeline_components-2.8.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     5853 b- defN 23-Dec-14 01:21 google_cloud_pipeline_components-2.8.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Dec-14 01:21 google_cloud_pipeline_components-2.8.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       33 b- defN 23-Dec-14 01:21 google_cloud_pipeline_components-2.8.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    70441 b- defN 23-Dec-14 01:21 google_cloud_pipeline_components-2.8.0.dist-info/RECORD
-514 files, 7735083 bytes uncompressed, 1285261 bytes compressed:  83.4%
+Zip file size: 1405561 bytes, number of entries: 520
+-rw-r--r--  2.0 unx     1189 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/__init__.py
+-rw-r--r--  2.0 unx      828 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_image.py
+-rw-r--r--  2.0 unx     1233 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_placeholders.py
+-rw-r--r--  2.0 unx    11045 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/utils.py
+-rw-r--r--  2.0 unx      677 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/version.py
+-rw-r--r--  2.0 unx      606 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/__init__.py
+-rw-r--r--  2.0 unx      606 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/__init__.py
+-rw-r--r--  2.0 unx     6252 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/arbiter_preprocess.py
+-rw-r--r--  2.0 unx     4638 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/autosxs_arbiter.py
+-rw-r--r--  2.0 unx     2591 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/autosxs_metrics_computer.py
+-rw-r--r--  2.0 unx     3598 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/bulk_inferrer.py
+-rw-r--r--  2.0 unx     5045 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/deploy_llm_model.py
+-rw-r--r--  2.0 unx     4671 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/deployment_graph.py
+-rw-r--r--  2.0 unx     1796 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/env.py
+-rw-r--r--  2.0 unx    21780 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/function_based.py
+-rw-r--r--  2.0 unx    11746 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/preprocess_chat_dataset.py
+-rw-r--r--  2.0 unx     3702 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/private_text_comparison_importer.py
+-rw-r--r--  2.0 unx     4060 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/private_text_importer.py
+-rw-r--r--  2.0 unx    10007 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/reinforcement_learning_graph.py
+-rw-r--r--  2.0 unx     5634 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/reinforcer.py
+-rw-r--r--  2.0 unx     9764 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/reward_model_graph.py
+-rw-r--r--  2.0 unx     4853 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/reward_model_trainer.py
+-rw-r--r--  2.0 unx     4931 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/supervised_fine_tuner.py
+-rw-r--r--  2.0 unx     4175 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/task_preprocess.py
+-rw-r--r--  2.0 unx     4734 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/upload_llm_model.py
+-rw-r--r--  2.0 unx     4010 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/upload_tensorboard_metrics.py
+-rw-r--r--  2.0 unx     4680 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/utils.py
+-rw-r--r--  2.0 unx     2864 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/llm/utils_test.py
+-rw-r--r--  2.0 unx      811 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model/__init__.py
+-rw-r--r--  2.0 unx      662 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model/get_model/__init__.py
+-rw-r--r--  2.0 unx     2307 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model/get_model/component.py
+-rw-r--r--  2.0 unx     5645 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/__init__.py
+-rw-r--r--  2.0 unx     3540 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/utils.py
+-rw-r--r--  2.0 unx      963 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/version.py
+-rw-r--r--  2.0 unx      665 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/chunking/__init__.py
+-rw-r--r--  2.0 unx     4436 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/chunking/component.py
+-rw-r--r--  2.0 unx     4167 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/chunking/feature_store_grounding_pipeline.py
+-rw-r--r--  2.0 unx      669 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/__init__.py
+-rw-r--r--  2.0 unx     5452 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/component.py
+-rw-r--r--  2.0 unx      677 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/__init__.py
+-rw-r--r--  2.0 unx     7039 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/component.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/endpoint_batch_predict/__init__.py
+-rw-r--r--  2.0 unx    12001 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/endpoint_batch_predict/component.py
+-rw-r--r--  2.0 unx      671 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/error_analysis_annotation/__init__.py
+-rw-r--r--  2.0 unx     4092 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/error_analysis_annotation/component.py
+-rw-r--r--  2.0 unx      677 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/evaluated_annotation/__init__.py
+-rw-r--r--  2.0 unx     5842 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/evaluated_annotation/component.py
+-rw-r--r--  2.0 unx      674 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/feature_attribution/__init__.py
+-rw-r--r--  2.0 unx     7664 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/feature_attribution/feature_attribution_component.py
+-rw-r--r--  2.0 unx    14290 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/feature_attribution/feature_attribution_graph_component.py
+-rw-r--r--  2.0 unx      674 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/feature_extractor/__init__.py
+-rw-r--r--  2.0 unx     5539 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/feature_extractor/component.py
+-rw-r--r--  2.0 unx      684 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluated_annotation/__init__.py
+-rw-r--r--  2.0 unx     3053 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluated_annotation/component.py
+-rw-r--r--  2.0 unx      674 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluation/__init__.py
+-rw-r--r--  2.0 unx     8026 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/import_evaluation/component.py
+-rw-r--r--  2.0 unx      697 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_classification_postprocessor/__init__.py
+-rw-r--r--  2.0 unx    11227 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_classification_postprocessor/component.py
+-rw-r--r--  2.0 unx      680 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_embedding/__init__.py
+-rw-r--r--  2.0 unx    14207 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_embedding/evaluation_llm_embedding_pipeline.py
+-rw-r--r--  2.0 unx      691 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_embedding_retrieval/__init__.py
+-rw-r--r--  2.0 unx     7265 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_embedding_retrieval/component.py
+-rw-r--r--  2.0 unx      671 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/__init__.py
+-rw-r--r--  2.0 unx     7478 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/component.py
+-rw-r--r--  2.0 unx      684 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation_preprocessor/__init__.py
+-rw-r--r--  2.0 unx     7952 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation_preprocessor/component.py
+-rw-r--r--  2.0 unx      695 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_information_retrieval_preprocessor/__init__.py
+-rw-r--r--  2.0 unx     8142 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_information_retrieval_preprocessor/component.py
+-rw-r--r--  2.0 unx      701 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_retrieval_metrics/__init__.py
+-rw-r--r--  2.0 unx     6483 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_retrieval_metrics/component.py
+-rw-r--r--  2.0 unx      673 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_safety_bias/__init__.py
+-rw-r--r--  2.0 unx     4932 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_safety_bias/component.py
+-rw-r--r--  2.0 unx     7005 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/llm_safety_bias/evaluation_llm_safety_bias_pipeline.py
+-rw-r--r--  2.0 unx      672 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/model_inference/__init__.py
+-rw-r--r--  2.0 unx    16211 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/model_inference/component.py
+-rw-r--r--  2.0 unx      682 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/__init__.py
+-rw-r--r--  2.0 unx     5738 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/component.py
+-rw-r--r--  2.0 unx      664 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql/__init__.py
+-rw-r--r--  2.0 unx     8386 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql/evaluation_llm_text2sql_pipeline.py
+-rw-r--r--  2.0 unx      680 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_evaluation/__init__.py
+-rw-r--r--  2.0 unx     5646 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_evaluation/component.py
+-rw-r--r--  2.0 unx      691 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_preprocess/__init__.py
+-rw-r--r--  2.0 unx     5197 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_preprocess/component.py
+-rw-r--r--  2.0 unx      701 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_validate_and_process/__init__.py
+-rw-r--r--  2.0 unx     5636 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_validate_and_process/component.py
+-rw-r--r--  2.0 unx      651 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/__init__.py
+-rw-r--r--  2.0 unx      659 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/_implementation/__init__.py
+-rw-r--r--  2.0 unx      665 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/_implementation/llm/__init__.py
+-rw-r--r--  2.0 unx      954 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/_implementation/llm/templated_custom_job/__init__.py
+-rw-r--r--  2.0 unx     5267 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/_implementation/llm/templated_custom_job/launcher.py
+-rw-r--r--  2.0 unx     6728 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/_implementation/llm/templated_custom_job/remote_runner.py
+-rw-r--r--  2.0 unx      668 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/_implementation/model/__init__.py
+-rw-r--r--  2.0 unx      685 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/_implementation/model/get_model/__init__.py
+-rw-r--r--  2.0 unx     4975 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/_implementation/model/get_model/get_model.py
+-rw-r--r--  2.0 unx      678 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/_implementation/model_evaluation/__init__.py
+-rw-r--r--  2.0 unx    10196 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/_implementation/model_evaluation/import_evaluated_annotation.py
+-rw-r--r--  2.0 unx    17835 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/_implementation/model_evaluation/import_model_evaluation.py
+-rw-r--r--  2.0 unx      651 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/preview/__init__.py
+-rw-r--r--  2.0 unx      686 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/preview/custom_job/__init__.py
+-rw-r--r--  2.0 unx     1922 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/preview/custom_job/launcher.py
+-rw-r--r--  2.0 unx     3591 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/preview/custom_job/remote_runner.py
+-rw-r--r--  2.0 unx      655 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/preview/dataflow/__init__.py
+-rw-r--r--  2.0 unx      698 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/preview/dataflow/flex_template/__init__.py
+-rw-r--r--  2.0 unx     1914 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/preview/dataflow/flex_template/launcher.py
+-rw-r--r--  2.0 unx     9632 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/preview/dataflow/flex_template/remote_runner.py
+-rw-r--r--  2.0 unx      662 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/preview/gcp_launcher/__init__.py
+-rw-r--r--  2.0 unx     8447 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/preview/gcp_launcher/job_remote_runner.py
+-rw-r--r--  2.0 unx      638 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/utils/__init__.py
+-rw-r--r--  2.0 unx     3027 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/utils/artifact_utils.py
+-rw-r--r--  2.0 unx     1841 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/utils/execution_context.py
+-rw-r--r--  2.0 unx      651 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/__init__.py
+-rw-r--r--  2.0 unx      651 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/aiplatform/__init__.py
+-rw-r--r--  2.0 unx    10719 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/aiplatform/remote_runner.py
+-rw-r--r--  2.0 unx     5257 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/aiplatform/utils.py
+-rw-r--r--  2.0 unx      689 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/automl_training_job/__init__.py
+-rw-r--r--  2.0 unx      701 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/automl_training_job/image/__init__.py
+-rw-r--r--  2.0 unx     6257 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/automl_training_job/image/launcher.py
+-rw-r--r--  2.0 unx     9061 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/automl_training_job/image/remote_runner.py
+-rw-r--r--  2.0 unx      696 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/batch_prediction_job/__init__.py
+-rw-r--r--  2.0 unx     2364 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/batch_prediction_job/launcher.py
+-rw-r--r--  2.0 unx     9920 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/batch_prediction_job/remote_runner.py
+-rw-r--r--  2.0 unx      655 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/__init__.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/create_model/__init__.py
+-rw-r--r--  2.0 unx     2502 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/create_model/launcher.py
+-rw-r--r--  2.0 unx     3825 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/create_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/__init__.py
+-rw-r--r--  2.0 unx     3284 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/launcher.py
+-rw-r--r--  2.0 unx     5770 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/detect_anomalies_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/drop_model/__init__.py
+-rw-r--r--  2.0 unx     2627 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/drop_model/launcher.py
+-rw-r--r--  2.0 unx     3306 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/drop_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/__init__.py
+-rw-r--r--  2.0 unx     3307 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/launcher.py
+-rw-r--r--  2.0 unx     5574 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/evaluate_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/__init__.py
+-rw-r--r--  2.0 unx     2958 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/launcher.py
+-rw-r--r--  2.0 unx     3432 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/explain_forecast_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/__init__.py
+-rw-r--r--  2.0 unx     3639 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/launcher.py
+-rw-r--r--  2.0 unx     5843 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/explain_predict_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/export_model/__init__.py
+-rw-r--r--  2.0 unx     2775 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/export_model/launcher.py
+-rw-r--r--  2.0 unx     4706 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/export_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/__init__.py
+-rw-r--r--  2.0 unx     2656 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/launcher.py
+-rw-r--r--  2.0 unx     3562 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/feature_importance/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/__init__.py
+-rw-r--r--  2.0 unx     2932 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/launcher.py
+-rw-r--r--  2.0 unx     3384 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/forecast_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/global_explain/__init__.py
+-rw-r--r--  2.0 unx     2807 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/global_explain/launcher.py
+-rw-r--r--  2.0 unx     2676 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/global_explain/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/__init__.py
+-rw-r--r--  2.0 unx     2653 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/launcher.py
+-rw-r--r--  2.0 unx     4046 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_advanced_weights/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/__init__.py
+-rw-r--r--  2.0 unx     2655 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/launcher.py
+-rw-r--r--  2.0 unx     3441 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_coefficients/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/__init__.py
+-rw-r--r--  2.0 unx     2822 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/launcher.py
+-rw-r--r--  2.0 unx     4662 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_arima_evaluate/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/__init__.py
+-rw-r--r--  2.0 unx     2780 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/launcher.py
+-rw-r--r--  2.0 unx     4637 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_centroids/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/__init__.py
+-rw-r--r--  2.0 unx     3319 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/launcher.py
+-rw-r--r--  2.0 unx     4731 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_confusion_matrix/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/__init__.py
+-rw-r--r--  2.0 unx     2641 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/launcher.py
+-rw-r--r--  2.0 unx     4018 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_feature_info/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/__init__.py
+-rw-r--r--  2.0 unx     2676 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/launcher.py
+-rw-r--r--  2.0 unx     3194 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_component_info/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/__init__.py
+-rw-r--r--  2.0 unx     2665 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/launcher.py
+-rw-r--r--  2.0 unx     3166 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_principal_components/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/__init__.py
+-rw-r--r--  2.0 unx     3086 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/launcher.py
+-rw-r--r--  2.0 unx     4050 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_recommend/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/__init__.py
+-rw-r--r--  2.0 unx     3115 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/launcher.py
+-rw-r--r--  2.0 unx     4337 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_reconstruction_loss/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/__init__.py
+-rw-r--r--  2.0 unx     3302 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/launcher.py
+-rw-r--r--  2.0 unx     4613 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_roc_curve/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/__init__.py
+-rw-r--r--  2.0 unx     2644 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/launcher.py
+-rw-r--r--  2.0 unx     4032 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_training_info/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/__init__.py
+-rw-r--r--  2.0 unx     2635 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/launcher.py
+-rw-r--r--  2.0 unx     4211 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_trial_info/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/__init__.py
+-rw-r--r--  2.0 unx     2627 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/launcher.py
+-rw-r--r--  2.0 unx     4001 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/ml_weights/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/predict_model/__init__.py
+-rw-r--r--  2.0 unx     3302 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/predict_model/launcher.py
+-rw-r--r--  2.0 unx     4578 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/predict_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/query_job/__init__.py
+-rw-r--r--  2.0 unx     2473 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/query_job/launcher.py
+-rw-r--r--  2.0 unx     2539 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/query_job/remote_runner.py
+-rw-r--r--  2.0 unx      673 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/utils/__init__.py
+-rw-r--r--  2.0 unx    12236 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/bigquery/utils/bigquery_util.py
+-rw-r--r--  2.0 unx      686 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/custom_job/__init__.py
+-rw-r--r--  2.0 unx     1917 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/custom_job/launcher.py
+-rw-r--r--  2.0 unx     3586 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/custom_job/remote_runner.py
+-rw-r--r--  2.0 unx      662 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataflow/__init__.py
+-rw-r--r--  2.0 unx     3303 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataflow/dataflow_launcher.py
+-rw-r--r--  2.0 unx     7415 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataflow/dataflow_python_job_remote_runner.py
+-rw-r--r--  2.0 unx      699 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/__init__.py
+-rw-r--r--  2.0 unx      705 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/create_pyspark_batch/__init__.py
+-rw-r--r--  2.0 unx     2196 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/create_pyspark_batch/launcher.py
+-rw-r--r--  2.0 unx      804 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/create_pyspark_batch/remote_runner.py
+-rw-r--r--  2.0 unx      703 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/create_spark_batch/__init__.py
+-rw-r--r--  2.0 unx     2188 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/create_spark_batch/launcher.py
+-rw-r--r--  2.0 unx      800 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/create_spark_batch/remote_runner.py
+-rw-r--r--  2.0 unx      705 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/create_spark_r_batch/__init__.py
+-rw-r--r--  2.0 unx     2195 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/create_spark_r_batch/launcher.py
+-rw-r--r--  2.0 unx      804 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/create_spark_r_batch/remote_runner.py
+-rw-r--r--  2.0 unx      707 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/__init__.py
+-rw-r--r--  2.0 unx     2203 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/launcher.py
+-rw-r--r--  2.0 unx      808 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/create_spark_sql_batch/remote_runner.py
+-rw-r--r--  2.0 unx      663 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/utils/__init__.py
+-rw-r--r--  2.0 unx    12872 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/dataproc/utils/dataproc_util.py
+-rw-r--r--  2.0 unx      655 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/__init__.py
+-rw-r--r--  2.0 unx      691 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/__init__.py
+-rw-r--r--  2.0 unx     2288 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/launcher.py
+-rw-r--r--  2.0 unx     2363 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/create_endpoint/remote_runner.py
+-rw-r--r--  2.0 unx      691 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/__init__.py
+-rw-r--r--  2.0 unx     1938 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/launcher.py
+-rw-r--r--  2.0 unx     2308 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/delete_endpoint/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/__init__.py
+-rw-r--r--  2.0 unx     1926 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/launcher.py
+-rw-r--r--  2.0 unx     2375 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/deploy_model/remote_runner.py
+-rw-r--r--  2.0 unx      690 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/__init__.py
+-rw-r--r--  2.0 unx     1934 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/launcher.py
+-rw-r--r--  2.0 unx     4711 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/endpoint/undeploy_model/remote_runner.py
+-rw-r--r--  2.0 unx      662 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/gcp_launcher/__init__.py
+-rw-r--r--  2.0 unx     8350 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/gcp_launcher/job_remote_runner.py
+-rw-r--r--  2.0 unx     4945 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/gcp_launcher/lro_remote_runner.py
+-rw-r--r--  2.0 unx    10456 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/gcp_launcher/pipeline_remote_runner.py
+-rw-r--r--  2.0 unx      660 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/__init__.py
+-rw-r--r--  2.0 unx      876 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/error_util.py
+-rw-r--r--  2.0 unx     2193 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/gcp_labels_util.py
+-rw-r--r--  2.0 unx     2951 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/json_util.py
+-rw-r--r--  2.0 unx     1882 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/gcp_launcher/utils/parser_util.py
+-rw-r--r--  2.0 unx      701 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/__init__.py
+-rw-r--r--  2.0 unx     2240 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/launcher.py
+-rw-r--r--  2.0 unx     4402 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/hyperparameter_tuning_job/remote_runner.py
+-rw-r--r--  2.0 unx      696 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/infra_validation_job/__init__.py
+-rw-r--r--  2.0 unx     2280 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/infra_validation_job/launcher.py
+-rw-r--r--  2.0 unx     4068 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/infra_validation_job/remote_runner.py
+-rw-r--r--  2.0 unx      652 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/__init__.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/delete_model/__init__.py
+-rw-r--r--  2.0 unx     1923 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/delete_model/launcher.py
+-rw-r--r--  2.0 unx     2464 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/delete_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/export_model/__init__.py
+-rw-r--r--  2.0 unx     2201 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/export_model/launcher.py
+-rw-r--r--  2.0 unx     2479 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/export_model/remote_runner.py
+-rw-r--r--  2.0 unx      685 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/get_model/__init__.py
+-rw-r--r--  2.0 unx     1909 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/get_model/launcher.py
+-rw-r--r--  2.0 unx     2062 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/get_model/remote_runner.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/upload_model/__init__.py
+-rw-r--r--  2.0 unx     2331 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/upload_model/launcher.py
+-rw-r--r--  2.0 unx     4480 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/model/upload_model/remote_runner.py
+-rw-r--r--  2.0 unx      662 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/vertex_notification_email/__init__.py
+-rw-r--r--  2.0 unx     1074 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/vertex_notification_email/executor.py
+-rw-r--r--  2.0 unx      694 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/wait_gcp_resources/__init__.py
+-rw-r--r--  2.0 unx     1928 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/wait_gcp_resources/launcher.py
+-rw-r--r--  2.0 unx     5968 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/container/v1/wait_gcp_resources/remote_runner.py
+-rw-r--r--  2.0 unx      606 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/__init__.py
+-rw-r--r--  2.0 unx      606 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/__init__.py
+-rw-r--r--  2.0 unx     3532 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/forecasting/__init__.py
+-rw-r--r--  2.0 unx     5655 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/forecasting/forecasting_ensemble.py
+-rw-r--r--  2.0 unx     6564 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_1_tuner.py
+-rw-r--r--  2.0 unx     6376 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_2_tuner.py
+-rw-r--r--  2.0 unx   361458 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/forecasting/learn_to_learn_forecasting_pipeline.yaml
+-rw-r--r--  2.0 unx   359637 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/forecasting/sequence_to_sequence_forecasting_pipeline.yaml
+-rw-r--r--  2.0 unx   358743 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/forecasting/temporal_fusion_transformer_forecasting_pipeline.yaml
+-rw-r--r--  2.0 unx   361533 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/forecasting/time_series_dense_encoder_forecasting_pipeline.yaml
+-rw-r--r--  2.0 unx    53462 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/forecasting/utils.py
+-rw-r--r--  2.0 unx     4247 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/__init__.py
+-rw-r--r--  2.0 unx     3643 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/auto_feature_engineering.py
+-rw-r--r--  2.0 unx   515159 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_feature_selection_pipeline.yaml
+-rw-r--r--  2.0 unx   582988 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_v2_pipeline.yaml
+-rw-r--r--  2.0 unx     9629 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/distillation_stage_feature_transform_engine.py
+-rw-r--r--  2.0 unx     7546 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/feature_selection.py
+-rw-r--r--  2.0 unx    75354 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/feature_selection_pipeline.yaml
+-rw-r--r--  2.0 unx    47372 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/feature_transform_engine.py
+-rw-r--r--  2.0 unx     9980 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job.py
+-rw-r--r--  2.0 unx   242808 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job_pipeline.yaml
+-rw-r--r--  2.0 unx    12748 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer.py
+-rw-r--r--  2.0 unx   201551 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer_pipeline.yaml
+-rw-r--r--  2.0 unx   168283 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/utils.py
+-rw-r--r--  2.0 unx     9975 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job.py
+-rw-r--r--  2.0 unx   194816 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job_pipeline.yaml
+-rw-r--r--  2.0 unx    11922 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer.py
+-rw-r--r--  2.0 unx   190208 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer_pipeline.yaml
+-rw-r--r--  2.0 unx     4787 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/xgboost_hyperparameter_tuning_job.py
+-rw-r--r--  2.0 unx   223628 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/xgboost_hyperparameter_tuning_job_pipeline.yaml
+-rw-r--r--  2.0 unx     2333 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/xgboost_trainer.py
+-rw-r--r--  2.0 unx   207747 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/xgboost_trainer_pipeline.yaml
+-rw-r--r--  2.0 unx     3292 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_large_search_space.json
+-rw-r--r--  2.0 unx     3261 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_medium_search_space.json
+-rw-r--r--  2.0 unx     2975 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_small_search_space.json
+-rw-r--r--  2.0 unx     3287 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_large_search_space.json
+-rw-r--r--  2.0 unx     3266 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_medium_search_space.json
+-rw-r--r--  2.0 unx     2974 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_small_search_space.json
+-rw-r--r--  2.0 unx     3269 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_large_search_space.json
+-rw-r--r--  2.0 unx     3254 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_medium_search_space.json
+-rw-r--r--  2.0 unx     2966 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_small_search_space.json
+-rw-r--r--  2.0 unx     2647 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/configs/wide_and_deep_params.json
+-rw-r--r--  2.0 unx     7748 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/tabular/configs/xgboost_params.json
+-rw-r--r--  2.0 unx      758 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/vision/__init__.py
+-rw-r--r--  2.0 unx     6310 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/vision/data_converter.py
+-rw-r--r--  2.0 unx     6001 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/automl/vision/json_utils.py
+-rw-r--r--  2.0 unx     1295 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/custom_job/__init__.py
+-rw-r--r--  2.0 unx     7098 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/custom_job/component.py
+-rw-r--r--  2.0 unx    16062 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/custom_job/utils.py
+-rw-r--r--  2.0 unx      773 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/dataflow/__init__.py
+-rw-r--r--  2.0 unx      886 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/llm/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/llm/infer/__init__.py
+-rw-r--r--  2.0 unx     7384 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/llm/infer/component.py
+-rw-r--r--  2.0 unx      606 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/llm/rlhf/__init__.py
+-rw-r--r--  2.0 unx    10629 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/llm/rlhf/component.py
+-rw-r--r--  2.0 unx     1949 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/model_evaluation/__init__.py
+-rw-r--r--  2.0 unx     5791 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/model_evaluation/data_bias_component.py
+-rw-r--r--  2.0 unx    11262 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/model_evaluation/evaluation_llm_classification_pipeline.py
+-rw-r--r--  2.0 unx     9600 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/model_evaluation/evaluation_llm_text_generation_pipeline.py
+-rw-r--r--  2.0 unx     7450 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_component.py
+-rw-r--r--  2.0 unx    13624 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_graph_component.py
+-rw-r--r--  2.0 unx     6674 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/model_evaluation/model_bias_component.py
+-rw-r--r--  2.0 unx     7539 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/model_evaluation/utils.py
+-rw-r--r--  2.0 unx      838 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/model_evaluation/model_based_llm_evaluation/__init__.py
+-rw-r--r--  2.0 unx      606 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/model_evaluation/model_based_llm_evaluation/autosxs/__init__.py
+-rw-r--r--  2.0 unx    11205 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/preview/model_evaluation/model_based_llm_evaluation/autosxs/autosxs_pipeline.py
+-rw-r--r--  2.0 unx      661 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/proto/__init__.py
+-rw-r--r--  2.0 unx     2134 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/proto/gcp_resources_pb2.py
+-rw-r--r--  2.0 unx      606 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/types/__init__.py
+-rw-r--r--  2.0 unx    23725 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/types/artifact_types.py
+-rw-r--r--  2.0 unx      765 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/__init__.py
+-rw-r--r--  2.0 unx      631 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/__init__.py
+-rw-r--r--  2.0 unx     1488 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/forecasting/__init__.py
+-rw-r--r--  2.0 unx    52484 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_predict_pipeline.yaml
+-rw-r--r--  2.0 unx   247001 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_train_pipeline.yaml
+-rw-r--r--  2.0 unx    95718 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/forecasting/prophet_predict_pipeline.yaml
+-rw-r--r--  2.0 unx     8363 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer.py
+-rw-r--r--  2.0 unx   144072 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer_pipeline.yaml
+-rw-r--r--  2.0 unx    14855 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/forecasting/utils.py
+-rw-r--r--  2.0 unx     2480 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/__init__.py
+-rw-r--r--  2.0 unx   502694 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/automl_tabular_pipeline.yaml
+-rw-r--r--  2.0 unx     6708 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/cv_trainer.py
+-rw-r--r--  2.0 unx     6935 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/ensemble.py
+-rw-r--r--  2.0 unx     3108 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/finalizer.py
+-rw-r--r--  2.0 unx     1333 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/infra_validator.py
+-rw-r--r--  2.0 unx     5571 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/split_materialized_data.py
+-rw-r--r--  2.0 unx     7761 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/stage_1_tuner.py
+-rw-r--r--  2.0 unx    13367 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/stats_and_example_gen.py
+-rw-r--r--  2.0 unx    12202 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/training_configurator_and_validator.py
+-rw-r--r--  2.0 unx     8241 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/transform.py
+-rw-r--r--  2.0 unx    74625 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/utils.py
+-rw-r--r--  2.0 unx   321170 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/tabular/deprecated/default_pipeline.json
+-rw-r--r--  2.0 unx     1803 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/training_job/__init__.py
+-rw-r--r--  2.0 unx      655 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/training_job/automl_forecasting_training_job/__init__.py
+-rw-r--r--  2.0 unx    27062 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/training_job/automl_forecasting_training_job/component.py
+-rw-r--r--  2.0 unx      649 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/training_job/automl_image_training_job/__init__.py
+-rw-r--r--  2.0 unx    18382 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/training_job/automl_image_training_job/component.py
+-rw-r--r--  2.0 unx      651 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/training_job/automl_tabular_training_job/__init__.py
+-rw-r--r--  2.0 unx    20801 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/training_job/automl_tabular_training_job/component.py
+-rw-r--r--  2.0 unx      648 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/training_job/automl_text_training_job/__init__.py
+-rw-r--r--  2.0 unx     9406 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/training_job/automl_text_training_job/component.py
+-rw-r--r--  2.0 unx      649 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/training_job/automl_video_training_job/__init__.py
+-rw-r--r--  2.0 unx     9237 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/automl/training_job/automl_video_training_job/component.py
+-rw-r--r--  2.0 unx      992 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/batch_predict_job/__init__.py
+-rw-r--r--  2.0 unx    20638 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/batch_predict_job/component.py
+-rw-r--r--  2.0 unx     5500 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/__init__.py
+-rw-r--r--  2.0 unx      670 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/create_model/__init__.py
+-rw-r--r--  2.0 unx     4164 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/create_model/component.py
+-rw-r--r--  2.0 unx      680 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/detect_anomalies_model/__init__.py
+-rw-r--r--  2.0 unx     6965 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/detect_anomalies_model/component.py
+-rw-r--r--  2.0 unx      668 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/drop_model/__init__.py
+-rw-r--r--  2.0 unx     4055 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/drop_model/component.py
+-rw-r--r--  2.0 unx      672 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/evaluate_model/__init__.py
+-rw-r--r--  2.0 unx     6320 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/evaluate_model/component.py
+-rw-r--r--  2.0 unx      680 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/explain_forecast_model/__init__.py
+-rw-r--r--  2.0 unx     5890 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/explain_forecast_model/component.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/explain_predict_model/__init__.py
+-rw-r--r--  2.0 unx     7174 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/explain_predict_model/component.py
+-rw-r--r--  2.0 unx      670 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/export_model/__init__.py
+-rw-r--r--  2.0 unx     4044 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/export_model/component.py
+-rw-r--r--  2.0 unx      676 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/feature_importance/__init__.py
+-rw-r--r--  2.0 unx     5158 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/feature_importance/component.py
+-rw-r--r--  2.0 unx      672 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/forecast_model/__init__.py
+-rw-r--r--  2.0 unx     5764 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/forecast_model/component.py
+-rw-r--r--  2.0 unx      672 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/global_explain/__init__.py
+-rw-r--r--  2.0 unx     4372 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/global_explain/component.py
+-rw-r--r--  2.0 unx      677 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_advanced_weights/__init__.py
+-rw-r--r--  2.0 unx     4578 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_advanced_weights/component.py
+-rw-r--r--  2.0 unx      679 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_arima_coefficients/__init__.py
+-rw-r--r--  2.0 unx     4772 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_arima_coefficients/component.py
+-rw-r--r--  2.0 unx      675 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_arima_evaluate/__init__.py
+-rw-r--r--  2.0 unx     5666 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_arima_evaluate/component.py
+-rw-r--r--  2.0 unx      670 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_centroids/__init__.py
+-rw-r--r--  2.0 unx     5477 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_centroids/component.py
+-rw-r--r--  2.0 unx      677 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_confusion_matrix/__init__.py
+-rw-r--r--  2.0 unx     5514 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_confusion_matrix/component.py
+-rw-r--r--  2.0 unx      673 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_feature_info/__init__.py
+-rw-r--r--  2.0 unx     4539 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_feature_info/component.py
+-rw-r--r--  2.0 unx      685 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_principal_component_info/__init__.py
+-rw-r--r--  2.0 unx     5353 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_principal_component_info/component.py
+-rw-r--r--  2.0 unx      681 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_principal_components/__init__.py
+-rw-r--r--  2.0 unx     5318 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_principal_components/component.py
+-rw-r--r--  2.0 unx      670 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_recommend/__init__.py
+-rw-r--r--  2.0 unx     5849 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_recommend/component.py
+-rw-r--r--  2.0 unx      680 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_reconstruction_loss/__init__.py
+-rw-r--r--  2.0 unx     5942 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_reconstruction_loss/component.py
+-rw-r--r--  2.0 unx      670 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_roc_curve/__init__.py
+-rw-r--r--  2.0 unx     5398 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_roc_curve/component.py
+-rw-r--r--  2.0 unx      674 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_training_info/__init__.py
+-rw-r--r--  2.0 unx     4571 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_training_info/component.py
+-rw-r--r--  2.0 unx      671 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_trial_info/__init__.py
+-rw-r--r--  2.0 unx     5083 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_trial_info/component.py
+-rw-r--r--  2.0 unx      668 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_weights/__init__.py
+-rw-r--r--  2.0 unx     4502 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/ml_weights/component.py
+-rw-r--r--  2.0 unx      671 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/predict_model/__init__.py
+-rw-r--r--  2.0 unx     6279 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/predict_model/component.py
+-rw-r--r--  2.0 unx      667 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/query_job/__init__.py
+-rw-r--r--  2.0 unx     5187 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/bigquery/query_job/component.py
+-rw-r--r--  2.0 unx     1278 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/custom_job/__init__.py
+-rw-r--r--  2.0 unx     6070 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/custom_job/component.py
+-rw-r--r--  2.0 unx    15509 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/custom_job/utils.py
+-rw-r--r--  2.0 unx     1063 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataflow/__init__.py
+-rw-r--r--  2.0 unx      669 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataflow/flex_template/__init__.py
+-rw-r--r--  2.0 unx    11701 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataflow/flex_template/component.py
+-rw-r--r--  2.0 unx      664 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataflow/python_job/__init__.py
+-rw-r--r--  2.0 unx     2692 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataflow/python_job/component.py
+-rw-r--r--  2.0 unx     1445 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataproc/__init__.py
+-rw-r--r--  2.0 unx      670 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataproc/create_pyspark_batch/__init__.py
+-rw-r--r--  2.0 unx     6776 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataproc/create_pyspark_batch/component.py
+-rw-r--r--  2.0 unx      668 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataproc/create_spark_batch/__init__.py
+-rw-r--r--  2.0 unx     6735 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataproc/create_spark_batch/component.py
+-rw-r--r--  2.0 unx      669 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataproc/create_spark_r_batch/__init__.py
+-rw-r--r--  2.0 unx     6307 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataproc/create_spark_r_batch/component.py
+-rw-r--r--  2.0 unx      673 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataproc/create_spark_sql_batch/__init__.py
+-rw-r--r--  2.0 unx     6111 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataproc/create_spark_sql_batch/component.py
+-rw-r--r--  2.0 unx     3108 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/__init__.py
+-rw-r--r--  2.0 unx      644 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/create_image_dataset/__init__.py
+-rw-r--r--  2.0 unx     5608 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/create_image_dataset/component.py
+-rw-r--r--  2.0 unx      646 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/create_tabular_dataset/__init__.py
+-rw-r--r--  2.0 unx     4368 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/create_tabular_dataset/component.py
+-rw-r--r--  2.0 unx      643 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/create_text_dataset/__init__.py
+-rw-r--r--  2.0 unx     5496 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/create_text_dataset/component.py
+-rw-r--r--  2.0 unx      642 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/create_time_series_dataset/__init__.py
+-rw-r--r--  2.0 unx     4381 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/create_time_series_dataset/component.py
+-rw-r--r--  2.0 unx      644 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/create_video_dataset/__init__.py
+-rw-r--r--  2.0 unx     5478 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/create_video_dataset/component.py
+-rw-r--r--  2.0 unx      644 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/export_image_dataset/__init__.py
+-rw-r--r--  2.0 unx     2982 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/export_image_dataset/component.py
+-rw-r--r--  2.0 unx      646 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/export_tabular_dataset/__init__.py
+-rw-r--r--  2.0 unx     2986 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/export_tabular_dataset/component.py
+-rw-r--r--  2.0 unx      643 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/export_text_dataset/__init__.py
+-rw-r--r--  2.0 unx     2980 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/export_text_dataset/component.py
+-rw-r--r--  2.0 unx      650 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/export_time_series_dataset/__init__.py
+-rw-r--r--  2.0 unx     2991 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/export_time_series_dataset/component.py
+-rw-r--r--  2.0 unx      644 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/export_video_dataset/__init__.py
+-rw-r--r--  2.0 unx     2982 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/export_video_dataset/component.py
+-rw-r--r--  2.0 unx      642 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/get_vertex_dataset/__init__.py
+-rw-r--r--  2.0 unx     3878 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/get_vertex_dataset/component.py
+-rw-r--r--  2.0 unx      644 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/import_image_dataset/__init__.py
+-rw-r--r--  2.0 unx     4114 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/import_image_dataset/component.py
+-rw-r--r--  2.0 unx      643 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/import_text_dataset/__init__.py
+-rw-r--r--  2.0 unx     4108 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/import_text_dataset/component.py
+-rw-r--r--  2.0 unx      644 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/import_video_dataset/__init__.py
+-rw-r--r--  2.0 unx     4114 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/dataset/import_video_dataset/component.py
+-rw-r--r--  2.0 unx     1381 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/endpoint/__init__.py
+-rw-r--r--  2.0 unx      664 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/endpoint/create_endpoint/__init__.py
+-rw-r--r--  2.0 unx     4751 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/endpoint/create_endpoint/component.py
+-rw-r--r--  2.0 unx      639 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/endpoint/delete_endpoint/__init__.py
+-rw-r--r--  2.0 unx     2311 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/endpoint/delete_endpoint/component.py
+-rw-r--r--  2.0 unx      661 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/endpoint/deploy_model/__init__.py
+-rw-r--r--  2.0 unx    10066 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/endpoint/deploy_model/component.py
+-rw-r--r--  2.0 unx      638 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/endpoint/undeploy_model/__init__.py
+-rw-r--r--  2.0 unx     3284 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/endpoint/undeploy_model/component.py
+-rw-r--r--  2.0 unx     1293 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/forecasting/__init__.py
+-rw-r--r--  2.0 unx      658 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/forecasting/prepare_data_for_train/__init__.py
+-rw-r--r--  2.0 unx     7404 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/forecasting/prepare_data_for_train/component.py
+-rw-r--r--  2.0 unx      646 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/forecasting/preprocess/__init__.py
+-rw-r--r--  2.0 unx     2241 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/forecasting/preprocess/component.py
+-rw-r--r--  2.0 unx      644 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/forecasting/validate/__init__.py
+-rw-r--r--  2.0 unx     1891 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/forecasting/validate/component.py
+-rw-r--r--  2.0 unx     1300 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/__init__.py
+-rw-r--r--  2.0 unx     9479 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/component.py
+-rw-r--r--  2.0 unx     3118 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/utils.py
+-rw-r--r--  2.0 unx     1264 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model/__init__.py
+-rw-r--r--  2.0 unx      638 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model/delete_model/__init__.py
+-rw-r--r--  2.0 unx     2490 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model/delete_model/component.py
+-rw-r--r--  2.0 unx      661 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model/export_model/__init__.py
+-rw-r--r--  2.0 unx     4908 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model/export_model/component.py
+-rw-r--r--  2.0 unx      662 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model/get_model/__init__.py
+-rw-r--r--  2.0 unx     2117 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model/get_model/component.py
+-rw-r--r--  2.0 unx      661 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model/upload_model/__init__.py
+-rw-r--r--  2.0 unx     7280 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model/upload_model/component.py
+-rw-r--r--  2.0 unx     2358 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model_evaluation/__init__.py
+-rw-r--r--  2.0 unx    12122 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model_evaluation/classification_component.py
+-rw-r--r--  2.0 unx    20122 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model_evaluation/error_analysis_pipeline.py
+-rw-r--r--  2.0 unx    12142 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model_evaluation/evaluated_annotation_pipeline.py
+-rw-r--r--  2.0 unx    46187 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_feature_attribution_pipeline.py
+-rw-r--r--  2.0 unx    37108 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_pipeline.py
+-rw-r--r--  2.0 unx    42404 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_unstructure_data_pipeline.py
+-rw-r--r--  2.0 unx    51173 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model_evaluation/evaluation_feature_attribution_pipeline.py
+-rw-r--r--  2.0 unx    10017 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model_evaluation/forecasting_component.py
+-rw-r--r--  2.0 unx     9117 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/model_evaluation/regression_component.py
+-rw-r--r--  2.0 unx      863 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/vertex_notification_email/__init__.py
+-rw-r--r--  2.0 unx     2163 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/vertex_notification_email/component.py
+-rw-r--r--  2.0 unx      878 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/wait_gcp_resources/__init__.py
+-rw-r--r--  2.0 unx     2468 b- defN 24-Jan-25 20:38 google_cloud_pipeline_components/v1/wait_gcp_resources/component.py
+-rw-r--r--  2.0 unx    13118 b- defN 24-Jan-25 20:53 google_cloud_pipeline_components-2.9.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     5843 b- defN 24-Jan-25 20:53 google_cloud_pipeline_components-2.9.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Jan-25 20:53 google_cloud_pipeline_components-2.9.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       33 b- defN 24-Jan-25 20:53 google_cloud_pipeline_components-2.9.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    71245 b- defN 24-Jan-25 20:53 google_cloud_pipeline_components-2.9.0.dist-info/RECORD
+520 files, 7670754 bytes uncompressed, 1282485 bytes compressed:  83.3%
```

## zipnote {}

```diff
@@ -102,14 +102,17 @@
 
 Filename: google_cloud_pipeline_components/_implementation/model_evaluation/chunking/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/_implementation/model_evaluation/chunking/component.py
 Comment: 
 
+Filename: google_cloud_pipeline_components/_implementation/model_evaluation/chunking/feature_store_grounding_pipeline.py
+Comment: 
+
 Filename: google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/component.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/__init__.py
@@ -765,14 +768,23 @@
 
 Filename: google_cloud_pipeline_components/container/v1/model/export_model/launcher.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/container/v1/model/export_model/remote_runner.py
 Comment: 
 
+Filename: google_cloud_pipeline_components/container/v1/model/get_model/__init__.py
+Comment: 
+
+Filename: google_cloud_pipeline_components/container/v1/model/get_model/launcher.py
+Comment: 
+
+Filename: google_cloud_pipeline_components/container/v1/model/get_model/remote_runner.py
+Comment: 
+
 Filename: google_cloud_pipeline_components/container/v1/model/upload_model/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/container/v1/model/upload_model/launcher.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/container/v1/model/upload_model/remote_runner.py
@@ -1473,14 +1485,20 @@
 
 Filename: google_cloud_pipeline_components/v1/model/export_model/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/v1/model/export_model/component.py
 Comment: 
 
+Filename: google_cloud_pipeline_components/v1/model/get_model/__init__.py
+Comment: 
+
+Filename: google_cloud_pipeline_components/v1/model/get_model/component.py
+Comment: 
+
 Filename: google_cloud_pipeline_components/v1/model/upload_model/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/v1/model/upload_model/component.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/v1/model_evaluation/__init__.py
@@ -1521,23 +1539,23 @@
 
 Filename: google_cloud_pipeline_components/v1/wait_gcp_resources/__init__.py
 Comment: 
 
 Filename: google_cloud_pipeline_components/v1/wait_gcp_resources/component.py
 Comment: 
 
-Filename: google_cloud_pipeline_components-2.8.0.dist-info/LICENSE
+Filename: google_cloud_pipeline_components-2.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: google_cloud_pipeline_components-2.8.0.dist-info/METADATA
+Filename: google_cloud_pipeline_components-2.9.0.dist-info/METADATA
 Comment: 
 
-Filename: google_cloud_pipeline_components-2.8.0.dist-info/WHEEL
+Filename: google_cloud_pipeline_components-2.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: google_cloud_pipeline_components-2.8.0.dist-info/top_level.txt
+Filename: google_cloud_pipeline_components-2.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: google_cloud_pipeline_components-2.8.0.dist-info/RECORD
+Filename: google_cloud_pipeline_components-2.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## google_cloud_pipeline_components/_placeholders.py

```diff
@@ -9,10 +9,21 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Placeholders for use in component authoring."""
 
-# prefer not using placeholder suffix like KFP does for reduce verbosity
+# prefer not using PIPELINE_TASK_ prefix like KFP does for reduced verbosity
 PROJECT_ID_PLACEHOLDER = "{{$.pipeline_google_cloud_project_id}}"
 LOCATION_PLACEHOLDER = "{{$.pipeline_google_cloud_location}}"
+
+
+# omit placeholder type annotation to avoid dependency on KFP SDK internals
+# placeholder is type kfp.dsl.placeholders.Placeholder
+def json_escape(placeholder, level: int) -> str:
+  if level not in {0, 1}:
+    raise ValueError(f"Invalid level: {level}")
+  # Placeholder implements __str__
+  s = str(placeholder)
+
+  return s.replace("}}", f".json_escape[{level}]}}}}")
```

## google_cloud_pipeline_components/version.py

```diff
@@ -9,8 +9,8 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Google Cloud Pipeline Components version."""
 
-__version__ = "2.8.0"
+__version__ = "2.9.0"
```

## google_cloud_pipeline_components/_implementation/llm/autosxs_arbiter.py

```diff
@@ -37,14 +37,15 @@
 @dsl.container_component
 def autosxs_arbiter(
     inference_output_uri: str,
     id_columns: List[str],
     task: str,
     judgments: dsl.Output[dsl.Dataset],  # pylint: disable=unused-argument # pytype: disable=unsupported-operands
     judgments_uri: dsl.OutputPath(str),  # pytype: disable=invalid-annotation
+    error_messages: dsl.Output[dsl.Dataset],  # pylint: disable=unused-argument # pytype: disable=unsupported-operands
     gcp_resources: dsl.OutputPath(str),
     metadata: dsl.OutputPath(str),
     human_preference_column: str = '',
     judgments_format: str = 'jsonl',
     bigquery_destination_prefix: str = '',
     experimental_args: Dict[str, Any] = {},
 ) -> dsl.ContainerSpec:  # pylint: disable=g-doc-args
@@ -52,25 +53,26 @@
 
   Args:
     inference_output_uri: Directory of model A's inference output.
     id_columns: The columns which distinguish unique evaluation examples.
     human_preference_column: Human preference column included in our inference
       output.
     task: Evaluation task in the form {task}@{version}. task can be one of
-      "summarization", "question_answer". Version is an integer with 3 digits or
-      "latest". Ex: summarization@001 or question_answer@latest.
+      "summarization", "question_answering". Version is an integer with 3 digits
+      or "latest". Ex: summarization@001 or question_answering@latest.
     judgments_format: The format to write judgments to. Can be either 'json' or
       'bigquery'.
     bigquery_destination_prefix: BigQuery table to write judgments to if the
       specified format is 'bigquery'.
     experimental_args: Experimentally released arguments. Subject to change.
 
   Returns:
     judgments: Individual judgments used to calculate the win rates.
     judgments_uri: URI of the Judgments Artifact.
+    error_messages: Error messages of failed samples of each evaluation example.
     gcp_resources: Tracker for GCP resources created by this component.
     metadata: Computed runtime metrics metadata from this component.
   """
   return gcpc_utils.build_serverless_customjob_container_spec(
       project=_placeholders.PROJECT_ID_PLACEHOLDER,
       # Hardcode location to us-central1 for text-bison availability.
       location='us-central1',
```

## google_cloud_pipeline_components/_implementation/llm/deployment_graph.py

```diff
@@ -57,18 +57,22 @@
       artifact_class=kfp.dsl.Artifact,
   ).set_display_name('Import Tuned Adapter')
 
   regional_endpoint = function_based.resolve_regional_endpoint(
       upload_location=upload_location
   ).set_display_name('Resolve Regional Endpoint')
 
-  display_name = function_based.resolve_model_display_name(
-      large_model_reference=large_model_reference,
-      model_display_name=model_display_name,
-  ).set_display_name('Resolve Model Display Name')
+  display_name = (
+      function_based.resolve_model_display_name(
+          large_model_reference=large_model_reference,
+          model_display_name=model_display_name,
+      )
+      .set_caching_options(False)
+      .set_display_name('Resolve Model Display Name')
+  )
 
   reference_model_metadata = function_based.resolve_reference_model_metadata(
       large_model_reference=large_model_reference,
   ).set_display_name('Resolve Model Metadata')
 
   upload_model = function_based.resolve_upload_model(
       large_model_reference=reference_model_metadata.outputs[
@@ -77,15 +81,15 @@
   ).set_display_name('Resolve Upload Model')
   upload_task = upload_llm_model.upload_llm_model(
       project=_placeholders.PROJECT_ID_PLACEHOLDER,
       location=upload_location,
       regional_endpoint=regional_endpoint.output,
       artifact_uri=adapter_artifact.output,
       model_display_name=display_name.output,
-      model_reference_name='text-bison@001',
+      model_reference_name=large_model_reference,
       upload_model=upload_model.output,
       tune_type='rlhf',
   ).set_display_name('Upload Model')
   deploy_model = function_based.resolve_deploy_model(
       deploy_model=deploy_model,
       large_model_reference=reference_model_metadata.outputs[
           'large_model_reference'
```

## google_cloud_pipeline_components/_implementation/llm/env.py

```diff
@@ -10,31 +10,47 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """A collection of constants shared across components and pipelines."""
 import os
 
+_DEFAULT_AUTOSXS_IMAGE_TAG = '20240116_0507_RC00'
 
 def get_private_image_tag() -> str:
-  return os.getenv('PRIVATE_IMAGE_TAG', '20231213_0507_RC00')
+  return os.getenv('PRIVATE_IMAGE_TAG') or '20240124_0507_RC00'
+
+
+def get_autosxs_image_tag() -> str:
+  return os.getenv('PRIVATE_IMAGE_TAG') or _DEFAULT_AUTOSXS_IMAGE_TAG
 
 
 def get_use_test_machine_spec() -> bool:
   str_value = os.getenv('USE_TEST_MACHINE_SPEC', 'False')
   return str_value.lower() in {'true', '1'}
 
 
 # Variables associated with private images:
 CLOUD_ML_REGION = os.getenv('CLOUD_ML_REGION', 'europe-west4')
-PRIVATE_ARTIFACT_REGISTRY_PROJECT: str = os.getenv(
-    'PRIVATE_ARTIFACT_REGISTRY_PROJECT', 'vertex-ai-restricted'
+PRIVATE_ARTIFACT_REGISTRY_PROJECT: str = (
+    os.getenv(
+        'PRIVATE_ARTIFACT_REGISTRY_PROJECT',
+    )
+    or 'vertex-ai-restricted'
+)
+PRIVATE_ARTIFACT_REGISTRY_LOCATION: str = (
+    os.getenv(
+        'PRIVATE_ARTIFACT_REGISTRY_LOCATION',
+    )
+    or 'us'
+)
+PRIVATE_ARTIFACT_REGISTRY: str = (
+    os.getenv('PRIVATE_ARTIFACT_REGISTRY') or 'rlhf'
 )
-PRIVATE_ARTIFACT_REGISTRY_LOCATION: str = os.getenv(
-    'PRIVATE_ARTIFACT_REGISTRY_LOCATION', 'us'
+PRIVATE_IMAGE_NAME_PREFIX: str = (
+    os.getenv('PRIVATE_IMAGE_NAME_PREFIX') or 'rlhf_'
 )
-PRIVATE_ARTIFACT_REGISTRY: str = os.getenv('PRIVATE_ARTIFACT_REGISTRY', 'rlhf')
-PRIVATE_IMAGE_NAME_PREFIX: str = os.getenv('PRIVATE_IMAGE_NAME_PREFIX', 'rlhf_')
 PRIVATE_IMAGE_TAG: str = get_private_image_tag()
+AUTOSXS_IMAGE_TAG: str = get_autosxs_image_tag()
 
 # Dataset variables:
 TRAIN_SPLIT: str = 'train'
```

## google_cloud_pipeline_components/_implementation/llm/function_based.py

```diff
@@ -568,7 +568,35 @@
     return os.path.join(artifact.uri, '*')
   return artifact.uri
 
 
 @dsl.component(base_image=_image.GCPC_IMAGE_TAG, install_kfp_package=False)
 def get_empty_string() -> str:
   return ''
+
+
+@dsl.component(base_image=_image.GCPC_IMAGE_TAG, install_kfp_package=False)
+def validate_rlhf_inputs(
+    large_model_reference: str,
+    eval_dataset: Optional[str] = None,
+) -> None:
+  """Checks user-provided arguments are valid for the RLHF pipeline."""
+  models_that_support_bulk_inference = {
+      't5-small',
+      't5-large',
+      't5-xl',
+      't5-xxl',
+      'llama-2-7b',
+      'llama-2-7b-chat',
+      'llama-2-13b',
+      'llama-2-13b-chat',
+  }
+  if (
+      eval_dataset
+      and large_model_reference not in models_that_support_bulk_inference
+  ):
+    raise ValueError(
+        f'eval_dataset not supported for {large_model_reference}. '
+        'Please set this value to None when tuning this model. '
+        'This model can be evaluated after tuning using Batch or Online '
+        'Prediction.'
+    )
```

## google_cloud_pipeline_components/_implementation/llm/utils.py

```diff
@@ -9,29 +9,31 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Utility functions used to create custom Kubeflow components."""
 import os
-from typing import Any, Dict, List
+from typing import Any, Dict, List, Optional
 
 from google_cloud_pipeline_components._implementation.llm import env
 import kfp
 
 
 def build_payload(
     *,
     display_name: str,
     machine_type: str,
     image_uri: str,
     args: List[str],
     accelerator_type: str = '',
     accelerator_count: int = 0,
     encryption_spec_key_name: str = '',
+    labels: Optional[Dict[str, str]] = None,
+    scheduling: Optional[Dict[str, Any]] = None,
 ) -> Dict[str, Any]:
   """Generates payload for a custom training job.
 
   Args:
     display_name: Component display name. Can contain up to 128 UTF-8
       characters.
     machine_type: The type of the machine to provision for the custom job. Must
@@ -42,14 +44,16 @@
       requested.
     accelerator_count: Number of accelerators. By default no accelerators are
       requested.
     encryption_spec_key_name: Customer-managed encryption key. If this is set,
       then all resources created by the CustomJob will be encrypted with the
       provided encryption key. Note that this is not supported for TPU at the
       moment.
+    labels: The labels with user-defined metadata to organize CustomJobs.
+    scheduling: Scheduling options for a CustomJob.
 
   Returns:
     Custom job payload.
 
   Raises:
     ValueError: if one of ``accelerator_count`` or ``accelerator_type`` is
       specified, but the corresponding field is not valid.
@@ -82,14 +86,20 @@
         'Accelerator type must be specified if accelerator count is not 0.'
         f'Received accelerator_type == {accelerator_type}.'
     )
 
   if encryption_spec_key_name:
     payload['encryption_spec'] = {'kms_key_name': encryption_spec_key_name}
 
+  if labels:
+    payload['labels'] = labels
+
+  if scheduling:
+    payload['job_spec']['scheduling'] = scheduling
+
   return payload
 
 
 def get_temp_location() -> str:
   """Gets a task-specific location to store temporary files."""
   return os.path.join(
       kfp.dsl.PIPELINE_ROOT_PLACEHOLDER,
@@ -109,13 +119,18 @@
 
   Args:
     image_name: Name of the image to resolve.
 
   Returns:
     URI of the image.
   """
+  if image_name.find('autosxs') != -1:
+    image_tag = env.get_autosxs_image_tag()
+  else:
+    image_tag = env.get_private_image_tag()
+
   return '/'.join([
       f'{env.PRIVATE_ARTIFACT_REGISTRY_LOCATION}-docker.pkg.dev',
       env.PRIVATE_ARTIFACT_REGISTRY_PROJECT,
       env.PRIVATE_ARTIFACT_REGISTRY,
-      f'{env.PRIVATE_IMAGE_NAME_PREFIX}{image_name}:{env.get_private_image_tag()}',
+      f'{env.PRIVATE_IMAGE_NAME_PREFIX}{image_name}:{image_tag}',
   ])
```

## google_cloud_pipeline_components/_implementation/llm/utils_test.py

```diff
@@ -53,10 +53,40 @@
         machine_type=machine_type,
         image_uri=image_uri,
         args=args,
         encryption_spec_key_name=encryption_spec_key_name,
     )
     self.assertDictEqual(expected_payload, actual_payload)
 
+  def test_build_payload_with_labels_and_scheduling(self):
+    machine_type = "n1-standard-1"
+    image_uri = "fake_image_uri"
+    args = ["--foo=bar"]
+    labels = {"vertex-internal-enable-custom-job-retries": ""}
+    scheduling = {"disable_retries": False}
+
+    expected_payload = {
+        "display_name": "test_with_encryption_spec_key_name",
+        "job_spec": {
+            "worker_pool_specs": [{
+                "replica_count": "1",
+                "machine_spec": {"machine_type": machine_type},
+                "container_spec": {"image_uri": image_uri, "args": args},
+            }],
+            "scheduling": scheduling,
+        },
+        "labels": labels,
+    }
+
+    actual_payload = utils.build_payload(
+        display_name="test_with_encryption_spec_key_name",
+        machine_type=machine_type,
+        image_uri=image_uri,
+        args=args,
+        labels=labels,
+        scheduling=scheduling,
+    )
+    self.assertDictEqual(expected_payload, actual_payload)
+
 
 if __name__ == "__main__":
   unittest.main()
```

## google_cloud_pipeline_components/_implementation/model_evaluation/chunking/component.py

```diff
@@ -27,14 +27,15 @@
 def chunking(
     gcp_resources: OutputPath(str),
     project: str,
     location: str,
     input_text_gcs_dir: str,
     output_bq_destination: str,
     output_text_gcs_dir: str,
+    output_error_file_path: str,
     generation_threshold_microseconds: str,
     display_name: str = 'chunking',
     machine_type: str = 'n1-standard-8',
     service_account: str = '',
     network: str = '',
     encryption_spec_key_name: str = '',
 ):
@@ -44,14 +45,15 @@
     project: The GCP project that runs the pipeline component.
     location: The GCP region that runs the pipeline component.
     input_text_gcs_dir: the GCS directory containing the files to chunk. DO NOT
       include '/' at the end of the path.
     output_bq_destination: The BigQuery table URI where the component will write
       chunks to.
     output_text_gcs_dir: The GCS folder to hold intermediate data.
+    output_error_file_path: The path to the file containing chunking error.
     generation_threshold_microseconds: only files created on/after this
       generation threshold will be processed, in microseconds.
     display_name: The name of the chunking job/component.
     machine_type: The machine type of this custom job.
     service_account: Sets the default service account for workload run-as
       account. The service account running the pipeline
       (https://cloud.google.com/vertex-ai/docs/pipelines/configure-project#service-account)
@@ -81,14 +83,15 @@
               f'--process_chunking={True}',
               f'--project={project}',
               f'--location={location}',
               f'--root_dir={PIPELINE_ROOT_PLACEHOLDER}',
               f'--input_text_gcs_dir={input_text_gcs_dir}',
               f'--output_bq_destination={output_bq_destination}',
               f'--output_text_gcs_dir={output_text_gcs_dir}',
+              f'--output_error_file_path={output_error_file_path}',
               f'--generation_threshold_microseconds={generation_threshold_microseconds}',
               f'--gcp_resources={gcp_resources}',
               '--executor_input={{$.json_escape[1]}}',
           ],
           service_account=service_account,
           network=network,
           encryption_spec_key_name=encryption_spec_key_name,
```

## google_cloud_pipeline_components/_implementation/model_evaluation/endpoint_batch_predict/component.py

```diff
@@ -12,35 +12,36 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Endpoint batch predict component used in KFP pipelines."""
 
 from typing import Dict, List, NamedTuple, Optional, Union
 from google_cloud_pipeline_components import utils as gcpc_utils
 from google_cloud_pipeline_components._implementation.model_evaluation import utils
+from google_cloud_pipeline_components._implementation.model_evaluation import version
 from kfp import dsl
 from kfp.dsl import Artifact
 from kfp.dsl import container_component
 from kfp.dsl import Output
 from kfp.dsl import OutputPath
 from kfp.dsl import PIPELINE_ROOT_PLACEHOLDER
 
 _IMAGE_URI = 'us-docker.pkg.dev/vertex-evaluation/public/llm:wjess-fishfooding'
 
 
-@dsl.component
+@dsl.component(base_image=version.LLM_EVAL_IMAGE_TAG)
 def add_json_escape_parameters(parameters: dict) -> str:
   if not parameters:
     return
   import json
 
   json_escaped_parameters = json.dumps(parameters).replace('"', '\\"')
   return json_escaped_parameters
 
 
-@dsl.component
+@dsl.component(base_image=version.LLM_EVAL_IMAGE_TAG)
 def add_json_escape_paths(paths: list) -> str:
   if not paths:
     return
   import json
 
   json_escaped_paths = json.dumps(paths).replace('"', '\\"')
   return json_escaped_paths
```

## google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/component.py

```diff
@@ -58,16 +58,15 @@
     evaluation_task: The task that the large language model will be evaluated
       on. The evaluation component computes a set of metrics relevant to that
       specific task. Currently supported tasks are: `summarization`,
       `question-answering`, and `text-generation`.
     target_field_name: The full name path of the features target field in the
       predictions file. Formatted to be able to find nested columns, delimited
       by `.`. Alternatively referred to as the ground truth (or
-      ground_truth_column) field. If not set, defaulted to
-      `inputs.ground_truth`.
+      ground_truth_column) field. If not set, defaulted to `inputs.output_text`.
     prediction_field_name: The full name path of the prediction field in the
       prediction file. Formatted to be able to find nested columns, delimited by
       `.`. If not set, defaulted to `predictions.content`.
     predictions_format: The file format for the LLM Batch Prediction results.
       `jsonl` is currently the only allowed format. If not set, defaulted to
       `jsonl`.
     joined_predictions_gcs_source: An Artifact with an URI pointing toward a GCS
```

## google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation_preprocessor/component.py

```diff
@@ -18,15 +18,15 @@
 from google_cloud_pipeline_components import utils as gcpc_utils
 from google_cloud_pipeline_components._implementation.model_evaluation import utils
 from google_cloud_pipeline_components._implementation.model_evaluation import version
 from kfp import dsl
 
 
 # pylint: disable=g-import-not-at-top, g-doc-args, unexpected-keyword-arg
-@dsl.component
+@dsl.component(base_image=version.LLM_EVAL_IMAGE_TAG)
 def add_json_escape_to_list(input_list: List[str]) -> str:
   import json
 
   json_escaped_list = json.dumps(input_list).replace('"', '\\"')
   return json_escaped_list
```

## google_cloud_pipeline_components/_implementation/model_evaluation/model_inference/component.py

```diff
@@ -13,57 +13,69 @@
 # limitations under the License.
 """Third party inference component."""
 from typing import Any, Dict, List, NamedTuple
 
 from google_cloud_pipeline_components import utils as gcpc_utils
 from google_cloud_pipeline_components._implementation.model_evaluation import LLMEvaluationTextGenerationOp
 from google_cloud_pipeline_components._implementation.model_evaluation import utils
+from google_cloud_pipeline_components._implementation.model_evaluation import version
 from kfp.dsl import Artifact
 from kfp.dsl import container_component
 from kfp.dsl import Metrics
 from kfp.dsl import Output
 from kfp.dsl import OutputPath
 from kfp.dsl import pipeline
 
 
 _IMAGE_URI = 'gcr.io/model-evaluation-dev/llm_eval:clyu-test'
 
 
 @container_component
 def model_inference_component_internal(
     gcp_resources: OutputPath(str),
-    gcs_output_directory: Output[Artifact],
+    gcs_output_path: Output[Artifact],
     project: str,
     location: str,
     client_api_key_path: str,
     prediction_instances_source_uri: str,
-    output_inference_gcs_prefix: str,
     inference_platform: str = 'openai_chat_completions',
     model_id: str = 'gpt-3.5-turbo',
     request_params: Dict[str, Any] = {},
-    max_request_per_second: float = 3,
-    max_tokens_per_minute: float = 100,
+    max_request_per_minute: float = 3,
+    max_tokens_per_minute: float = 10000,
+    query_field_name: str = '',
     display_name: str = 'third-party-inference',
     machine_type: str = 'e2-highmem-16',
     service_account: str = '',
     network: str = '',
     reserved_ip_ranges: List[str] = [],
     encryption_spec_key_name: str = '',
 ):
   """Internal component to run Third Party Model Inference.
 
   Args:
       gcp_resources (str): Serialized gcp_resources proto tracking the custom
         job.
-      model_inference_output_gcs_uri: The storage URI pointing toward a GCS
-        location to store CSV for third party inference.
+      gcs_output_path: The storage URI pointing toward a GCS location to store
+        CSV for third party inference.
       project: Required. The GCP project that runs the pipeline component.
       location: Required. The GCP region that runs the pipeline component.
       client_api_key_path: The GCS URI where client API key.
-      output_inference_gcs_prefix: GCS file prefix for writing output.
+      prediction_instances_source_uri: GCS file path to prediction requests.
+      inference_platform: Name of the inference platform.
+      model_id: Name of the model to send requests against.
+      request_params: Parameters to confirgure requests.
+      max_request_per_minute: Maximum number of requests can be sent in a
+        minute.
+      max_tokens_per_minute: float = 10000,
+      query_field_name: The full name path of the features prompt field in the
+        request file. Formatted to be able to find nested columns, delimited by
+        `.`. Alternatively referred to as the ground truth (or
+        ground_truth_column) field. If not set, defaulted to
+        `inputs.ground_truth`.
       display_name: display name of the pipeline.
       machine_type: The machine type of this custom job. If not set, defaulted
         to `e2-highmem-16`. More details:
         https://cloud.google.com/compute/docs/machine-resource
       service_account: Sets the default service account for workload run-as
         account. The service account running the pipeline
         (https://cloud.google.com/vertex-ai/docs/pipelines/configure-project#service-account)
@@ -84,38 +96,37 @@
       encryption_spec_key_name: Customer-managed encryption key options for the
         CustomJob. If this is set, then all resources created by the CustomJob
         will be encrypted with the provided encryption key.
 
   Returns:
       gcp_resources (str): Serialized gcp_resources proto tracking the custom
         job.
-      model_inference_output_gcs_uri: The storage URI pointing toward a
+      gcs_output_path: The storage URI pointing toward a
         GCS location to store CSV for third party inference.
   """
   return gcpc_utils.build_serverless_customjob_container_spec(
       project=project,
       location=location,
       custom_job_payload=utils.build_custom_job_payload(
           display_name=display_name,
           machine_type=machine_type,
-          image_uri=_IMAGE_URI,
+          image_uri=version.LLM_EVAL_IMAGE_TAG,  # for local test and validation, use _IMAGE_URI.
           args=[
               f'--3p_model_inference={True}',
               f'--project={project}',
               f'--location={location}',
               f'--prediction_instances_source_uri={prediction_instances_source_uri}',
               f'--inference_platform={inference_platform}',
-              f'--output_inference_gcs_prefix={output_inference_gcs_prefix}',
               f'--model_id={model_id}',
               f'--request_params={request_params}',
               f'--client_api_key_path={client_api_key_path}',
-              f'--max_request_per_second={max_request_per_second}',
+              f'--max_request_per_minute={max_request_per_minute}',
               f'--max_tokens_per_minute={max_tokens_per_minute}',
-              # f'--gcs_output_directory={gcs_output_directory}',
-              f'--gcs_output_directory={gcs_output_directory.path}',
+              f'--query_field_name={query_field_name}',
+              f'--gcs_output_path={gcs_output_path.path}',
               '--executor_input={{$.json_escape[1]}}',
           ],
           service_account=service_account,
           network=network,
           reserved_ip_ranges=reserved_ip_ranges,
           encryption_spec_key_name=encryption_spec_key_name,
       ),
@@ -125,37 +136,48 @@
 
 @pipeline(name='ModelEvaluationModelInferenceOp')
 def model_inference_component(
     project: str,
     location: str,
     client_api_key_path: str,
     prediction_instances_source_uri: str,
-    output_inference_gcs_prefix: str,
     inference_platform: str = 'openai_chat_completions',
     model_id: str = 'gpt-3.5-turbo',
     request_params: Dict[str, Any] = {},
-    max_request_per_second: float = 3,
-    max_tokens_per_minute: float = 100,
+    query_field_name: str = 'prompt',
+    max_request_per_minute: float = 3,
+    max_tokens_per_minute: float = 10000,
     display_name: str = 'third-party-inference',
     machine_type: str = 'e2-highmem-16',
     service_account: str = '',
     network: str = '',
     reserved_ip_ranges: List[str] = [],
     encryption_spec_key_name: str = '',
 ) -> NamedTuple(
     'outputs',
-    gcs_output_directory=Artifact,
+    gcs_output_path=Artifact,
 ):
   """Component to run Third Party Model Inference.
 
   Args:
     project: Required. The GCP project that runs the pipeline component.
       location: Required. The GCP region that runs the pipeline component.
       client_api_key_path: The GCS URI where client API key.
-      output_inference_gcs_prefix: GCS file prefix for writing output.
+      prediction_instances_source_uri: GCS file path to prediction requests.
+      inference_platform: Name of the inference platform.
+      model_id: Name of the model to send requests against.
+      request_params: Parameters to confirgure requests.
+      query_field_name: The full name path of the features prompt field in the
+        request file. Formatted to be able to find nested columns, delimited by
+        `.`. Alternatively referred to as the ground truth (or
+        ground_truth_column) field. If not set, defaulted to
+        `inputs.ground_truth`.
+      max_request_per_minute: Maximum number of requests can be sent in a
+        minute.
+      max_tokens_per_minute: float = 10000,
       display_name: display name of the pipeline.
       machine_type: The machine type of this custom job. If not set, defaulted
         to `e2-highmem-16`. More details:
         https://cloud.google.com/compute/docs/machine-resource
       service_account: Sets the default service account for workload run-as
         account. The service account running the pipeline
         (https://cloud.google.com/vertex-ai/docs/pipelines/configure-project#service-account)
@@ -175,77 +197,93 @@
         to any ip ranges under the provided VPC network.
       encryption_spec_key_name: Customer-managed encryption key options for the
         CustomJob. If this is set, then all resources created by the CustomJob
         will be encrypted with the provided encryption key.
 
   Returns:
     NamedTuple:
-      model_inference_output_gcs_uri: CSV file output containing third
+      gcs_output_path: CSV file output containing third
       party prediction results.
   """
   outputs = NamedTuple(
       'outputs',
-      gcs_output_directory=Artifact,
+      gcs_output_path=Artifact,
   )
 
   inference_task = model_inference_component_internal(
       project=project,
       location=location,
       client_api_key_path=client_api_key_path,
       prediction_instances_source_uri=prediction_instances_source_uri,
       inference_platform=inference_platform,
       model_id=model_id,
       request_params=request_params,
-      max_request_per_second=max_request_per_second,
+      max_request_per_minute=max_request_per_minute,
       max_tokens_per_minute=max_tokens_per_minute,
-      output_inference_gcs_prefix=output_inference_gcs_prefix,
       display_name=display_name,
+      query_field_name=query_field_name,
       machine_type=machine_type,
       service_account=service_account,
       network=network,
       reserved_ip_ranges=reserved_ip_ranges,
       encryption_spec_key_name=encryption_spec_key_name,
   )
 
   return outputs(
-      gcs_output_directory=inference_task.outputs['gcs_output_directory'],
+      gcs_output_path=inference_task.outputs['gcs_output_path'],
   )
 
 
 @pipeline(name='ModelEvaluationModelInferenceAndEvaluationPipeline')
 def model_inference_and_evaluation_component(
     project: str,
     location: str,
     client_api_key_path: str,
     prediction_instances_source_uri: str,
-    output_inference_gcs_prefix: str,
-    target_field_name: str = '',
     inference_platform: str = 'openai_chat_completions',
     model_id: str = 'gpt-3.5-turbo',
     request_params: Dict[str, Any] = {},
-    max_request_per_second: float = 3,
-    max_tokens_per_minute: float = 100,
+    target_field_name: str = 'ground_truth',
+    query_field_name: str = 'prompt',
+    max_request_per_minute: float = 3,
+    max_tokens_per_minute: float = 10000,
     display_name: str = 'third-party-inference',
     machine_type: str = 'e2-highmem-16',
     service_account: str = '',
     network: str = '',
     reserved_ip_ranges: List[str] = [],
     encryption_spec_key_name: str = '',
 ) -> NamedTuple(
     'outputs',
-    gcs_output_directory=Artifact,
+    gcs_output_path=Artifact,
     evaluation_metrics=Metrics,
 ):
   """Component tun Third Party Model Inference and evaluation.
 
   Args:
     project: Required. The GCP project that runs the pipeline component.
       location: Required. The GCP region that runs the pipeline component.
       client_api_key_path: The GCS URI where client API key.
-      output_inference_gcs_prefix: GCS file prefix for writing output.
+      prediction_instances_source_uri: GCS file path to prediction requests.
+      inference_platform: Name of the inference platform.
+      model_id: Name of the model to send requests against.
+      request_params: Parameters to confirgure requests.
+      target_field_name: The full name path of the features target field in the
+        predictions file. Formatted to be able to find nested columns, delimited
+        by `.`. Alternatively referred to as the ground truth (or
+        ground_truth_column) field. If not set, defaulted to
+        `inputs.ground_truth`.
+      query_field_name: The full name path of the features prompt field in the
+        request file. Formatted to be able to find nested columns, delimited by
+        `.`. Alternatively referred to as the ground truth (or
+        ground_truth_column) field. If not set, defaulted to
+        `inputs.ground_truth`.
+      max_request_per_minute: Maximum number of requests can be sent in a
+        minute.
+      max_tokens_per_minute: float = 10000,
       display_name: display name of the pipeline.
       machine_type: The machine type of this custom job. If not set, defaulted
         to `e2-highmem-16`. More details:
         https://cloud.google.com/compute/docs/machine-resource
       service_account: Sets the default service account for workload run-as
         account. The service account running the pipeline
         (https://cloud.google.com/vertex-ai/docs/pipelines/configure-project#service-account)
@@ -265,60 +303,50 @@
         to any ip ranges under the provided VPC network.
       encryption_spec_key_name: Customer-managed encryption key options for the
         CustomJob. If this is set, then all resources created by the CustomJob
         will be encrypted with the provided encryption key.
 
   Returns:
     NamedTuple:
-      model_inference_output_gcs_uri: CSV file output containing third
+      gcs_output_path: CSV file output containing third
       party prediction results.
   """
   outputs = NamedTuple(
       'outputs',
-      gcs_output_directory=Artifact,
+      gcs_output_path=Artifact,
       evaluation_metrics=Metrics,
   )
 
   inference_task = model_inference_component_internal(
       project=project,
       location=location,
       client_api_key_path=client_api_key_path,
       prediction_instances_source_uri=prediction_instances_source_uri,
       inference_platform=inference_platform,
       model_id=model_id,
       request_params=request_params,
-      max_request_per_second=max_request_per_second,
+      max_request_per_minute=max_request_per_minute,
       max_tokens_per_minute=max_tokens_per_minute,
-      output_inference_gcs_prefix=output_inference_gcs_prefix,
+      query_field_name=query_field_name,
       display_name=display_name,
       machine_type=machine_type,
       service_account=service_account,
       network=network,
       reserved_ip_ranges=reserved_ip_ranges,
       encryption_spec_key_name=encryption_spec_key_name,
   )
 
-  if inference_platform == 'openai_chat_completions':
-    prediction_field_name = 'predictions.0.message.content'
-  elif inference_platform == 'anthropic_predictions':
-    prediction_field_name = 'predictions'
-  else:
-    prediction_field_name = ''
-
   eval_task = LLMEvaluationTextGenerationOp(
       project=project,
       location=location,
       evaluation_task='text-generation',
-      target_field_name=target_field_name,
-      prediction_field_name=prediction_field_name,
+      target_field_name='.'.join(['instance', str(target_field_name)]),
       predictions_format='jsonl',
-      joined_predictions_gcs_source=inference_task.outputs[
-          'gcs_output_directory'
-      ],
+      joined_predictions_gcs_source=inference_task.outputs['gcs_output_path'],
       machine_type=machine_type,
       encryption_spec_key_name=encryption_spec_key_name,
   )
 
   return outputs(
-      gcs_output_directory=inference_task.outputs['gcs_output_directory'],
+      gcs_output_path=inference_task.outputs['gcs_output_path'],
       evaluation_metrics=eval_task.outputs['evaluation_metrics'],
   )
```

## google_cloud_pipeline_components/preview/automl/forecasting/__init__.py

```diff
@@ -7,26 +7,35 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 """Experimental AutoML forecasting components."""
 import os
 
 from google_cloud_pipeline_components.preview.automl.forecasting.forecasting_ensemble import automl_forecasting_ensemble as ForecastingEnsembleOp
 from google_cloud_pipeline_components.preview.automl.forecasting.forecasting_stage_1_tuner import automl_forecasting_stage_1_tuner as ForecastingStage1TunerOp
 from google_cloud_pipeline_components.preview.automl.forecasting.forecasting_stage_2_tuner import automl_forecasting_stage_2_tuner as ForecastingStage2TunerOp
+from google_cloud_pipeline_components.preview.automl.forecasting.utils import get_learn_to_learn_forecasting_pipeline_and_parameters
+from google_cloud_pipeline_components.preview.automl.forecasting.utils import get_sequence_to_sequence_forecasting_pipeline_and_parameters
+from google_cloud_pipeline_components.preview.automl.forecasting.utils import get_temporal_fusion_transformer_forecasting_pipeline_and_parameters
+from google_cloud_pipeline_components.preview.automl.forecasting.utils import get_time_series_dense_encoder_forecasting_pipeline_and_parameters
 from kfp import components
 
 __all__ = [
-    'ForecastingStage1TunerOp',
     'ForecastingEnsembleOp',
+    'ForecastingStage1TunerOp',
     'ForecastingStage2TunerOp',
+    'get_learn_to_learn_forecasting_pipeline_and_parameters',
+    'get_sequence_to_sequence_forecasting_pipeline_and_parameters',
+    'get_temporal_fusion_transformer_forecasting_pipeline_and_parameters',
+    'get_time_series_dense_encoder_forecasting_pipeline_and_parameters',
     'learn_to_learn_forecasting_pipeline',
     'sequence_to_sequence_forecasting_pipeline',
     'temporal_fusion_transformer_forecasting_pipeline',
     'time_series_dense_encoder_forecasting_pipeline',
 ]
 
 learn_to_learn_forecasting_pipeline = components.load_component_from_file(
```

## google_cloud_pipeline_components/preview/automl/forecasting/forecasting_ensemble.py

```diff
@@ -68,15 +68,15 @@
     explanation_metadata_artifact: The explanation metadata used by Vertex online and batch explanations in the format of a KFP Artifact.
     explanation_parameters: The explanation parameters used by Vertex online and batch explanations.
     example_instance: An example instance which may be used as an input for predictions.
   """
   # fmt: on
   job_id = dsl.PIPELINE_JOB_ID_PLACEHOLDER
   task_id = dsl.PIPELINE_TASK_ID_PLACEHOLDER
-  image_uri = 'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125'
+  image_uri = 'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125'
   display_name = f'automl-forecasting-ensemble-{job_id}-{task_id}'
 
   error_file_path = f'{root_dir}/{job_id}/{task_id}/error.pb'
   model_relative_path = f'{job_id}/{task_id}/model'
   explanation_metadata_paths = (
       f'{explanation_metadata},{explanation_metadata_artifact.uri}'
   )
```

## google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_1_tuner.py

```diff
@@ -95,22 +95,22 @@
                   ),
                   encryption_spec_key_name,
                   (
                       '"}, "job_spec": {"worker_pool_specs": [{"replica_count":'
                       ' 1, "machine_spec": {"machine_type": "n1-standard-8"},'
                       ' "container_spec": {"image_uri":"'
                   ),
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125',
                   '", "args": ["forecasting_mp_l2l_stage_1_tuner',
                   '", "--region=',
                   location,
                   '", "--transform_output_path=',
                   transform_output.uri,
                   '", "--training_docker_uri=',
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125',
                   '", "--reduce_search_space_mode=',
                   reduce_search_space_mode,
                   f'", "--component_id={dsl.PIPELINE_TASK_ID_PLACEHOLDER}',
                   '", "--training_base_dir=',
                   root_dir,
                   f'/{dsl.PIPELINE_JOB_ID_PLACEHOLDER}/{dsl.PIPELINE_TASK_ID_PLACEHOLDER}/train',
                   '", "--num_parallel_trial=',
```

## google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_2_tuner.py

```diff
@@ -93,22 +93,22 @@
                   ),
                   encryption_spec_key_name,
                   (
                       '"}, "job_spec": {"worker_pool_specs": [{"replica_count":'
                       ' 1, "machine_spec": {"machine_type": "n1-standard-8"},'
                       ' "container_spec": {"image_uri":"'
                   ),
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125',
                   '", "args": ["forecasting_mp_l2l_stage_2_tuner',
                   '", "--region=',
                   location,
                   '", "--transform_output_path=',
                   transform_output.uri,
                   '", "--training_docker_uri=',
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125',
                   f'", "--component_id={dsl.PIPELINE_TASK_ID_PLACEHOLDER}',
                   '", "--training_base_dir=',
                   root_dir,
                   f'/{dsl.PIPELINE_JOB_ID_PLACEHOLDER}/{dsl.PIPELINE_TASK_ID_PLACEHOLDER}/train',
                   '", "--num_parallel_trial=',
                   num_parallel_trials,
                   '", "--single_run_max_secs=',
```

## google_cloud_pipeline_components/preview/automl/forecasting/learn_to_learn_forecasting_pipeline.yaml

```diff
@@ -74,56 +74,49 @@
     executorLabel: exec-automl-forecasting-ensemble
     inputDefinitions:
       artifacts:
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         instance_schema_path:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The path to the instance schema,
-
-            describing the input data for the tf_model at serving time.'
+          description: The path to the instance schema, describing the input data
+            for the tf_model at serving time.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Region to run the job in.
           parameterType: STRING
         prediction_image_uri:
-          description: 'URI of the Docker image to be used as the
-
-            container for serving predictions. This URI must identify an image in
-
-            Artifact Registry or Container Registry.'
+          description: URI of the Docker image to be used as the container for serving
+            predictions. This URI must identify an image in Artifact Registry or Container
+            Registry.
           parameterType: STRING
         project:
           description: Project to run the job in.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage path to store the output.
           parameterType: STRING
@@ -155,69 +148,60 @@
           description: The explanation metadata used by Vertex online and batch explanations.
           parameterType: STRUCT
         explanation_parameters:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-forecasting-ensemble-2:
     executorLabel: exec-automl-forecasting-ensemble-2
     inputDefinitions:
       artifacts:
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         instance_schema_path:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The path to the instance schema,
-
-            describing the input data for the tf_model at serving time.'
+          description: The path to the instance schema, describing the input data
+            for the tf_model at serving time.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Region to run the job in.
           parameterType: STRING
         prediction_image_uri:
-          description: 'URI of the Docker image to be used as the
-
-            container for serving predictions. This URI must identify an image in
-
-            Artifact Registry or Container Registry.'
+          description: URI of the Docker image to be used as the container for serving
+            predictions. This URI must identify an image in Artifact Registry or Container
+            Registry.
           parameterType: STRING
         project:
           description: Project to run the job in.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage path to store the output.
           parameterType: STRING
@@ -249,209 +233,174 @@
           description: The explanation metadata used by Vertex online and batch explanations.
           parameterType: STRUCT
         explanation_parameters:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-forecasting-stage-1-tuner:
     executorLabel: exec-automl-forecasting-stage-1-tuner
     inputDefinitions:
       artifacts:
         materialized_eval_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         deadline_hours:
-          description: 'Number of hours the hyperparameter tuning should
-
-            run.'
+          description: Number of hours the hyperparameter tuning should run.
           parameterType: NUMBER_DOUBLE
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Location for running the hyperparameter tuning.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run hyperparameter tuning.
           parameterType: STRING
         reduce_search_space_mode:
           defaultValue: regular
-          description: 'The reduce search space mode. Possible
-
-            values: "regular" (default), "minimal", "full".'
+          description: 'The reduce search space mode. Possible values: "regular" (default),
+            "minimal", "full".'
           isOptional: true
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         study_spec_parameters_override:
           defaultValue: []
-          description: 'JSON study spec. E.g.,
-
-            [{"parameter_id": "activation","categorical_value_spec": {"values":
-
-            ["tanh"]}}]'
+          description: 'JSON study spec. E.g., [{"parameter_id": "activation","categorical_value_spec":
+            {"values": ["tanh"]}}]'
           isOptional: true
           parameterType: LIST
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component.
-
-            For more details, see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-forecasting-stage-2-tuner:
     executorLabel: exec-automl-forecasting-stage-2-tuner
     inputDefinitions:
       artifacts:
         materialized_eval_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The forecasting example gen
-
-            metadata.'
+          description: The forecasting example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input_path:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Path to the json of hyperparameter
-
-            tuning results to use when evaluating models.'
+          description: Path to the json of hyperparameter tuning results to use when
+            evaluating models.
       parameters:
         deadline_hours:
-          description: 'Number of hours the cross-validation trainer
-
-            should run.'
+          description: Number of hours the cross-validation trainer should run.
           parameterType: NUMBER_DOUBLE
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: 'Cloud region for running the component: us-central1).'
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model.'
+          description: Number of selected trials. The number of weak learners in the
+            final model.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run stage 2 tuner.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained (private) model artifact paths and their hyperparameters.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component.
-
-            For more details, see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-finalizer:
     executorLabel: exec-automl-tabular-finalizer
     inputDefinitions:
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
@@ -466,18 +415,16 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-calculate-training-parameters:
     executorLabel: exec-calculate-training-parameters
     inputDefinitions:
       parameters:
         fast_testing:
           defaultValue: false
@@ -967,14 +914,17 @@
                 componentInputParameter: pipelinechannel--dataflow_subnetwork
               dataflow_use_public_ips:
                 componentInputParameter: pipelinechannel--dataflow_use_public_ips
               dataflow_workers_num:
                 componentInputParameter: pipelinechannel--evaluation_dataflow_starting_num_workers
               encryption_spec_key_name:
                 componentInputParameter: pipelinechannel--encryption_spec_key_name
+              force_runner_mode:
+                runtimeValue:
+                  constant: Dataflow
               location:
                 componentInputParameter: pipelinechannel--location
               predictions_format:
                 runtimeValue:
                   constant: jsonl
               problem_type:
                 runtimeValue:
@@ -1685,14 +1635,17 @@
                 componentInputParameter: pipelinechannel--dataflow_subnetwork
               dataflow_use_public_ips:
                 componentInputParameter: pipelinechannel--dataflow_use_public_ips
               dataflow_workers_num:
                 componentInputParameter: pipelinechannel--evaluation_dataflow_starting_num_workers
               encryption_spec_key_name:
                 componentInputParameter: pipelinechannel--encryption_spec_key_name
+              force_runner_mode:
+                runtimeValue:
+                  constant: Dataflow
               location:
                 componentInputParameter: pipelinechannel--location
               predictions_format:
                 runtimeValue:
                   constant: jsonl
               problem_type:
                 runtimeValue:
@@ -2741,194 +2694,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -2936,42 +2853,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -3013,38 +2922,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -3069,116 +2971,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -3194,272 +3069,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -3485,19 +3308,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -3510,44 +3331,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-finalize-eval-quantile-parameters:
     executorLabel: exec-finalize-eval-quantile-parameters
     inputDefinitions:
       parameters:
         quantiles:
           isOptional: true
@@ -5557,224 +5370,178 @@
     executorLabel: exec-training-configurator-and-validator
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -5802,15 +5569,15 @@
         - '{{$.inputs.parameters[''location'']}}'
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"display_name": "automl-forecasting-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "encryption_spec": {"kms_key_name": "{{$.inputs.parameters[''encryption_spec_key_name'']}}"},
           "job_spec": {"worker_pool_specs": [{"replica_count": 1, "machine_spec":
-          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "args": ["forecasting_mp_ensemble", "--transform_output_path={{$.inputs.artifacts[''transform_output''].uri}}",
           "--error_file_path={{$.inputs.parameters[''root_dir'']}}/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb",
           "--metadata_path={{$.inputs.artifacts[''metadata''].uri}}", "--tuning_result_input_path={{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "--instance_baseline_path={{$.inputs.artifacts[''instance_baseline''].uri}}",
           "--instance_schema_path={{$.inputs.artifacts[''instance_schema_path''].uri}}",
           "--prediction_docker_uri={{$.inputs.parameters[''prediction_image_uri'']}}",
           "--model_relative_output_path={{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model",
@@ -5836,15 +5603,15 @@
         - '{{$.inputs.parameters[''location'']}}'
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"display_name": "automl-forecasting-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "encryption_spec": {"kms_key_name": "{{$.inputs.parameters[''encryption_spec_key_name'']}}"},
           "job_spec": {"worker_pool_specs": [{"replica_count": 1, "machine_spec":
-          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "args": ["forecasting_mp_ensemble", "--transform_output_path={{$.inputs.artifacts[''transform_output''].uri}}",
           "--error_file_path={{$.inputs.parameters[''root_dir'']}}/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb",
           "--metadata_path={{$.inputs.artifacts[''metadata''].uri}}", "--tuning_result_input_path={{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "--instance_baseline_path={{$.inputs.artifacts[''instance_baseline''].uri}}",
           "--instance_schema_path={{$.inputs.artifacts[''instance_schema_path''].uri}}",
           "--prediction_docker_uri={{$.inputs.parameters[''prediction_image_uri'']}}",
           "--model_relative_output_path={{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model",
@@ -5871,19 +5638,19 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-forecasting-stage-1-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"args\": [\"forecasting_mp_l2l_stage_1_tuner", "\", \"--region=",
           "{{$.inputs.parameters[''location'']}}", "\", \"--transform_output_path=",
           "{{$.inputs.artifacts[''transform_output''].uri}}", "\", \"--training_docker_uri=",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"--reduce_search_space_mode=", "{{$.inputs.parameters[''reduce_search_space_mode'']}}",
           "\", \"--component_id={{$.pipeline_task_uuid}}", "\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train",
           "\", \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--num_selected_trials=", "{{$.inputs.parameters[''num_selected_trials'']}}",
@@ -5914,19 +5681,19 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-forecasting-stage-2-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"args\": [\"forecasting_mp_l2l_stage_2_tuner", "\", \"--region=",
           "{{$.inputs.parameters[''location'']}}", "\", \"--transform_output_path=",
           "{{$.inputs.artifacts[''transform_output''].uri}}", "\", \"--training_docker_uri=",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"--component_id={{$.pipeline_task_uuid}}", "\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train",
           "\", \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--num_selected_trials=", "{{$.inputs.parameters[''num_selected_trials'']}}",
           "\", \"--lro_job_info=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro",
@@ -5957,15 +5724,15 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
@@ -6281,16 +6048,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -6299,15 +6066,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-finalize-eval-quantile-parameters:
       container:
         args:
         - --executor_input
@@ -6469,18 +6236,18 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef _get_prediction_image_uri(model_type: str) -> str:\n  \"\"\"\
           Returns the prediction image corresponding to the given model type.\"\"\"\
           \n  # Keys come from AutoMlTimeSeriesForecastingTrainSpec.\n  # The URIs\
           \ must be hardcoded without any breaks in the code so string\n  # replacement\
-          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20231029_0125',\n\
-          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20231029_0125',\n\
-          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20231029_0125',\n\
-          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20231029_0125',\n\
+          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20240119_0125',\n\
+          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20240119_0125',\n\
+          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20240119_0125',\n\
+          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20240119_0125',\n\
           \  }\n  if model_type not in images:\n    raise ValueError(\n        f'Invalid\
           \ forecasting model type: {model_type}. Valid options are: '\n        f'{images.keys()}.'\n\
           \    )\n  return images[model_type]\n\n"
         image: python:3.7
     exec-get-prediction-image-uri-2:
       container:
         args:
@@ -6505,18 +6272,18 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef _get_prediction_image_uri(model_type: str) -> str:\n  \"\"\"\
           Returns the prediction image corresponding to the given model type.\"\"\"\
           \n  # Keys come from AutoMlTimeSeriesForecastingTrainSpec.\n  # The URIs\
           \ must be hardcoded without any breaks in the code so string\n  # replacement\
-          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20231029_0125',\n\
-          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20231029_0125',\n\
-          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20231029_0125',\n\
-          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20231029_0125',\n\
+          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20240119_0125',\n\
+          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20240119_0125',\n\
+          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20240119_0125',\n\
+          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20240119_0125',\n\
           \  }\n  if model_type not in images:\n    raise ValueError(\n        f'Invalid\
           \ forecasting model type: {model_type}. Valid options are: '\n        f'{images.keys()}.'\n\
           \    )\n  return images[model_type]\n\n"
         image: python:3.7
     exec-get-predictions-column:
       container:
         args:
@@ -6541,15 +6308,15 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef get_predictions_column(forecasting_type: str, target_column:\
           \ str) -> str:\n  \"\"\"Generates the BP output's target column name.\"\"\
           \"\n  if forecasting_type == 'quantile':\n    return f'predicted_{target_column}.quantile_predictions'\n\
           \  return f'predicted_{target_column}.value'\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-predictions-column-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_predictions_column
@@ -6570,15 +6337,15 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef get_predictions_column(forecasting_type: str, target_column:\
           \ str) -> str:\n  \"\"\"Generates the BP output's target column name.\"\"\
           \"\n  if forecasting_type == 'quantile':\n    return f'predicted_{target_column}.quantile_predictions'\n\
           \  return f'predicted_{target_column}.value'\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-importer:
       importer:
         artifactUri:
           runtimeParameter: uri
         typeSchema:
           schemaTitle: system.Artifact
           schemaVersion: 0.0.1
@@ -7016,15 +6783,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-model-upload-2:
       container:
         args:
         - --type
         - UploadModel
         - --payload
         - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
@@ -7045,15 +6812,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-set-optional-inputs:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _set_optional_inputs
@@ -7108,15 +6875,15 @@
           \    raise ValueError(\n        'One of vertex_dataset, data_source_csv_filenames,'\n\
           \        ' data_source_bigquery_table_path must be specified'\n    )\n\n\
           \  return collections.namedtuple(\n      'Outputs',\n      [\n         \
           \ 'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
           \          'model_display_name',\n          'transformations',\n      ],\n\
           \  )(\n      data_source_csv_filenames,\n      data_source_bigquery_table_path,\n\
           \      model_display_name,\n      transformations,\n  )\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-split-materialized-data:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _split_materialized_data
@@ -7154,15 +6921,15 @@
           \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
           \ data source: {materialized_data_json}')\n\n  # we map indices to file\
           \ patterns based on the ordering of insertion order\n  # in our transform_data\
           \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
           \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
           \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
           \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
     exec-string-not-empty:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _string_not_empty
@@ -7220,15 +6987,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-table-to-uri-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - table_to_uri
@@ -7256,15 +7023,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-training-configurator-and-validator:
       container:
         args:
         - training_configurator_and_validator
         - '{"Concat": ["--instance_schema_path=", "{{$.inputs.artifacts[''instance_schema''].uri}}"]}'
         - '{"Concat": ["--training_schema_path=", "{{$.inputs.artifacts[''training_schema''].uri}}"]}'
         - '{"Concat": ["--dataset_stats_path=", "{{$.inputs.artifacts[''dataset_stats''].uri}}"]}'
@@ -7301,15 +7068,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
 pipelineInfo:
   description: The AutoML Forecasting pipeline.
   name: learn-to-learn-forecasting
 root:
   dag:
     outputs:
       artifacts:
```

## google_cloud_pipeline_components/preview/automl/forecasting/sequence_to_sequence_forecasting_pipeline.yaml

```diff
@@ -72,56 +72,49 @@
     executorLabel: exec-automl-forecasting-ensemble
     inputDefinitions:
       artifacts:
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         instance_schema_path:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The path to the instance schema,
-
-            describing the input data for the tf_model at serving time.'
+          description: The path to the instance schema, describing the input data
+            for the tf_model at serving time.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Region to run the job in.
           parameterType: STRING
         prediction_image_uri:
-          description: 'URI of the Docker image to be used as the
-
-            container for serving predictions. This URI must identify an image in
-
-            Artifact Registry or Container Registry.'
+          description: URI of the Docker image to be used as the container for serving
+            predictions. This URI must identify an image in Artifact Registry or Container
+            Registry.
           parameterType: STRING
         project:
           description: Project to run the job in.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage path to store the output.
           parameterType: STRING
@@ -153,69 +146,60 @@
           description: The explanation metadata used by Vertex online and batch explanations.
           parameterType: STRUCT
         explanation_parameters:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-forecasting-ensemble-2:
     executorLabel: exec-automl-forecasting-ensemble-2
     inputDefinitions:
       artifacts:
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         instance_schema_path:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The path to the instance schema,
-
-            describing the input data for the tf_model at serving time.'
+          description: The path to the instance schema, describing the input data
+            for the tf_model at serving time.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Region to run the job in.
           parameterType: STRING
         prediction_image_uri:
-          description: 'URI of the Docker image to be used as the
-
-            container for serving predictions. This URI must identify an image in
-
-            Artifact Registry or Container Registry.'
+          description: URI of the Docker image to be used as the container for serving
+            predictions. This URI must identify an image in Artifact Registry or Container
+            Registry.
           parameterType: STRING
         project:
           description: Project to run the job in.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage path to store the output.
           parameterType: STRING
@@ -247,209 +231,174 @@
           description: The explanation metadata used by Vertex online and batch explanations.
           parameterType: STRUCT
         explanation_parameters:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-forecasting-stage-1-tuner:
     executorLabel: exec-automl-forecasting-stage-1-tuner
     inputDefinitions:
       artifacts:
         materialized_eval_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         deadline_hours:
-          description: 'Number of hours the hyperparameter tuning should
-
-            run.'
+          description: Number of hours the hyperparameter tuning should run.
           parameterType: NUMBER_DOUBLE
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Location for running the hyperparameter tuning.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run hyperparameter tuning.
           parameterType: STRING
         reduce_search_space_mode:
           defaultValue: regular
-          description: 'The reduce search space mode. Possible
-
-            values: "regular" (default), "minimal", "full".'
+          description: 'The reduce search space mode. Possible values: "regular" (default),
+            "minimal", "full".'
           isOptional: true
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         study_spec_parameters_override:
           defaultValue: []
-          description: 'JSON study spec. E.g.,
-
-            [{"parameter_id": "activation","categorical_value_spec": {"values":
-
-            ["tanh"]}}]'
+          description: 'JSON study spec. E.g., [{"parameter_id": "activation","categorical_value_spec":
+            {"values": ["tanh"]}}]'
           isOptional: true
           parameterType: LIST
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component.
-
-            For more details, see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-forecasting-stage-2-tuner:
     executorLabel: exec-automl-forecasting-stage-2-tuner
     inputDefinitions:
       artifacts:
         materialized_eval_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The forecasting example gen
-
-            metadata.'
+          description: The forecasting example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input_path:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Path to the json of hyperparameter
-
-            tuning results to use when evaluating models.'
+          description: Path to the json of hyperparameter tuning results to use when
+            evaluating models.
       parameters:
         deadline_hours:
-          description: 'Number of hours the cross-validation trainer
-
-            should run.'
+          description: Number of hours the cross-validation trainer should run.
           parameterType: NUMBER_DOUBLE
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: 'Cloud region for running the component: us-central1).'
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model.'
+          description: Number of selected trials. The number of weak learners in the
+            final model.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run stage 2 tuner.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained (private) model artifact paths and their hyperparameters.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component.
-
-            For more details, see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-finalizer:
     executorLabel: exec-automl-tabular-finalizer
     inputDefinitions:
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
@@ -464,18 +413,16 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-calculate-training-parameters:
     executorLabel: exec-calculate-training-parameters
     inputDefinitions:
       parameters:
         fast_testing:
           defaultValue: false
@@ -961,14 +908,17 @@
                 componentInputParameter: pipelinechannel--dataflow_subnetwork
               dataflow_use_public_ips:
                 componentInputParameter: pipelinechannel--dataflow_use_public_ips
               dataflow_workers_num:
                 componentInputParameter: pipelinechannel--evaluation_dataflow_starting_num_workers
               encryption_spec_key_name:
                 componentInputParameter: pipelinechannel--encryption_spec_key_name
+              force_runner_mode:
+                runtimeValue:
+                  constant: Dataflow
               location:
                 componentInputParameter: pipelinechannel--location
               predictions_format:
                 runtimeValue:
                   constant: jsonl
               problem_type:
                 runtimeValue:
@@ -1674,14 +1624,17 @@
                 componentInputParameter: pipelinechannel--dataflow_subnetwork
               dataflow_use_public_ips:
                 componentInputParameter: pipelinechannel--dataflow_use_public_ips
               dataflow_workers_num:
                 componentInputParameter: pipelinechannel--evaluation_dataflow_starting_num_workers
               encryption_spec_key_name:
                 componentInputParameter: pipelinechannel--encryption_spec_key_name
+              force_runner_mode:
+                runtimeValue:
+                  constant: Dataflow
               location:
                 componentInputParameter: pipelinechannel--location
               predictions_format:
                 runtimeValue:
                   constant: jsonl
               problem_type:
                 runtimeValue:
@@ -2723,194 +2676,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -2918,42 +2835,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -2995,38 +2904,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -3051,116 +2953,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -3176,272 +3051,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -3467,19 +3290,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -3492,44 +3313,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-finalize-eval-quantile-parameters:
     executorLabel: exec-finalize-eval-quantile-parameters
     inputDefinitions:
       parameters:
         quantiles:
           isOptional: true
@@ -5539,224 +5352,178 @@
     executorLabel: exec-training-configurator-and-validator
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -5784,15 +5551,15 @@
         - '{{$.inputs.parameters[''location'']}}'
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"display_name": "automl-forecasting-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "encryption_spec": {"kms_key_name": "{{$.inputs.parameters[''encryption_spec_key_name'']}}"},
           "job_spec": {"worker_pool_specs": [{"replica_count": 1, "machine_spec":
-          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "args": ["forecasting_mp_ensemble", "--transform_output_path={{$.inputs.artifacts[''transform_output''].uri}}",
           "--error_file_path={{$.inputs.parameters[''root_dir'']}}/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb",
           "--metadata_path={{$.inputs.artifacts[''metadata''].uri}}", "--tuning_result_input_path={{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "--instance_baseline_path={{$.inputs.artifacts[''instance_baseline''].uri}}",
           "--instance_schema_path={{$.inputs.artifacts[''instance_schema_path''].uri}}",
           "--prediction_docker_uri={{$.inputs.parameters[''prediction_image_uri'']}}",
           "--model_relative_output_path={{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model",
@@ -5818,15 +5585,15 @@
         - '{{$.inputs.parameters[''location'']}}'
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"display_name": "automl-forecasting-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "encryption_spec": {"kms_key_name": "{{$.inputs.parameters[''encryption_spec_key_name'']}}"},
           "job_spec": {"worker_pool_specs": [{"replica_count": 1, "machine_spec":
-          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "args": ["forecasting_mp_ensemble", "--transform_output_path={{$.inputs.artifacts[''transform_output''].uri}}",
           "--error_file_path={{$.inputs.parameters[''root_dir'']}}/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb",
           "--metadata_path={{$.inputs.artifacts[''metadata''].uri}}", "--tuning_result_input_path={{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "--instance_baseline_path={{$.inputs.artifacts[''instance_baseline''].uri}}",
           "--instance_schema_path={{$.inputs.artifacts[''instance_schema_path''].uri}}",
           "--prediction_docker_uri={{$.inputs.parameters[''prediction_image_uri'']}}",
           "--model_relative_output_path={{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model",
@@ -5853,19 +5620,19 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-forecasting-stage-1-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"args\": [\"forecasting_mp_l2l_stage_1_tuner", "\", \"--region=",
           "{{$.inputs.parameters[''location'']}}", "\", \"--transform_output_path=",
           "{{$.inputs.artifacts[''transform_output''].uri}}", "\", \"--training_docker_uri=",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"--reduce_search_space_mode=", "{{$.inputs.parameters[''reduce_search_space_mode'']}}",
           "\", \"--component_id={{$.pipeline_task_uuid}}", "\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train",
           "\", \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--num_selected_trials=", "{{$.inputs.parameters[''num_selected_trials'']}}",
@@ -5896,19 +5663,19 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-forecasting-stage-2-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"args\": [\"forecasting_mp_l2l_stage_2_tuner", "\", \"--region=",
           "{{$.inputs.parameters[''location'']}}", "\", \"--transform_output_path=",
           "{{$.inputs.artifacts[''transform_output''].uri}}", "\", \"--training_docker_uri=",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"--component_id={{$.pipeline_task_uuid}}", "\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train",
           "\", \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--num_selected_trials=", "{{$.inputs.parameters[''num_selected_trials'']}}",
           "\", \"--lro_job_info=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro",
@@ -5939,15 +5706,15 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
@@ -6263,16 +6030,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -6281,15 +6048,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-finalize-eval-quantile-parameters:
       container:
         args:
         - --executor_input
@@ -6451,18 +6218,18 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef _get_prediction_image_uri(model_type: str) -> str:\n  \"\"\"\
           Returns the prediction image corresponding to the given model type.\"\"\"\
           \n  # Keys come from AutoMlTimeSeriesForecastingTrainSpec.\n  # The URIs\
           \ must be hardcoded without any breaks in the code so string\n  # replacement\
-          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20231029_0125',\n\
-          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20231029_0125',\n\
-          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20231029_0125',\n\
-          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20231029_0125',\n\
+          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20240119_0125',\n\
+          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20240119_0125',\n\
+          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20240119_0125',\n\
+          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20240119_0125',\n\
           \  }\n  if model_type not in images:\n    raise ValueError(\n        f'Invalid\
           \ forecasting model type: {model_type}. Valid options are: '\n        f'{images.keys()}.'\n\
           \    )\n  return images[model_type]\n\n"
         image: python:3.7
     exec-get-prediction-image-uri-2:
       container:
         args:
@@ -6487,18 +6254,18 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef _get_prediction_image_uri(model_type: str) -> str:\n  \"\"\"\
           Returns the prediction image corresponding to the given model type.\"\"\"\
           \n  # Keys come from AutoMlTimeSeriesForecastingTrainSpec.\n  # The URIs\
           \ must be hardcoded without any breaks in the code so string\n  # replacement\
-          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20231029_0125',\n\
-          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20231029_0125',\n\
-          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20231029_0125',\n\
-          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20231029_0125',\n\
+          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20240119_0125',\n\
+          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20240119_0125',\n\
+          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20240119_0125',\n\
+          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20240119_0125',\n\
           \  }\n  if model_type not in images:\n    raise ValueError(\n        f'Invalid\
           \ forecasting model type: {model_type}. Valid options are: '\n        f'{images.keys()}.'\n\
           \    )\n  return images[model_type]\n\n"
         image: python:3.7
     exec-get-predictions-column:
       container:
         args:
@@ -6523,15 +6290,15 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef get_predictions_column(forecasting_type: str, target_column:\
           \ str) -> str:\n  \"\"\"Generates the BP output's target column name.\"\"\
           \"\n  if forecasting_type == 'quantile':\n    return f'predicted_{target_column}.quantile_predictions'\n\
           \  return f'predicted_{target_column}.value'\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-predictions-column-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_predictions_column
@@ -6552,15 +6319,15 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef get_predictions_column(forecasting_type: str, target_column:\
           \ str) -> str:\n  \"\"\"Generates the BP output's target column name.\"\"\
           \"\n  if forecasting_type == 'quantile':\n    return f'predicted_{target_column}.quantile_predictions'\n\
           \  return f'predicted_{target_column}.value'\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-importer:
       importer:
         artifactUri:
           runtimeParameter: uri
         typeSchema:
           schemaTitle: system.Artifact
           schemaVersion: 0.0.1
@@ -6998,15 +6765,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-model-upload-2:
       container:
         args:
         - --type
         - UploadModel
         - --payload
         - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
@@ -7027,15 +6794,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-set-optional-inputs:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _set_optional_inputs
@@ -7090,15 +6857,15 @@
           \    raise ValueError(\n        'One of vertex_dataset, data_source_csv_filenames,'\n\
           \        ' data_source_bigquery_table_path must be specified'\n    )\n\n\
           \  return collections.namedtuple(\n      'Outputs',\n      [\n         \
           \ 'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
           \          'model_display_name',\n          'transformations',\n      ],\n\
           \  )(\n      data_source_csv_filenames,\n      data_source_bigquery_table_path,\n\
           \      model_display_name,\n      transformations,\n  )\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-split-materialized-data:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _split_materialized_data
@@ -7136,15 +6903,15 @@
           \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
           \ data source: {materialized_data_json}')\n\n  # we map indices to file\
           \ patterns based on the ordering of insertion order\n  # in our transform_data\
           \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
           \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
           \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
           \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
     exec-string-not-empty:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _string_not_empty
@@ -7202,15 +6969,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-table-to-uri-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - table_to_uri
@@ -7238,15 +7005,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-training-configurator-and-validator:
       container:
         args:
         - training_configurator_and_validator
         - '{"Concat": ["--instance_schema_path=", "{{$.inputs.artifacts[''instance_schema''].uri}}"]}'
         - '{"Concat": ["--training_schema_path=", "{{$.inputs.artifacts[''training_schema''].uri}}"]}'
         - '{"Concat": ["--dataset_stats_path=", "{{$.inputs.artifacts[''dataset_stats''].uri}}"]}'
@@ -7283,15 +7050,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
 pipelineInfo:
   description: The Sequence to Sequence (Seq2Seq) Forecasting pipeline.
   name: sequence-to-sequence-forecasting
 root:
   dag:
     outputs:
       artifacts:
```

## google_cloud_pipeline_components/preview/automl/forecasting/temporal_fusion_transformer_forecasting_pipeline.yaml

```diff
@@ -71,56 +71,49 @@
     executorLabel: exec-automl-forecasting-ensemble
     inputDefinitions:
       artifacts:
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         instance_schema_path:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The path to the instance schema,
-
-            describing the input data for the tf_model at serving time.'
+          description: The path to the instance schema, describing the input data
+            for the tf_model at serving time.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Region to run the job in.
           parameterType: STRING
         prediction_image_uri:
-          description: 'URI of the Docker image to be used as the
-
-            container for serving predictions. This URI must identify an image in
-
-            Artifact Registry or Container Registry.'
+          description: URI of the Docker image to be used as the container for serving
+            predictions. This URI must identify an image in Artifact Registry or Container
+            Registry.
           parameterType: STRING
         project:
           description: Project to run the job in.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage path to store the output.
           parameterType: STRING
@@ -152,69 +145,60 @@
           description: The explanation metadata used by Vertex online and batch explanations.
           parameterType: STRUCT
         explanation_parameters:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-forecasting-ensemble-2:
     executorLabel: exec-automl-forecasting-ensemble-2
     inputDefinitions:
       artifacts:
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         instance_schema_path:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The path to the instance schema,
-
-            describing the input data for the tf_model at serving time.'
+          description: The path to the instance schema, describing the input data
+            for the tf_model at serving time.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Region to run the job in.
           parameterType: STRING
         prediction_image_uri:
-          description: 'URI of the Docker image to be used as the
-
-            container for serving predictions. This URI must identify an image in
-
-            Artifact Registry or Container Registry.'
+          description: URI of the Docker image to be used as the container for serving
+            predictions. This URI must identify an image in Artifact Registry or Container
+            Registry.
           parameterType: STRING
         project:
           description: Project to run the job in.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage path to store the output.
           parameterType: STRING
@@ -246,209 +230,174 @@
           description: The explanation metadata used by Vertex online and batch explanations.
           parameterType: STRUCT
         explanation_parameters:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-forecasting-stage-1-tuner:
     executorLabel: exec-automl-forecasting-stage-1-tuner
     inputDefinitions:
       artifacts:
         materialized_eval_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         deadline_hours:
-          description: 'Number of hours the hyperparameter tuning should
-
-            run.'
+          description: Number of hours the hyperparameter tuning should run.
           parameterType: NUMBER_DOUBLE
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Location for running the hyperparameter tuning.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run hyperparameter tuning.
           parameterType: STRING
         reduce_search_space_mode:
           defaultValue: regular
-          description: 'The reduce search space mode. Possible
-
-            values: "regular" (default), "minimal", "full".'
+          description: 'The reduce search space mode. Possible values: "regular" (default),
+            "minimal", "full".'
           isOptional: true
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         study_spec_parameters_override:
           defaultValue: []
-          description: 'JSON study spec. E.g.,
-
-            [{"parameter_id": "activation","categorical_value_spec": {"values":
-
-            ["tanh"]}}]'
+          description: 'JSON study spec. E.g., [{"parameter_id": "activation","categorical_value_spec":
+            {"values": ["tanh"]}}]'
           isOptional: true
           parameterType: LIST
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component.
-
-            For more details, see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-forecasting-stage-2-tuner:
     executorLabel: exec-automl-forecasting-stage-2-tuner
     inputDefinitions:
       artifacts:
         materialized_eval_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The forecasting example gen
-
-            metadata.'
+          description: The forecasting example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input_path:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Path to the json of hyperparameter
-
-            tuning results to use when evaluating models.'
+          description: Path to the json of hyperparameter tuning results to use when
+            evaluating models.
       parameters:
         deadline_hours:
-          description: 'Number of hours the cross-validation trainer
-
-            should run.'
+          description: Number of hours the cross-validation trainer should run.
           parameterType: NUMBER_DOUBLE
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: 'Cloud region for running the component: us-central1).'
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model.'
+          description: Number of selected trials. The number of weak learners in the
+            final model.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run stage 2 tuner.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained (private) model artifact paths and their hyperparameters.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component.
-
-            For more details, see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-finalizer:
     executorLabel: exec-automl-tabular-finalizer
     inputDefinitions:
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
@@ -463,18 +412,16 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-calculate-training-parameters:
     executorLabel: exec-calculate-training-parameters
     inputDefinitions:
       parameters:
         fast_testing:
           defaultValue: false
@@ -960,14 +907,17 @@
                 componentInputParameter: pipelinechannel--dataflow_subnetwork
               dataflow_use_public_ips:
                 componentInputParameter: pipelinechannel--dataflow_use_public_ips
               dataflow_workers_num:
                 componentInputParameter: pipelinechannel--evaluation_dataflow_starting_num_workers
               encryption_spec_key_name:
                 componentInputParameter: pipelinechannel--encryption_spec_key_name
+              force_runner_mode:
+                runtimeValue:
+                  constant: Dataflow
               location:
                 componentInputParameter: pipelinechannel--location
               predictions_format:
                 runtimeValue:
                   constant: jsonl
               problem_type:
                 runtimeValue:
@@ -1673,14 +1623,17 @@
                 componentInputParameter: pipelinechannel--dataflow_subnetwork
               dataflow_use_public_ips:
                 componentInputParameter: pipelinechannel--dataflow_use_public_ips
               dataflow_workers_num:
                 componentInputParameter: pipelinechannel--evaluation_dataflow_starting_num_workers
               encryption_spec_key_name:
                 componentInputParameter: pipelinechannel--encryption_spec_key_name
+              force_runner_mode:
+                runtimeValue:
+                  constant: Dataflow
               location:
                 componentInputParameter: pipelinechannel--location
               predictions_format:
                 runtimeValue:
                   constant: jsonl
               problem_type:
                 runtimeValue:
@@ -2716,194 +2669,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -2911,42 +2828,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -2988,38 +2897,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -3044,116 +2946,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -3169,272 +3044,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -3460,19 +3283,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -3485,44 +3306,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-finalize-eval-quantile-parameters:
     executorLabel: exec-finalize-eval-quantile-parameters
     inputDefinitions:
       parameters:
         quantiles:
           isOptional: true
@@ -5532,224 +5345,178 @@
     executorLabel: exec-training-configurator-and-validator
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -5777,15 +5544,15 @@
         - '{{$.inputs.parameters[''location'']}}'
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"display_name": "automl-forecasting-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "encryption_spec": {"kms_key_name": "{{$.inputs.parameters[''encryption_spec_key_name'']}}"},
           "job_spec": {"worker_pool_specs": [{"replica_count": 1, "machine_spec":
-          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "args": ["forecasting_mp_ensemble", "--transform_output_path={{$.inputs.artifacts[''transform_output''].uri}}",
           "--error_file_path={{$.inputs.parameters[''root_dir'']}}/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb",
           "--metadata_path={{$.inputs.artifacts[''metadata''].uri}}", "--tuning_result_input_path={{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "--instance_baseline_path={{$.inputs.artifacts[''instance_baseline''].uri}}",
           "--instance_schema_path={{$.inputs.artifacts[''instance_schema_path''].uri}}",
           "--prediction_docker_uri={{$.inputs.parameters[''prediction_image_uri'']}}",
           "--model_relative_output_path={{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model",
@@ -5811,15 +5578,15 @@
         - '{{$.inputs.parameters[''location'']}}'
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"display_name": "automl-forecasting-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "encryption_spec": {"kms_key_name": "{{$.inputs.parameters[''encryption_spec_key_name'']}}"},
           "job_spec": {"worker_pool_specs": [{"replica_count": 1, "machine_spec":
-          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "args": ["forecasting_mp_ensemble", "--transform_output_path={{$.inputs.artifacts[''transform_output''].uri}}",
           "--error_file_path={{$.inputs.parameters[''root_dir'']}}/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb",
           "--metadata_path={{$.inputs.artifacts[''metadata''].uri}}", "--tuning_result_input_path={{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "--instance_baseline_path={{$.inputs.artifacts[''instance_baseline''].uri}}",
           "--instance_schema_path={{$.inputs.artifacts[''instance_schema_path''].uri}}",
           "--prediction_docker_uri={{$.inputs.parameters[''prediction_image_uri'']}}",
           "--model_relative_output_path={{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model",
@@ -5846,19 +5613,19 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-forecasting-stage-1-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"args\": [\"forecasting_mp_l2l_stage_1_tuner", "\", \"--region=",
           "{{$.inputs.parameters[''location'']}}", "\", \"--transform_output_path=",
           "{{$.inputs.artifacts[''transform_output''].uri}}", "\", \"--training_docker_uri=",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"--reduce_search_space_mode=", "{{$.inputs.parameters[''reduce_search_space_mode'']}}",
           "\", \"--component_id={{$.pipeline_task_uuid}}", "\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train",
           "\", \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--num_selected_trials=", "{{$.inputs.parameters[''num_selected_trials'']}}",
@@ -5889,19 +5656,19 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-forecasting-stage-2-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"args\": [\"forecasting_mp_l2l_stage_2_tuner", "\", \"--region=",
           "{{$.inputs.parameters[''location'']}}", "\", \"--transform_output_path=",
           "{{$.inputs.artifacts[''transform_output''].uri}}", "\", \"--training_docker_uri=",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"--component_id={{$.pipeline_task_uuid}}", "\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train",
           "\", \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--num_selected_trials=", "{{$.inputs.parameters[''num_selected_trials'']}}",
           "\", \"--lro_job_info=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro",
@@ -5932,15 +5699,15 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
@@ -6256,16 +6023,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -6274,15 +6041,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-finalize-eval-quantile-parameters:
       container:
         args:
         - --executor_input
@@ -6444,18 +6211,18 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef _get_prediction_image_uri(model_type: str) -> str:\n  \"\"\"\
           Returns the prediction image corresponding to the given model type.\"\"\"\
           \n  # Keys come from AutoMlTimeSeriesForecastingTrainSpec.\n  # The URIs\
           \ must be hardcoded without any breaks in the code so string\n  # replacement\
-          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20231029_0125',\n\
-          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20231029_0125',\n\
-          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20231029_0125',\n\
-          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20231029_0125',\n\
+          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20240119_0125',\n\
+          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20240119_0125',\n\
+          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20240119_0125',\n\
+          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20240119_0125',\n\
           \  }\n  if model_type not in images:\n    raise ValueError(\n        f'Invalid\
           \ forecasting model type: {model_type}. Valid options are: '\n        f'{images.keys()}.'\n\
           \    )\n  return images[model_type]\n\n"
         image: python:3.7
     exec-get-prediction-image-uri-2:
       container:
         args:
@@ -6480,18 +6247,18 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef _get_prediction_image_uri(model_type: str) -> str:\n  \"\"\"\
           Returns the prediction image corresponding to the given model type.\"\"\"\
           \n  # Keys come from AutoMlTimeSeriesForecastingTrainSpec.\n  # The URIs\
           \ must be hardcoded without any breaks in the code so string\n  # replacement\
-          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20231029_0125',\n\
-          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20231029_0125',\n\
-          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20231029_0125',\n\
-          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20231029_0125',\n\
+          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20240119_0125',\n\
+          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20240119_0125',\n\
+          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20240119_0125',\n\
+          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20240119_0125',\n\
           \  }\n  if model_type not in images:\n    raise ValueError(\n        f'Invalid\
           \ forecasting model type: {model_type}. Valid options are: '\n        f'{images.keys()}.'\n\
           \    )\n  return images[model_type]\n\n"
         image: python:3.7
     exec-get-predictions-column:
       container:
         args:
@@ -6516,15 +6283,15 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef get_predictions_column(forecasting_type: str, target_column:\
           \ str) -> str:\n  \"\"\"Generates the BP output's target column name.\"\"\
           \"\n  if forecasting_type == 'quantile':\n    return f'predicted_{target_column}.quantile_predictions'\n\
           \  return f'predicted_{target_column}.value'\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-predictions-column-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_predictions_column
@@ -6545,15 +6312,15 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef get_predictions_column(forecasting_type: str, target_column:\
           \ str) -> str:\n  \"\"\"Generates the BP output's target column name.\"\"\
           \"\n  if forecasting_type == 'quantile':\n    return f'predicted_{target_column}.quantile_predictions'\n\
           \  return f'predicted_{target_column}.value'\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-importer:
       importer:
         artifactUri:
           runtimeParameter: uri
         typeSchema:
           schemaTitle: system.Artifact
           schemaVersion: 0.0.1
@@ -6991,15 +6758,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-model-upload-2:
       container:
         args:
         - --type
         - UploadModel
         - --payload
         - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
@@ -7020,15 +6787,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-set-optional-inputs:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _set_optional_inputs
@@ -7083,15 +6850,15 @@
           \    raise ValueError(\n        'One of vertex_dataset, data_source_csv_filenames,'\n\
           \        ' data_source_bigquery_table_path must be specified'\n    )\n\n\
           \  return collections.namedtuple(\n      'Outputs',\n      [\n         \
           \ 'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
           \          'model_display_name',\n          'transformations',\n      ],\n\
           \  )(\n      data_source_csv_filenames,\n      data_source_bigquery_table_path,\n\
           \      model_display_name,\n      transformations,\n  )\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-split-materialized-data:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _split_materialized_data
@@ -7129,15 +6896,15 @@
           \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
           \ data source: {materialized_data_json}')\n\n  # we map indices to file\
           \ patterns based on the ordering of insertion order\n  # in our transform_data\
           \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
           \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
           \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
           \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
     exec-string-not-empty:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _string_not_empty
@@ -7195,15 +6962,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-table-to-uri-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - table_to_uri
@@ -7231,15 +6998,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-training-configurator-and-validator:
       container:
         args:
         - training_configurator_and_validator
         - '{"Concat": ["--instance_schema_path=", "{{$.inputs.artifacts[''instance_schema''].uri}}"]}'
         - '{"Concat": ["--training_schema_path=", "{{$.inputs.artifacts[''training_schema''].uri}}"]}'
         - '{"Concat": ["--dataset_stats_path=", "{{$.inputs.artifacts[''dataset_stats''].uri}}"]}'
@@ -7276,15 +7043,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
 pipelineInfo:
   description: The Temporal Fusion Transformer (TFT) Forecasting pipeline.
   name: temporal-fusion-transformer-forecasting
 root:
   dag:
     outputs:
       artifacts:
```

## google_cloud_pipeline_components/preview/automl/forecasting/time_series_dense_encoder_forecasting_pipeline.yaml

```diff
@@ -74,56 +74,49 @@
     executorLabel: exec-automl-forecasting-ensemble
     inputDefinitions:
       artifacts:
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         instance_schema_path:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The path to the instance schema,
-
-            describing the input data for the tf_model at serving time.'
+          description: The path to the instance schema, describing the input data
+            for the tf_model at serving time.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Region to run the job in.
           parameterType: STRING
         prediction_image_uri:
-          description: 'URI of the Docker image to be used as the
-
-            container for serving predictions. This URI must identify an image in
-
-            Artifact Registry or Container Registry.'
+          description: URI of the Docker image to be used as the container for serving
+            predictions. This URI must identify an image in Artifact Registry or Container
+            Registry.
           parameterType: STRING
         project:
           description: Project to run the job in.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage path to store the output.
           parameterType: STRING
@@ -155,69 +148,60 @@
           description: The explanation metadata used by Vertex online and batch explanations.
           parameterType: STRUCT
         explanation_parameters:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-forecasting-ensemble-2:
     executorLabel: exec-automl-forecasting-ensemble-2
     inputDefinitions:
       artifacts:
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         instance_schema_path:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The path to the instance schema,
-
-            describing the input data for the tf_model at serving time.'
+          description: The path to the instance schema, describing the input data
+            for the tf_model at serving time.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Region to run the job in.
           parameterType: STRING
         prediction_image_uri:
-          description: 'URI of the Docker image to be used as the
-
-            container for serving predictions. This URI must identify an image in
-
-            Artifact Registry or Container Registry.'
+          description: URI of the Docker image to be used as the container for serving
+            predictions. This URI must identify an image in Artifact Registry or Container
+            Registry.
           parameterType: STRING
         project:
           description: Project to run the job in.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage path to store the output.
           parameterType: STRING
@@ -249,209 +233,174 @@
           description: The explanation metadata used by Vertex online and batch explanations.
           parameterType: STRUCT
         explanation_parameters:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-forecasting-stage-1-tuner:
     executorLabel: exec-automl-forecasting-stage-1-tuner
     inputDefinitions:
       artifacts:
         materialized_eval_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         deadline_hours:
-          description: 'Number of hours the hyperparameter tuning should
-
-            run.'
+          description: Number of hours the hyperparameter tuning should run.
           parameterType: NUMBER_DOUBLE
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Location for running the hyperparameter tuning.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run hyperparameter tuning.
           parameterType: STRING
         reduce_search_space_mode:
           defaultValue: regular
-          description: 'The reduce search space mode. Possible
-
-            values: "regular" (default), "minimal", "full".'
+          description: 'The reduce search space mode. Possible values: "regular" (default),
+            "minimal", "full".'
           isOptional: true
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         study_spec_parameters_override:
           defaultValue: []
-          description: 'JSON study spec. E.g.,
-
-            [{"parameter_id": "activation","categorical_value_spec": {"values":
-
-            ["tanh"]}}]'
+          description: 'JSON study spec. E.g., [{"parameter_id": "activation","categorical_value_spec":
+            {"values": ["tanh"]}}]'
           isOptional: true
           parameterType: LIST
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component.
-
-            For more details, see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-forecasting-stage-2-tuner:
     executorLabel: exec-automl-forecasting-stage-2-tuner
     inputDefinitions:
       artifacts:
         materialized_eval_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The forecasting example gen
-
-            metadata.'
+          description: The forecasting example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input_path:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Path to the json of hyperparameter
-
-            tuning results to use when evaluating models.'
+          description: Path to the json of hyperparameter tuning results to use when
+            evaluating models.
       parameters:
         deadline_hours:
-          description: 'Number of hours the cross-validation trainer
-
-            should run.'
+          description: Number of hours the cross-validation trainer should run.
           parameterType: NUMBER_DOUBLE
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: 'Cloud region for running the component: us-central1).'
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model.'
+          description: Number of selected trials. The number of weak learners in the
+            final model.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run stage 2 tuner.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained (private) model artifact paths and their hyperparameters.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component.
-
-            For more details, see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-finalizer:
     executorLabel: exec-automl-tabular-finalizer
     inputDefinitions:
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
@@ -466,18 +415,16 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-calculate-training-parameters:
     executorLabel: exec-calculate-training-parameters
     inputDefinitions:
       parameters:
         fast_testing:
           defaultValue: false
@@ -967,14 +914,17 @@
                 componentInputParameter: pipelinechannel--dataflow_subnetwork
               dataflow_use_public_ips:
                 componentInputParameter: pipelinechannel--dataflow_use_public_ips
               dataflow_workers_num:
                 componentInputParameter: pipelinechannel--evaluation_dataflow_starting_num_workers
               encryption_spec_key_name:
                 componentInputParameter: pipelinechannel--encryption_spec_key_name
+              force_runner_mode:
+                runtimeValue:
+                  constant: Dataflow
               location:
                 componentInputParameter: pipelinechannel--location
               predictions_format:
                 runtimeValue:
                   constant: jsonl
               problem_type:
                 runtimeValue:
@@ -1685,14 +1635,17 @@
                 componentInputParameter: pipelinechannel--dataflow_subnetwork
               dataflow_use_public_ips:
                 componentInputParameter: pipelinechannel--dataflow_use_public_ips
               dataflow_workers_num:
                 componentInputParameter: pipelinechannel--evaluation_dataflow_starting_num_workers
               encryption_spec_key_name:
                 componentInputParameter: pipelinechannel--encryption_spec_key_name
+              force_runner_mode:
+                runtimeValue:
+                  constant: Dataflow
               location:
                 componentInputParameter: pipelinechannel--location
               predictions_format:
                 runtimeValue:
                   constant: jsonl
               problem_type:
                 runtimeValue:
@@ -2741,194 +2694,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -2936,42 +2853,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -3013,38 +2922,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -3069,116 +2971,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -3194,272 +3069,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -3485,19 +3308,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -3510,44 +3331,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-finalize-eval-quantile-parameters:
     executorLabel: exec-finalize-eval-quantile-parameters
     inputDefinitions:
       parameters:
         quantiles:
           isOptional: true
@@ -5557,224 +5370,178 @@
     executorLabel: exec-training-configurator-and-validator
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -5802,15 +5569,15 @@
         - '{{$.inputs.parameters[''location'']}}'
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"display_name": "automl-forecasting-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "encryption_spec": {"kms_key_name": "{{$.inputs.parameters[''encryption_spec_key_name'']}}"},
           "job_spec": {"worker_pool_specs": [{"replica_count": 1, "machine_spec":
-          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "args": ["forecasting_mp_ensemble", "--transform_output_path={{$.inputs.artifacts[''transform_output''].uri}}",
           "--error_file_path={{$.inputs.parameters[''root_dir'']}}/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb",
           "--metadata_path={{$.inputs.artifacts[''metadata''].uri}}", "--tuning_result_input_path={{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "--instance_baseline_path={{$.inputs.artifacts[''instance_baseline''].uri}}",
           "--instance_schema_path={{$.inputs.artifacts[''instance_schema_path''].uri}}",
           "--prediction_docker_uri={{$.inputs.parameters[''prediction_image_uri'']}}",
           "--model_relative_output_path={{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model",
@@ -5836,15 +5603,15 @@
         - '{{$.inputs.parameters[''location'']}}'
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"display_name": "automl-forecasting-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "encryption_spec": {"kms_key_name": "{{$.inputs.parameters[''encryption_spec_key_name'']}}"},
           "job_spec": {"worker_pool_specs": [{"replica_count": 1, "machine_spec":
-          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          {"machine_type": "n1-highmem-8"}, "container_spec": {"image_uri": "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "args": ["forecasting_mp_ensemble", "--transform_output_path={{$.inputs.artifacts[''transform_output''].uri}}",
           "--error_file_path={{$.inputs.parameters[''root_dir'']}}/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb",
           "--metadata_path={{$.inputs.artifacts[''metadata''].uri}}", "--tuning_result_input_path={{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "--instance_baseline_path={{$.inputs.artifacts[''instance_baseline''].uri}}",
           "--instance_schema_path={{$.inputs.artifacts[''instance_schema_path''].uri}}",
           "--prediction_docker_uri={{$.inputs.parameters[''prediction_image_uri'']}}",
           "--model_relative_output_path={{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model",
@@ -5871,19 +5638,19 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-forecasting-stage-1-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"args\": [\"forecasting_mp_l2l_stage_1_tuner", "\", \"--region=",
           "{{$.inputs.parameters[''location'']}}", "\", \"--transform_output_path=",
           "{{$.inputs.artifacts[''transform_output''].uri}}", "\", \"--training_docker_uri=",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"--reduce_search_space_mode=", "{{$.inputs.parameters[''reduce_search_space_mode'']}}",
           "\", \"--component_id={{$.pipeline_task_uuid}}", "\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train",
           "\", \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--num_selected_trials=", "{{$.inputs.parameters[''num_selected_trials'']}}",
@@ -5914,19 +5681,19 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-forecasting-stage-2-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"args\": [\"forecasting_mp_l2l_stage_2_tuner", "\", \"--region=",
           "{{$.inputs.parameters[''location'']}}", "\", \"--transform_output_path=",
           "{{$.inputs.artifacts[''transform_output''].uri}}", "\", \"--training_docker_uri=",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20231029_0125",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/forecasting-training:20240119_0125",
           "\", \"--component_id={{$.pipeline_task_uuid}}", "\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train",
           "\", \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--num_selected_trials=", "{{$.inputs.parameters[''num_selected_trials'']}}",
           "\", \"--lro_job_info=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro",
@@ -5957,15 +5724,15 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
@@ -6281,16 +6048,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -6299,15 +6066,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-finalize-eval-quantile-parameters:
       container:
         args:
         - --executor_input
@@ -6469,18 +6236,18 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef _get_prediction_image_uri(model_type: str) -> str:\n  \"\"\"\
           Returns the prediction image corresponding to the given model type.\"\"\"\
           \n  # Keys come from AutoMlTimeSeriesForecastingTrainSpec.\n  # The URIs\
           \ must be hardcoded without any breaks in the code so string\n  # replacement\
-          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20231029_0125',\n\
-          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20231029_0125',\n\
-          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20231029_0125',\n\
-          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20231029_0125',\n\
+          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20240119_0125',\n\
+          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20240119_0125',\n\
+          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20240119_0125',\n\
+          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20240119_0125',\n\
           \  }\n  if model_type not in images:\n    raise ValueError(\n        f'Invalid\
           \ forecasting model type: {model_type}. Valid options are: '\n        f'{images.keys()}.'\n\
           \    )\n  return images[model_type]\n\n"
         image: python:3.7
     exec-get-prediction-image-uri-2:
       container:
         args:
@@ -6505,18 +6272,18 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef _get_prediction_image_uri(model_type: str) -> str:\n  \"\"\"\
           Returns the prediction image corresponding to the given model type.\"\"\"\
           \n  # Keys come from AutoMlTimeSeriesForecastingTrainSpec.\n  # The URIs\
           \ must be hardcoded without any breaks in the code so string\n  # replacement\
-          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20231029_0125',\n\
-          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20231029_0125',\n\
-          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20231029_0125',\n\
-          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20231029_0125',\n\
+          \ will work correctly.\n  images = {\n      'l2l': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-l2l:20240119_0125',\n\
+          \      'seq2seq': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-seq2seq:20240119_0125',\n\
+          \      'tft': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tft:20240119_0125',\n\
+          \      'tide': 'us-docker.pkg.dev/vertex-ai/automl-tabular/forecasting-prediction-server-tide:20240119_0125',\n\
           \  }\n  if model_type not in images:\n    raise ValueError(\n        f'Invalid\
           \ forecasting model type: {model_type}. Valid options are: '\n        f'{images.keys()}.'\n\
           \    )\n  return images[model_type]\n\n"
         image: python:3.7
     exec-get-predictions-column:
       container:
         args:
@@ -6541,15 +6308,15 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef get_predictions_column(forecasting_type: str, target_column:\
           \ str) -> str:\n  \"\"\"Generates the BP output's target column name.\"\"\
           \"\n  if forecasting_type == 'quantile':\n    return f'predicted_{target_column}.quantile_predictions'\n\
           \  return f'predicted_{target_column}.value'\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-predictions-column-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_predictions_column
@@ -6570,15 +6337,15 @@
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef get_predictions_column(forecasting_type: str, target_column:\
           \ str) -> str:\n  \"\"\"Generates the BP output's target column name.\"\"\
           \"\n  if forecasting_type == 'quantile':\n    return f'predicted_{target_column}.quantile_predictions'\n\
           \  return f'predicted_{target_column}.value'\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-importer:
       importer:
         artifactUri:
           runtimeParameter: uri
         typeSchema:
           schemaTitle: system.Artifact
           schemaVersion: 0.0.1
@@ -7016,15 +6783,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-model-upload-2:
       container:
         args:
         - --type
         - UploadModel
         - --payload
         - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
@@ -7045,15 +6812,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-set-optional-inputs:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _set_optional_inputs
@@ -7108,15 +6875,15 @@
           \    raise ValueError(\n        'One of vertex_dataset, data_source_csv_filenames,'\n\
           \        ' data_source_bigquery_table_path must be specified'\n    )\n\n\
           \  return collections.namedtuple(\n      'Outputs',\n      [\n         \
           \ 'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
           \          'model_display_name',\n          'transformations',\n      ],\n\
           \  )(\n      data_source_csv_filenames,\n      data_source_bigquery_table_path,\n\
           \      model_display_name,\n      transformations,\n  )\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-split-materialized-data:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _split_materialized_data
@@ -7154,15 +6921,15 @@
           \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
           \ data source: {materialized_data_json}')\n\n  # we map indices to file\
           \ patterns based on the ordering of insertion order\n  # in our transform_data\
           \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
           \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
           \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
           \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
     exec-string-not-empty:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _string_not_empty
@@ -7220,15 +6987,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-table-to-uri-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - table_to_uri
@@ -7256,15 +7023,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-training-configurator-and-validator:
       container:
         args:
         - training_configurator_and_validator
         - '{"Concat": ["--instance_schema_path=", "{{$.inputs.artifacts[''instance_schema''].uri}}"]}'
         - '{"Concat": ["--training_schema_path=", "{{$.inputs.artifacts[''training_schema''].uri}}"]}'
         - '{"Concat": ["--dataset_stats_path=", "{{$.inputs.artifacts[''dataset_stats''].uri}}"]}'
@@ -7301,15 +7068,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
 pipelineInfo:
   description: The Timeseries Dense Encoder (TiDE) Forecasting pipeline.
   name: time-series-dense-encoder-forecasting
 root:
   dag:
     outputs:
       artifacts:
```

## google_cloud_pipeline_components/preview/automl/forecasting/utils.py

```diff
@@ -240,112 +240,80 @@
     model_description: Optional[str] = None,
     run_evaluation: bool = True,
     group_columns: Optional[List[str]] = None,
     group_total_weight: float = 0.0,
     temporal_total_weight: float = 0.0,
     group_temporal_total_weight: float = 0.0,
 ) -> Tuple[str, Dict[str, Any]]:
+  # fmt: off
   """Returns l2l_forecasting pipeline and formatted parameters.
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region that runs the pipeline components.
     root_dir: The root GCS directory for the pipeline components.
     target_column: The target column name.
-    optimization_objective: "minimize-rmse", "minimize-mae", "minimize-rmsle",
-      "minimize-rmspe", "minimize-wape-mae", "minimize-mape", or
-      "minimize-quantile-loss".
-    transformations: Dict mapping auto and/or type-resolutions to feature
-      columns. The supported types are: auto, categorical, numeric, text, and
-      timestamp.
-    train_budget_milli_node_hours: The train budget of creating this model,
-      expressed in milli node hours i.e. 1,000 value in this field means 1 node
-      hour.
+    optimization_objective: "minimize-rmse", "minimize-mae", "minimize-rmsle", "minimize-rmspe", "minimize-wape-mae", "minimize-mape", or "minimize-quantile-loss".
+    transformations: Dict mapping auto and/or type-resolutions to feature columns. The supported types are: auto, categorical, numeric, text, and timestamp.
+    train_budget_milli_node_hours: The train budget of creating this model, expressed in milli node hours i.e. 1,000 value in this field means 1 node hour.
     time_column: The column that indicates the time.
-    time_series_identifier_columns: The columns which distinguish different time
-      series.
-    time_series_identifier_column: [Deprecated] The column which distinguishes
-      different time series.
-    time_series_attribute_columns: The columns that are invariant across the
-      same time series.
-    available_at_forecast_columns: The columns that are available at the
-      forecast time.
-    unavailable_at_forecast_columns: The columns that are unavailable at the
-      forecast time.
+    time_series_identifier_columns: The columns which distinguish different time series.
+    time_series_identifier_column: [Deprecated] The column which distinguishes different time series.
+    time_series_attribute_columns: The columns that are invariant across the same time series.
+    available_at_forecast_columns: The columns that are available at the forecast time.
+    unavailable_at_forecast_columns: The columns that are unavailable at the forecast time.
     forecast_horizon: The length of the horizon.
     context_window: The length of the context window.
-    evaluated_examples_bigquery_path: The bigquery dataset to write the
-      predicted examples into for evaluation, in the format
-      `bq://project.dataset`.
+    evaluated_examples_bigquery_path: The bigquery dataset to write the predicted examples into for evaluation, in the format `bq://project.dataset`.
     window_predefined_column: The column that indicate the start of each window.
     window_stride_length: The stride length to generate the window.
     window_max_count: The maximum number of windows that will be generated.
-    holiday_regions: The geographical regions where the holiday effect is
-      applied in modeling.
+    holiday_regions: The geographical regions where the holiday effect is applied in modeling.
     stage_1_num_parallel_trials: Number of parallel trails for stage 1.
-    stage_1_tuning_result_artifact_uri: The stage 1 tuning result artifact GCS
-      URI.
+    stage_1_tuning_result_artifact_uri: The stage 1 tuning result artifact GCS URI.
     stage_2_num_parallel_trials: Number of parallel trails for stage 2.
     num_selected_trials: Number of selected trails.
-    data_source_csv_filenames: A string that represents a list of comma
-      separated CSV filenames.
-    data_source_bigquery_table_path: The BigQuery table path of format
-      bq://bq_project.bq_dataset.bq_table
+    data_source_csv_filenames: A string that represents a list of comma separated CSV filenames.
+    data_source_bigquery_table_path: The BigQuery table path of format bq://bq_project.bq_dataset.bq_table
     predefined_split_key: The predefined_split column name.
     training_fraction: The training fraction.
     validation_fraction: The validation fraction.
     test_fraction: The test fraction.
     weight_column: The weight column name.
     dataflow_service_account: The full service account name.
     dataflow_subnetwork: The dataflow subnetwork.
     dataflow_use_public_ips: `True` to enable dataflow public IPs.
-    feature_transform_engine_bigquery_staging_full_dataset_id: The full id of
-      the feature transform engine staging dataset.
-    feature_transform_engine_dataflow_machine_type: The dataflow machine type of
-      the feature transform engine.
-    feature_transform_engine_dataflow_max_num_workers: The max number of
-      dataflow workers of the feature transform engine.
-    feature_transform_engine_dataflow_disk_size_gb: The disk size of the
-      dataflow workers of the feature transform engine.
-    evaluation_batch_predict_machine_type: Machine type for the batch prediction
-      job in evaluation, such as 'n1-standard-16'.
-    evaluation_batch_predict_starting_replica_count: Number of replicas to use
-      in the batch prediction cluster at startup time.
-    evaluation_batch_predict_max_replica_count: The maximum count of replicas
-      the batch prediction job can scale to.
-    evaluation_dataflow_machine_type: Machine type for the dataflow job in
-      evaluation, such as 'n1-standard-16'.
+    feature_transform_engine_bigquery_staging_full_dataset_id: The full id of the feature transform engine staging dataset.
+    feature_transform_engine_dataflow_machine_type: The dataflow machine type of the feature transform engine.
+    feature_transform_engine_dataflow_max_num_workers: The max number of dataflow workers of the feature transform engine.
+    feature_transform_engine_dataflow_disk_size_gb: The disk size of the dataflow workers of the feature transform engine.
+    evaluation_batch_predict_machine_type: Machine type for the batch prediction job in evaluation, such as 'n1-standard-16'.
+    evaluation_batch_predict_starting_replica_count: Number of replicas to use in the batch prediction cluster at startup time.
+    evaluation_batch_predict_max_replica_count: The maximum count of replicas the batch prediction job can scale to.
+    evaluation_dataflow_machine_type: Machine type for the dataflow job in evaluation, such as 'n1-standard-16'.
     evaluation_dataflow_max_num_workers: Maximum number of dataflow workers.
     evaluation_dataflow_disk_size_gb: The disk space in GB for dataflow.
     study_spec_parameters_override: The list for overriding study spec.
-    stage_1_tuner_worker_pool_specs_override: The dictionary for overriding
-      stage 1 tuner worker pool spec.
-    stage_2_trainer_worker_pool_specs_override: The dictionary for overriding
-      stage 2 trainer worker pool spec.
-    enable_probabilistic_inference: If probabilistic inference is enabled, the
-      model will fit a distribution that captures the uncertainty of a
-      prediction. If quantiles are specified, then the quantiles of the
-      distribution are also returned.
-    quantiles: Quantiles to use for probabilistic inference. Up to 5 quantiles
-      are allowed of values between 0 and 1, exclusive. Represents the quantiles
-      to use for that objective. Quantiles must be unique.
+    stage_1_tuner_worker_pool_specs_override: The dictionary for overriding stage 1 tuner worker pool spec.
+    stage_2_trainer_worker_pool_specs_override: The dictionary for overriding stage 2 trainer worker pool spec.
+    enable_probabilistic_inference: If probabilistic inference is enabled, the model will fit a distribution that captures the uncertainty of a prediction. If quantiles are specified, then the quantiles of the distribution are also returned.
+    quantiles: Quantiles to use for probabilistic inference. Up to 5 quantiles are allowed of values between 0 and 1, exclusive. Represents the quantiles to use for that objective. Quantiles must be unique.
     encryption_spec_key_name: The KMS key name.
     model_display_name: Optional display name for model.
     model_description: Optional description.
     run_evaluation: `True` to evaluate the ensembled model on the test split.
-    group_columns: A list of time series attribute column names that define the
-      time series hierarchy.
-    group_total_weight: The weight of the loss for predictions aggregated over
-      time series in the same group.
-    temporal_total_weight: The weight of the loss for predictions aggregated
-      over the horizon for a single time series.
-    group_temporal_total_weight: The weight of the loss for predictions
-      aggregated over both the horizon and time series in the same hierarchy
-      group.
+    group_columns: A list of time series attribute column names that define the time series hierarchy.
+    group_total_weight: The weight of the loss for predictions aggregated over time series in the same group.
+    temporal_total_weight: The weight of the loss for predictions aggregated over the horizon for a single time series.
+    group_temporal_total_weight: The weight of the loss for predictions aggregated over both the horizon and time series in the same hierarchy group.
+
+  Returns:
+    Tuple of pipeline_definition_path and parameter_values.
   """
+  # fmt: on
   parameter_values = _get_base_forecasting_parameters(
       project=project,
       location=location,
       root_dir=root_dir,
       target_column=target_column,
       evaluated_examples_bigquery_path=evaluated_examples_bigquery_path,
       optimization_objective=optimization_objective,
@@ -466,113 +434,80 @@
     model_description: Optional[str] = None,
     run_evaluation: bool = True,
     group_columns: Optional[List[str]] = None,
     group_total_weight: float = 0.0,
     temporal_total_weight: float = 0.0,
     group_temporal_total_weight: float = 0.0,
 ) -> Tuple[str, Dict[str, Any]]:
+  # fmt: off
   """Returns timeseries_dense_encoder_forecasting pipeline and parameters.
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region that runs the pipeline components.
     root_dir: The root GCS directory for the pipeline components.
     target_column: The target column name.
-    optimization_objective: "minimize-rmse", "minimize-mae", "minimize-rmsle",
-      "minimize-rmspe", "minimize-wape-mae", "minimize-mape", or
-      "minimize-quantile-loss".
-    transformations: Dict mapping auto and/or type-resolutions to feature
-      columns. The supported types are: auto, categorical, numeric, text, and
-      timestamp.
-    train_budget_milli_node_hours: The train budget of creating this model,
-      expressed in milli node hours i.e. 1,000 value in this field means 1 node
-      hour.
+    optimization_objective: "minimize-rmse", "minimize-mae", "minimize-rmsle", "minimize-rmspe", "minimize-wape-mae", "minimize-mape", or "minimize-quantile-loss".
+    transformations: Dict mapping auto and/or type-resolutions to feature columns. The supported types are: auto, categorical, numeric, text, and timestamp.
+    train_budget_milli_node_hours: The train budget of creating this model, expressed in milli node hours i.e. 1,000 value in this field means 1 node hour.
     time_column: The column that indicates the time.
-    time_series_identifier_columns: The columns which distinguish different time
-      series.
-    time_series_identifier_column: [Deprecated] The column which distinguishes
-      different time series.
-    time_series_attribute_columns: The columns that are invariant across the
-      same time series.
-    available_at_forecast_columns: The columns that are available at the
-      forecast time.
-    unavailable_at_forecast_columns: The columns that are unavailable at the
-      forecast time.
+    time_series_identifier_columns: The columns which distinguish different time series.
+    time_series_identifier_column: [Deprecated] The column which distinguishes different time series.
+    time_series_attribute_columns: The columns that are invariant across the same time series.
+    available_at_forecast_columns: The columns that are available at the forecast time.
+    unavailable_at_forecast_columns: The columns that are unavailable at the forecast time.
     forecast_horizon: The length of the horizon.
     context_window: The length of the context window.
-    evaluated_examples_bigquery_path: The bigquery dataset to write the
-      predicted examples into for evaluation, in the format
-      `bq://project.dataset`.
+    evaluated_examples_bigquery_path: The bigquery dataset to write the predicted examples into for evaluation, in the format `bq://project.dataset`.
     window_predefined_column: The column that indicate the start of each window.
     window_stride_length: The stride length to generate the window.
     window_max_count: The maximum number of windows that will be generated.
-    holiday_regions: The geographical regions where the holiday effect is
-      applied in modeling.
+    holiday_regions: The geographical regions where the holiday effect is applied in modeling.
     stage_1_num_parallel_trials: Number of parallel trails for stage 1.
-    stage_1_tuning_result_artifact_uri: The stage 1 tuning result artifact GCS
-      URI.
+    stage_1_tuning_result_artifact_uri: The stage 1 tuning result artifact GCS URI.
     stage_2_num_parallel_trials: Number of parallel trails for stage 2.
     num_selected_trials: Number of selected trails.
-    data_source_csv_filenames: A string that represents a list of comma
-      separated CSV filenames.
-    data_source_bigquery_table_path: The BigQuery table path of format
-      bq://bq_project.bq_dataset.bq_table
+    data_source_csv_filenames: A string that represents a list of comma separated CSV filenames.
+    data_source_bigquery_table_path: The BigQuery table path of format bq://bq_project.bq_dataset.bq_table
     predefined_split_key: The predefined_split column name.
     training_fraction: The training fraction.
     validation_fraction: The validation fraction.
     test_fraction: The test fraction.
     weight_column: The weight column name.
     dataflow_service_account: The full service account name.
     dataflow_subnetwork: The dataflow subnetwork.
     dataflow_use_public_ips: `True` to enable dataflow public IPs.
-    feature_transform_engine_bigquery_staging_full_dataset_id: The full id of
-      the feature transform engine staging dataset.
-    feature_transform_engine_dataflow_machine_type: The dataflow machine type of
-      the feature transform engine.
-    feature_transform_engine_dataflow_max_num_workers: The max number of
-      dataflow workers of the feature transform engine.
-    feature_transform_engine_dataflow_disk_size_gb: The disk size of the
-      dataflow workers of the feature transform engine.
-    evaluation_batch_predict_machine_type: Machine type for the batch prediction
-      job in evaluation, such as 'n1-standard-16'.
-    evaluation_batch_predict_starting_replica_count: Number of replicas to use
-      in the batch prediction cluster at startup time.
-    evaluation_batch_predict_max_replica_count: The maximum count of replicas
-      the batch prediction job can scale to.
-    evaluation_dataflow_machine_type: Machine type for the dataflow job in
-      evaluation, such as 'n1-standard-16'.
+    feature_transform_engine_bigquery_staging_full_dataset_id: The full id of the feature transform engine staging dataset.
+    feature_transform_engine_dataflow_machine_type: The dataflow machine type of the feature transform engine.
+    feature_transform_engine_dataflow_max_num_workers: The max number of dataflow workers of the feature transform engine.
+    feature_transform_engine_dataflow_disk_size_gb: The disk size of the dataflow workers of the feature transform engine.
+    evaluation_batch_predict_machine_type: Machine type for the batch prediction job in evaluation, such as 'n1-standard-16'.
+    evaluation_batch_predict_starting_replica_count: Number of replicas to use in the batch prediction cluster at startup time.
+    evaluation_batch_predict_max_replica_count: The maximum count of replicas the batch prediction job can scale to.
+    evaluation_dataflow_machine_type: Machine type for the dataflow job in evaluation, such as 'n1-standard-16'.
     evaluation_dataflow_max_num_workers: Maximum number of dataflow workers.
     evaluation_dataflow_disk_size_gb: The disk space in GB for dataflow.
     study_spec_parameters_override: The list for overriding study spec.
-    stage_1_tuner_worker_pool_specs_override: The dictionary for overriding
-      stage 1 tuner worker pool spec.
-    stage_2_trainer_worker_pool_specs_override: The dictionary for overriding
-      stage 2 trainer worker pool spec.
-    enable_probabilistic_inference: If probabilistic inference is enabled, the
-      model will fit a distribution that captures the uncertainty of a
-      prediction. If quantiles are specified, then the quantiles of the
-      distribution are also returned.
-    quantiles: Quantiles to use for probabilistic inference. Up to 5 quantiles
-      are allowed of values between 0 and 1, exclusive. Represents the quantiles
-      to use for that objective. Quantiles must be unique.
+    stage_1_tuner_worker_pool_specs_override: The dictionary for overriding stage 1 tuner worker pool spec.
+    stage_2_trainer_worker_pool_specs_override: The dictionary for overriding stage 2 trainer worker pool spec.
+    enable_probabilistic_inference: If probabilistic inference is enabled, the model will fit a distribution that captures the uncertainty of a prediction. If quantiles are specified, then the quantiles of the distribution are also returned.
+    quantiles: Quantiles to use for probabilistic inference. Up to 5 quantiles are allowed of values between 0 and 1, exclusive. Represents the quantiles to use for that objective. Quantiles must be unique.
     encryption_spec_key_name: The KMS key name.
     model_display_name: Optional display name for model.
     model_description: Optional description.
     run_evaluation: `True` to evaluate the ensembled model on the test split.
-    group_columns: A list of time series attribute column names that define the
-      time series hierarchy.
-    group_total_weight: The weight of the loss for predictions aggregated over
-      time series in the same group.
-    temporal_total_weight: The weight of the loss for predictions aggregated
-      over the horizon for a single time series.
-    group_temporal_total_weight: The weight of the loss for predictions
-      aggregated over both the horizon and time series in the same hierarchy
-      group.
-  """
+    group_columns: A list of time series attribute column names that define the time series hierarchy.
+    group_total_weight: The weight of the loss for predictions aggregated over time series in the same group.
+    temporal_total_weight: The weight of the loss for predictions aggregated over the horizon for a single time series.
+    group_temporal_total_weight: The weight of the loss for predictions aggregated over both the horizon and time series in the same hierarchy group.
 
+  Returns:
+    Tuple of pipeline_definition_path and parameter_values.
+  """
+  # fmt: on
   parameter_values = _get_base_forecasting_parameters(
       project=project,
       location=location,
       root_dir=root_dir,
       target_column=target_column,
       evaluated_examples_bigquery_path=evaluated_examples_bigquery_path,
       optimization_objective=optimization_objective,
@@ -686,95 +621,73 @@
     stage_1_tuner_worker_pool_specs_override: Optional[Dict[str, Any]] = None,
     stage_2_trainer_worker_pool_specs_override: Optional[Dict[str, Any]] = None,
     encryption_spec_key_name: Optional[str] = None,
     model_display_name: Optional[str] = None,
     model_description: Optional[str] = None,
     run_evaluation: bool = True,
 ):
+  # fmt: off
   """Returns tft_forecasting pipeline and formatted parameters.
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region that runs the pipeline components.
     root_dir: The root GCS directory for the pipeline components.
     target_column: The target column name.
-    optimization_objective: "minimize-rmse", "minimize-mae", "minimize-rmsle",
-      "minimize-rmspe", "minimize-wape-mae", "minimize-mape", or
-      "minimize-quantile-loss".
-    transformations: Dict mapping auto and/or type-resolutions to feature
-      columns. The supported types are: auto, categorical, numeric, text, and
-      timestamp.
-    train_budget_milli_node_hours: The train budget of creating this model,
-      expressed in milli node hours i.e. 1,000 value in this field means 1 node
-      hour.
+    optimization_objective: "minimize-rmse", "minimize-mae", "minimize-rmsle", "minimize-rmspe", "minimize-wape-mae", "minimize-mape", or "minimize-quantile-loss".
+    transformations: Dict mapping auto and/or type-resolutions to feature columns. The supported types are: auto, categorical, numeric, text, and timestamp.
+    train_budget_milli_node_hours: The train budget of creating this model, expressed in milli node hours i.e. 1,000 value in this field means 1 node hour.
     time_column: The column that indicates the time.
-    time_series_identifier_columns: The columns which distinguish different time
-      series.
-    time_series_identifier_column: [Deprecated] The column which distinguishes
-      different time series.
-    time_series_attribute_columns: The columns that are invariant across the
-      same time series.
-    available_at_forecast_columns: The columns that are available at the
-      forecast time.
-    unavailable_at_forecast_columns: The columns that are unavailable at the
-      forecast time.
+    time_series_identifier_columns: The columns which distinguish different time series.
+    time_series_identifier_column: [Deprecated] The column which distinguishes different time series.
+    time_series_attribute_columns: The columns that are invariant across the same time series.
+    available_at_forecast_columns: The columns that are available at the forecast time.
+    unavailable_at_forecast_columns: The columns that are unavailable at the forecast time.
     forecast_horizon: The length of the horizon.
     context_window: The length of the context window.
-    evaluated_examples_bigquery_path: The bigquery dataset to write the
-      predicted examples into for evaluation, in the format
-      `bq://project.dataset`.
+    evaluated_examples_bigquery_path: The bigquery dataset to write the predicted examples into for evaluation, in the format `bq://project.dataset`.
     window_predefined_column: The column that indicate the start of each window.
     window_stride_length: The stride length to generate the window.
     window_max_count: The maximum number of windows that will be generated.
-    holiday_regions: The geographical regions where the holiday effect is
-      applied in modeling.
+    holiday_regions: The geographical regions where the holiday effect is applied in modeling.
     stage_1_num_parallel_trials: Number of parallel trails for stage 1.
-    stage_1_tuning_result_artifact_uri: The stage 1 tuning result artifact GCS
-      URI.
+    stage_1_tuning_result_artifact_uri: The stage 1 tuning result artifact GCS URI.
     stage_2_num_parallel_trials: Number of parallel trails for stage 2.
-    data_source_csv_filenames: A string that represents a list of comma
-      separated CSV filenames.
-    data_source_bigquery_table_path: The BigQuery table path of format
-      bq://bq_project.bq_dataset.bq_table
+    data_source_csv_filenames: A string that represents a list of comma separated CSV filenames.
+    data_source_bigquery_table_path: The BigQuery table path of format bq://bq_project.bq_dataset.bq_table
     predefined_split_key: The predefined_split column name.
     training_fraction: The training fraction.
     validation_fraction: The validation fraction.
     test_fraction: The test fraction.
     weight_column: The weight column name.
     dataflow_service_account: The full service account name.
     dataflow_subnetwork: The dataflow subnetwork.
     dataflow_use_public_ips: `True` to enable dataflow public IPs.
-    feature_transform_engine_bigquery_staging_full_dataset_id: The full id of
-      the feature transform engine staging dataset.
-    feature_transform_engine_dataflow_machine_type: The dataflow machine type of
-      the feature transform engine.
-    feature_transform_engine_dataflow_max_num_workers: The max number of
-      dataflow workers of the feature transform engine.
-    feature_transform_engine_dataflow_disk_size_gb: The disk size of the
-      dataflow workers of the feature transform engine.
-    evaluation_batch_predict_machine_type: Machine type for the batch prediction
-      job in evaluation, such as 'n1-standard-16'.
-    evaluation_batch_predict_starting_replica_count: Number of replicas to use
-      in the batch prediction cluster at startup time.
-    evaluation_batch_predict_max_replica_count: The maximum count of replicas
-      the batch prediction job can scale to.
-    evaluation_dataflow_machine_type: Machine type for the dataflow job in
-      evaluation, such as 'n1-standard-16'.
+    feature_transform_engine_bigquery_staging_full_dataset_id: The full id of the feature transform engine staging dataset.
+    feature_transform_engine_dataflow_machine_type: The dataflow machine type of the feature transform engine.
+    feature_transform_engine_dataflow_max_num_workers: The max number of dataflow workers of the feature transform engine.
+    feature_transform_engine_dataflow_disk_size_gb: The disk size of the dataflow workers of the feature transform engine.
+    evaluation_batch_predict_machine_type: Machine type for the batch prediction job in evaluation, such as 'n1-standard-16'.
+    evaluation_batch_predict_starting_replica_count: Number of replicas to use in the batch prediction cluster at startup time.
+    evaluation_batch_predict_max_replica_count: The maximum count of replicas the batch prediction job can scale to.
+    evaluation_dataflow_machine_type: Machine type for the dataflow job in evaluation, such as 'n1-standard-16'.
     evaluation_dataflow_max_num_workers: Maximum number of dataflow workers.
     evaluation_dataflow_disk_size_gb: The disk space in GB for dataflow.
     study_spec_parameters_override: The list for overriding study spec.
-    stage_1_tuner_worker_pool_specs_override: The dictionary for overriding
-      stage 1 tuner worker pool spec.
-    stage_2_trainer_worker_pool_specs_override: The dictionary for overriding
-      stage 2 trainer worker pool spec.
+    stage_1_tuner_worker_pool_specs_override: The dictionary for overriding stage 1 tuner worker pool spec.
+    stage_2_trainer_worker_pool_specs_override: The dictionary for overriding stage 2 trainer worker pool spec.
     encryption_spec_key_name: The KMS key name.
     model_display_name: Optional display name for model.
     model_description: Optional description.
     run_evaluation: `True` to evaluate the ensembled model on the test split.
+
+  Returns:
+    Tuple of pipeline_definition_path and parameter_values.
   """
+  # fmt: on
   # TFT should only have 1 selected trial to freeze the ensemble size at 1.
   excluded_parameters = _RETAIL_MODEL_DISABLED_OPTIONS.union({
       'num_selected_trials',
   })
   parameter_values = _get_base_forecasting_parameters(
       project=project,
       location=location,
@@ -887,97 +800,74 @@
     stage_1_tuner_worker_pool_specs_override: Optional[Dict[str, Any]] = None,
     stage_2_trainer_worker_pool_specs_override: Optional[Dict[str, Any]] = None,
     encryption_spec_key_name: Optional[str] = None,
     model_display_name: Optional[str] = None,
     model_description: Optional[str] = None,
     run_evaluation: bool = True,
 ):
+  # fmt: off
   """Returns seq2seq forecasting pipeline and formatted parameters.
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region that runs the pipeline components.
     root_dir: The root GCS directory for the pipeline components.
     target_column: The target column name.
-    optimization_objective: "minimize-rmse", "minimize-mae", "minimize-rmsle",
-      "minimize-rmspe", "minimize-wape-mae", "minimize-mape", or
-      "minimize-quantile-loss".
-    transformations: Dict mapping auto and/or type-resolutions to feature
-      columns. The supported types are: auto, categorical, numeric, text, and
-      timestamp.
-    train_budget_milli_node_hours: The train budget of creating this model,
-      expressed in milli node hours i.e. 1,000 value in this field means 1 node
-      hour.
+    optimization_objective: "minimize-rmse", "minimize-mae", "minimize-rmsle", "minimize-rmspe", "minimize-wape-mae", "minimize-mape", or "minimize-quantile-loss".
+    transformations: Dict mapping auto and/or type-resolutions to feature columns. The supported types are: auto, categorical, numeric, text, and timestamp.
+    train_budget_milli_node_hours: The train budget of creating this model, expressed in milli node hours i.e. 1,000 value in this field means 1 node hour.
     time_column: The column that indicates the time.
-    time_series_identifier_columns: The columns which distinguish different time
-      series.
-    time_series_identifier_column: [Deprecated] The column which distinguishes
-      different time series.
-    time_series_attribute_columns: The columns that are invariant across the
-      same time series.
-    available_at_forecast_columns: The columns that are available at the
-      forecast time.
-    unavailable_at_forecast_columns: The columns that are unavailable at the
-      forecast time.
+    time_series_identifier_columns: The columns which distinguish different time series.
+    time_series_identifier_column: [Deprecated] The column which distinguishes different time series.
+    time_series_attribute_columns: The columns that are invariant across the same time series.
+    available_at_forecast_columns: The columns that are available at the forecast time.
+    unavailable_at_forecast_columns: The columns that are unavailable at the forecast time.
     forecast_horizon: The length of the horizon.
     context_window: The length of the context window.
-    evaluated_examples_bigquery_path: The bigquery dataset to write the
-      predicted examples into for evaluation, in the format
-      `bq://project.dataset`.
+    evaluated_examples_bigquery_path: The bigquery dataset to write the predicted examples into for evaluation, in the format `bq://project.dataset`.
     window_predefined_column: The column that indicate the start of each window.
     window_stride_length: The stride length to generate the window.
     window_max_count: The maximum number of windows that will be generated.
-    holiday_regions: The geographical regions where the holiday effect is
-      applied in modeling.
+    holiday_regions: The geographical regions where the holiday effect is applied in modeling.
     stage_1_num_parallel_trials: Number of parallel trails for stage 1.
-    stage_1_tuning_result_artifact_uri: The stage 1 tuning result artifact GCS
-      URI.
+    stage_1_tuning_result_artifact_uri: The stage 1 tuning result artifact GCS URI.
     stage_2_num_parallel_trials: Number of parallel trails for stage 2.
     num_selected_trials: Number of selected trails.
-    data_source_csv_filenames: A string that represents a list of comma
-      separated CSV filenames.
-    data_source_bigquery_table_path: The BigQuery table path of format
-      bq://bq_project.bq_dataset.bq_table
+    data_source_csv_filenames: A string that represents a list of comma separated CSV filenames.
+    data_source_bigquery_table_path: The BigQuery table path of format bq://bq_project.bq_dataset.bq_table
     predefined_split_key: The predefined_split column name.
     training_fraction: The training fraction.
     validation_fraction: The validation fraction.
     test_fraction: The test fraction.
     weight_column: The weight column name.
     dataflow_service_account: The full service account name.
     dataflow_subnetwork: The dataflow subnetwork.
     dataflow_use_public_ips: `True` to enable dataflow public IPs.
-    feature_transform_engine_bigquery_staging_full_dataset_id: The full id of
-      the feature transform engine staging dataset.
-    feature_transform_engine_dataflow_machine_type: The dataflow machine type of
-      the feature transform engine.
-    feature_transform_engine_dataflow_max_num_workers: The max number of
-      dataflow workers of the feature transform engine.
-    feature_transform_engine_dataflow_disk_size_gb: The disk size of the
-      dataflow workers of the feature transform engine.
-    evaluation_batch_predict_machine_type: Machine type for the batch prediction
-      job in evaluation, such as 'n1-standard-16'.
-    evaluation_batch_predict_starting_replica_count: Number of replicas to use
-      in the batch prediction cluster at startup time.
-    evaluation_batch_predict_max_replica_count: The maximum count of replicas
-      the batch prediction job can scale to.
-    evaluation_dataflow_machine_type: Machine type for the dataflow job in
-      evaluation, such as 'n1-standard-16'.
+    feature_transform_engine_bigquery_staging_full_dataset_id: The full id of the feature transform engine staging dataset.
+    feature_transform_engine_dataflow_machine_type: The dataflow machine type of the feature transform engine.
+    feature_transform_engine_dataflow_max_num_workers: The max number of dataflow workers of the feature transform engine.
+    feature_transform_engine_dataflow_disk_size_gb: The disk size of the dataflow workers of the feature transform engine.
+    evaluation_batch_predict_machine_type: Machine type for the batch prediction job in evaluation, such as 'n1-standard-16'.
+    evaluation_batch_predict_starting_replica_count: Number of replicas to use in the batch prediction cluster at startup time.
+    evaluation_batch_predict_max_replica_count: The maximum count of replicas the batch prediction job can scale to.
+    evaluation_dataflow_machine_type: Machine type for the dataflow job in evaluation, such as 'n1-standard-16'.
     evaluation_dataflow_max_num_workers: Maximum number of dataflow workers.
     evaluation_dataflow_disk_size_gb: The disk space in GB for dataflow.
     study_spec_parameters_override: The list for overriding study spec.
-    stage_1_tuner_worker_pool_specs_override: The dictionary for overriding
-      stage 1 tuner worker pool spec.
-    stage_2_trainer_worker_pool_specs_override: The dictionary for overriding
-      stage 2 trainer worker pool spec.
+    stage_1_tuner_worker_pool_specs_override: The dictionary for overriding stage 1 tuner worker pool spec.
+    stage_2_trainer_worker_pool_specs_override: The dictionary for overriding stage 2 trainer worker pool spec.
     encryption_spec_key_name: The KMS key name.
     model_display_name: Optional display name for model.
     model_description: Optional description.
     run_evaluation: `True` to evaluate the ensembled model on the test split.
-  """
 
+  Returns:
+    Tuple of pipeline_definition_path and parameter_values.
+  """
+  # fmt: on
   parameter_values = _get_base_forecasting_parameters(
       project=project,
       location=location,
       root_dir=root_dir,
       target_column=target_column,
       evaluated_examples_bigquery_path=evaluated_examples_bigquery_path,
       optimization_objective=optimization_objective,
```

## google_cloud_pipeline_components/preview/automl/tabular/__init__.py

```diff
@@ -18,31 +18,43 @@
 
 from google_cloud_pipeline_components.preview.automl.tabular.auto_feature_engineering import automated_feature_engineering as AutoFeatureEngineeringOp
 from google_cloud_pipeline_components.preview.automl.tabular.distillation_stage_feature_transform_engine import distillation_stage_feature_transform_engine as DistillationStageFeatureTransformEngineOp
 from google_cloud_pipeline_components.preview.automl.tabular.feature_selection import tabular_feature_ranking_and_selection as FeatureSelectionOp
 from google_cloud_pipeline_components.preview.automl.tabular.feature_transform_engine import feature_transform_engine as FeatureTransformEngineOp
 from google_cloud_pipeline_components.preview.automl.tabular.tabnet_hyperparameter_tuning_job import tabnet_hyperparameter_tuning_job as TabNetHyperparameterTuningJobOp
 from google_cloud_pipeline_components.preview.automl.tabular.tabnet_trainer import tabnet_trainer as TabNetTrainerOp
+from google_cloud_pipeline_components.preview.automl.tabular.utils import get_tabnet_hyperparameter_tuning_job_pipeline_and_parameters
+from google_cloud_pipeline_components.preview.automl.tabular.utils import get_tabnet_trainer_pipeline_and_parameters
+from google_cloud_pipeline_components.preview.automl.tabular.utils import get_wide_and_deep_hyperparameter_tuning_job_pipeline_and_parameters
+from google_cloud_pipeline_components.preview.automl.tabular.utils import get_wide_and_deep_trainer_pipeline_and_parameters
+from google_cloud_pipeline_components.preview.automl.tabular.utils import get_xgboost_hyperparameter_tuning_job_pipeline_and_parameters
+from google_cloud_pipeline_components.preview.automl.tabular.utils import get_xgboost_trainer_pipeline_and_parameters
 from google_cloud_pipeline_components.preview.automl.tabular.wide_and_deep_hyperparameter_tuning_job import wide_and_deep_hyperparameter_tuning_job as WideAndDeepHyperparameterTuningJobOp
 from google_cloud_pipeline_components.preview.automl.tabular.wide_and_deep_trainer import wide_and_deep_trainer as WideAndDeepTrainerOp
 from google_cloud_pipeline_components.preview.automl.tabular.xgboost_hyperparameter_tuning_job import xgboost_hyperparameter_tuning_job as XGBoostHyperparameterTuningJobOp
 from google_cloud_pipeline_components.preview.automl.tabular.xgboost_trainer import xgboost_trainer as XGBoostTrainerOp
 from kfp import components
 
 __all__ = [
     'AutoFeatureEngineeringOp',
+    'DistillationStageFeatureTransformEngineOp',
     'FeatureSelectionOp',
-    'WideAndDeepHyperparameterTuningJobOp',
-    'WideAndDeepTrainerOp',
+    'FeatureTransformEngineOp',
     'TabNetHyperparameterTuningJobOp',
     'TabNetTrainerOp',
-    'FeatureTransformEngineOp',
-    'DistillationStageFeatureTransformEngineOp',
+    'WideAndDeepHyperparameterTuningJobOp',
+    'WideAndDeepTrainerOp',
     'XGBoostHyperparameterTuningJobOp',
     'XGBoostTrainerOp',
+    'get_tabnet_hyperparameter_tuning_job_pipeline_and_parameters',
+    'get_tabnet_trainer_pipeline_and_parameters',
+    'get_wide_and_deep_hyperparameter_tuning_job_pipeline_and_parameters',
+    'get_wide_and_deep_trainer_pipeline_and_parameters',
+    'get_xgboost_hyperparameter_tuning_job_pipeline_and_parameters',
+    'get_xgboost_trainer_pipeline_and_parameters',
 ]
 
 tabnet_trainer_pipeline = components.load_component_from_file(
     # Note, please don't name it as `component.yaml` which will conflict with
     # the generated file.
     os.path.join(os.path.dirname(__file__), 'tabnet_trainer_pipeline.yaml')
 )
```

## google_cloud_pipeline_components/preview/automl/tabular/auto_feature_engineering.py

```diff
@@ -61,15 +61,15 @@
                       f' "auto-feature-engineering-{dsl.PIPELINE_JOB_ID_PLACEHOLDER}-{dsl.PIPELINE_TASK_ID_PLACEHOLDER}",'
                   ),
                   (
                       '"job_spec": {"worker_pool_specs": [{"replica_count":'
                       ' 1, "machine_spec": {"machine_type": "n1-standard-16"},'
                       ' "container_spec": {"image_uri":"'
                   ),
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125',
                   '", "args": ["feature_engineering", "--project=', project,
                   '", "--location=', location, '", "--data_source_bigquery_table_path=',
                   data_source_bigquery_table_path,
                   '", "--target_column=',
                   target_column,
                   '", "--weight_column=',
                   weight_column,
```

## google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_feature_selection_pipeline.yaml

```diff
@@ -108,60 +108,51 @@
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_features:
           defaultValue: 0.0
-          description: 'Number of selected features. The number of
-
-            features to learn in the NN models.'
+          description: Number of selected features. The number of features to learn
+            in the NN models.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run Cross-validation trainer.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         execution_metrics:
           description: Core metrics in dictionary of component execution.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-cv-trainer-2:
     executorLabel: exec-automl-tabular-cv-trainer-2
     inputDefinitions:
       artifacts:
         materialized_cv_splits:
           artifactType:
@@ -196,117 +187,99 @@
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_features:
           defaultValue: 0.0
-          description: 'Number of selected features. The number of
-
-            features to learn in the NN models.'
+          description: Number of selected features. The number of features to learn
+            in the NN models.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run Cross-validation trainer.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         execution_metrics:
           description: Core metrics in dictionary of component execution.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-ensemble:
     executorLabel: exec-automl-tabular-ensemble
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The schema of the dataset.
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
         warmup_data:
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
-          description: 'The warm up data. Ensemble component will save the
-
-            warm up data together with the model artifact, used to warm up the model
-
-            when prediction server starts.'
+          description: The warm up data. Ensemble component will save the warm up
+            data together with the model artifact, used to warm up the model when
+            prediction server starts.
           isOptional: true
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         export_additional_model_without_custom_ops:
           defaultValue: false
-          description: 'True if export
-
-            an additional model without custom TF operators to the
-
-            `model_without_custom_ops` output.'
+          description: True if export an additional model without custom TF operators
+            to the `model_without_custom_ops` output.
           isOptional: true
           parameterType: BOOLEAN
         location:
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         project:
           description: Project to run Cross-validation trainer.
@@ -344,75 +317,64 @@
         explanation_metadata:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         explanation_parameters:
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-ensemble-2:
     executorLabel: exec-automl-tabular-ensemble-2
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The schema of the dataset.
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
         warmup_data:
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
-          description: 'The warm up data. Ensemble component will save the
-
-            warm up data together with the model artifact, used to warm up the model
-
-            when prediction server starts.'
+          description: The warm up data. Ensemble component will save the warm up
+            data together with the model artifact, used to warm up the model when
+            prediction server starts.
           isOptional: true
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         export_additional_model_without_custom_ops:
           defaultValue: false
-          description: 'True if export
-
-            an additional model without custom TF operators to the
-
-            `model_without_custom_ops` output.'
+          description: True if export an additional model without custom TF operators
+            to the `model_without_custom_ops` output.
           isOptional: true
           parameterType: BOOLEAN
         location:
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         project:
           description: Project to run Cross-validation trainer.
@@ -450,75 +412,64 @@
         explanation_metadata:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         explanation_parameters:
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-ensemble-3:
     executorLabel: exec-automl-tabular-ensemble-3
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The schema of the dataset.
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
         warmup_data:
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
-          description: 'The warm up data. Ensemble component will save the
-
-            warm up data together with the model artifact, used to warm up the model
-
-            when prediction server starts.'
+          description: The warm up data. Ensemble component will save the warm up
+            data together with the model artifact, used to warm up the model when
+            prediction server starts.
           isOptional: true
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         export_additional_model_without_custom_ops:
           defaultValue: false
-          description: 'True if export
-
-            an additional model without custom TF operators to the
-
-            `model_without_custom_ops` output.'
+          description: True if export an additional model without custom TF operators
+            to the `model_without_custom_ops` output.
           isOptional: true
           parameterType: BOOLEAN
         location:
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         project:
           description: Project to run Cross-validation trainer.
@@ -556,18 +507,16 @@
         explanation_metadata:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         explanation_parameters:
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-finalizer:
     executorLabel: exec-automl-tabular-finalizer
     inputDefinitions:
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
@@ -582,52 +531,44 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-infra-validator:
     executorLabel: exec-automl-tabular-infra-validator
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-automl-tabular-infra-validator-2:
     executorLabel: exec-automl-tabular-infra-validator-2
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-automl-tabular-infra-validator-3:
     executorLabel: exec-automl-tabular-infra-validator-3
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-automl-tabular-stage-1-tuner:
     executorLabel: exec-automl-tabular-stage-1-tuner
     inputDefinitions:
       artifacts:
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
@@ -638,38 +579,32 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         deadline_hours:
-          description: 'Number of hours the cross-validation trainer
-
-            should run.'
+          description: Number of hours the cross-validation trainer should run.
           parameterType: NUMBER_DOUBLE
         disable_early_stopping:
           defaultValue: false
-          description: 'True if disable early stopping. Default
-
-            value is false.'
+          description: True if disable early stopping. Default value is false.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
@@ -677,87 +612,72 @@
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_features:
           defaultValue: 0.0
-          description: 'Number of selected features. The number of
-
-            features to learn in the NN models.'
+          description: Number of selected features. The number of features to learn
+            in the NN models.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run Cross-validation trainer.
           parameterType: STRING
         reduce_search_space_mode:
           defaultValue: regular
-          description: 'The reduce search space mode. Possible
-
-            values: "regular" (default), "minimal", "full".'
+          description: 'The reduce search space mode. Possible values: "regular" (default),
+            "minimal", "full".'
           isOptional: true
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distillation:
           defaultValue: false
-          description: 'True if in distillation mode. The default value
-
-            is false.'
+          description: True if in distillation mode. The default value is false.
           isOptional: true
           parameterType: BOOLEAN
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         study_spec_parameters_override:
           defaultValue: []
-          description: 'JSON study spec. E.g.,
-
-            [{"parameter_id": "model_type","categorical_value_spec": {"values":
-
-            ["nn"]}}]'
+          description: 'JSON study spec. E.g., [{"parameter_id": "model_type","categorical_value_spec":
+            {"values": ["nn"]}}]'
           isOptional: true
           parameterType: LIST
         tune_feature_selection_rate:
           defaultValue: false
           isOptional: true
           parameterType: BOOLEAN
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         execution_metrics:
           description: Core metrics in dictionary of component execution.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-stage-1-tuner-2:
     executorLabel: exec-automl-tabular-stage-1-tuner-2
     inputDefinitions:
       artifacts:
         feature_ranking:
           artifactType:
@@ -769,38 +689,32 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         deadline_hours:
-          description: 'Number of hours the cross-validation trainer
-
-            should run.'
+          description: Number of hours the cross-validation trainer should run.
           parameterType: NUMBER_DOUBLE
         disable_early_stopping:
           defaultValue: false
-          description: 'True if disable early stopping. Default
-
-            value is false.'
+          description: True if disable early stopping. Default value is false.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
@@ -808,87 +722,72 @@
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_features:
           defaultValue: 0.0
-          description: 'Number of selected features. The number of
-
-            features to learn in the NN models.'
+          description: Number of selected features. The number of features to learn
+            in the NN models.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run Cross-validation trainer.
           parameterType: STRING
         reduce_search_space_mode:
           defaultValue: regular
-          description: 'The reduce search space mode. Possible
-
-            values: "regular" (default), "minimal", "full".'
+          description: 'The reduce search space mode. Possible values: "regular" (default),
+            "minimal", "full".'
           isOptional: true
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distillation:
           defaultValue: false
-          description: 'True if in distillation mode. The default value
-
-            is false.'
+          description: True if in distillation mode. The default value is false.
           isOptional: true
           parameterType: BOOLEAN
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         study_spec_parameters_override:
           defaultValue: []
-          description: 'JSON study spec. E.g.,
-
-            [{"parameter_id": "model_type","categorical_value_spec": {"values":
-
-            ["nn"]}}]'
+          description: 'JSON study spec. E.g., [{"parameter_id": "model_type","categorical_value_spec":
+            {"values": ["nn"]}}]'
           isOptional: true
           parameterType: LIST
         tune_feature_selection_rate:
           defaultValue: false
           isOptional: true
           parameterType: BOOLEAN
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         execution_metrics:
           description: Core metrics in dictionary of component execution.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-transform:
     executorLabel: exec-automl-tabular-transform
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
@@ -914,54 +813,44 @@
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
           description: The train split.
       parameters:
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            dataflow jobs.'
+          description: Custom service account to run dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More
-
-            details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
@@ -998,18 +887,16 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-transform-2:
     executorLabel: exec-automl-tabular-transform-2
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
@@ -1035,54 +922,44 @@
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
           description: The train split.
       parameters:
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            dataflow jobs.'
+          description: Custom service account to run dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More
-
-            details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
@@ -1119,18 +996,16 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-bool-identity:
     executorLabel: exec-bool-identity
     inputDefinitions:
       parameters:
         value:
           description: Boolean value to return
@@ -8407,81 +8282,62 @@
           parameterType: STRING
         binary_classification:
           defaultValue: 'false'
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            dataflow jobs.'
+          description: Custom service account to run dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More
-
-            details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
-          description: 'Customer-managed encryption key.
-
-            If this is set, then all resources will be encrypted with the provided
-
-            encryption key. data_source(Dataset): The input dataset artifact which
-
-            references csv, BigQuery, or TF Records. target_column_name(str): Target
-
-            column name of the input dataset.'
+          description: 'Customer-managed encryption key. If this is set, then all
+            resources will be encrypted with the provided encryption key. data_source(Dataset):
+            The input dataset artifact which references csv, BigQuery, or TF Records.
+            target_column_name(str): Target column name of the input dataset.'
           isOptional: true
           parameterType: STRING
         location:
-          description: 'Location for running the feature selection. If not set,
-
-            default to us-central1.'
+          description: Location for running the feature selection. If not set, default
+            to us-central1.
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'number of features to select by the
-
-            algorithm. If not set, default to 1000.'
+          description: number of features to select by the algorithm. If not set,
+            default to 1000.
           isOptional: true
           parameterType: NUMBER_INTEGER
         prediction_type:
           defaultValue: unknown
           isOptional: true
           parameterType: STRING
         project:
@@ -8502,19 +8358,16 @@
         selected_features:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: A json array of selected feature names.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component.
-
-            For more details, see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-tabular-stats-and-example-gen:
     executorLabel: exec-tabular-stats-and-example-gen
     inputDefinitions:
       parameters:
         additional_experiments:
           defaultValue: ''
@@ -8530,138 +8383,112 @@
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            dataflow jobs.'
+          description: Custom service account to run dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More
-
-            details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         enable_probabilistic_inference:
           defaultValue: false
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
-          description: 'Location for running dataset statistics and example
-
-            generation.'
+          description: Location for running dataset statistics and example generation.
           parameterType: STRING
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         predefined_split_key:
           defaultValue: ''
           isOptional: true
           parameterType: STRING
         prediction_type:
-          description: 'The prediction type. Supported values:
-
-            "classification", "regression".'
+          description: 'The prediction type. Supported values: "classification", "regression".'
           parameterType: STRING
         project:
-          description: 'Project to run dataset statistics and example
-
-            generation.'
+          description: Project to run dataset statistics and example generation.
           parameterType: STRING
         quantiles:
           defaultValue: []
           isOptional: true
           parameterType: LIST
         request_type:
           defaultValue: COLUMN_STATS_ONLY
           isOptional: true
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distillation:
           defaultValue: false
-          description: 'True if in distillation mode. The default value
-
-            is false.'
+          description: True if in distillation mode. The default value is false.
           isOptional: true
           parameterType: BOOLEAN
         stratified_split_key:
           defaultValue: ''
           isOptional: true
           parameterType: STRING
         target_column_name:
@@ -8676,29 +8503,22 @@
           isOptional: true
           parameterType: STRING
         training_fraction:
           defaultValue: -1.0
           isOptional: true
           parameterType: NUMBER_DOUBLE
         transformations:
-          description: 'Quote escaped JSON string for transformations. Each
-
-            transformation will apply transform function to given input column. And
-
-            the result will be used for training. When creating transformation for
-
-            BigQuery Struct column, the column should be flattened using "." as the
-
-            delimiter.'
+          description: Quote escaped JSON string for transformations. Each transformation
+            will apply transform function to given input column. And the result will
+            be used for training. When creating transformation for BigQuery Struct
+            column, the column should be flattened using "." as the delimiter.
           parameterType: STRING
         transformations_path:
           defaultValue: ''
-          description: 'Path to a GCS file containing JSON
-
-            string for transformations.'
+          description: Path to a GCS file containing JSON string for transformations.
           isOptional: true
           parameterType: STRING
         validation_fraction:
           defaultValue: -1.0
           isOptional: true
           parameterType: NUMBER_DOUBLE
         weight_column_name:
@@ -8744,18 +8564,16 @@
             schemaVersion: 0.0.1
           description: The train split.
       parameters:
         downsampled_test_split_json:
           description: The downsampled test split JSON object.
           parameterType: LIST
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         test_split_json:
           description: The test split JSON object.
           parameterType: LIST
   comp-write-bp-result-path:
     executorLabel: exec-write-bp-result-path
     inputDefinitions:
@@ -8800,17 +8618,17 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-cv-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"l2l_cv_tuner\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
-          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125",
+          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125",
           "\", \"--component_id={{$.pipeline_task_uuid}}\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--valid_trials_completed_threshold=0.7\", \"--num_selected_trials=",
           "{{$.inputs.parameters[''num_selected_trials'']}}", "\", \"--num_selected_features=",
@@ -8843,17 +8661,17 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-cv-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"l2l_cv_tuner\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
-          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125",
+          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125",
           "\", \"--component_id={{$.pipeline_task_uuid}}\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--valid_trials_completed_threshold=0.7\", \"--num_selected_trials=",
           "{{$.inputs.parameters[''num_selected_trials'']}}", "\", \"--num_selected_features=",
@@ -8886,27 +8704,27 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-highmem-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"ensemble\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--model_output_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model\",
           \"--custom_model_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/custom_model\", \"--error_file_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--export_custom_model=", "{{$.inputs.parameters[''export_additional_model_without_custom_ops'']}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--dataset_schema_path=", "{{$.inputs.artifacts[''dataset_schema''].uri}}",
           "\", \"--tuning_result_input_path=", "{{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "\", \"--instance_baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--warmup_data=", "{{$.inputs.artifacts[''warmup_data''].uri}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--model_path=", "{{$.outputs.artifacts[''model''].uri}}", "\", \"--custom_model_path=",
           "{{$.outputs.artifacts[''model_without_custom_ops''].uri}}", "\", \"--explanation_metadata_path=",
           "{{$.outputs.parameters[''explanation_metadata''].output_file}}", ",", "{{$.outputs.artifacts[''explanation_metadata_artifact''].uri}}",
           "\", \"--explanation_parameters_path=", "{{$.outputs.parameters[''explanation_parameters''].output_file}}",
           "\", \"--model_architecture_path=", "{{$.outputs.artifacts[''model_architecture''].uri}}",
           "\", \"--use_json=true\", \"--executor_input={{$.json_escape[1]}}\"]}}]}}"]}'
         command:
@@ -8927,27 +8745,27 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-highmem-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"ensemble\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--model_output_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model\",
           \"--custom_model_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/custom_model\", \"--error_file_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--export_custom_model=", "{{$.inputs.parameters[''export_additional_model_without_custom_ops'']}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--dataset_schema_path=", "{{$.inputs.artifacts[''dataset_schema''].uri}}",
           "\", \"--tuning_result_input_path=", "{{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "\", \"--instance_baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--warmup_data=", "{{$.inputs.artifacts[''warmup_data''].uri}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--model_path=", "{{$.outputs.artifacts[''model''].uri}}", "\", \"--custom_model_path=",
           "{{$.outputs.artifacts[''model_without_custom_ops''].uri}}", "\", \"--explanation_metadata_path=",
           "{{$.outputs.parameters[''explanation_metadata''].output_file}}", ",", "{{$.outputs.artifacts[''explanation_metadata_artifact''].uri}}",
           "\", \"--explanation_parameters_path=", "{{$.outputs.parameters[''explanation_parameters''].output_file}}",
           "\", \"--model_architecture_path=", "{{$.outputs.artifacts[''model_architecture''].uri}}",
           "\", \"--use_json=true\", \"--executor_input={{$.json_escape[1]}}\"]}}]}}"]}'
         command:
@@ -8968,27 +8786,27 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-highmem-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"ensemble\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--model_output_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model\",
           \"--custom_model_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/custom_model\", \"--error_file_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--export_custom_model=", "{{$.inputs.parameters[''export_additional_model_without_custom_ops'']}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--dataset_schema_path=", "{{$.inputs.artifacts[''dataset_schema''].uri}}",
           "\", \"--tuning_result_input_path=", "{{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "\", \"--instance_baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--warmup_data=", "{{$.inputs.artifacts[''warmup_data''].uri}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--model_path=", "{{$.outputs.artifacts[''model''].uri}}", "\", \"--custom_model_path=",
           "{{$.outputs.artifacts[''model_without_custom_ops''].uri}}", "\", \"--explanation_metadata_path=",
           "{{$.outputs.parameters[''explanation_metadata''].output_file}}", ",", "{{$.outputs.artifacts[''explanation_metadata_artifact''].uri}}",
           "\", \"--explanation_parameters_path=", "{{$.outputs.parameters[''explanation_parameters''].output_file}}",
           "\", \"--model_architecture_path=", "{{$.outputs.artifacts[''model_architecture''].uri}}",
           "\", \"--use_json=true\", \"--executor_input={{$.json_escape[1]}}\"]}}]}}"]}'
         command:
@@ -9009,48 +8827,48 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
         - google_cloud_pipeline_components.container.v1.custom_job.launcher
         image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.44
     exec-automl-tabular-infra-validator:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-automl-tabular-infra-validator-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-automl-tabular-infra-validator-3:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-automl-tabular-stage-1-tuner:
       container:
         args:
         - --type
@@ -9062,17 +8880,17 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-stage-1-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"l2l_stage_1_tuner\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
-          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125",
+          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125",
           "\", \"--feature_selection_result_path=", "{{$.inputs.artifacts[''feature_ranking''].uri}}",
           "\", \"--disable_early_stopping=", "{{$.inputs.parameters[''disable_early_stopping'']}}",
           "\", \"--tune_feature_selection_rate=", "{{$.inputs.parameters[''tune_feature_selection_rate'']}}",
           "\", \"--reduce_search_space_mode=", "{{$.inputs.parameters[''reduce_search_space_mode'']}}",
           "\", \"--component_id={{$.pipeline_task_uuid}}\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
@@ -9109,17 +8927,17 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-stage-1-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"l2l_stage_1_tuner\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
-          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125",
+          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125",
           "\", \"--feature_selection_result_path=", "{{$.inputs.artifacts[''feature_ranking''].uri}}",
           "\", \"--disable_early_stopping=", "{{$.inputs.parameters[''disable_early_stopping'']}}",
           "\", \"--tune_feature_selection_rate=", "{{$.inputs.parameters[''tune_feature_selection_rate'']}}",
           "\", \"--reduce_search_space_mode=", "{{$.inputs.parameters[''reduce_search_space_mode'']}}",
           "\", \"--component_id={{$.pipeline_task_uuid}}\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
@@ -9156,15 +8974,15 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-transform-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"transform\", \"--is_mp=true\", \"--transform_output_artifact_path=",
           "{{$.outputs.artifacts[''transform_output''].uri}}", "\", \"--transform_output_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/transform\",
           \"--materialized_splits_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/transform_materialized\",
           \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\", \"--dataset_schema_path=",
           "{{$.inputs.artifacts[''dataset_schema''].uri}}", "\", \"--train_split=",
@@ -9177,15 +8995,15 @@
           "\", \"--job_name=automl-tabular-transform-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "\", \"--dataflow_project=", "{{$.inputs.parameters[''project'']}}", "\",
           \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging\",
           \"--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp\",
           \"--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}",
           "\", \"--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}",
-          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125",
+          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125",
           "\", \"--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}",
           "\", \"--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}",
           "\", \"--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}",
           "\", \"--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\", \"--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}",
           "\", \"--lro_job_info=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\",
           \"--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}",
@@ -9208,15 +9026,15 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-transform-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"transform\", \"--is_mp=true\", \"--transform_output_artifact_path=",
           "{{$.outputs.artifacts[''transform_output''].uri}}", "\", \"--transform_output_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/transform\",
           \"--materialized_splits_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/transform_materialized\",
           \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\", \"--dataset_schema_path=",
           "{{$.inputs.artifacts[''dataset_schema''].uri}}", "\", \"--train_split=",
@@ -9229,15 +9047,15 @@
           "\", \"--job_name=automl-tabular-transform-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "\", \"--dataflow_project=", "{{$.inputs.parameters[''project'']}}", "\",
           \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging\",
           \"--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp\",
           \"--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}",
           "\", \"--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}",
-          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125",
+          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125",
           "\", \"--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}",
           "\", \"--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}",
           "\", \"--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}",
           "\", \"--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\", \"--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}",
           "\", \"--lro_job_info=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\",
           \"--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}",
@@ -10495,15 +10313,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-model-upload-2:
       container:
         args:
         - --type
         - UploadModel
         - --payload
         - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
@@ -10524,15 +10342,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-model-upload-3:
       container:
         args:
         - --type
         - UploadModel
         - --payload
         - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
@@ -10553,15 +10371,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-purge-unused-features:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _purge_unused_features
@@ -10724,28 +10542,28 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"tabular-feature-selection-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"feature_selection\", \"--data_source=", "{{$.inputs.artifacts[''data_source''].uri}}",
           "\", \"--target_column=", "{{$.inputs.parameters[''target_column_name'']}}",
           "\", \"--prediction_type=", "{{$.inputs.parameters[''prediction_type'']}}",
           "\", \"--binary_classification=", "{{$.inputs.parameters[''binary_classification'']}}",
           "\", \"--algorithm=", "{{$.inputs.parameters[''algorithm'']}}", "\", \"--feature_selection_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/feature_selection/\",
           \"--job_name=tabular-feature-selection-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "\", \"--dataflow_project=", "{{$.inputs.parameters[''project'']}}", "\",
           \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging\",
           \"--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp\",
           \"--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}",
-          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125",
+          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125",
           "\", \"--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}",
           "\", \"--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}",
           "\", \"--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}",
           "\", \"--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}",
           "\", \"--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}",
           "\", \"--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\", \"--max_selected_features=", "{{$.inputs.parameters[''max_selected_features'']}}",
@@ -10770,15 +10588,15 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"tabular-stats-and-example-gen-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"stats_generator\",", "\"--train_spec={\\\"prediction_type\\\":
           \\\"", "{{$.inputs.parameters[''prediction_type'']}}", "\\\", \\\"target_column\\\":
           \\\"", "{{$.inputs.parameters[''target_column_name'']}}", "\\\", \\\"optimization_objective\\\":
           \\\"", "{{$.inputs.parameters[''optimization_objective'']}}", "\\\", \\\"weight_column_name\\\":
           \\\"", "{{$.inputs.parameters[''weight_column_name'']}}", "\\\", \\\"transformations\\\":
           ", "{{$.inputs.parameters[''transformations'']}}", ", \\\"quantiles\\\":
           ", "{{$.inputs.parameters[''quantiles'']}}", ", \\\"enable_probabilistic_inference\\\":
@@ -10803,15 +10621,15 @@
           "\", \"--dataset_schema_path=", "{{$.outputs.artifacts[''dataset_schema''].uri}}",
           "\", \"--job_name=tabular-stats-and-example-gen-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "\", \"--dataflow_project=", "{{$.inputs.parameters[''project'']}}", "\",
           \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging\",
           \"--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp\",
           \"--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}",
-          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125",
+          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125",
           "\", \"--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}",
           "\", \"--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}",
           "\", \"--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\", \"--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}",
           "\", \"--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}",
           "\", \"--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}",
           "\", \"--is_distill=", "{{$.inputs.parameters[''run_distillation'']}}",
```

## google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_v2_pipeline.yaml

```diff
@@ -116,60 +116,51 @@
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_features:
           defaultValue: 0.0
-          description: 'Number of selected features. The number of
-
-            features to learn in the NN models.'
+          description: Number of selected features. The number of features to learn
+            in the NN models.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run Cross-validation trainer.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         execution_metrics:
           description: Core metrics in dictionary of component execution.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-cv-trainer-2:
     executorLabel: exec-automl-tabular-cv-trainer-2
     inputDefinitions:
       artifacts:
         materialized_cv_splits:
           artifactType:
@@ -204,117 +195,99 @@
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_features:
           defaultValue: 0.0
-          description: 'Number of selected features. The number of
-
-            features to learn in the NN models.'
+          description: Number of selected features. The number of features to learn
+            in the NN models.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run Cross-validation trainer.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         execution_metrics:
           description: Core metrics in dictionary of component execution.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-ensemble:
     executorLabel: exec-automl-tabular-ensemble
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The schema of the dataset.
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
         warmup_data:
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
-          description: 'The warm up data. Ensemble component will save the
-
-            warm up data together with the model artifact, used to warm up the model
-
-            when prediction server starts.'
+          description: The warm up data. Ensemble component will save the warm up
+            data together with the model artifact, used to warm up the model when
+            prediction server starts.
           isOptional: true
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         export_additional_model_without_custom_ops:
           defaultValue: false
-          description: 'True if export
-
-            an additional model without custom TF operators to the
-
-            `model_without_custom_ops` output.'
+          description: True if export an additional model without custom TF operators
+            to the `model_without_custom_ops` output.
           isOptional: true
           parameterType: BOOLEAN
         location:
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         project:
           description: Project to run Cross-validation trainer.
@@ -352,75 +325,64 @@
         explanation_metadata:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         explanation_parameters:
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-ensemble-2:
     executorLabel: exec-automl-tabular-ensemble-2
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The schema of the dataset.
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
         warmup_data:
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
-          description: 'The warm up data. Ensemble component will save the
-
-            warm up data together with the model artifact, used to warm up the model
-
-            when prediction server starts.'
+          description: The warm up data. Ensemble component will save the warm up
+            data together with the model artifact, used to warm up the model when
+            prediction server starts.
           isOptional: true
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         export_additional_model_without_custom_ops:
           defaultValue: false
-          description: 'True if export
-
-            an additional model without custom TF operators to the
-
-            `model_without_custom_ops` output.'
+          description: True if export an additional model without custom TF operators
+            to the `model_without_custom_ops` output.
           isOptional: true
           parameterType: BOOLEAN
         location:
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         project:
           description: Project to run Cross-validation trainer.
@@ -458,75 +420,64 @@
         explanation_metadata:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         explanation_parameters:
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-ensemble-3:
     executorLabel: exec-automl-tabular-ensemble-3
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The schema of the dataset.
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
         warmup_data:
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
-          description: 'The warm up data. Ensemble component will save the
-
-            warm up data together with the model artifact, used to warm up the model
-
-            when prediction server starts.'
+          description: The warm up data. Ensemble component will save the warm up
+            data together with the model artifact, used to warm up the model when
+            prediction server starts.
           isOptional: true
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         export_additional_model_without_custom_ops:
           defaultValue: false
-          description: 'True if export
-
-            an additional model without custom TF operators to the
-
-            `model_without_custom_ops` output.'
+          description: True if export an additional model without custom TF operators
+            to the `model_without_custom_ops` output.
           isOptional: true
           parameterType: BOOLEAN
         location:
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         project:
           description: Project to run Cross-validation trainer.
@@ -564,18 +515,16 @@
         explanation_metadata:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         explanation_parameters:
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-finalizer:
     executorLabel: exec-automl-tabular-finalizer
     inputDefinitions:
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
@@ -590,52 +539,44 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-infra-validator:
     executorLabel: exec-automl-tabular-infra-validator
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-automl-tabular-infra-validator-2:
     executorLabel: exec-automl-tabular-infra-validator-2
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-automl-tabular-infra-validator-3:
     executorLabel: exec-automl-tabular-infra-validator-3
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-automl-tabular-stage-1-tuner:
     executorLabel: exec-automl-tabular-stage-1-tuner
     inputDefinitions:
       artifacts:
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
@@ -646,38 +587,32 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         deadline_hours:
-          description: 'Number of hours the cross-validation trainer
-
-            should run.'
+          description: Number of hours the cross-validation trainer should run.
           parameterType: NUMBER_DOUBLE
         disable_early_stopping:
           defaultValue: false
-          description: 'True if disable early stopping. Default
-
-            value is false.'
+          description: True if disable early stopping. Default value is false.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
@@ -685,87 +620,72 @@
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_features:
           defaultValue: 0.0
-          description: 'Number of selected features. The number of
-
-            features to learn in the NN models.'
+          description: Number of selected features. The number of features to learn
+            in the NN models.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run Cross-validation trainer.
           parameterType: STRING
         reduce_search_space_mode:
           defaultValue: regular
-          description: 'The reduce search space mode. Possible
-
-            values: "regular" (default), "minimal", "full".'
+          description: 'The reduce search space mode. Possible values: "regular" (default),
+            "minimal", "full".'
           isOptional: true
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distillation:
           defaultValue: false
-          description: 'True if in distillation mode. The default value
-
-            is false.'
+          description: True if in distillation mode. The default value is false.
           isOptional: true
           parameterType: BOOLEAN
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         study_spec_parameters_override:
           defaultValue: []
-          description: 'JSON study spec. E.g.,
-
-            [{"parameter_id": "model_type","categorical_value_spec": {"values":
-
-            ["nn"]}}]'
+          description: 'JSON study spec. E.g., [{"parameter_id": "model_type","categorical_value_spec":
+            {"values": ["nn"]}}]'
           isOptional: true
           parameterType: LIST
         tune_feature_selection_rate:
           defaultValue: false
           isOptional: true
           parameterType: BOOLEAN
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         execution_metrics:
           description: Core metrics in dictionary of component execution.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-stage-1-tuner-2:
     executorLabel: exec-automl-tabular-stage-1-tuner-2
     inputDefinitions:
       artifacts:
         feature_ranking:
           artifactType:
@@ -777,38 +697,32 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         deadline_hours:
-          description: 'Number of hours the cross-validation trainer
-
-            should run.'
+          description: Number of hours the cross-validation trainer should run.
           parameterType: NUMBER_DOUBLE
         disable_early_stopping:
           defaultValue: false
-          description: 'True if disable early stopping. Default
-
-            value is false.'
+          description: True if disable early stopping. Default value is false.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
@@ -816,87 +730,72 @@
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_features:
           defaultValue: 0.0
-          description: 'Number of selected features. The number of
-
-            features to learn in the NN models.'
+          description: Number of selected features. The number of features to learn
+            in the NN models.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run Cross-validation trainer.
           parameterType: STRING
         reduce_search_space_mode:
           defaultValue: regular
-          description: 'The reduce search space mode. Possible
-
-            values: "regular" (default), "minimal", "full".'
+          description: 'The reduce search space mode. Possible values: "regular" (default),
+            "minimal", "full".'
           isOptional: true
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distillation:
           defaultValue: false
-          description: 'True if in distillation mode. The default value
-
-            is false.'
+          description: True if in distillation mode. The default value is false.
           isOptional: true
           parameterType: BOOLEAN
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         study_spec_parameters_override:
           defaultValue: []
-          description: 'JSON study spec. E.g.,
-
-            [{"parameter_id": "model_type","categorical_value_spec": {"values":
-
-            ["nn"]}}]'
+          description: 'JSON study spec. E.g., [{"parameter_id": "model_type","categorical_value_spec":
+            {"values": ["nn"]}}]'
           isOptional: true
           parameterType: LIST
         tune_feature_selection_rate:
           defaultValue: false
           isOptional: true
           parameterType: BOOLEAN
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         execution_metrics:
           description: Core metrics in dictionary of component execution.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-bool-identity:
     executorLabel: exec-bool-identity
     inputDefinitions:
       parameters:
         value:
           description: Boolean value to return
@@ -3784,115 +3683,89 @@
       parameters:
         autodetect_csv_schema:
           defaultValue: false
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            ''projectId.datasetId'' format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            ''vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}''.
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in 'projectId.datasetId' format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called 'vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}'.  All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         bigquery_train_full_table_uri:
-          description: 'BigQuery full table id for our
-
-            train split output by pre-distillation FTE with soft target included.'
+          description: BigQuery full table id for our train split output by pre-distillation
+            FTE with soft target included.
           parameterType: STRING
         bigquery_validate_full_table_uri:
-          description: 'BigQuery full table id for our
-
-            validation split output by pre-distillation FTE with soft target
-
-            included.'
+          description: BigQuery full table id for our validation split output by pre-distillation
+            FTE with soft target included.
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         prediction_type:
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         target_column:
           description: Target column of input data.
           parameterType: STRING
         transform_config_path:
-          description: 'Path to the transform config output by the
-
-            pre-distillation FTE component.'
+          description: Path to the transform config output by the pre-distillation
+            FTE component.
           parameterType: STRING
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
     outputDefinitions:
@@ -3905,19 +3778,16 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component.
-
-            For more details, see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component.  For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-exit-handler-1:
     dag:
       outputs:
         artifacts:
           feature-attribution-2-feature_attributions:
             artifactSelectors:
@@ -4772,194 +4642,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -4967,42 +4801,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -5044,38 +4870,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -5100,116 +4919,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -5225,272 +5017,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -5516,19 +5256,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -5541,44 +5279,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-get-bigquery-destination-output-uri:
     executorLabel: exec-get-bigquery-destination-output-uri
     inputDefinitions:
       parameters:
         bigquery_source_input_uri:
           parameterType: STRING
@@ -9323,224 +9053,178 @@
     executorLabel: exec-training-configurator-and-validator
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -9559,224 +9243,178 @@
     executorLabel: exec-training-configurator-and-validator-2
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -9805,17 +9443,17 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-cv-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"l2l_cv_tuner\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
-          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125",
+          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125",
           "\", \"--component_id={{$.pipeline_task_uuid}}\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--valid_trials_completed_threshold=0.7\", \"--num_selected_trials=",
           "{{$.inputs.parameters[''num_selected_trials'']}}", "\", \"--num_selected_features=",
@@ -9848,17 +9486,17 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-cv-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"l2l_cv_tuner\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
-          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125",
+          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125",
           "\", \"--component_id={{$.pipeline_task_uuid}}\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--valid_trials_completed_threshold=0.7\", \"--num_selected_trials=",
           "{{$.inputs.parameters[''num_selected_trials'']}}", "\", \"--num_selected_features=",
@@ -9891,27 +9529,27 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-highmem-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"ensemble\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--model_output_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model\",
           \"--custom_model_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/custom_model\", \"--error_file_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--export_custom_model=", "{{$.inputs.parameters[''export_additional_model_without_custom_ops'']}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--dataset_schema_path=", "{{$.inputs.artifacts[''dataset_schema''].uri}}",
           "\", \"--tuning_result_input_path=", "{{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "\", \"--instance_baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--warmup_data=", "{{$.inputs.artifacts[''warmup_data''].uri}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--model_path=", "{{$.outputs.artifacts[''model''].uri}}", "\", \"--custom_model_path=",
           "{{$.outputs.artifacts[''model_without_custom_ops''].uri}}", "\", \"--explanation_metadata_path=",
           "{{$.outputs.parameters[''explanation_metadata''].output_file}}", ",", "{{$.outputs.artifacts[''explanation_metadata_artifact''].uri}}",
           "\", \"--explanation_parameters_path=", "{{$.outputs.parameters[''explanation_parameters''].output_file}}",
           "\", \"--model_architecture_path=", "{{$.outputs.artifacts[''model_architecture''].uri}}",
           "\", \"--use_json=true\", \"--executor_input={{$.json_escape[1]}}\"]}}]}}"]}'
         command:
@@ -9932,27 +9570,27 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-highmem-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"ensemble\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--model_output_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model\",
           \"--custom_model_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/custom_model\", \"--error_file_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--export_custom_model=", "{{$.inputs.parameters[''export_additional_model_without_custom_ops'']}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--dataset_schema_path=", "{{$.inputs.artifacts[''dataset_schema''].uri}}",
           "\", \"--tuning_result_input_path=", "{{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "\", \"--instance_baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--warmup_data=", "{{$.inputs.artifacts[''warmup_data''].uri}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--model_path=", "{{$.outputs.artifacts[''model''].uri}}", "\", \"--custom_model_path=",
           "{{$.outputs.artifacts[''model_without_custom_ops''].uri}}", "\", \"--explanation_metadata_path=",
           "{{$.outputs.parameters[''explanation_metadata''].output_file}}", ",", "{{$.outputs.artifacts[''explanation_metadata_artifact''].uri}}",
           "\", \"--explanation_parameters_path=", "{{$.outputs.parameters[''explanation_parameters''].output_file}}",
           "\", \"--model_architecture_path=", "{{$.outputs.artifacts[''model_architecture''].uri}}",
           "\", \"--use_json=true\", \"--executor_input={{$.json_escape[1]}}\"]}}]}}"]}'
         command:
@@ -9973,27 +9611,27 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-highmem-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"ensemble\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--model_output_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model\",
           \"--custom_model_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/custom_model\", \"--error_file_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--export_custom_model=", "{{$.inputs.parameters[''export_additional_model_without_custom_ops'']}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--dataset_schema_path=", "{{$.inputs.artifacts[''dataset_schema''].uri}}",
           "\", \"--tuning_result_input_path=", "{{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "\", \"--instance_baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--warmup_data=", "{{$.inputs.artifacts[''warmup_data''].uri}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--model_path=", "{{$.outputs.artifacts[''model''].uri}}", "\", \"--custom_model_path=",
           "{{$.outputs.artifacts[''model_without_custom_ops''].uri}}", "\", \"--explanation_metadata_path=",
           "{{$.outputs.parameters[''explanation_metadata''].output_file}}", ",", "{{$.outputs.artifacts[''explanation_metadata_artifact''].uri}}",
           "\", \"--explanation_parameters_path=", "{{$.outputs.parameters[''explanation_parameters''].output_file}}",
           "\", \"--model_architecture_path=", "{{$.outputs.artifacts[''model_architecture''].uri}}",
           "\", \"--use_json=true\", \"--executor_input={{$.json_escape[1]}}\"]}}]}}"]}'
         command:
@@ -10014,48 +9652,48 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
         - google_cloud_pipeline_components.container.v1.custom_job.launcher
         image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.44
     exec-automl-tabular-infra-validator:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-automl-tabular-infra-validator-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-automl-tabular-infra-validator-3:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-automl-tabular-stage-1-tuner:
       container:
         args:
         - --type
@@ -10067,17 +9705,17 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-stage-1-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"l2l_stage_1_tuner\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
-          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125",
+          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125",
           "\", \"--feature_selection_result_path=", "{{$.inputs.artifacts[''feature_ranking''].uri}}",
           "\", \"--disable_early_stopping=", "{{$.inputs.parameters[''disable_early_stopping'']}}",
           "\", \"--tune_feature_selection_rate=", "{{$.inputs.parameters[''tune_feature_selection_rate'']}}",
           "\", \"--reduce_search_space_mode=", "{{$.inputs.parameters[''reduce_search_space_mode'']}}",
           "\", \"--component_id={{$.pipeline_task_uuid}}\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
@@ -10114,17 +9752,17 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-stage-1-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"l2l_stage_1_tuner\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
-          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125",
+          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125",
           "\", \"--feature_selection_result_path=", "{{$.inputs.artifacts[''feature_ranking''].uri}}",
           "\", \"--disable_early_stopping=", "{{$.inputs.parameters[''disable_early_stopping'']}}",
           "\", \"--tune_feature_selection_rate=", "{{$.inputs.parameters[''tune_feature_selection_rate'']}}",
           "\", \"--reduce_search_space_mode=", "{{$.inputs.parameters[''reduce_search_space_mode'']}}",
           "\", \"--component_id={{$.pipeline_task_uuid}}\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
@@ -10458,22 +10096,22 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-feature-attribution:
       container:
         args:
         - --task
@@ -10712,16 +10350,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -10730,15 +10368,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-get-bigquery-destination-output-uri:
       container:
         args:
         - --executor_input
@@ -10916,15 +10554,15 @@
           \ enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  with tf.io.gfile.GFile(transform_output_dir_artifact.uri, 'r') as f:\n\
           \    transform_output_dir = f.read()\n\n  transform_config_path = os.path.join(\n\
           \      transform_output_dir,\n      'feature_transform_engine',\n      'transform_config.json',\n\
           \  )\n\n  return collections.namedtuple(\n      'Outputs',\n      [\n  \
           \        'transform_config_path',\n      ],\n  )(\n      transform_config_path,\n\
           \  )\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-importer:
       importer:
         artifactUri:
           runtimeParameter: uri
         typeSchema:
           schemaTitle: system.Artifact
           schemaVersion: 0.0.1
@@ -11695,15 +11333,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-model-upload-2:
       container:
         args:
         - --type
         - UploadModel
         - --payload
         - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
@@ -11724,15 +11362,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-model-upload-3:
       container:
         args:
         - --type
         - UploadModel
         - --payload
         - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
@@ -11753,15 +11391,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-set-optional-inputs:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _set_optional_inputs
@@ -11810,15 +11448,15 @@
           \    raise ValueError(\n        'One of vertex_dataset, data_source_csv_filenames,'\n\
           \        ' data_source_bigquery_table_path must be specified'\n    )\n\n\
           \  return collections.namedtuple(\n      'Outputs',\n      [\n         \
           \ 'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
           \          'model_display_name',\n      ],\n  )(\n      data_source_csv_filenames,\n\
           \      data_source_bigquery_table_path,\n      model_display_name,\n  )\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-split-materialized-data:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _split_materialized_data
@@ -11856,15 +11494,15 @@
           \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
           \ data source: {materialized_data_json}')\n\n  # we map indices to file\
           \ patterns based on the ordering of insertion order\n  # in our transform_data\
           \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
           \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
           \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
           \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
     exec-split-materialized-data-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _split_materialized_data
@@ -11902,15 +11540,15 @@
           \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
           \ data source: {materialized_data_json}')\n\n  # we map indices to file\
           \ patterns based on the ordering of insertion order\n  # in our transform_data\
           \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
           \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
           \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
           \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
     exec-string-not-empty:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _string_not_empty
@@ -11977,15 +11615,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
     exec-training-configurator-and-validator-2:
       container:
         args:
         - training_configurator_and_validator
         - '{"Concat": ["--instance_schema_path=", "{{$.inputs.artifacts[''instance_schema''].uri}}"]}'
         - '{"Concat": ["--training_schema_path=", "{{$.inputs.artifacts[''training_schema''].uri}}"]}'
         - '{"Concat": ["--dataset_stats_path=", "{{$.inputs.artifacts[''dataset_stats''].uri}}"]}'
@@ -12022,15 +11660,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
 pipelineInfo:
   description: The AutoML Tabular pipeline v2.
   name: automl-tabular-v2
 root:
   dag:
     outputs:
       artifacts:
```

## google_cloud_pipeline_components/preview/automl/tabular/distillation_stage_feature_transform_engine.py

```diff
@@ -47,39 +47,41 @@
   """Feature Transform Engine (FTE) component to transform raw data to engineered features during model distilation.
 
   The FTE transform configuration is generated as part of the FTE stage prior
   to distillation.  This distillation-stage FTE component re-uses this config to
   transform the input datasets with predicted outputs included (soft targets).
 
   Args:
-    root_dir: The Cloud Storage location to store the output.
-    project: Project to run feature transform engine.
-    location: Location for the created GCP services.
-    transform_config_path: Path to the transform config output by the pre-distillation FTE component.
-    bigquery_train_full_table_uri: BigQuery full table id for our train split output by pre-distillation FTE with soft target included.
-    bigquery_validate_full_table_uri: BigQuery full table id for our validation split output by pre-distillation FTE with soft target included.
-    target_column: Target column of input data. prediction_type (str): Model prediction type. One of "classification", "regression", "time_series".
-    bigquery_staging_full_dataset_id: Dataset in 'projectId.datasetId' format for storing intermediate-FTE BigQuery tables.  If the specified dataset does not exist in BigQuery, FTE will create the dataset. If no bigquery_staging_full_dataset_id is specified, all intermediate tables will be stored in a dataset created under the provided project in the input data source's location during FTE execution called 'vertex_feature_transform_engine_staging_{location.replace('-', '_')}'. All tables generated by FTE will have a 30 day TTL.
-    weight_column: Weight column of input data.
-    dataflow_machine_type: The machine type used for dataflow jobs. If not set, default to n1-standard-16.
-    dataflow_max_num_workers: The number of workers to run the dataflow job. If not set, default to 25.
-    dataflow_disk_size_gb: The disk size, in gigabytes, to use on each Dataflow worker instance. If not set, default to 40.
-    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications dataflow_use_public_ips (Optional[bool]): Specifies whether Dataflow workers use public IP addresses.
-    dataflow_service_account: Custom service account to run Dataflow jobs.
-    encryption_spec_key_name: Customer-managed encryption key.
+      root_dir: The Cloud Storage location to store the output.
+      project: Project to run feature transform engine.
+      location: Location for the created GCP services.
+      transform_config_path: Path to the transform config output by the pre-distillation FTE component.
+      bigquery_train_full_table_uri: BigQuery full table id for our train split output by pre-distillation FTE with soft target included.
+      bigquery_validate_full_table_uri: BigQuery full table id for our validation split output by pre-distillation FTE with soft target included.
+      target_column: Target column of input data.
+      prediction_type: Model prediction type. One of "classification", "regression", "time_series".
+      bigquery_staging_full_dataset_id: Dataset in 'projectId.datasetId' format for storing intermediate-FTE BigQuery tables.  If the specified dataset does not exist in BigQuery, FTE will create the dataset. If no bigquery_staging_full_dataset_id is specified, all intermediate tables will be stored in a dataset created under the provided project in the input data source's location during FTE execution called 'vertex_feature_transform_engine_staging_{location.replace('-', '_')}'.  All tables generated by FTE will have a 30 day TTL.
+      weight_column: Weight column of input data.
+      dataflow_machine_type: The machine type used for dataflow jobs. If not set, default to n1-standard-16.
+      dataflow_max_num_workers: The number of workers to run the dataflow job. If not set, default to 25.
+      dataflow_disk_size_gb: The disk size, in gigabytes, to use on each Dataflow worker instance. If not set, default to 40.
+      dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
+      dataflow_use_public_ips: Specifies whether Dataflow workers use public IP addresses.
+      dataflow_service_account: Custom service account to run Dataflow jobs.
+      encryption_spec_key_name: Customer-managed encryption key.
 
   Returns:
-    materialized_data: The materialized dataset.
-    transform_output: The transform output artifact.
-    gcp_resources: GCP resources created by this component. For more details, see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
+      materialized_data: The materialized dataset.
+      transform_output: The transform output artifact.
+      gcp_resources: GCP resources created by this component.  For more details, see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
   """
   # fmt: on
 
   return dsl.ContainerSpec(
-      image='us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125',
+      image='us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125',
       command=[],
       args=[
           'distillation_stage_feature_transform_engine',
           dsl.ConcatPlaceholder(items=['--project=', project]),
           dsl.ConcatPlaceholder(items=['--location=', location]),
           dsl.ConcatPlaceholder(
               items=[
@@ -179,15 +181,15 @@
           ),
           dsl.ConcatPlaceholder(
               items=[
                   '--dataflow_machine_type=',
                   dataflow_machine_type,
               ]
           ),
-          '--dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125',
+          '--dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125',
           dsl.ConcatPlaceholder(
               items=[
                   '--dataflow_disk_size_gb=',
                   dataflow_disk_size_gb,
               ]
           ),
           dsl.ConcatPlaceholder(
```

## google_cloud_pipeline_components/preview/automl/tabular/feature_selection.py

```diff
@@ -96,15 +96,15 @@
                   ),
                   encryption_spec_key_name,
                   (
                       '"}, "job_spec": {"worker_pool_specs": [{"replica_count":'
                       ' 1, "machine_spec": {"machine_type": "n1-standard-8"},'
                       ' "container_spec": {"image_uri":"'
                   ),
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125',
                   '", "args": ["feature_selection", "--data_source=',
                   data_source.uri,
                   '", "--target_column=',
                   target_column_name,
                   '", "--prediction_type=',
                   prediction_type,
                   '", "--binary_classification=',
@@ -133,15 +133,15 @@
                   root_dir,
                   (
                       f'/{dsl.PIPELINE_JOB_ID_PLACEHOLDER}/{dsl.PIPELINE_TASK_ID_PLACEHOLDER}/dataflow_tmp",'
                       ' "--dataflow_max_num_workers='
                   ),
                   dataflow_max_num_workers,
                   '", "--dataflow_worker_container_image=',
-                  'us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125',
                   '", "--dataflow_machine_type=',
                   dataflow_machine_type,
                   '", "--dataflow_disk_size_gb=',
                   dataflow_disk_size_gb,
                   '", "--dataflow_subnetwork_fully_qualified=',
                   dataflow_subnetwork,
                   '", "--dataflow_use_public_ips=',
```

## google_cloud_pipeline_components/preview/automl/tabular/feature_selection_pipeline.yaml

```diff
@@ -43,194 +43,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -238,42 +202,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -315,38 +271,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -371,116 +320,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -496,272 +418,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -787,19 +657,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -812,263 +680,209 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-training-configurator-and-validator:
     executorLabel: exec-training-configurator-and-validator
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -1165,16 +979,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -1183,15 +997,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-training-configurator-and-validator:
       container:
         args:
         - training_configurator_and_validator
@@ -1231,15 +1045,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
 pipelineInfo:
   description: Defines pipeline for feature transform engine component.
   name: feature-selection
 root:
   dag:
     tasks:
       feature-transform-engine:
```

## google_cloud_pipeline_components/preview/automl/tabular/feature_transform_engine.py

```diff
@@ -304,15 +304,15 @@
       group_total_weight: The weight of the loss for predictions aggregated over time series in the same group.
       temporal_total_weight: The weight of the loss for predictions aggregated over the horizon for a single time series.
       group_temporal_total_weight: The weight of the loss for predictions aggregated over both the horizon and time series in the same hierarchy group.
   """
   # fmt: on
 
   return dsl.ContainerSpec(
-      image='us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125',
+      image='us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125',
       command=[],
       args=[
           'feature_transform_engine',
           dsl.ConcatPlaceholder(items=['--project=', project]),
           dsl.ConcatPlaceholder(items=['--location=', location]),
           dsl.ConcatPlaceholder(
               items=[
@@ -633,16 +633,16 @@
           ),
           dsl.ConcatPlaceholder(
               items=['--dataflow_max_num_workers=', dataflow_max_num_workers]
           ),
           dsl.ConcatPlaceholder(
               items=['--dataflow_machine_type=', dataflow_machine_type]
           ),
-          '--dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125',
-          '--feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125',
+          '--dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125',
+          '--feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125',
           dsl.ConcatPlaceholder(
               items=['--dataflow_disk_size_gb=', dataflow_disk_size_gb]
           ),
           dsl.ConcatPlaceholder(
               items=[
                   '--dataflow_subnetwork_fully_qualified=',
                   dataflow_subnetwork,
```

## google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job.py

```diff
@@ -154,23 +154,23 @@
                   ),
                   '1',
                   '", "machine_spec": ',
                   training_machine_spec,
                   ', "disk_spec": ',
                   training_disk_spec,
                   ', "container_spec": {"image_uri":"',
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/tabnet-training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/tabnet-training:20240119_0125',
                   '", "args": ["--target_column=',
                   target_column,
                   '", "--weight_column=',
                   weight_column,
                   '", "--model_type=',
                   prediction_type,
                   '", "--prediction_docker_uri=',
-                  'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125',
                   '", "--prediction_docker_uri_artifact_path=',
                   prediction_docker_uri_output,
                   '", "--baseline_path=',
                   instance_baseline.uri,
                   '", "--metadata_path=',
                   metadata.uri,
                   '", "--transform_output_path=',
```

## google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job_pipeline.yaml

```diff
@@ -79,30 +79,26 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-infra-validator:
     executorLabel: exec-automl-tabular-infra-validator
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-bool-identity:
     executorLabel: exec-bool-identity
     inputDefinitions:
       parameters:
         value:
           description: Boolean value to return
           parameterType: BOOLEAN
@@ -832,194 +828,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -1027,42 +987,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -1104,38 +1056,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -1160,116 +1105,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -1285,272 +1203,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -1576,19 +1442,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -1601,44 +1465,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-get-best-hyperparameter-tuning-job-trial:
     executorLabel: exec-get-best-hyperparameter-tuning-job-trial
     inputDefinitions:
       parameters:
         gcp_resources:
           description: Proto tracking the hyperparameter tuning job.
@@ -2626,146 +2482,113 @@
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The path to transform output.
       parameters:
         cache_data:
           defaultValue: auto
-          description: 'Whether to cache data or not. If set to
-
-            ''auto'', caching is determined based on the dataset size.'
+          description: Whether to cache data or not. If set to 'auto', caching is
+            determined based on the dataset size.
           isOptional: true
           parameterType: STRING
         enable_profiler:
           defaultValue: false
-          description: 'Enables profiling and saves a trace
-
-            during evaluation.'
+          description: Enables profiling and saves a trace during evaluation.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: The KMS key name.
           isOptional: true
           parameterType: STRING
         eval_frequency_secs:
           defaultValue: 600.0
-          description: 'Frequency at which evaluation and
-
-            checkpointing will take place.'
+          description: Frequency at which evaluation and checkpointing will take place.
           isOptional: true
           parameterType: NUMBER_INTEGER
         eval_steps:
           defaultValue: 0.0
-          description: 'Number of steps to run evaluation for. If not
-
-            specified or negative, it means run evaluation on the whole validation
-
-            dataset. If set to 0, it means run evaluation for a fixed number of
-
-            samples.'
+          description: Number of steps to run evaluation for. If not specified or
+            negative, it means run evaluation on the whole validation dataset. If
+            set to 0, it means run evaluation for a fixed number of samples.
           isOptional: true
           parameterType: NUMBER_INTEGER
         location:
           description: The GCP region that runs the pipeline components.
           parameterType: STRING
         max_failed_trial_count:
           defaultValue: 0.0
-          description: 'The number of failed trials that
-
-            need to be seen before failing the HyperparameterTuningJob. If set to
-            0,
-
-            Vertex AI decides how many trials must fail before the whole job fails.'
+          description: The number of failed trials that need to be seen before failing
+            the HyperparameterTuningJob. If set to 0, Vertex AI decides how many trials
+            must fail before the whole job fails.
           isOptional: true
           parameterType: NUMBER_INTEGER
         max_trial_count:
           description: The desired total number of trials.
           parameterType: NUMBER_INTEGER
         parallel_trial_count:
-          description: 'The desired number of trials to run
-
-            in parallel.'
+          description: The desired number of trials to run in parallel.
           parameterType: NUMBER_INTEGER
         prediction_type:
-          description: 'The type of prediction the model is to
-
-            produce. "classification" or "regression".'
+          description: The type of prediction the model is to produce. "classification"
+            or "regression".
           parameterType: STRING
         project:
           description: The GCP project that runs the pipeline components.
           parameterType: STRING
         root_dir:
           description: The root GCS directory for the pipeline components.
           parameterType: STRING
         seed:
           defaultValue: 1.0
           description: Seed to be used for this run.
           isOptional: true
           parameterType: NUMBER_INTEGER
         study_spec_algorithm:
           defaultValue: ALGORITHM_UNSPECIFIED
-          description: 'The search algorithm specified for
-
-            the study. One of ''ALGORITHM_UNSPECIFIED'', ''GRID_SEARCH'', or
-
-            ''RANDOM_SEARCH''.'
+          description: The search algorithm specified for the study. One of 'ALGORITHM_UNSPECIFIED',
+            'GRID_SEARCH', or 'RANDOM_SEARCH'.
           isOptional: true
           parameterType: STRING
         study_spec_measurement_selection_type:
           defaultValue: BEST_MEASUREMENT
-          description: 'Which measurement
-
-            to use if/when the service automatically selects the final measurement
-
-            from previously reported intermediate measurements. One of
-
-            "BEST_MEASUREMENT" or "LAST_MEASUREMENT".'
+          description: Which measurement to use if/when the service automatically
+            selects the final measurement from previously reported intermediate measurements.
+            One of "BEST_MEASUREMENT" or "LAST_MEASUREMENT".
           isOptional: true
           parameterType: STRING
         study_spec_metric_goal:
-          description: 'Optimization goal of the metric,
-
-            possible values: "MAXIMIZE", "MINIMIZE".'
+          description: 'Optimization goal of the metric, possible values: "MAXIMIZE",
+            "MINIMIZE".'
           parameterType: STRING
         study_spec_metric_id:
-          description: 'Metric to optimize, possible
-
-            values: [ ''loss'', ''average_loss'', ''rmse'', ''mae'', ''mql'', ''accuracy'',
-            ''auc'', ''precision'', ''recall''].'
+          description: 'Metric to optimize, possible values: [ ''loss'', ''average_loss'',
+            ''rmse'', ''mae'', ''mql'', ''accuracy'', ''auc'', ''precision'', ''recall''].'
           parameterType: STRING
         study_spec_parameters_override:
-          description: 'List of dictionaries
-
-            representing parameters to optimize. The dictionary key is the
-
-            parameter_id, which is passed to training job as a command line
-
-            argument, and the dictionary value is the parameter specification of the
-
-            metric.'
+          description: List of dictionaries representing parameters to optimize. The
+            dictionary key is the parameter_id, which is passed to training job as
+            a command line argument, and the dictionary value is the parameter specification
+            of the metric.
           parameterType: LIST
         target_column:
           description: The target column name.
           parameterType: STRING
         training_disk_spec:
           defaultValue:
             boot_disk_size_gb: 100.0
             boot_disk_type: pd-ssd
           description: The training disk spec.
           isOptional: true
           parameterType: STRUCT
         training_machine_spec:
           defaultValue:
             machine_type: c2-standard-16
-          description: 'The training machine
-
-            spec. See https://cloud.google.com/compute/docs/machine-types for
-
-            options.'
+          description: The training machine spec. See https://cloud.google.com/compute/docs/machine-types
+            for options.
           isOptional: true
           parameterType: STRUCT
         weight_column:
           defaultValue: ''
           description: The weight column name.
           isOptional: true
           parameterType: STRING
@@ -2794,224 +2617,178 @@
     executorLabel: exec-training-configurator-and-validator
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -3040,30 +2817,30 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
         - google_cloud_pipeline_components.container.v1.custom_job.launcher
         image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.44
     exec-automl-tabular-infra-validator:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-bool-identity:
       container:
         args:
         - --executor_input
@@ -3171,16 +2948,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -3189,15 +2966,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-get-best-hyperparameter-tuning-job-trial:
       container:
         args:
         - --executor_input
@@ -3263,15 +3040,15 @@
           \ = best_fn(trials_list, key=lambda trial: trial['objective_value'])\n\n\
           \  # Build unmanaged_container_model\n  unmanaged_container_model.metadata['containerSpec']\
           \ = {\n      'imageUri': prediction_docker_uri,\n      'healthRoute': '/health',\n\
           \      'predictRoute': '/predict',\n  }\n  unmanaged_container_model.metadata['predictSchemata']\
           \ = {\n      'instanceSchemaUri': instance_schema_uri,\n      'predictionSchemaUri':\
           \ prediction_schema_uri,\n  }\n  unmanaged_container_model.uri = os.path.join(\n\
           \      trials_dir, 'trial_{}'.format(best_trial['id']), 'model'\n  )\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-tabnet-study-spec-parameters:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_tabnet_study_spec_parameters
@@ -3779,15 +3556,15 @@
           \ = param\n\n  study_spec_parameters = []\n  for param in formatted_params:\n\
           \    study_spec_parameters.append(\n        override_params.get(param['parameter_id'],\
           \ param)\n    )\n\n  extra_overrides = set(override_params) - set(\n   \
           \   p['parameter_id'] for p in params\n  )\n  if extra_overrides:\n    extra_override_str\
           \ = ', '.join(extra_overrides)\n    warnings.warn(\n        f'The overrides\
           \ {extra_override_str} were not found in the params and '\n        'will\
           \ be ignored.'\n    )\n\n  return study_spec_parameters\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-model-batch-predict:
       container:
         args:
         - --type
         - BatchPredictionJob
         - --payload
         - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''job_display_name'']}}",
@@ -4083,15 +3860,15 @@
           \    raise ValueError(\n        'One of vertex_dataset, data_source_csv_filenames,'\n\
           \        ' data_source_bigquery_table_path must be specified'\n    )\n\n\
           \  return collections.namedtuple(\n      'Outputs',\n      [\n         \
           \ 'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
           \          'model_display_name',\n      ],\n  )(\n      data_source_csv_filenames,\n\
           \      data_source_bigquery_table_path,\n      model_display_name,\n  )\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-split-materialized-data:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _split_materialized_data
@@ -4129,15 +3906,15 @@
           \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
           \ data source: {materialized_data_json}')\n\n  # we map indices to file\
           \ patterns based on the ordering of insertion order\n  # in our transform_data\
           \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
           \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
           \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
           \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
     exec-tabnet-hyperparameter-tuning-job:
       container:
         args:
         - --type
         - HyperparameterTuningJobWithMetrics
         - --project
         - '{{$.inputs.parameters[''project'']}}'
@@ -4157,19 +3934,19 @@
           "\", \"measurement_selection_type\": \"", "{{$.inputs.parameters[''study_spec_measurement_selection_type'']}}",
           "\"}, \"max_trial_count\": ", "{{$.inputs.parameters[''max_trial_count'']}}",
           ", \"parallel_trial_count\": ", "{{$.inputs.parameters[''parallel_trial_count'']}}",
           ", \"max_failed_trial_count\": ", "{{$.inputs.parameters[''max_failed_trial_count'']}}",
           ", \"trial_job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\"",
           "1", "\", \"machine_spec\": ", "{{$.inputs.parameters[''training_machine_spec'']}}",
           ", \"disk_spec\": ", "{{$.inputs.parameters[''training_disk_spec'']}}",
-          ", \"container_spec\": {\"image_uri\":\"", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/tabnet-training:20231029_0125",
+          ", \"container_spec\": {\"image_uri\":\"", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/tabnet-training:20240119_0125",
           "\", \"args\": [\"--target_column=", "{{$.inputs.parameters[''target_column'']}}",
           "\", \"--weight_column=", "{{$.inputs.parameters[''weight_column'']}}",
           "\", \"--model_type=", "{{$.inputs.parameters[''prediction_type'']}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--prediction_docker_uri_artifact_path=", "{{$.outputs.parameters[''prediction_docker_uri_output''].output_file}}",
           "\", \"--baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--training_schema_path=", "{{$.inputs.artifacts[''training_schema_uri''].uri}}",
           "\", \"--instance_schema_path=", "{{$.outputs.parameters[''instance_schema_uri''].output_file}}",
           "\", \"--prediction_schema_path=", "{{$.outputs.parameters[''prediction_schema_uri''].output_file}}",
@@ -4230,15 +4007,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
 pipelineInfo:
   description: The TabNet built-in algorithm HyperparameterTuningJob pipeline.
   name: automl-tabular-tabnet-hyperparameter-tuning-job
 root:
   dag:
     outputs:
       artifacts:
```

## google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer.py

```diff
@@ -161,23 +161,23 @@
                   '"}, "job_spec": {"worker_pool_specs": [{"replica_count":"',
                   '1',
                   '", "machine_spec": ',
                   training_machine_spec,
                   ', "disk_spec": ',
                   training_disk_spec,
                   ', "container_spec": {"image_uri":"',
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/tabnet-training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/tabnet-training:20240119_0125',
                   '", "args": ["--target_column=',
                   target_column,
                   '", "--weight_column=',
                   weight_column,
                   '", "--model_type=',
                   prediction_type,
                   '", "--prediction_docker_uri=',
-                  'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125',
                   '", "--baseline_path=',
                   instance_baseline.uri,
                   '", "--metadata_path=',
                   metadata.uri,
                   '", "--transform_output_path=',
                   transform_output.uri,
                   '", "--training_schema_path=',
```

## google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer_pipeline.yaml

```diff
@@ -98,30 +98,26 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-infra-validator:
     executorLabel: exec-automl-tabular-infra-validator
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-bool-identity:
     executorLabel: exec-bool-identity
     inputDefinitions:
       parameters:
         value:
           description: Boolean value to return
           parameterType: BOOLEAN
@@ -861,194 +857,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -1056,42 +1016,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -1133,38 +1085,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -1189,116 +1134,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -1314,272 +1232,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -1605,19 +1471,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -1630,44 +1494,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-model-batch-predict:
     executorLabel: exec-model-batch-predict
     inputDefinitions:
       artifacts:
         model:
           artifactType:
@@ -2581,242 +2437,194 @@
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The path to transform output.
       parameters:
         alpha_focal_loss:
           defaultValue: 0.25
-          description: 'Alpha value (balancing factor) in
-
-            focal_loss function. Only used for classification.'
+          description: Alpha value (balancing factor) in focal_loss function. Only
+            used for classification.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         batch_momentum:
           defaultValue: 0.95
           description: Momentum in ghost batch normalization.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         batch_size:
           defaultValue: 100.0
           description: Batch size for training.
           isOptional: true
           parameterType: NUMBER_INTEGER
         batch_size_ratio:
           defaultValue: 0.25
-          description: 'The ratio of virtual batch size (size
-
-            of the ghost batch normalization) to batch size.'
+          description: The ratio of virtual batch size (size of the ghost batch normalization)
+            to batch size.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         cache_data:
           defaultValue: auto
-          description: 'Whether to cache data or not. If set to
-
-            ''auto'', caching is determined based on the dataset size.'
+          description: Whether to cache data or not. If set to 'auto', caching is
+            determined based on the dataset size.
           isOptional: true
           parameterType: STRING
         class_weight:
           defaultValue: 1.0
-          description: 'The class weight is used to computes a
-
-            weighted cross entropy which is helpful in classify imbalanced dataset.
-
-            Only used for classification.'
+          description: The class weight is used to computes a weighted cross entropy
+            which is helpful in classify imbalanced dataset. Only used for classification.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         decay_every:
           defaultValue: 100.0
-          description: 'Number of iterations for periodically
-
-            applying learning rate decaying.'
+          description: Number of iterations for periodically applying learning rate
+            decaying.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         decay_rate:
           defaultValue: 0.95
           description: Learning rate decaying.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         enable_profiler:
           defaultValue: false
-          description: 'Enables profiling and saves a trace
-
-            during evaluation.'
+          description: Enables profiling and saves a trace during evaluation.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: The KMS key name.
           isOptional: true
           parameterType: STRING
         eval_frequency_secs:
           defaultValue: 600.0
-          description: 'Frequency at which evaluation and
-
-            checkpointing will take place.'
+          description: Frequency at which evaluation and checkpointing will take place.
           isOptional: true
           parameterType: NUMBER_INTEGER
         eval_steps:
           defaultValue: 0.0
-          description: 'Number of steps to run evaluation for. If not
-
-            specified or negative, it means run evaluation on the whole validation
-
-            dataset. If set to 0, it means run evaluation for a fixed number of
-
-            samples.'
+          description: Number of steps to run evaluation for. If not specified or
+            negative, it means run evaluation on the whole validation dataset. If
+            set to 0, it means run evaluation for a fixed number of samples.
           isOptional: true
           parameterType: NUMBER_INTEGER
         feature_dim:
           defaultValue: 64.0
-          description: 'Dimensionality of the hidden representation
-
-            in feature transformation block.'
+          description: Dimensionality of the hidden representation in feature transformation
+            block.
           isOptional: true
           parameterType: NUMBER_INTEGER
         feature_dim_ratio:
           defaultValue: 0.5
-          description: 'The ratio of output dimension
-
-            (dimensionality of the outputs of each decision step) to feature
-
-            dimension.'
+          description: The ratio of output dimension (dimensionality of the outputs
+            of each decision step) to feature dimension.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         gamma_focal_loss:
           defaultValue: 2.0
-          description: 'Gamma value (modulating factor) for
-
-            focal loss for focal loss. Only used for classification.'
+          description: Gamma value (modulating factor) for focal loss for focal loss.
+            Only used for classification.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         gradient_thresh:
           defaultValue: 2000.0
           description: Threshold for the norm of gradients for clipping.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         large_category_dim:
           defaultValue: 1.0
-          description: 'Embedding dimension for categorical
-
-            feature with large number of categories.'
+          description: Embedding dimension for categorical feature with large number
+            of categories.
           isOptional: true
           parameterType: NUMBER_INTEGER
         large_category_thresh:
           defaultValue: 300.0
-          description: 'Threshold for number of categories
-
-            to apply large_category_dim embedding dimension to.'
+          description: Threshold for number of categories to apply large_category_dim
+            embedding dimension to.
           isOptional: true
           parameterType: NUMBER_INTEGER
         learning_rate:
           description: The learning rate used by the linear optimizer.
           parameterType: NUMBER_DOUBLE
         location:
           description: The GCP region that runs the pipeline components.
           parameterType: STRING
         loss_function_type:
           defaultValue: default
-          description: 'Loss function type. Loss function in
-
-            classification [cross_entropy, weighted_cross_entropy, focal_loss],
-
-            default is cross_entropy. Loss function in regression: [rmse, mae, mse],
-
-            default is mse.'
+          description: 'Loss function type. Loss function in classification [cross_entropy,
+            weighted_cross_entropy, focal_loss], default is cross_entropy. Loss function
+            in regression: [rmse, mae, mse], default is mse.'
           isOptional: true
           parameterType: STRING
         max_steps:
           defaultValue: -1.0
           description: Number of steps to run the trainer for.
           isOptional: true
           parameterType: NUMBER_INTEGER
         max_train_secs:
           defaultValue: -1.0
-          description: 'Amount of time in seconds to run the
-
-            trainer for.'
+          description: Amount of time in seconds to run the trainer for.
           isOptional: true
           parameterType: NUMBER_INTEGER
         measurement_selection_type:
           defaultValue: BEST_MEASUREMENT
-          description: 'Which measurement to use
-
-            if/when the service automatically selects the final measurement from
-
-            previously reported intermediate measurements. One of "BEST_MEASUREMENT"
-
-            or "LAST_MEASUREMENT".'
+          description: Which measurement to use if/when the service automatically
+            selects the final measurement from previously reported intermediate measurements.
+            One of "BEST_MEASUREMENT" or "LAST_MEASUREMENT".
           isOptional: true
           parameterType: STRING
         num_decision_steps:
           defaultValue: 6.0
           description: Number of sequential decision steps.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_transformer_layers:
           defaultValue: 4.0
-          description: 'The number of transformer layers
-
-            for each decision step. used only at one decision step and as it
-
-            increases, more flexibility is provided to use a feature at multiple
-
-            decision steps.'
+          description: The number of transformer layers for each decision step. used
+            only at one decision step and as it increases, more flexibility is provided
+            to use a feature at multiple decision steps.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_transformer_layers_ratio:
           defaultValue: 0.25
-          description: 'The ratio of shared
-
-            transformer layer to transformer layers.'
+          description: The ratio of shared transformer layer to transformer layers.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_metric:
           defaultValue: ''
-          description: 'Optimization metric used for
-
-            `measurement_selection_type`. Default is "rmse" for regression and "auc"
-
-            for classification.'
+          description: Optimization metric used for `measurement_selection_type`.
+            Default is "rmse" for regression and "auc" for classification.
           isOptional: true
           parameterType: STRING
         prediction_type:
-          description: 'The type of prediction the model is to
-
-            produce. "classification" or "regression".'
+          description: The type of prediction the model is to produce. "classification"
+            or "regression".
           parameterType: STRING
         project:
           description: The GCP project that runs the pipeline components.
           parameterType: STRING
         relaxation_factor:
           defaultValue: 1.5
-          description: 'Relaxation factor that promotes the
-
-            reuse of each feature at different decision steps. When it is 1, a
-
-            feature is enforced to be used only at one decision step and as it
-
-            increases, more flexibility is provided to use a feature at multiple
-
-            decision steps.'
+          description: Relaxation factor that promotes the reuse of each feature at
+            different decision steps. When it is 1, a feature is enforced to be used
+            only at one decision step and as it increases, more flexibility is provided
+            to use a feature at multiple decision steps.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         root_dir:
           description: The root GCS directory for the pipeline components.
           parameterType: STRING
         seed:
           defaultValue: 1.0
           description: Seed to be used for this run.
           isOptional: true
           parameterType: NUMBER_INTEGER
         sparsity_loss_weight:
           defaultValue: 1.0e-05
-          description: 'Weight of the loss for sparsity
-
-            regularization (increasing it will yield more sparse feature selection).'
+          description: Weight of the loss for sparsity regularization (increasing
+            it will yield more sparse feature selection).
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           description: The target column name.
           parameterType: STRING
         training_disk_spec:
           defaultValue:
@@ -2824,31 +2632,26 @@
             boot_disk_type: pd-ssd
           description: The training disk spec.
           isOptional: true
           parameterType: STRUCT
         training_machine_spec:
           defaultValue:
             machine_type: c2-standard-16
-          description: 'The training machine
-
-            spec. See https://cloud.google.com/compute/docs/machine-types for
-
-            options.'
+          description: The training machine spec. See https://cloud.google.com/compute/docs/machine-types
+            for options.
           isOptional: true
           parameterType: STRUCT
         weight_column:
           defaultValue: ''
           description: The weight column name.
           isOptional: true
           parameterType: STRING
         yeo_johnson_transform:
           defaultValue: true
-          description: 'Enables trainable Yeo-Johnson
-
-            power transform.'
+          description: Enables trainable Yeo-Johnson power transform.
           isOptional: true
           parameterType: BOOLEAN
     outputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
@@ -2863,224 +2666,178 @@
     executorLabel: exec-training-configurator-and-validator
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -3109,30 +2866,30 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
         - google_cloud_pipeline_components.container.v1.custom_job.launcher
         image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.44
     exec-automl-tabular-infra-validator:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-bool-identity:
       container:
         args:
         - --executor_input
@@ -3240,16 +2997,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -3258,15 +3015,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-model-batch-predict:
       container:
         args:
         - --type
@@ -3565,15 +3322,15 @@
           \    raise ValueError(\n        'One of vertex_dataset, data_source_csv_filenames,'\n\
           \        ' data_source_bigquery_table_path must be specified'\n    )\n\n\
           \  return collections.namedtuple(\n      'Outputs',\n      [\n         \
           \ 'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
           \          'model_display_name',\n      ],\n  )(\n      data_source_csv_filenames,\n\
           \      data_source_bigquery_table_path,\n      model_display_name,\n  )\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-split-materialized-data:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _split_materialized_data
@@ -3611,15 +3368,15 @@
           \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
           \ data source: {materialized_data_json}')\n\n  # we map indices to file\
           \ patterns based on the ordering of insertion order\n  # in our transform_data\
           \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
           \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
           \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
           \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
     exec-tabnet-trainer:
       container:
         args:
         - --type
         - CustomJob
         - --project
         - '{{$.inputs.parameters[''project'']}}'
@@ -3629,19 +3386,19 @@
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"tabnet-trainer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\"", "1",
           "\", \"machine_spec\": ", "{{$.inputs.parameters[''training_machine_spec'']}}",
           ", \"disk_spec\": ", "{{$.inputs.parameters[''training_disk_spec'']}}",
-          ", \"container_spec\": {\"image_uri\":\"", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/tabnet-training:20231029_0125",
+          ", \"container_spec\": {\"image_uri\":\"", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/tabnet-training:20240119_0125",
           "\", \"args\": [\"--target_column=", "{{$.inputs.parameters[''target_column'']}}",
           "\", \"--weight_column=", "{{$.inputs.parameters[''weight_column'']}}",
           "\", \"--model_type=", "{{$.inputs.parameters[''prediction_type'']}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--training_schema_path=", "{{$.inputs.artifacts[''training_schema_uri''].uri}}",
           "\", \"--job_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--training_data_path=", "{{$.inputs.artifacts[''materialized_train_split''].uri}}",
           "\", \"--validation_data_path=", "{{$.inputs.artifacts[''materialized_eval_split''].uri}}",
@@ -3720,15 +3477,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
 pipelineInfo:
   description: 'Train a model using the Tabular Workflow for TabNet pipelines.
 
     TabNet uses sequential attention to choose which features to reason from at
 
     each decision step, promoting interpretability and more efficient learning.'
   name: automl-tabular-tabnet-trainer
```

## google_cloud_pipeline_components/preview/automl/tabular/utils.py

```diff
@@ -3878,6728 +3878,6641 @@
 0000f250: 7220 3d20 2727 2c0a 2020 2020 6461 7461  r = '',.    data
 0000f260: 666c 6f77 5f75 7365 5f70 7562 6c69 635f  flow_use_public_
 0000f270: 6970 733a 2062 6f6f 6c20 3d20 5472 7565  ips: bool = True
 0000f280: 2c0a 2020 2020 656e 6372 7970 7469 6f6e  ,.    encryption
 0000f290: 5f73 7065 635f 6b65 795f 6e61 6d65 3a20  _spec_key_name: 
 0000f2a0: 7374 7220 3d20 2727 2c0a 2920 2d3e 2054  str = '',.) -> T
 0000f2b0: 7570 6c65 5b73 7472 2c20 4469 6374 5b73  uple[str, Dict[s
-0000f2c0: 7472 2c20 416e 795d 5d3a 0a20 2022 2222  tr, Any]]:.  """
-0000f2d0: 4765 7420 7468 6520 5769 6465 2026 2044  Get the Wide & D
-0000f2e0: 6565 7020 7472 6169 6e69 6e67 2070 6970  eep training pip
-0000f2f0: 656c 696e 652e 0a0a 2020 4172 6773 3a0a  eline...  Args:.
-0000f300: 2020 2020 7072 6f6a 6563 743a 2054 6865      project: The
-0000f310: 2047 4350 2070 726f 6a65 6374 2074 6861   GCP project tha
-0000f320: 7420 7275 6e73 2074 6865 2070 6970 656c  t runs the pipel
-0000f330: 696e 6520 636f 6d70 6f6e 656e 7473 2e0a  ine components..
-0000f340: 2020 2020 6c6f 6361 7469 6f6e 3a20 5468      location: Th
-0000f350: 6520 4743 5020 7265 6769 6f6e 2074 6861  e GCP region tha
-0000f360: 7420 7275 6e73 2074 6865 2070 6970 656c  t runs the pipel
-0000f370: 696e 6520 636f 6d70 6f6e 656e 7473 2e0a  ine components..
-0000f380: 2020 2020 726f 6f74 5f64 6972 3a20 5468      root_dir: Th
-0000f390: 6520 726f 6f74 2047 4353 2064 6972 6563  e root GCS direc
-0000f3a0: 746f 7279 2066 6f72 2074 6865 2070 6970  tory for the pip
-0000f3b0: 656c 696e 6520 636f 6d70 6f6e 656e 7473  eline components
-0000f3c0: 2e0a 2020 2020 7461 7267 6574 5f63 6f6c  ..    target_col
-0000f3d0: 756d 6e3a 2054 6865 2074 6172 6765 7420  umn: The target 
-0000f3e0: 636f 6c75 6d6e 206e 616d 652e 0a20 2020  column name..   
-0000f3f0: 2070 7265 6469 6374 696f 6e5f 7479 7065   prediction_type
-0000f400: 3a20 5468 6520 7479 7065 206f 6620 7072  : The type of pr
-0000f410: 6564 6963 7469 6f6e 2074 6865 206d 6f64  ediction the mod
-0000f420: 656c 2069 7320 746f 2070 726f 6475 6365  el is to produce
-0000f430: 2e0a 2020 2020 2020 2763 6c61 7373 6966  ..      'classif
-0000f440: 6963 6174 696f 6e27 206f 7220 2772 6567  ication' or 'reg
-0000f450: 7265 7373 696f 6e27 2e0a 2020 2020 6c65  ression'..    le
-0000f460: 6172 6e69 6e67 5f72 6174 653a 2054 6865  arning_rate: The
-0000f470: 206c 6561 726e 696e 6720 7261 7465 2075   learning rate u
-0000f480: 7365 6420 6279 2074 6865 206c 696e 6561  sed by the linea
-0000f490: 7220 6f70 7469 6d69 7a65 722e 0a20 2020  r optimizer..   
-0000f4a0: 2064 6e6e 5f6c 6561 726e 696e 675f 7261   dnn_learning_ra
-0000f4b0: 7465 3a20 5468 6520 6c65 6172 6e69 6e67  te: The learning
-0000f4c0: 2072 6174 6520 666f 7220 7472 6169 6e69   rate for traini
-0000f4d0: 6e67 2074 6865 2064 6565 7020 7061 7274  ng the deep part
-0000f4e0: 206f 6620 7468 650a 2020 2020 2020 6d6f   of the.      mo
-0000f4f0: 6465 6c2e 0a20 2020 2074 7261 6e73 666f  del..    transfo
-0000f500: 726d 5f63 6f6e 6669 673a 2050 6174 6820  rm_config: Path 
-0000f510: 746f 2076 3120 5446 2074 7261 6e73 666f  to v1 TF transfo
-0000f520: 726d 6174 696f 6e20 636f 6e66 6967 7572  rmation configur
-0000f530: 6174 696f 6e2e 0a20 2020 2064 6174 6173  ation..    datas
-0000f540: 6574 5f6c 6576 656c 5f63 7573 746f 6d5f  et_level_custom_
-0000f550: 7472 616e 7366 6f72 6d61 7469 6f6e 5f64  transformation_d
-0000f560: 6566 696e 6974 696f 6e73 3a20 4461 7461  efinitions: Data
-0000f570: 7365 742d 6c65 7665 6c20 6375 7374 6f6d  set-level custom
-0000f580: 0a20 2020 2020 2074 7261 6e73 666f 726d  .      transform
-0000f590: 6174 696f 6e20 6465 6669 6e69 7469 6f6e  ation definition
-0000f5a0: 7320 696e 2073 7472 696e 6720 666f 726d  s in string form
-0000f5b0: 6174 2e0a 2020 2020 6461 7461 7365 745f  at..    dataset_
-0000f5c0: 6c65 7665 6c5f 7472 616e 7366 6f72 6d61  level_transforma
-0000f5d0: 7469 6f6e 733a 2044 6174 6173 6574 2d6c  tions: Dataset-l
-0000f5e0: 6576 656c 2074 7261 6e73 666f 726d 6174  evel transformat
-0000f5f0: 696f 6e20 636f 6e66 6967 7572 6174 696f  ion configuratio
-0000f600: 6e20 696e 0a20 2020 2020 2073 7472 696e  n in.      strin
-0000f610: 6720 666f 726d 6174 2e0a 2020 2020 7275  g format..    ru
-0000f620: 6e5f 6665 6174 7572 655f 7365 6c65 6374  n_feature_select
-0000f630: 696f 6e3a 2057 6865 7468 6572 2074 6f20  ion: Whether to 
-0000f640: 656e 6162 6c65 2066 6561 7475 7265 2073  enable feature s
-0000f650: 656c 6563 7469 6f6e 2e0a 2020 2020 6665  election..    fe
-0000f660: 6174 7572 655f 7365 6c65 6374 696f 6e5f  ature_selection_
-0000f670: 616c 676f 7269 7468 6d3a 2046 6561 7475  algorithm: Featu
-0000f680: 7265 2073 656c 6563 7469 6f6e 2061 6c67  re selection alg
-0000f690: 6f72 6974 686d 2e0a 2020 2020 6d61 7465  orithm..    mate
-0000f6a0: 7269 616c 697a 6564 5f65 7861 6d70 6c65  rialized_example
-0000f6b0: 735f 666f 726d 6174 3a20 5468 6520 666f  s_format: The fo
-0000f6c0: 726d 6174 2066 6f72 2074 6865 206d 6174  rmat for the mat
-0000f6d0: 6572 6961 6c69 7a65 6420 6578 616d 706c  erialized exampl
-0000f6e0: 6573 2e0a 2020 2020 6d61 785f 7365 6c65  es..    max_sele
-0000f6f0: 6374 6564 5f66 6561 7475 7265 733a 204d  cted_features: M
-0000f700: 6178 696d 756d 206e 756d 6265 7220 6f66  aximum number of
-0000f710: 2066 6561 7475 7265 7320 746f 2073 656c   features to sel
-0000f720: 6563 742e 0a20 2020 2070 7265 6465 6669  ect..    predefi
-0000f730: 6e65 645f 7370 6c69 745f 6b65 793a 2050  ned_split_key: P
-0000f740: 7265 6465 6669 6e65 6420 7370 6c69 7420  redefined split 
-0000f750: 6b65 792e 0a20 2020 2073 7472 6174 6966  key..    stratif
-0000f760: 6965 645f 7370 6c69 745f 6b65 793a 2053  ied_split_key: S
-0000f770: 7472 6174 6966 6965 6420 7370 6c69 7420  tratified split 
-0000f780: 6b65 792e 0a20 2020 2074 7261 696e 696e  key..    trainin
-0000f790: 675f 6672 6163 7469 6f6e 3a20 5472 6169  g_fraction: Trai
-0000f7a0: 6e69 6e67 2066 7261 6374 696f 6e2e 0a20  ning fraction.. 
-0000f7b0: 2020 2076 616c 6964 6174 696f 6e5f 6672     validation_fr
-0000f7c0: 6163 7469 6f6e 3a20 5661 6c69 6461 7469  action: Validati
-0000f7d0: 6f6e 2066 7261 6374 696f 6e2e 0a20 2020  on fraction..   
-0000f7e0: 2074 6573 745f 6672 6163 7469 6f6e 3a20   test_fraction: 
-0000f7f0: 5465 7374 2066 7261 6374 696f 6e2e 0a20  Test fraction.. 
-0000f800: 2020 2074 665f 7472 616e 7366 6f72 6d5f     tf_transform_
-0000f810: 6578 6563 7574 696f 6e5f 656e 6769 6e65  execution_engine
-0000f820: 3a20 5468 6520 6578 6563 7574 696f 6e20  : The execution 
-0000f830: 656e 6769 6e65 2075 7365 6420 746f 2065  engine used to e
-0000f840: 7865 6375 7465 2054 462d 6261 7365 640a  xecute TF-based.
-0000f850: 2020 2020 2020 7472 616e 7366 6f72 6d61        transforma
-0000f860: 7469 6f6e 732e 0a20 2020 2074 665f 6175  tions..    tf_au
-0000f870: 746f 5f74 7261 6e73 666f 726d 5f66 6561  to_transform_fea
-0000f880: 7475 7265 733a 204c 6973 7420 6f66 2061  tures: List of a
-0000f890: 7574 6f20 7472 616e 7366 6f72 6d20 6665  uto transform fe
-0000f8a0: 6174 7572 6573 2069 6e20 7468 650a 2020  atures in the.  
-0000f8b0: 2020 2020 636f 6d6d 612d 7365 7061 7261      comma-separa
-0000f8c0: 7465 6420 7374 7269 6e67 2066 6f72 6d61  ted string forma
-0000f8d0: 742e 0a20 2020 2074 665f 6375 7374 6f6d  t..    tf_custom
-0000f8e0: 5f74 7261 6e73 666f 726d 6174 696f 6e5f  _transformation_
-0000f8f0: 6465 6669 6e69 7469 6f6e 733a 2054 4620  definitions: TF 
-0000f900: 6375 7374 6f6d 2074 7261 6e73 666f 726d  custom transform
-0000f910: 6174 696f 6e20 6465 6669 6e69 7469 6f6e  ation definition
-0000f920: 730a 2020 2020 2020 696e 2073 7472 696e  s.      in strin
-0000f930: 6720 666f 726d 6174 2e0a 2020 2020 7466  g format..    tf
-0000f940: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
-0000f950: 5f70 6174 683a 2050 6174 6820 746f 2054  _path: Path to T
-0000f960: 4620 7472 616e 7366 6f72 6d61 7469 6f6e  F transformation
-0000f970: 2063 6f6e 6669 6775 7261 7469 6f6e 2e0a   configuration..
-0000f980: 2020 2020 6f70 7469 6d69 7a65 725f 7479      optimizer_ty
-0000f990: 7065 3a20 5468 6520 7479 7065 206f 6620  pe: The type of 
-0000f9a0: 6f70 7469 6d69 7a65 7220 746f 2075 7365  optimizer to use
-0000f9b0: 2e20 4368 6f69 6365 7320 6172 6520 2261  . Choices are "a
-0000f9c0: 6461 6d22 2c20 2266 7472 6c22 2061 6e64  dam", "ftrl" and
-0000f9d0: 0a20 2020 2020 2022 7367 6422 2066 6f72  .      "sgd" for
-0000f9e0: 2074 6865 2041 6461 6d2c 2046 5452 4c2c   the Adam, FTRL,
-0000f9f0: 2061 6e64 2047 7261 6469 656e 7420 4465   and Gradient De
-0000fa00: 7363 656e 7420 4f70 7469 6d69 7a65 7273  scent Optimizers
-0000fa10: 2c20 7265 7370 6563 7469 7665 6c79 2e0a  , respectively..
-0000fa20: 2020 2020 6d61 785f 7374 6570 733a 204e      max_steps: N
-0000fa30: 756d 6265 7220 6f66 2073 7465 7073 2074  umber of steps t
-0000fa40: 6f20 7275 6e20 7468 6520 7472 6169 6e65  o run the traine
-0000fa50: 7220 666f 722e 0a20 2020 206d 6178 5f74  r for..    max_t
-0000fa60: 7261 696e 5f73 6563 733a 2041 6d6f 756e  rain_secs: Amoun
-0000fa70: 7420 6f66 2074 696d 6520 696e 2073 6563  t of time in sec
-0000fa80: 6f6e 6473 2074 6f20 7275 6e20 7468 6520  onds to run the 
-0000fa90: 7472 6169 6e65 7220 666f 722e 0a20 2020  trainer for..   
-0000faa0: 206c 315f 7265 6775 6c61 7269 7a61 7469   l1_regularizati
-0000fab0: 6f6e 5f73 7472 656e 6774 683a 204c 3120  on_strength: L1 
-0000fac0: 7265 6775 6c61 7269 7a61 7469 6f6e 2073  regularization s
-0000fad0: 7472 656e 6774 6820 666f 720a 2020 2020  trength for.    
-0000fae0: 2020 6f70 7469 6d69 7a65 725f 7479 7065    optimizer_type
-0000faf0: 3d22 6674 726c 222e 0a20 2020 206c 325f  ="ftrl"..    l2_
-0000fb00: 7265 6775 6c61 7269 7a61 7469 6f6e 5f73  regularization_s
-0000fb10: 7472 656e 6774 683a 204c 3220 7265 6775  trength: L2 regu
-0000fb20: 6c61 7269 7a61 7469 6f6e 2073 7472 656e  larization stren
-0000fb30: 6774 6820 666f 720a 2020 2020 2020 6f70  gth for.      op
-0000fb40: 7469 6d69 7a65 725f 7479 7065 3d22 6674  timizer_type="ft
-0000fb50: 726c 222e 0a20 2020 206c 325f 7368 7269  rl"..    l2_shri
-0000fb60: 6e6b 6167 655f 7265 6775 6c61 7269 7a61  nkage_regulariza
-0000fb70: 7469 6f6e 5f73 7472 656e 6774 683a 204c  tion_strength: L
-0000fb80: 3220 7368 7269 6e6b 6167 6520 7265 6775  2 shrinkage regu
-0000fb90: 6c61 7269 7a61 7469 6f6e 2073 7472 656e  larization stren
-0000fba0: 6774 680a 2020 2020 2020 666f 7220 6f70  gth.      for op
-0000fbb0: 7469 6d69 7a65 725f 7479 7065 3d22 6674  timizer_type="ft
-0000fbc0: 726c 222e 0a20 2020 2062 6574 615f 313a  rl"..    beta_1:
-0000fbd0: 2042 6574 6120 3120 7661 6c75 6520 666f   Beta 1 value fo
-0000fbe0: 7220 6f70 7469 6d69 7a65 725f 7479 7065  r optimizer_type
-0000fbf0: 3d22 6164 616d 222e 0a20 2020 2062 6574  ="adam"..    bet
-0000fc00: 615f 323a 2042 6574 6120 3220 7661 6c75  a_2: Beta 2 valu
-0000fc10: 6520 666f 7220 6f70 7469 6d69 7a65 725f  e for optimizer_
-0000fc20: 7479 7065 3d22 6164 616d 222e 0a20 2020  type="adam"..   
-0000fc30: 2068 6964 6465 6e5f 756e 6974 733a 2048   hidden_units: H
-0000fc40: 6964 6465 6e20 6c61 7965 7220 7369 7a65  idden layer size
-0000fc50: 7320 746f 2075 7365 2066 6f72 2044 4e4e  s to use for DNN
-0000fc60: 2066 6561 7475 7265 2063 6f6c 756d 6e73   feature columns
-0000fc70: 2c20 7072 6f76 6964 6564 2069 6e0a 2020  , provided in.  
-0000fc80: 2020 2020 636f 6d6d 612d 7365 7061 7261      comma-separa
-0000fc90: 7465 6420 6c61 7965 7273 2e0a 2020 2020  ted layers..    
-0000fca0: 7573 655f 7769 6465 3a20 4966 2073 6574  use_wide: If set
-0000fcb0: 2074 6f20 7472 7565 2c20 7468 6520 6361   to true, the ca
-0000fcc0: 7465 676f 7269 6361 6c20 636f 6c75 6d6e  tegorical column
-0000fcd0: 7320 7769 6c6c 2062 6520 7573 6564 2069  s will be used i
-0000fce0: 6e20 7468 6520 7769 6465 0a20 2020 2020  n the wide.     
-0000fcf0: 2070 6172 7420 6f66 2074 6865 2044 4e4e   part of the DNN
-0000fd00: 206d 6f64 656c 2e0a 2020 2020 656d 6265   model..    embe
-0000fd10: 645f 6361 7465 676f 7269 6573 3a20 4966  d_categories: If
-0000fd20: 2073 6574 2074 6f20 7472 7565 2c20 7468   set to true, th
-0000fd30: 6520 6361 7465 676f 7269 6361 6c20 636f  e categorical co
-0000fd40: 6c75 6d6e 7320 7769 6c6c 2062 6520 7573  lumns will be us
-0000fd50: 6564 0a20 2020 2020 2065 6d62 6564 6465  ed.      embedde
-0000fd60: 6420 616e 6420 7573 6564 2069 6e20 7468  d and used in th
-0000fd70: 6520 6465 6570 2070 6172 7420 6f66 2074  e deep part of t
-0000fd80: 6865 206d 6f64 656c 2e20 456d 6265 6464  he model. Embedd
-0000fd90: 696e 6720 7369 7a65 2069 7320 7468 650a  ing size is the.
-0000fda0: 2020 2020 2020 7371 7561 7265 2072 6f6f        square roo
-0000fdb0: 7420 6f66 2074 6865 2063 6f6c 756d 6e20  t of the column 
-0000fdc0: 6361 7264 696e 616c 6974 792e 0a20 2020  cardinality..   
-0000fdd0: 2064 6e6e 5f64 726f 706f 7574 3a20 5468   dnn_dropout: Th
-0000fde0: 6520 7072 6f62 6162 696c 6974 7920 7765  e probability we
-0000fdf0: 2077 696c 6c20 6472 6f70 206f 7574 2061   will drop out a
-0000fe00: 2067 6976 656e 2063 6f6f 7264 696e 6174   given coordinat
-0000fe10: 652e 0a20 2020 2064 6e6e 5f6f 7074 696d  e..    dnn_optim
-0000fe20: 697a 6572 5f74 7970 653a 2054 6865 2074  izer_type: The t
-0000fe30: 7970 6520 6f66 206f 7074 696d 697a 6572  ype of optimizer
-0000fe40: 2074 6f20 7573 6520 666f 7220 7468 6520   to use for the 
-0000fe50: 6465 6570 2070 6172 7420 6f66 2074 6865  deep part of the
-0000fe60: 0a20 2020 2020 206d 6f64 656c 2e20 4368  .      model. Ch
-0000fe70: 6f69 6365 7320 6172 6520 2261 6461 6d22  oices are "adam"
-0000fe80: 2c20 2266 7472 6c22 2061 6e64 2022 7367  , "ftrl" and "sg
-0000fe90: 6422 2e20 666f 7220 7468 6520 4164 616d  d". for the Adam
-0000fea0: 2c20 4654 524c 2c20 616e 640a 2020 2020  , FTRL, and.    
-0000feb0: 2020 4772 6164 6965 6e74 2044 6573 6365    Gradient Desce
-0000fec0: 6e74 204f 7074 696d 697a 6572 732c 2072  nt Optimizers, r
-0000fed0: 6573 7065 6374 6976 656c 792e 0a20 2020  espectively..   
-0000fee0: 2064 6e6e 5f6c 315f 7265 6775 6c61 7269   dnn_l1_regulari
-0000fef0: 7a61 7469 6f6e 5f73 7472 656e 6774 683a  zation_strength:
-0000ff00: 204c 3120 7265 6775 6c61 7269 7a61 7469   L1 regularizati
-0000ff10: 6f6e 2073 7472 656e 6774 6820 666f 720a  on strength for.
-0000ff20: 2020 2020 2020 646e 6e5f 6f70 7469 6d69        dnn_optimi
-0000ff30: 7a65 725f 7479 7065 3d22 6674 726c 222e  zer_type="ftrl".
-0000ff40: 0a20 2020 2064 6e6e 5f6c 325f 7265 6775  .    dnn_l2_regu
-0000ff50: 6c61 7269 7a61 7469 6f6e 5f73 7472 656e  larization_stren
-0000ff60: 6774 683a 204c 3220 7265 6775 6c61 7269  gth: L2 regulari
-0000ff70: 7a61 7469 6f6e 2073 7472 656e 6774 6820  zation strength 
-0000ff80: 666f 720a 2020 2020 2020 646e 6e5f 6f70  for.      dnn_op
-0000ff90: 7469 6d69 7a65 725f 7479 7065 3d22 6674  timizer_type="ft
-0000ffa0: 726c 222e 0a20 2020 2064 6e6e 5f6c 325f  rl"..    dnn_l2_
-0000ffb0: 7368 7269 6e6b 6167 655f 7265 6775 6c61  shrinkage_regula
-0000ffc0: 7269 7a61 7469 6f6e 5f73 7472 656e 6774  rization_strengt
-0000ffd0: 683a 204c 3220 7368 7269 6e6b 6167 6520  h: L2 shrinkage 
-0000ffe0: 7265 6775 6c61 7269 7a61 7469 6f6e 0a20  regularization. 
-0000fff0: 2020 2020 2073 7472 656e 6774 6820 666f       strength fo
-00010000: 7220 646e 6e5f 6f70 7469 6d69 7a65 725f  r dnn_optimizer_
-00010010: 7479 7065 3d22 6674 726c 222e 0a20 2020  type="ftrl"..   
-00010020: 2064 6e6e 5f62 6574 615f 313a 2042 6574   dnn_beta_1: Bet
-00010030: 6120 3120 7661 6c75 6520 666f 7220 646e  a 1 value for dn
-00010040: 6e5f 6f70 7469 6d69 7a65 725f 7479 7065  n_optimizer_type
-00010050: 3d22 6164 616d 222e 0a20 2020 2064 6e6e  ="adam"..    dnn
-00010060: 5f62 6574 615f 323a 2042 6574 6120 3220  _beta_2: Beta 2 
-00010070: 7661 6c75 6520 666f 7220 646e 6e5f 6f70  value for dnn_op
-00010080: 7469 6d69 7a65 725f 7479 7065 3d22 6164  timizer_type="ad
-00010090: 616d 222e 0a20 2020 2065 6e61 626c 655f  am"..    enable_
-000100a0: 7072 6f66 696c 6572 3a20 456e 6162 6c65  profiler: Enable
-000100b0: 7320 7072 6f66 696c 696e 6720 616e 6420  s profiling and 
-000100c0: 7361 7665 7320 6120 7472 6163 6520 6475  saves a trace du
-000100d0: 7269 6e67 2065 7661 6c75 6174 696f 6e2e  ring evaluation.
-000100e0: 0a20 2020 2063 6163 6865 5f64 6174 613a  .    cache_data:
-000100f0: 2057 6865 7468 6572 2074 6f20 6361 6368   Whether to cach
-00010100: 6520 6461 7461 206f 7220 6e6f 742e 2049  e data or not. I
-00010110: 6620 7365 7420 746f 2027 6175 746f 272c  f set to 'auto',
-00010120: 2063 6163 6869 6e67 2069 730a 2020 2020   caching is.    
-00010130: 2020 6465 7465 726d 696e 6564 2062 6173    determined bas
-00010140: 6564 206f 6e20 7468 6520 6461 7461 7365  ed on the datase
-00010150: 7420 7369 7a65 2e0a 2020 2020 7365 6564  t size..    seed
-00010160: 3a20 5365 6564 2074 6f20 6265 2075 7365  : Seed to be use
-00010170: 6420 666f 7220 7468 6973 2072 756e 2e0a  d for this run..
-00010180: 2020 2020 6576 616c 5f73 7465 7073 3a20      eval_steps: 
-00010190: 4e75 6d62 6572 206f 6620 7374 6570 7320  Number of steps 
-000101a0: 746f 2072 756e 2065 7661 6c75 6174 696f  to run evaluatio
-000101b0: 6e20 666f 722e 2049 6620 6e6f 7420 7370  n for. If not sp
-000101c0: 6563 6966 6965 6420 6f72 0a20 2020 2020  ecified or.     
-000101d0: 206e 6567 6174 6976 652c 2069 7420 6d65   negative, it me
-000101e0: 616e 7320 7275 6e20 6576 616c 7561 7469  ans run evaluati
-000101f0: 6f6e 206f 6e20 7468 6520 7768 6f6c 6520  on on the whole 
-00010200: 7661 6c69 6461 7469 6f6e 2064 6174 6173  validation datas
-00010210: 6574 2e20 4966 2073 6574 0a20 2020 2020  et. If set.     
-00010220: 2074 6f20 302c 2069 7420 6d65 616e 7320   to 0, it means 
-00010230: 7275 6e20 6576 616c 7561 7469 6f6e 2066  run evaluation f
-00010240: 6f72 2061 2066 6978 6564 206e 756d 6265  or a fixed numbe
-00010250: 7220 6f66 2073 616d 706c 6573 2e0a 2020  r of samples..  
-00010260: 2020 6261 7463 685f 7369 7a65 3a20 4261    batch_size: Ba
-00010270: 7463 6820 7369 7a65 2066 6f72 2074 7261  tch size for tra
-00010280: 696e 696e 672e 0a20 2020 206d 6561 7375  ining..    measu
-00010290: 7265 6d65 6e74 5f73 656c 6563 7469 6f6e  rement_selection
-000102a0: 5f74 7970 653a 2057 6869 6368 206d 6561  _type: Which mea
-000102b0: 7375 7265 6d65 6e74 2074 6f20 7573 6520  surement to use 
-000102c0: 6966 2f77 6865 6e20 7468 6520 7365 7276  if/when the serv
-000102d0: 6963 650a 2020 2020 2020 6175 746f 6d61  ice.      automa
-000102e0: 7469 6361 6c6c 7920 7365 6c65 6374 7320  tically selects 
-000102f0: 7468 6520 6669 6e61 6c20 6d65 6173 7572  the final measur
-00010300: 656d 656e 7420 6672 6f6d 2070 7265 7669  ement from previ
-00010310: 6f75 736c 7920 7265 706f 7274 6564 0a20  ously reported. 
-00010320: 2020 2020 2069 6e74 6572 6d65 6469 6174       intermediat
-00010330: 6520 6d65 6173 7572 656d 656e 7473 2e20  e measurements. 
-00010340: 4f6e 6520 6f66 2022 4245 5354 5f4d 4541  One of "BEST_MEA
-00010350: 5355 5245 4d45 4e54 2220 6f72 0a20 2020  SUREMENT" or.   
-00010360: 2020 2022 4c41 5354 5f4d 4541 5355 5245     "LAST_MEASURE
-00010370: 4d45 4e54 222e 0a20 2020 206f 7074 696d  MENT"..    optim
-00010380: 697a 6174 696f 6e5f 6d65 7472 6963 3a20  ization_metric: 
-00010390: 4f70 7469 6d69 7a61 7469 6f6e 206d 6574  Optimization met
-000103a0: 7269 6320 7573 6564 2066 6f72 0a20 2020  ric used for.   
-000103b0: 2020 2060 6d65 6173 7572 656d 656e 745f     `measurement_
-000103c0: 7365 6c65 6374 696f 6e5f 7479 7065 602e  selection_type`.
-000103d0: 2044 6566 6175 6c74 2069 7320 2272 6d73   Default is "rms
-000103e0: 6522 2066 6f72 2072 6567 7265 7373 696f  e" for regressio
-000103f0: 6e20 616e 6420 2261 7563 220a 2020 2020  n and "auc".    
-00010400: 2020 666f 7220 636c 6173 7369 6669 6361    for classifica
-00010410: 7469 6f6e 2e0a 2020 2020 6576 616c 5f66  tion..    eval_f
-00010420: 7265 7175 656e 6379 5f73 6563 733a 2046  requency_secs: F
-00010430: 7265 7175 656e 6379 2061 7420 7768 6963  requency at whic
-00010440: 6820 6576 616c 7561 7469 6f6e 2061 6e64  h evaluation and
-00010450: 2063 6865 636b 706f 696e 7469 6e67 2077   checkpointing w
-00010460: 696c 6c0a 2020 2020 2020 7461 6b65 2070  ill.      take p
-00010470: 6c61 6365 2e0a 2020 2020 6461 7461 5f73  lace..    data_s
-00010480: 6f75 7263 655f 6373 765f 6669 6c65 6e61  ource_csv_filena
-00010490: 6d65 733a 2054 6865 2043 5356 2064 6174  mes: The CSV dat
-000104a0: 6120 736f 7572 6365 2e0a 2020 2020 6461  a source..    da
-000104b0: 7461 5f73 6f75 7263 655f 6269 6771 7565  ta_source_bigque
-000104c0: 7279 5f74 6162 6c65 5f70 6174 683a 2054  ry_table_path: T
-000104d0: 6865 2042 6967 5175 6572 7920 6461 7461  he BigQuery data
-000104e0: 2073 6f75 7263 652e 0a20 2020 2062 6967   source..    big
-000104f0: 7175 6572 795f 7374 6167 696e 675f 6675  query_staging_fu
-00010500: 6c6c 5f64 6174 6173 6574 5f69 643a 2054  ll_dataset_id: T
-00010510: 6865 2042 6967 5175 6572 7920 7374 6167  he BigQuery stag
-00010520: 696e 6720 6675 6c6c 2064 6174 6173 6574  ing full dataset
-00010530: 2069 6420 666f 720a 2020 2020 2020 7374   id for.      st
-00010540: 6f72 696e 6720 696e 7465 726d 6564 6961  oring intermedia
-00010550: 7465 2074 6162 6c65 732e 0a20 2020 2077  te tables..    w
-00010560: 6569 6768 745f 636f 6c75 6d6e 3a20 5468  eight_column: Th
-00010570: 6520 7765 6967 6874 2063 6f6c 756d 6e20  e weight column 
-00010580: 6e61 6d65 2e0a 2020 2020 7472 616e 7366  name..    transf
-00010590: 6f72 6d5f 6461 7461 666c 6f77 5f6d 6163  orm_dataflow_mac
-000105a0: 6869 6e65 5f74 7970 653a 2054 6865 2064  hine_type: The d
-000105b0: 6174 6166 6c6f 7720 6d61 6368 696e 6520  ataflow machine 
-000105c0: 7479 7065 2066 6f72 2074 7261 6e73 666f  type for transfo
-000105d0: 726d 0a20 2020 2020 2063 6f6d 706f 6e65  rm.      compone
-000105e0: 6e74 2e0a 2020 2020 7472 616e 7366 6f72  nt..    transfor
-000105f0: 6d5f 6461 7461 666c 6f77 5f6d 6178 5f6e  m_dataflow_max_n
-00010600: 756d 5f77 6f72 6b65 7273 3a20 5468 6520  um_workers: The 
-00010610: 6d61 7820 6e75 6d62 6572 206f 6620 4461  max number of Da
-00010620: 7461 666c 6f77 2077 6f72 6b65 7273 2066  taflow workers f
-00010630: 6f72 0a20 2020 2020 2074 7261 6e73 666f  or.      transfo
-00010640: 726d 2063 6f6d 706f 6e65 6e74 2e0a 2020  rm component..  
-00010650: 2020 7472 616e 7366 6f72 6d5f 6461 7461    transform_data
-00010660: 666c 6f77 5f64 6973 6b5f 7369 7a65 5f67  flow_disk_size_g
-00010670: 623a 2044 6174 6166 6c6f 7720 776f 726b  b: Dataflow work
-00010680: 6572 2773 2064 6973 6b20 7369 7a65 2069  er's disk size i
-00010690: 6e20 4742 2066 6f72 0a20 2020 2020 2074  n GB for.      t
-000106a0: 7261 6e73 666f 726d 2063 6f6d 706f 6e65  ransform compone
-000106b0: 6e74 2e0a 2020 2020 776f 726b 6572 5f70  nt..    worker_p
-000106c0: 6f6f 6c5f 7370 6563 735f 6f76 6572 7269  ool_specs_overri
-000106d0: 6465 3a20 5468 6520 6469 6374 696f 6e61  de: The dictiona
-000106e0: 7279 2066 6f72 206f 7665 7272 6964 696e  ry for overridin
-000106f0: 6720 7472 6169 6e69 6e67 2061 6e64 0a20  g training and. 
-00010700: 2020 2020 2065 7661 6c75 6174 696f 6e20       evaluation 
-00010710: 776f 726b 6572 2070 6f6f 6c20 7370 6563  worker pool spec
-00010720: 732e 2054 6865 2064 6963 7469 6f6e 6172  s. The dictionar
-00010730: 7920 7368 6f75 6c64 2062 6520 6f66 2066  y should be of f
-00010740: 6f72 6d61 740a 2020 2020 2020 2020 2020  ormat.          
-00010750: 6874 7470 733a 2f2f 6769 7468 7562 2e63  https://github.c
-00010760: 6f6d 2f67 6f6f 676c 6561 7069 732f 676f  om/googleapis/go
-00010770: 6f67 6c65 6170 6973 2f62 6c6f 622f 3465  ogleapis/blob/4e
-00010780: 3833 3663 3763 3235 3765 3365 3230 6231  836c7c257e3e20b1
-00010790: 6465 3134 6434 3730 3939 3361 3262 3166  de14d470993a2b1f
-000107a0: 3437 3336 6138 2f67 6f6f 676c 652f 636c  4736a8/google/cl
-000107b0: 6f75 642f 6169 706c 6174 666f 726d 2f76  oud/aiplatform/v
-000107c0: 3162 6574 6131 2f63 7573 746f 6d5f 6a6f  1beta1/custom_jo
-000107d0: 622e 7072 6f74 6f23 4c31 3732 2e0a 2020  b.proto#L172..  
-000107e0: 2020 7275 6e5f 6576 616c 7561 7469 6f6e    run_evaluation
-000107f0: 3a20 5768 6574 6865 7220 746f 2072 756e  : Whether to run
-00010800: 2065 7661 6c75 6174 696f 6e20 7374 6570   evaluation step
-00010810: 7320 6475 7269 6e67 2074 7261 696e 696e  s during trainin
-00010820: 672e 0a20 2020 2065 7661 6c75 6174 696f  g..    evaluatio
-00010830: 6e5f 6261 7463 685f 7072 6564 6963 745f  n_batch_predict_
-00010840: 6d61 6368 696e 655f 7479 7065 3a20 5468  machine_type: Th
-00010850: 6520 7072 6564 6963 7469 6f6e 2073 6572  e prediction ser
-00010860: 7665 7220 6d61 6368 696e 6520 7479 7065  ver machine type
-00010870: 0a20 2020 2020 2066 6f72 2062 6174 6368  .      for batch
-00010880: 2070 7265 6469 6374 2063 6f6d 706f 6e65   predict compone
-00010890: 6e74 7320 6475 7269 6e67 2065 7661 6c75  nts during evalu
-000108a0: 6174 696f 6e2e 0a20 2020 2065 7661 6c75  ation..    evalu
-000108b0: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
-000108c0: 6963 745f 7374 6172 7469 6e67 5f72 6570  ict_starting_rep
-000108d0: 6c69 6361 5f63 6f75 6e74 3a20 5468 6520  lica_count: The 
-000108e0: 696e 6974 6961 6c20 6e75 6d62 6572 206f  initial number o
-000108f0: 660a 2020 2020 2020 7072 6564 6963 7469  f.      predicti
-00010900: 6f6e 2073 6572 7665 7220 666f 7220 6261  on server for ba
-00010910: 7463 6820 7072 6564 6963 7420 636f 6d70  tch predict comp
-00010920: 6f6e 656e 7473 2064 7572 696e 6720 6576  onents during ev
-00010930: 616c 7561 7469 6f6e 2e0a 2020 2020 6576  aluation..    ev
-00010940: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
-00010950: 7265 6469 6374 5f6d 6178 5f72 6570 6c69  redict_max_repli
-00010960: 6361 5f63 6f75 6e74 3a20 5468 6520 6d61  ca_count: The ma
-00010970: 7820 6e75 6d62 6572 206f 6620 7072 6564  x number of pred
-00010980: 6963 7469 6f6e 0a20 2020 2020 2073 6572  iction.      ser
-00010990: 7665 7220 666f 7220 6261 7463 6820 7072  ver for batch pr
-000109a0: 6564 6963 7420 636f 6d70 6f6e 656e 7473  edict components
-000109b0: 2064 7572 696e 6720 6576 616c 7561 7469   during evaluati
-000109c0: 6f6e 2e0a 2020 2020 6576 616c 7561 7469  on..    evaluati
-000109d0: 6f6e 5f64 6174 6166 6c6f 775f 6d61 6368  on_dataflow_mach
-000109e0: 696e 655f 7479 7065 3a20 5468 6520 6461  ine_type: The da
-000109f0: 7461 666c 6f77 206d 6163 6869 6e65 2074  taflow machine t
-00010a00: 7970 6520 666f 7220 6576 616c 7561 7469  ype for evaluati
-00010a10: 6f6e 0a20 2020 2020 2063 6f6d 706f 6e65  on.      compone
-00010a20: 6e74 732e 0a20 2020 2065 7661 6c75 6174  nts..    evaluat
-00010a30: 696f 6e5f 6461 7461 666c 6f77 5f73 7461  ion_dataflow_sta
-00010a40: 7274 696e 675f 6e75 6d5f 776f 726b 6572  rting_num_worker
-00010a50: 733a 2054 6865 2069 6e69 7469 616c 206e  s: The initial n
-00010a60: 756d 6265 7220 6f66 2044 6174 6166 6c6f  umber of Dataflo
-00010a70: 770a 2020 2020 2020 776f 726b 6572 7320  w.      workers 
-00010a80: 666f 7220 6576 616c 7561 7469 6f6e 2063  for evaluation c
-00010a90: 6f6d 706f 6e65 6e74 732e 0a20 2020 2065  omponents..    e
-00010aa0: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
-00010ab0: 6f77 5f6d 6178 5f6e 756d 5f77 6f72 6b65  ow_max_num_worke
-00010ac0: 7273 3a20 5468 6520 6d61 7820 6e75 6d62  rs: The max numb
-00010ad0: 6572 206f 6620 4461 7461 666c 6f77 2077  er of Dataflow w
-00010ae0: 6f72 6b65 7273 2066 6f72 0a20 2020 2020  orkers for.     
-00010af0: 2065 7661 6c75 6174 696f 6e20 636f 6d70   evaluation comp
-00010b00: 6f6e 656e 7473 2e0a 2020 2020 6576 616c  onents..    eval
-00010b10: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
-00010b20: 6469 736b 5f73 697a 655f 6762 3a20 4461  disk_size_gb: Da
-00010b30: 7461 666c 6f77 2077 6f72 6b65 7227 7320  taflow worker's 
-00010b40: 6469 736b 2073 697a 6520 696e 2047 4220  disk size in GB 
-00010b50: 666f 720a 2020 2020 2020 6576 616c 7561  for.      evalua
-00010b60: 7469 6f6e 2063 6f6d 706f 6e65 6e74 732e  tion components.
-00010b70: 0a20 2020 2064 6174 6166 6c6f 775f 7365  .    dataflow_se
-00010b80: 7276 6963 655f 6163 636f 756e 743a 2043  rvice_account: C
-00010b90: 7573 746f 6d20 7365 7276 6963 6520 6163  ustom service ac
-00010ba0: 636f 756e 7420 746f 2072 756e 2064 6174  count to run dat
-00010bb0: 6166 6c6f 7720 6a6f 6273 2e0a 2020 2020  aflow jobs..    
-00010bc0: 6461 7461 666c 6f77 5f73 7562 6e65 7477  dataflow_subnetw
-00010bd0: 6f72 6b3a 2044 6174 6166 6c6f 7727 7320  ork: Dataflow's 
-00010be0: 6675 6c6c 7920 7175 616c 6966 6965 6420  fully qualified 
-00010bf0: 7375 626e 6574 776f 726b 206e 616d 652c  subnetwork name,
-00010c00: 2077 6865 6e20 656d 7074 790a 2020 2020   when empty.    
-00010c10: 2020 7468 6520 6465 6661 756c 7420 7375    the default su
-00010c20: 626e 6574 776f 726b 2077 696c 6c20 6265  bnetwork will be
-00010c30: 2075 7365 642e 2045 7861 6d70 6c65 3a0a   used. Example:.
-00010c40: 2020 2020 2020 2020 6874 7470 733a 2f2f          https://
-00010c50: 636c 6f75 642e 676f 6f67 6c65 2e63 6f6d  cloud.google.com
-00010c60: 2f64 6174 6166 6c6f 772f 646f 6373 2f67  /dataflow/docs/g
-00010c70: 7569 6465 732f 7370 6563 6966 7969 6e67  uides/specifying
-00010c80: 2d6e 6574 776f 726b 7323 6578 616d 706c  -networks#exampl
-00010c90: 655f 6e65 7477 6f72 6b5f 616e 645f 7375  e_network_and_su
-00010ca0: 626e 6574 776f 726b 5f73 7065 6369 6669  bnetwork_specifi
-00010cb0: 6361 7469 6f6e 730a 2020 2020 6461 7461  cations.    data
-00010cc0: 666c 6f77 5f75 7365 5f70 7562 6c69 635f  flow_use_public_
-00010cd0: 6970 733a 2053 7065 6369 6669 6573 2077  ips: Specifies w
-00010ce0: 6865 7468 6572 2044 6174 6166 6c6f 7720  hether Dataflow 
-00010cf0: 776f 726b 6572 7320 7573 6520 7075 626c  workers use publ
-00010d00: 6963 2049 500a 2020 2020 2020 6164 6472  ic IP.      addr
-00010d10: 6573 7365 732e 0a20 2020 2065 6e63 7279  esses..    encry
-00010d20: 7074 696f 6e5f 7370 6563 5f6b 6579 5f6e  ption_spec_key_n
-00010d30: 616d 653a 2054 6865 204b 4d53 206b 6579  ame: The KMS key
-00010d40: 206e 616d 652e 0a0a 2020 5265 7475 726e   name...  Return
-00010d50: 733a 0a20 2020 2054 7570 6c65 206f 6620  s:.    Tuple of 
-00010d60: 7069 7065 6c69 6e65 5f64 6566 696e 6974  pipeline_definit
-00010d70: 696f 6e5f 7061 7468 2061 6e64 2070 6172  ion_path and par
-00010d80: 616d 6574 6572 5f76 616c 7565 732e 0a20  ameter_values.. 
-00010d90: 2022 2222 0a20 2069 6620 6973 696e 7374   """.  if isinst
-00010da0: 616e 6365 2874 665f 6175 746f 5f74 7261  ance(tf_auto_tra
-00010db0: 6e73 666f 726d 5f66 6561 7475 7265 732c  nsform_features,
-00010dc0: 206c 6973 7429 3a0a 2020 2020 7466 5f61   list):.    tf_a
-00010dd0: 7574 6f5f 7472 616e 7366 6f72 6d5f 6665  uto_transform_fe
-00010de0: 6174 7572 6573 203d 207b 2761 7574 6f27  atures = {'auto'
-00010df0: 3a20 7466 5f61 7574 6f5f 7472 616e 7366  : tf_auto_transf
-00010e00: 6f72 6d5f 6665 6174 7572 6573 7d0a 0a20  orm_features}.. 
-00010e10: 2069 6620 7472 616e 7366 6f72 6d5f 636f   if transform_co
-00010e20: 6e66 6967 2061 6e64 2074 665f 7472 616e  nfig and tf_tran
-00010e30: 7366 6f72 6d61 7469 6f6e 735f 7061 7468  sformations_path
-00010e40: 3a0a 2020 2020 7261 6973 6520 5661 6c75  :.    raise Valu
-00010e50: 6545 7272 6f72 280a 2020 2020 2020 2020  eError(.        
-00010e60: 274f 6e6c 7920 6f6e 6520 6f66 2074 7261  'Only one of tra
-00010e70: 6e73 666f 726d 5f63 6f6e 6669 6720 616e  nsform_config an
-00010e80: 6420 7466 5f74 7261 6e73 666f 726d 6174  d tf_transformat
-00010e90: 696f 6e73 5f70 6174 6820 6361 6e20 270a  ions_path can '.
-00010ea0: 2020 2020 2020 2020 2762 6520 7370 6563          'be spec
-00010eb0: 6966 6965 642e 270a 2020 2020 290a 0a20  ified.'.    ).. 
-00010ec0: 2065 6c69 6620 7472 616e 7366 6f72 6d5f   elif transform_
-00010ed0: 636f 6e66 6967 3a0a 2020 2020 7761 726e  config:.    warn
-00010ee0: 696e 6773 2e77 6172 6e28 0a20 2020 2020  ings.warn(.     
-00010ef0: 2020 2027 7472 616e 7366 6f72 6d5f 636f     'transform_co
-00010f00: 6e66 6967 2070 6172 616d 6574 6572 2069  nfig parameter i
-00010f10: 7320 6465 7072 6563 6174 6564 2e20 270a  s deprecated. '.
-00010f20: 2020 2020 2020 2020 2750 6c65 6173 6520          'Please 
-00010f30: 7573 6520 7468 6520 666c 6174 7465 6e65  use the flattene
-00010f40: 6420 7472 616e 7366 6f72 6d20 636f 6e66  d transform conf
-00010f50: 6967 2061 7267 756d 656e 7473 2069 6e73  ig arguments ins
-00010f60: 7465 6164 2e27 0a20 2020 2029 0a20 2020  tead.'.    ).   
-00010f70: 2074 665f 7472 616e 7366 6f72 6d61 7469   tf_transformati
-00010f80: 6f6e 735f 7061 7468 203d 2074 7261 6e73  ons_path = trans
-00010f90: 666f 726d 5f63 6f6e 6669 670a 0a20 2069  form_config..  i
-00010fa0: 6620 6e6f 7420 776f 726b 6572 5f70 6f6f  f not worker_poo
-00010fb0: 6c5f 7370 6563 735f 6f76 6572 7269 6465  l_specs_override
-00010fc0: 3a0a 2020 2020 776f 726b 6572 5f70 6f6f  :.    worker_poo
-00010fd0: 6c5f 7370 6563 735f 6f76 6572 7269 6465  l_specs_override
-00010fe0: 203d 205b 5d0a 0a20 2070 6172 616d 6574   = []..  paramet
-00010ff0: 6572 5f76 616c 7565 7320 3d20 7b7d 0a20  er_values = {}. 
-00011000: 2074 7261 696e 696e 675f 616e 645f 6576   training_and_ev
-00011010: 616c 5f70 6172 616d 6574 6572 7320 3d20  al_parameters = 
-00011020: 7b0a 2020 2020 2020 2770 726f 6a65 6374  {.      'project
-00011030: 273a 2070 726f 6a65 6374 2c0a 2020 2020  ': project,.    
-00011040: 2020 276c 6f63 6174 696f 6e27 3a20 6c6f    'location': lo
-00011050: 6361 7469 6f6e 2c0a 2020 2020 2020 2772  cation,.      'r
-00011060: 6f6f 745f 6469 7227 3a20 726f 6f74 5f64  oot_dir': root_d
-00011070: 6972 2c0a 2020 2020 2020 2774 6172 6765  ir,.      'targe
-00011080: 745f 636f 6c75 6d6e 273a 2074 6172 6765  t_column': targe
-00011090: 745f 636f 6c75 6d6e 2c0a 2020 2020 2020  t_column,.      
-000110a0: 2770 7265 6469 6374 696f 6e5f 7479 7065  'prediction_type
-000110b0: 273a 2070 7265 6469 6374 696f 6e5f 7479  ': prediction_ty
-000110c0: 7065 2c0a 2020 2020 2020 276c 6561 726e  pe,.      'learn
-000110d0: 696e 675f 7261 7465 273a 206c 6561 726e  ing_rate': learn
-000110e0: 696e 675f 7261 7465 2c0a 2020 2020 2020  ing_rate,.      
-000110f0: 2764 6e6e 5f6c 6561 726e 696e 675f 7261  'dnn_learning_ra
-00011100: 7465 273a 2064 6e6e 5f6c 6561 726e 696e  te': dnn_learnin
-00011110: 675f 7261 7465 2c0a 2020 2020 2020 276f  g_rate,.      'o
-00011120: 7074 696d 697a 6572 5f74 7970 6527 3a20  ptimizer_type': 
-00011130: 6f70 7469 6d69 7a65 725f 7479 7065 2c0a  optimizer_type,.
-00011140: 2020 2020 2020 276d 6178 5f73 7465 7073        'max_steps
-00011150: 273a 206d 6178 5f73 7465 7073 2c0a 2020  ': max_steps,.  
-00011160: 2020 2020 276d 6178 5f74 7261 696e 5f73      'max_train_s
-00011170: 6563 7327 3a20 6d61 785f 7472 6169 6e5f  ecs': max_train_
-00011180: 7365 6373 2c0a 2020 2020 2020 276c 315f  secs,.      'l1_
-00011190: 7265 6775 6c61 7269 7a61 7469 6f6e 5f73  regularization_s
-000111a0: 7472 656e 6774 6827 3a20 6c31 5f72 6567  trength': l1_reg
-000111b0: 756c 6172 697a 6174 696f 6e5f 7374 7265  ularization_stre
-000111c0: 6e67 7468 2c0a 2020 2020 2020 276c 325f  ngth,.      'l2_
-000111d0: 7265 6775 6c61 7269 7a61 7469 6f6e 5f73  regularization_s
-000111e0: 7472 656e 6774 6827 3a20 6c32 5f72 6567  trength': l2_reg
-000111f0: 756c 6172 697a 6174 696f 6e5f 7374 7265  ularization_stre
-00011200: 6e67 7468 2c0a 2020 2020 2020 276c 325f  ngth,.      'l2_
-00011210: 7368 7269 6e6b 6167 655f 7265 6775 6c61  shrinkage_regula
-00011220: 7269 7a61 7469 6f6e 5f73 7472 656e 6774  rization_strengt
-00011230: 6827 3a20 280a 2020 2020 2020 2020 2020  h': (.          
-00011240: 6c32 5f73 6872 696e 6b61 6765 5f72 6567  l2_shrinkage_reg
-00011250: 756c 6172 697a 6174 696f 6e5f 7374 7265  ularization_stre
-00011260: 6e67 7468 0a20 2020 2020 2029 2c0a 2020  ngth.      ),.  
-00011270: 2020 2020 2762 6574 615f 3127 3a20 6265      'beta_1': be
-00011280: 7461 5f31 2c0a 2020 2020 2020 2762 6574  ta_1,.      'bet
-00011290: 615f 3227 3a20 6265 7461 5f32 2c0a 2020  a_2': beta_2,.  
-000112a0: 2020 2020 2768 6964 6465 6e5f 756e 6974      'hidden_unit
-000112b0: 7327 3a20 6869 6464 656e 5f75 6e69 7473  s': hidden_units
-000112c0: 2c0a 2020 2020 2020 2775 7365 5f77 6964  ,.      'use_wid
-000112d0: 6527 3a20 7573 655f 7769 6465 2c0a 2020  e': use_wide,.  
-000112e0: 2020 2020 2765 6d62 6564 5f63 6174 6567      'embed_categ
-000112f0: 6f72 6965 7327 3a20 656d 6265 645f 6361  ories': embed_ca
-00011300: 7465 676f 7269 6573 2c0a 2020 2020 2020  tegories,.      
-00011310: 2764 6e6e 5f64 726f 706f 7574 273a 2064  'dnn_dropout': d
-00011320: 6e6e 5f64 726f 706f 7574 2c0a 2020 2020  nn_dropout,.    
-00011330: 2020 2764 6e6e 5f6f 7074 696d 697a 6572    'dnn_optimizer
-00011340: 5f74 7970 6527 3a20 646e 6e5f 6f70 7469  _type': dnn_opti
-00011350: 6d69 7a65 725f 7479 7065 2c0a 2020 2020  mizer_type,.    
-00011360: 2020 2764 6e6e 5f6c 315f 7265 6775 6c61    'dnn_l1_regula
-00011370: 7269 7a61 7469 6f6e 5f73 7472 656e 6774  rization_strengt
-00011380: 6827 3a20 646e 6e5f 6c31 5f72 6567 756c  h': dnn_l1_regul
-00011390: 6172 697a 6174 696f 6e5f 7374 7265 6e67  arization_streng
-000113a0: 7468 2c0a 2020 2020 2020 2764 6e6e 5f6c  th,.      'dnn_l
-000113b0: 325f 7265 6775 6c61 7269 7a61 7469 6f6e  2_regularization
-000113c0: 5f73 7472 656e 6774 6827 3a20 646e 6e5f  _strength': dnn_
-000113d0: 6c32 5f72 6567 756c 6172 697a 6174 696f  l2_regularizatio
-000113e0: 6e5f 7374 7265 6e67 7468 2c0a 2020 2020  n_strength,.    
-000113f0: 2020 2764 6e6e 5f6c 325f 7368 7269 6e6b    'dnn_l2_shrink
-00011400: 6167 655f 7265 6775 6c61 7269 7a61 7469  age_regularizati
-00011410: 6f6e 5f73 7472 656e 6774 6827 3a20 280a  on_strength': (.
-00011420: 2020 2020 2020 2020 2020 646e 6e5f 6c32            dnn_l2
-00011430: 5f73 6872 696e 6b61 6765 5f72 6567 756c  _shrinkage_regul
-00011440: 6172 697a 6174 696f 6e5f 7374 7265 6e67  arization_streng
-00011450: 7468 0a20 2020 2020 2029 2c0a 2020 2020  th.      ),.    
-00011460: 2020 2764 6e6e 5f62 6574 615f 3127 3a20    'dnn_beta_1': 
-00011470: 646e 6e5f 6265 7461 5f31 2c0a 2020 2020  dnn_beta_1,.    
-00011480: 2020 2764 6e6e 5f62 6574 615f 3227 3a20    'dnn_beta_2': 
-00011490: 646e 6e5f 6265 7461 5f32 2c0a 2020 2020  dnn_beta_2,.    
-000114a0: 2020 2765 6e61 626c 655f 7072 6f66 696c    'enable_profil
-000114b0: 6572 273a 2065 6e61 626c 655f 7072 6f66  er': enable_prof
-000114c0: 696c 6572 2c0a 2020 2020 2020 2763 6163  iler,.      'cac
-000114d0: 6865 5f64 6174 6127 3a20 6361 6368 655f  he_data': cache_
-000114e0: 6461 7461 2c0a 2020 2020 2020 2773 6565  data,.      'see
-000114f0: 6427 3a20 7365 6564 2c0a 2020 2020 2020  d': seed,.      
-00011500: 2765 7661 6c5f 7374 6570 7327 3a20 6576  'eval_steps': ev
-00011510: 616c 5f73 7465 7073 2c0a 2020 2020 2020  al_steps,.      
-00011520: 2762 6174 6368 5f73 697a 6527 3a20 6261  'batch_size': ba
-00011530: 7463 685f 7369 7a65 2c0a 2020 2020 2020  tch_size,.      
-00011540: 276d 6561 7375 7265 6d65 6e74 5f73 656c  'measurement_sel
-00011550: 6563 7469 6f6e 5f74 7970 6527 3a20 6d65  ection_type': me
-00011560: 6173 7572 656d 656e 745f 7365 6c65 6374  asurement_select
-00011570: 696f 6e5f 7479 7065 2c0a 2020 2020 2020  ion_type,.      
-00011580: 276f 7074 696d 697a 6174 696f 6e5f 6d65  'optimization_me
-00011590: 7472 6963 273a 206f 7074 696d 697a 6174  tric': optimizat
-000115a0: 696f 6e5f 6d65 7472 6963 2c0a 2020 2020  ion_metric,.    
-000115b0: 2020 2765 7661 6c5f 6672 6571 7565 6e63    'eval_frequenc
-000115c0: 795f 7365 6373 273a 2065 7661 6c5f 6672  y_secs': eval_fr
-000115d0: 6571 7565 6e63 795f 7365 6373 2c0a 2020  equency_secs,.  
-000115e0: 2020 2020 2777 6569 6768 745f 636f 6c75      'weight_colu
-000115f0: 6d6e 273a 2077 6569 6768 745f 636f 6c75  mn': weight_colu
-00011600: 6d6e 2c0a 2020 2020 2020 2774 7261 6e73  mn,.      'trans
-00011610: 666f 726d 5f64 6174 6166 6c6f 775f 6d61  form_dataflow_ma
-00011620: 6368 696e 655f 7479 7065 273a 2074 7261  chine_type': tra
-00011630: 6e73 666f 726d 5f64 6174 6166 6c6f 775f  nsform_dataflow_
-00011640: 6d61 6368 696e 655f 7479 7065 2c0a 2020  machine_type,.  
-00011650: 2020 2020 2774 7261 6e73 666f 726d 5f64      'transform_d
-00011660: 6174 6166 6c6f 775f 6d61 785f 6e75 6d5f  ataflow_max_num_
-00011670: 776f 726b 6572 7327 3a20 7472 616e 7366  workers': transf
-00011680: 6f72 6d5f 6461 7461 666c 6f77 5f6d 6178  orm_dataflow_max
-00011690: 5f6e 756d 5f77 6f72 6b65 7273 2c0a 2020  _num_workers,.  
-000116a0: 2020 2020 2774 7261 6e73 666f 726d 5f64      'transform_d
-000116b0: 6174 6166 6c6f 775f 6469 736b 5f73 697a  ataflow_disk_siz
-000116c0: 655f 6762 273a 2074 7261 6e73 666f 726d  e_gb': transform
-000116d0: 5f64 6174 6166 6c6f 775f 6469 736b 5f73  _dataflow_disk_s
-000116e0: 697a 655f 6762 2c0a 2020 2020 2020 2777  ize_gb,.      'w
-000116f0: 6f72 6b65 725f 706f 6f6c 5f73 7065 6373  orker_pool_specs
-00011700: 5f6f 7665 7272 6964 6527 3a20 776f 726b  _override': work
-00011710: 6572 5f70 6f6f 6c5f 7370 6563 735f 6f76  er_pool_specs_ov
-00011720: 6572 7269 6465 2c0a 2020 2020 2020 2772  erride,.      'r
-00011730: 756e 5f65 7661 6c75 6174 696f 6e27 3a20  un_evaluation': 
-00011740: 7275 6e5f 6576 616c 7561 7469 6f6e 2c0a  run_evaluation,.
-00011750: 2020 2020 2020 2765 7661 6c75 6174 696f        'evaluatio
-00011760: 6e5f 6261 7463 685f 7072 6564 6963 745f  n_batch_predict_
-00011770: 6d61 6368 696e 655f 7479 7065 273a 2028  machine_type': (
-00011780: 0a20 2020 2020 2020 2020 2065 7661 6c75  .          evalu
-00011790: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
-000117a0: 6963 745f 6d61 6368 696e 655f 7479 7065  ict_machine_type
-000117b0: 0a20 2020 2020 2029 2c0a 2020 2020 2020  .      ),.      
-000117c0: 2765 7661 6c75 6174 696f 6e5f 6261 7463  'evaluation_batc
-000117d0: 685f 7072 6564 6963 745f 7374 6172 7469  h_predict_starti
-000117e0: 6e67 5f72 6570 6c69 6361 5f63 6f75 6e74  ng_replica_count
-000117f0: 273a 2028 0a20 2020 2020 2020 2020 2065  ': (.          e
-00011800: 7661 6c75 6174 696f 6e5f 6261 7463 685f  valuation_batch_
-00011810: 7072 6564 6963 745f 7374 6172 7469 6e67  predict_starting
-00011820: 5f72 6570 6c69 6361 5f63 6f75 6e74 0a20  _replica_count. 
-00011830: 2020 2020 2029 2c0a 2020 2020 2020 2765       ),.      'e
-00011840: 7661 6c75 6174 696f 6e5f 6261 7463 685f  valuation_batch_
-00011850: 7072 6564 6963 745f 6d61 785f 7265 706c  predict_max_repl
-00011860: 6963 615f 636f 756e 7427 3a20 280a 2020  ica_count': (.  
-00011870: 2020 2020 2020 2020 6576 616c 7561 7469          evaluati
-00011880: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
-00011890: 5f6d 6178 5f72 6570 6c69 6361 5f63 6f75  _max_replica_cou
-000118a0: 6e74 0a20 2020 2020 2029 2c0a 2020 2020  nt.      ),.    
-000118b0: 2020 2765 7661 6c75 6174 696f 6e5f 6461    'evaluation_da
-000118c0: 7461 666c 6f77 5f6d 6163 6869 6e65 5f74  taflow_machine_t
-000118d0: 7970 6527 3a20 6576 616c 7561 7469 6f6e  ype': evaluation
-000118e0: 5f64 6174 6166 6c6f 775f 6d61 6368 696e  _dataflow_machin
-000118f0: 655f 7479 7065 2c0a 2020 2020 2020 2765  e_type,.      'e
-00011900: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
-00011910: 6f77 5f73 7461 7274 696e 675f 6e75 6d5f  ow_starting_num_
-00011920: 776f 726b 6572 7327 3a20 280a 2020 2020  workers': (.    
-00011930: 2020 2020 2020 6576 616c 7561 7469 6f6e        evaluation
-00011940: 5f64 6174 6166 6c6f 775f 7374 6172 7469  _dataflow_starti
-00011950: 6e67 5f6e 756d 5f77 6f72 6b65 7273 0a20  ng_num_workers. 
-00011960: 2020 2020 2029 2c0a 2020 2020 2020 2765       ),.      'e
-00011970: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
-00011980: 6f77 5f6d 6178 5f6e 756d 5f77 6f72 6b65  ow_max_num_worke
-00011990: 7273 273a 2028 0a20 2020 2020 2020 2020  rs': (.         
-000119a0: 2065 7661 6c75 6174 696f 6e5f 6461 7461   evaluation_data
-000119b0: 666c 6f77 5f6d 6178 5f6e 756d 5f77 6f72  flow_max_num_wor
-000119c0: 6b65 7273 0a20 2020 2020 2029 2c0a 2020  kers.      ),.  
-000119d0: 2020 2020 2765 7661 6c75 6174 696f 6e5f      'evaluation_
-000119e0: 6461 7461 666c 6f77 5f64 6973 6b5f 7369  dataflow_disk_si
-000119f0: 7a65 5f67 6227 3a20 6576 616c 7561 7469  ze_gb': evaluati
-00011a00: 6f6e 5f64 6174 6166 6c6f 775f 6469 736b  on_dataflow_disk
-00011a10: 5f73 697a 655f 6762 2c0a 2020 2020 2020  _size_gb,.      
-00011a20: 2764 6174 6166 6c6f 775f 7365 7276 6963  'dataflow_servic
-00011a30: 655f 6163 636f 756e 7427 3a20 6461 7461  e_account': data
-00011a40: 666c 6f77 5f73 6572 7669 6365 5f61 6363  flow_service_acc
-00011a50: 6f75 6e74 2c0a 2020 2020 2020 2764 6174  ount,.      'dat
-00011a60: 6166 6c6f 775f 7375 626e 6574 776f 726b  aflow_subnetwork
-00011a70: 273a 2064 6174 6166 6c6f 775f 7375 626e  ': dataflow_subn
-00011a80: 6574 776f 726b 2c0a 2020 2020 2020 2764  etwork,.      'd
-00011a90: 6174 6166 6c6f 775f 7573 655f 7075 626c  ataflow_use_publ
-00011aa0: 6963 5f69 7073 273a 2064 6174 6166 6c6f  ic_ips': dataflo
-00011ab0: 775f 7573 655f 7075 626c 6963 5f69 7073  w_use_public_ips
-00011ac0: 2c0a 2020 2020 2020 2765 6e63 7279 7074  ,.      'encrypt
-00011ad0: 696f 6e5f 7370 6563 5f6b 6579 5f6e 616d  ion_spec_key_nam
-00011ae0: 6527 3a20 656e 6372 7970 7469 6f6e 5f73  e': encryption_s
-00011af0: 7065 635f 6b65 795f 6e61 6d65 2c0a 2020  pec_key_name,.  
-00011b00: 7d0a 2020 5f75 7064 6174 655f 7061 7261  }.  _update_para
-00011b10: 6d65 7465 7273 2870 6172 616d 6574 6572  meters(parameter
-00011b20: 5f76 616c 7565 732c 2074 7261 696e 696e  _values, trainin
-00011b30: 675f 616e 645f 6576 616c 5f70 6172 616d  g_and_eval_param
-00011b40: 6574 6572 7329 0a0a 2020 6674 655f 7061  eters)..  fte_pa
-00011b50: 7261 6d73 203d 207b 0a20 2020 2020 2027  rams = {.      '
-00011b60: 6461 7461 7365 745f 6c65 7665 6c5f 6375  dataset_level_cu
-00011b70: 7374 6f6d 5f74 7261 6e73 666f 726d 6174  stom_transformat
-00011b80: 696f 6e5f 6465 6669 6e69 7469 6f6e 7327  ion_definitions'
-00011b90: 3a20 280a 2020 2020 2020 2020 2020 6461  : (.          da
-00011ba0: 7461 7365 745f 6c65 7665 6c5f 6375 7374  taset_level_cust
-00011bb0: 6f6d 5f74 7261 6e73 666f 726d 6174 696f  om_transformatio
-00011bc0: 6e5f 6465 6669 6e69 7469 6f6e 730a 2020  n_definitions.  
-00011bd0: 2020 2020 2020 2020 6966 2064 6174 6173          if datas
-00011be0: 6574 5f6c 6576 656c 5f63 7573 746f 6d5f  et_level_custom_
-00011bf0: 7472 616e 7366 6f72 6d61 7469 6f6e 5f64  transformation_d
-00011c00: 6566 696e 6974 696f 6e73 0a20 2020 2020  efinitions.     
-00011c10: 2020 2020 2065 6c73 6520 5b5d 0a20 2020       else [].   
-00011c20: 2020 2029 2c0a 2020 2020 2020 2764 6174     ),.      'dat
-00011c30: 6173 6574 5f6c 6576 656c 5f74 7261 6e73  aset_level_trans
-00011c40: 666f 726d 6174 696f 6e73 273a 2028 0a20  formations': (. 
-00011c50: 2020 2020 2020 2020 2064 6174 6173 6574           dataset
-00011c60: 5f6c 6576 656c 5f74 7261 6e73 666f 726d  _level_transform
-00011c70: 6174 696f 6e73 2069 6620 6461 7461 7365  ations if datase
-00011c80: 745f 6c65 7665 6c5f 7472 616e 7366 6f72  t_level_transfor
-00011c90: 6d61 7469 6f6e 7320 656c 7365 205b 5d0a  mations else [].
-00011ca0: 2020 2020 2020 292c 0a20 2020 2020 2027        ),.      '
-00011cb0: 7275 6e5f 6665 6174 7572 655f 7365 6c65  run_feature_sele
-00011cc0: 6374 696f 6e27 3a20 7275 6e5f 6665 6174  ction': run_feat
-00011cd0: 7572 655f 7365 6c65 6374 696f 6e2c 0a20  ure_selection,. 
-00011ce0: 2020 2020 2027 6665 6174 7572 655f 7365       'feature_se
-00011cf0: 6c65 6374 696f 6e5f 616c 676f 7269 7468  lection_algorith
-00011d00: 6d27 3a20 6665 6174 7572 655f 7365 6c65  m': feature_sele
-00011d10: 6374 696f 6e5f 616c 676f 7269 7468 6d2c  ction_algorithm,
-00011d20: 0a20 2020 2020 2027 6d61 785f 7365 6c65  .      'max_sele
-00011d30: 6374 6564 5f66 6561 7475 7265 7327 3a20  cted_features': 
-00011d40: 6d61 785f 7365 6c65 6374 6564 5f66 6561  max_selected_fea
-00011d50: 7475 7265 732c 0a20 2020 2020 2027 7072  tures,.      'pr
-00011d60: 6564 6566 696e 6564 5f73 706c 6974 5f6b  edefined_split_k
-00011d70: 6579 273a 2070 7265 6465 6669 6e65 645f  ey': predefined_
-00011d80: 7370 6c69 745f 6b65 792c 0a20 2020 2020  split_key,.     
-00011d90: 2027 7374 7261 7469 6669 6564 5f73 706c   'stratified_spl
-00011da0: 6974 5f6b 6579 273a 2073 7472 6174 6966  it_key': stratif
-00011db0: 6965 645f 7370 6c69 745f 6b65 792c 0a20  ied_split_key,. 
-00011dc0: 2020 2020 2027 7472 6169 6e69 6e67 5f66       'training_f
-00011dd0: 7261 6374 696f 6e27 3a20 7472 6169 6e69  raction': traini
-00011de0: 6e67 5f66 7261 6374 696f 6e2c 0a20 2020  ng_fraction,.   
-00011df0: 2020 2027 7661 6c69 6461 7469 6f6e 5f66     'validation_f
-00011e00: 7261 6374 696f 6e27 3a20 7661 6c69 6461  raction': valida
-00011e10: 7469 6f6e 5f66 7261 6374 696f 6e2c 0a20  tion_fraction,. 
-00011e20: 2020 2020 2027 7465 7374 5f66 7261 6374       'test_fract
-00011e30: 696f 6e27 3a20 7465 7374 5f66 7261 6374  ion': test_fract
-00011e40: 696f 6e2c 0a20 2020 2020 2027 7466 5f61  ion,.      'tf_a
-00011e50: 7574 6f5f 7472 616e 7366 6f72 6d5f 6665  uto_transform_fe
-00011e60: 6174 7572 6573 273a 2028 0a20 2020 2020  atures': (.     
-00011e70: 2020 2020 2074 665f 6175 746f 5f74 7261       tf_auto_tra
-00011e80: 6e73 666f 726d 5f66 6561 7475 7265 7320  nsform_features 
-00011e90: 6966 2074 665f 6175 746f 5f74 7261 6e73  if tf_auto_trans
-00011ea0: 666f 726d 5f66 6561 7475 7265 7320 656c  form_features el
-00011eb0: 7365 207b 7d0a 2020 2020 2020 292c 0a20  se {}.      ),. 
-00011ec0: 2020 2020 2027 7466 5f63 7573 746f 6d5f       'tf_custom_
-00011ed0: 7472 616e 7366 6f72 6d61 7469 6f6e 5f64  transformation_d
-00011ee0: 6566 696e 6974 696f 6e73 273a 2028 0a20  efinitions': (. 
-00011ef0: 2020 2020 2020 2020 2074 665f 6375 7374           tf_cust
-00011f00: 6f6d 5f74 7261 6e73 666f 726d 6174 696f  om_transformatio
-00011f10: 6e5f 6465 6669 6e69 7469 6f6e 730a 2020  n_definitions.  
-00011f20: 2020 2020 2020 2020 6966 2074 665f 6375          if tf_cu
-00011f30: 7374 6f6d 5f74 7261 6e73 666f 726d 6174  stom_transformat
-00011f40: 696f 6e5f 6465 6669 6e69 7469 6f6e 730a  ion_definitions.
-00011f50: 2020 2020 2020 2020 2020 656c 7365 205b            else [
-00011f60: 5d0a 2020 2020 2020 292c 0a20 2020 2020  ].      ),.     
-00011f70: 2027 7466 5f74 7261 6e73 666f 726d 6174   'tf_transformat
-00011f80: 696f 6e73 5f70 6174 6827 3a20 7466 5f74  ions_path': tf_t
-00011f90: 7261 6e73 666f 726d 6174 696f 6e73 5f70  ransformations_p
-00011fa0: 6174 682c 0a20 2020 2020 2027 6d61 7465  ath,.      'mate
-00011fb0: 7269 616c 697a 6564 5f65 7861 6d70 6c65  rialized_example
-00011fc0: 735f 666f 726d 6174 273a 2028 0a20 2020  s_format': (.   
-00011fd0: 2020 2020 2020 206d 6174 6572 6961 6c69         materiali
-00011fe0: 7a65 645f 6578 616d 706c 6573 5f66 6f72  zed_examples_for
-00011ff0: 6d61 740a 2020 2020 2020 2020 2020 6966  mat.          if
-00012000: 206d 6174 6572 6961 6c69 7a65 645f 6578   materialized_ex
-00012010: 616d 706c 6573 5f66 6f72 6d61 740a 2020  amples_format.  
-00012020: 2020 2020 2020 2020 656c 7365 2027 7466          else 'tf
-00012030: 7265 636f 7264 735f 677a 6970 270a 2020  records_gzip'.  
-00012040: 2020 2020 292c 0a20 2020 2020 2027 7466      ),.      'tf
-00012050: 5f74 7261 6e73 666f 726d 5f65 7865 6375  _transform_execu
-00012060: 7469 6f6e 5f65 6e67 696e 6527 3a20 280a  tion_engine': (.
-00012070: 2020 2020 2020 2020 2020 7466 5f74 7261            tf_tra
-00012080: 6e73 666f 726d 5f65 7865 6375 7469 6f6e  nsform_execution
-00012090: 5f65 6e67 696e 650a 2020 2020 2020 2020  _engine.        
-000120a0: 2020 6966 2074 665f 7472 616e 7366 6f72    if tf_transfor
-000120b0: 6d5f 6578 6563 7574 696f 6e5f 656e 6769  m_execution_engi
-000120c0: 6e65 0a20 2020 2020 2020 2020 2065 6c73  ne.          els
-000120d0: 6520 2764 6174 6166 6c6f 7727 0a20 2020  e 'dataflow'.   
-000120e0: 2020 2029 2c0a 2020 7d0a 2020 5f75 7064     ),.  }.  _upd
-000120f0: 6174 655f 7061 7261 6d65 7465 7273 2870  ate_parameters(p
-00012100: 6172 616d 6574 6572 5f76 616c 7565 732c  arameter_values,
-00012110: 2066 7465 5f70 6172 616d 7329 0a0a 2020   fte_params)..  
-00012120: 6461 7461 5f73 6f75 7263 655f 616e 645f  data_source_and_
-00012130: 7370 6c69 745f 7061 7261 6d65 7465 7273  split_parameters
-00012140: 203d 207b 0a20 2020 2020 2027 6461 7461   = {.      'data
-00012150: 5f73 6f75 7263 655f 6373 765f 6669 6c65  _source_csv_file
-00012160: 6e61 6d65 7327 3a20 6461 7461 5f73 6f75  names': data_sou
-00012170: 7263 655f 6373 765f 6669 6c65 6e61 6d65  rce_csv_filename
-00012180: 732c 0a20 2020 2020 2027 6461 7461 5f73  s,.      'data_s
-00012190: 6f75 7263 655f 6269 6771 7565 7279 5f74  ource_bigquery_t
-000121a0: 6162 6c65 5f70 6174 6827 3a20 6461 7461  able_path': data
-000121b0: 5f73 6f75 7263 655f 6269 6771 7565 7279  _source_bigquery
-000121c0: 5f74 6162 6c65 5f70 6174 682c 0a20 2020  _table_path,.   
-000121d0: 2020 2027 6269 6771 7565 7279 5f73 7461     'bigquery_sta
-000121e0: 6769 6e67 5f66 756c 6c5f 6461 7461 7365  ging_full_datase
-000121f0: 745f 6964 273a 2062 6967 7175 6572 795f  t_id': bigquery_
-00012200: 7374 6167 696e 675f 6675 6c6c 5f64 6174  staging_full_dat
-00012210: 6173 6574 5f69 642c 0a20 207d 0a20 205f  aset_id,.  }.  _
-00012220: 7570 6461 7465 5f70 6172 616d 6574 6572  update_parameter
-00012230: 7328 7061 7261 6d65 7465 725f 7661 6c75  s(parameter_valu
-00012240: 6573 2c20 6461 7461 5f73 6f75 7263 655f  es, data_source_
-00012250: 616e 645f 7370 6c69 745f 7061 7261 6d65  and_split_parame
-00012260: 7465 7273 290a 0a20 2070 6970 656c 696e  ters)..  pipelin
-00012270: 655f 6465 6669 6e69 7469 6f6e 5f70 6174  e_definition_pat
-00012280: 6820 3d20 6f73 2e70 6174 682e 6a6f 696e  h = os.path.join
-00012290: 280a 2020 2020 2020 7061 7468 6c69 622e  (.      pathlib.
-000122a0: 5061 7468 285f 5f66 696c 655f 5f29 2e70  Path(__file__).p
-000122b0: 6172 656e 742e 7265 736f 6c76 6528 292c  arent.resolve(),
-000122c0: 0a20 2020 2020 2027 7769 6465 5f61 6e64  .      'wide_and
-000122d0: 5f64 6565 705f 7472 6169 6e65 725f 7069  _deep_trainer_pi
-000122e0: 7065 6c69 6e65 2e79 616d 6c27 2c0a 2020  peline.yaml',.  
-000122f0: 290a 0a20 2072 6574 7572 6e20 7069 7065  )..  return pipe
-00012300: 6c69 6e65 5f64 6566 696e 6974 696f 6e5f  line_definition_
-00012310: 7061 7468 2c20 7061 7261 6d65 7465 725f  path, parameter_
-00012320: 7661 6c75 6573 0a0a 0a64 6566 2067 6574  values...def get
-00012330: 5f62 7569 6c74 696e 5f61 6c67 6f72 6974  _builtin_algorit
-00012340: 686d 5f68 7970 6572 7061 7261 6d65 7465  hm_hyperparamete
-00012350: 725f 7475 6e69 6e67 5f6a 6f62 5f70 6970  r_tuning_job_pip
-00012360: 656c 696e 655f 616e 645f 7061 7261 6d65  eline_and_parame
-00012370: 7465 7273 280a 2020 2020 7072 6f6a 6563  ters(.    projec
-00012380: 743a 2073 7472 2c0a 2020 2020 6c6f 6361  t: str,.    loca
-00012390: 7469 6f6e 3a20 7374 722c 0a20 2020 2072  tion: str,.    r
-000123a0: 6f6f 745f 6469 723a 2073 7472 2c0a 2020  oot_dir: str,.  
-000123b0: 2020 7461 7267 6574 5f63 6f6c 756d 6e3a    target_column:
-000123c0: 2073 7472 2c0a 2020 2020 7072 6564 6963   str,.    predic
-000123d0: 7469 6f6e 5f74 7970 653a 2073 7472 2c0a  tion_type: str,.
-000123e0: 2020 2020 7374 7564 795f 7370 6563 5f6d      study_spec_m
-000123f0: 6574 7269 635f 6964 3a20 7374 722c 0a20  etric_id: str,. 
-00012400: 2020 2073 7475 6479 5f73 7065 635f 6d65     study_spec_me
-00012410: 7472 6963 5f67 6f61 6c3a 2073 7472 2c0a  tric_goal: str,.
-00012420: 2020 2020 7374 7564 795f 7370 6563 5f70      study_spec_p
-00012430: 6172 616d 6574 6572 735f 6f76 6572 7269  arameters_overri
-00012440: 6465 3a20 4c69 7374 5b44 6963 745b 7374  de: List[Dict[st
-00012450: 722c 2041 6e79 5d5d 2c0a 2020 2020 6d61  r, Any]],.    ma
-00012460: 785f 7472 6961 6c5f 636f 756e 743a 2069  x_trial_count: i
-00012470: 6e74 2c0a 2020 2020 7061 7261 6c6c 656c  nt,.    parallel
-00012480: 5f74 7269 616c 5f63 6f75 6e74 3a20 696e  _trial_count: in
-00012490: 742c 0a20 2020 2061 6c67 6f72 6974 686d  t,.    algorithm
-000124a0: 3a20 7374 722c 0a20 2020 2065 6e61 626c  : str,.    enabl
-000124b0: 655f 7072 6f66 696c 6572 3a20 626f 6f6c  e_profiler: bool
-000124c0: 203d 2046 616c 7365 2c0a 2020 2020 7365   = False,.    se
-000124d0: 6564 3a20 696e 7420 3d20 312c 0a20 2020  ed: int = 1,.   
-000124e0: 2065 7661 6c5f 7374 6570 733a 2069 6e74   eval_steps: int
-000124f0: 203d 2030 2c0a 2020 2020 6576 616c 5f66   = 0,.    eval_f
-00012500: 7265 7175 656e 6379 5f73 6563 733a 2069  requency_secs: i
-00012510: 6e74 203d 2036 3030 2c0a 2020 2020 7472  nt = 600,.    tr
-00012520: 616e 7366 6f72 6d5f 636f 6e66 6967 3a20  ansform_config: 
-00012530: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
-00012540: 4e6f 6e65 2c0a 2020 2020 6461 7461 7365  None,.    datase
-00012550: 745f 6c65 7665 6c5f 6375 7374 6f6d 5f74  t_level_custom_t
-00012560: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
-00012570: 6669 6e69 7469 6f6e 733a 204f 7074 696f  finitions: Optio
-00012580: 6e61 6c5b 0a20 2020 2020 2020 204c 6973  nal[.        Lis
-00012590: 745b 4469 6374 5b73 7472 2c20 416e 795d  t[Dict[str, Any]
-000125a0: 5d0a 2020 2020 5d20 3d20 4e6f 6e65 2c0a  ].    ] = None,.
-000125b0: 2020 2020 6461 7461 7365 745f 6c65 7665      dataset_leve
-000125c0: 6c5f 7472 616e 7366 6f72 6d61 7469 6f6e  l_transformation
-000125d0: 733a 204f 7074 696f 6e61 6c5b 4c69 7374  s: Optional[List
-000125e0: 5b44 6963 745b 7374 722c 2041 6e79 5d5d  [Dict[str, Any]]
-000125f0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7072  ] = None,.    pr
-00012600: 6564 6566 696e 6564 5f73 706c 6974 5f6b  edefined_split_k
-00012610: 6579 3a20 4f70 7469 6f6e 616c 5b73 7472  ey: Optional[str
-00012620: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7374  ] = None,.    st
-00012630: 7261 7469 6669 6564 5f73 706c 6974 5f6b  ratified_split_k
-00012640: 6579 3a20 4f70 7469 6f6e 616c 5b73 7472  ey: Optional[str
-00012650: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7472  ] = None,.    tr
-00012660: 6169 6e69 6e67 5f66 7261 6374 696f 6e3a  aining_fraction:
-00012670: 204f 7074 696f 6e61 6c5b 666c 6f61 745d   Optional[float]
-00012680: 203d 204e 6f6e 652c 0a20 2020 2076 616c   = None,.    val
-00012690: 6964 6174 696f 6e5f 6672 6163 7469 6f6e  idation_fraction
-000126a0: 3a20 4f70 7469 6f6e 616c 5b66 6c6f 6174  : Optional[float
-000126b0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7465  ] = None,.    te
-000126c0: 7374 5f66 7261 6374 696f 6e3a 204f 7074  st_fraction: Opt
-000126d0: 696f 6e61 6c5b 666c 6f61 745d 203d 204e  ional[float] = N
-000126e0: 6f6e 652c 0a20 2020 2074 665f 7472 616e  one,.    tf_tran
-000126f0: 7366 6f72 6d5f 6578 6563 7574 696f 6e5f  sform_execution_
-00012700: 656e 6769 6e65 3a20 4f70 7469 6f6e 616c  engine: Optional
-00012710: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 2020  [str] = None,.  
-00012720: 2020 7466 5f61 7574 6f5f 7472 616e 7366    tf_auto_transf
-00012730: 6f72 6d5f 6665 6174 7572 6573 3a20 4f70  orm_features: Op
-00012740: 7469 6f6e 616c 5b0a 2020 2020 2020 2020  tional[.        
-00012750: 556e 696f 6e5b 4c69 7374 5b73 7472 5d2c  Union[List[str],
-00012760: 2044 6963 745b 7374 722c 204c 6973 745b   Dict[str, List[
-00012770: 7374 725d 5d5d 0a20 2020 205d 203d 204e  str]]].    ] = N
-00012780: 6f6e 652c 0a20 2020 2074 665f 6375 7374  one,.    tf_cust
-00012790: 6f6d 5f74 7261 6e73 666f 726d 6174 696f  om_transformatio
-000127a0: 6e5f 6465 6669 6e69 7469 6f6e 733a 204f  n_definitions: O
-000127b0: 7074 696f 6e61 6c5b 4c69 7374 5b44 6963  ptional[List[Dic
-000127c0: 745b 7374 722c 2041 6e79 5d5d 5d20 3d20  t[str, Any]]] = 
-000127d0: 4e6f 6e65 2c0a 2020 2020 7466 5f74 7261  None,.    tf_tra
-000127e0: 6e73 666f 726d 6174 696f 6e73 5f70 6174  nsformations_pat
-000127f0: 683a 204f 7074 696f 6e61 6c5b 7374 725d  h: Optional[str]
-00012800: 203d 204e 6f6e 652c 0a20 2020 2064 6174   = None,.    dat
-00012810: 615f 736f 7572 6365 5f63 7376 5f66 696c  a_source_csv_fil
-00012820: 656e 616d 6573 3a20 4f70 7469 6f6e 616c  enames: Optional
-00012830: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 2020  [str] = None,.  
-00012840: 2020 6461 7461 5f73 6f75 7263 655f 6269    data_source_bi
-00012850: 6771 7565 7279 5f74 6162 6c65 5f70 6174  gquery_table_pat
-00012860: 683a 204f 7074 696f 6e61 6c5b 7374 725d  h: Optional[str]
-00012870: 203d 204e 6f6e 652c 0a20 2020 2062 6967   = None,.    big
-00012880: 7175 6572 795f 7374 6167 696e 675f 6675  query_staging_fu
-00012890: 6c6c 5f64 6174 6173 6574 5f69 643a 204f  ll_dataset_id: O
-000128a0: 7074 696f 6e61 6c5b 7374 725d 203d 204e  ptional[str] = N
-000128b0: 6f6e 652c 0a20 2020 2077 6569 6768 745f  one,.    weight_
-000128c0: 636f 6c75 6d6e 3a20 7374 7220 3d20 2727  column: str = ''
-000128d0: 2c0a 2020 2020 6d61 785f 6661 696c 6564  ,.    max_failed
-000128e0: 5f74 7269 616c 5f63 6f75 6e74 3a20 696e  _trial_count: in
-000128f0: 7420 3d20 302c 0a20 2020 2073 7475 6479  t = 0,.    study
-00012900: 5f73 7065 635f 616c 676f 7269 7468 6d3a  _spec_algorithm:
-00012910: 2073 7472 203d 2027 414c 474f 5249 5448   str = 'ALGORITH
-00012920: 4d5f 554e 5350 4543 4946 4945 4427 2c0a  M_UNSPECIFIED',.
-00012930: 2020 2020 7374 7564 795f 7370 6563 5f6d      study_spec_m
-00012940: 6561 7375 7265 6d65 6e74 5f73 656c 6563  easurement_selec
-00012950: 7469 6f6e 5f74 7970 653a 2073 7472 203d  tion_type: str =
-00012960: 2027 4245 5354 5f4d 4541 5355 5245 4d45   'BEST_MEASUREME
-00012970: 4e54 272c 0a20 2020 2074 7261 6e73 666f  NT',.    transfo
-00012980: 726d 5f64 6174 6166 6c6f 775f 6d61 6368  rm_dataflow_mach
-00012990: 696e 655f 7479 7065 3a20 7374 7220 3d20  ine_type: str = 
-000129a0: 276e 312d 7374 616e 6461 7264 2d31 3627  'n1-standard-16'
-000129b0: 2c0a 2020 2020 7472 616e 7366 6f72 6d5f  ,.    transform_
-000129c0: 6461 7461 666c 6f77 5f6d 6178 5f6e 756d  dataflow_max_num
-000129d0: 5f77 6f72 6b65 7273 3a20 696e 7420 3d20  _workers: int = 
-000129e0: 3235 2c0a 2020 2020 7472 616e 7366 6f72  25,.    transfor
-000129f0: 6d5f 6461 7461 666c 6f77 5f64 6973 6b5f  m_dataflow_disk_
-00012a00: 7369 7a65 5f67 623a 2069 6e74 203d 2034  size_gb: int = 4
-00012a10: 302c 0a20 2020 2077 6f72 6b65 725f 706f  0,.    worker_po
-00012a20: 6f6c 5f73 7065 6373 5f6f 7665 7272 6964  ol_specs_overrid
-00012a30: 653a 204f 7074 696f 6e61 6c5b 4469 6374  e: Optional[Dict
-00012a40: 5b73 7472 2c20 416e 795d 5d20 3d20 4e6f  [str, Any]] = No
-00012a50: 6e65 2c0a 2020 2020 7275 6e5f 6576 616c  ne,.    run_eval
-00012a60: 7561 7469 6f6e 3a20 626f 6f6c 203d 2054  uation: bool = T
-00012a70: 7275 652c 0a20 2020 2065 7661 6c75 6174  rue,.    evaluat
-00012a80: 696f 6e5f 6261 7463 685f 7072 6564 6963  ion_batch_predic
-00012a90: 745f 6d61 6368 696e 655f 7479 7065 3a20  t_machine_type: 
-00012aa0: 7374 7220 3d20 5f45 5641 4c55 4154 494f  str = _EVALUATIO
-00012ab0: 4e5f 4241 5443 485f 5052 4544 4943 545f  N_BATCH_PREDICT_
-00012ac0: 4d41 4348 494e 455f 5459 5045 2c0a 2020  MACHINE_TYPE,.  
-00012ad0: 2020 6576 616c 7561 7469 6f6e 5f62 6174    evaluation_bat
-00012ae0: 6368 5f70 7265 6469 6374 5f73 7461 7274  ch_predict_start
-00012af0: 696e 675f 7265 706c 6963 615f 636f 756e  ing_replica_coun
-00012b00: 743a 2069 6e74 203d 205f 4556 414c 5541  t: int = _EVALUA
-00012b10: 5449 4f4e 5f42 4154 4348 5f50 5245 4449  TION_BATCH_PREDI
-00012b20: 4354 5f53 5441 5254 494e 475f 5245 504c  CT_STARTING_REPL
-00012b30: 4943 415f 434f 554e 542c 0a20 2020 2065  ICA_COUNT,.    e
-00012b40: 7661 6c75 6174 696f 6e5f 6261 7463 685f  valuation_batch_
-00012b50: 7072 6564 6963 745f 6d61 785f 7265 706c  predict_max_repl
-00012b60: 6963 615f 636f 756e 743a 2069 6e74 203d  ica_count: int =
-00012b70: 205f 4556 414c 5541 5449 4f4e 5f42 4154   _EVALUATION_BAT
-00012b80: 4348 5f50 5245 4449 4354 5f4d 4158 5f52  CH_PREDICT_MAX_R
-00012b90: 4550 4c49 4341 5f43 4f55 4e54 2c0a 2020  EPLICA_COUNT,.  
-00012ba0: 2020 6576 616c 7561 7469 6f6e 5f64 6174    evaluation_dat
-00012bb0: 6166 6c6f 775f 6d61 6368 696e 655f 7479  aflow_machine_ty
-00012bc0: 7065 3a20 7374 7220 3d20 5f45 5641 4c55  pe: str = _EVALU
-00012bd0: 4154 494f 4e5f 4441 5441 464c 4f57 5f4d  ATION_DATAFLOW_M
-00012be0: 4143 4849 4e45 5f54 5950 452c 0a20 2020  ACHINE_TYPE,.   
-00012bf0: 2065 7661 6c75 6174 696f 6e5f 6461 7461   evaluation_data
-00012c00: 666c 6f77 5f73 7461 7274 696e 675f 6e75  flow_starting_nu
-00012c10: 6d5f 776f 726b 6572 733a 2069 6e74 203d  m_workers: int =
-00012c20: 205f 4556 414c 5541 5449 4f4e 5f44 4154   _EVALUATION_DAT
-00012c30: 4146 4c4f 575f 5354 4152 5449 4e47 5f4e  AFLOW_STARTING_N
-00012c40: 554d 5f57 4f52 4b45 5253 2c0a 2020 2020  UM_WORKERS,.    
-00012c50: 6576 616c 7561 7469 6f6e 5f64 6174 6166  evaluation_dataf
-00012c60: 6c6f 775f 6d61 785f 6e75 6d5f 776f 726b  low_max_num_work
-00012c70: 6572 733a 2069 6e74 203d 205f 4556 414c  ers: int = _EVAL
-00012c80: 5541 5449 4f4e 5f44 4154 4146 4c4f 575f  UATION_DATAFLOW_
-00012c90: 4d41 585f 4e55 4d5f 574f 524b 4552 532c  MAX_NUM_WORKERS,
-00012ca0: 0a20 2020 2065 7661 6c75 6174 696f 6e5f  .    evaluation_
-00012cb0: 6461 7461 666c 6f77 5f64 6973 6b5f 7369  dataflow_disk_si
-00012cc0: 7a65 5f67 623a 2069 6e74 203d 205f 4556  ze_gb: int = _EV
-00012cd0: 414c 5541 5449 4f4e 5f44 4154 4146 4c4f  ALUATION_DATAFLO
-00012ce0: 575f 4449 534b 5f53 495a 455f 4742 2c0a  W_DISK_SIZE_GB,.
-00012cf0: 2020 2020 6461 7461 666c 6f77 5f73 6572      dataflow_ser
-00012d00: 7669 6365 5f61 6363 6f75 6e74 3a20 7374  vice_account: st
-00012d10: 7220 3d20 2727 2c0a 2020 2020 6461 7461  r = '',.    data
-00012d20: 666c 6f77 5f73 7562 6e65 7477 6f72 6b3a  flow_subnetwork:
-00012d30: 2073 7472 203d 2027 272c 0a20 2020 2064   str = '',.    d
-00012d40: 6174 6166 6c6f 775f 7573 655f 7075 626c  ataflow_use_publ
-00012d50: 6963 5f69 7073 3a20 626f 6f6c 203d 2054  ic_ips: bool = T
-00012d60: 7275 652c 0a20 2020 2065 6e63 7279 7074  rue,.    encrypt
-00012d70: 696f 6e5f 7370 6563 5f6b 6579 5f6e 616d  ion_spec_key_nam
-00012d80: 653a 2073 7472 203d 2027 272c 0a29 202d  e: str = '',.) -
-00012d90: 3e20 5475 706c 655b 7374 722c 2044 6963  > Tuple[str, Dic
-00012da0: 745b 7374 722c 2041 6e79 5d5d 3a0a 2020  t[str, Any]]:.  
-00012db0: 2222 2247 6574 2074 6865 2062 7569 6c74  """Get the built
-00012dc0: 2d69 6e20 616c 676f 7269 7468 6d20 4879  -in algorithm Hy
-00012dd0: 7065 7270 6172 616d 6574 6572 5475 6e69  perparameterTuni
-00012de0: 6e67 4a6f 6220 7069 7065 6c69 6e65 2e0a  ngJob pipeline..
-00012df0: 0a20 2041 7267 733a 0a20 2020 2070 726f  .  Args:.    pro
-00012e00: 6a65 6374 3a20 5468 6520 4743 5020 7072  ject: The GCP pr
-00012e10: 6f6a 6563 7420 7468 6174 2072 756e 7320  oject that runs 
-00012e20: 7468 6520 7069 7065 6c69 6e65 2063 6f6d  the pipeline com
-00012e30: 706f 6e65 6e74 732e 0a20 2020 206c 6f63  ponents..    loc
-00012e40: 6174 696f 6e3a 2054 6865 2047 4350 2072  ation: The GCP r
-00012e50: 6567 696f 6e20 7468 6174 2072 756e 7320  egion that runs 
-00012e60: 7468 6520 7069 7065 6c69 6e65 2063 6f6d  the pipeline com
-00012e70: 706f 6e65 6e74 732e 0a20 2020 2072 6f6f  ponents..    roo
-00012e80: 745f 6469 723a 2054 6865 2072 6f6f 7420  t_dir: The root 
-00012e90: 4743 5320 6469 7265 6374 6f72 7920 666f  GCS directory fo
-00012ea0: 7220 7468 6520 7069 7065 6c69 6e65 2063  r the pipeline c
-00012eb0: 6f6d 706f 6e65 6e74 732e 0a20 2020 2074  omponents..    t
-00012ec0: 6172 6765 745f 636f 6c75 6d6e 3a20 5468  arget_column: Th
-00012ed0: 6520 7461 7267 6574 2063 6f6c 756d 6e20  e target column 
-00012ee0: 6e61 6d65 2e0a 2020 2020 7072 6564 6963  name..    predic
-00012ef0: 7469 6f6e 5f74 7970 653a 2054 6865 2074  tion_type: The t
-00012f00: 7970 6520 6f66 2070 7265 6469 6374 696f  ype of predictio
-00012f10: 6e20 7468 6520 6d6f 6465 6c20 6973 2074  n the model is t
-00012f20: 6f20 7072 6f64 7563 652e 0a20 2020 2020  o produce..     
-00012f30: 2022 636c 6173 7369 6669 6361 7469 6f6e   "classification
-00012f40: 2220 6f72 2022 7265 6772 6573 7369 6f6e  " or "regression
-00012f50: 222e 0a20 2020 2073 7475 6479 5f73 7065  "..    study_spe
-00012f60: 635f 6d65 7472 6963 5f69 643a 204d 6574  c_metric_id: Met
-00012f70: 7269 6320 746f 206f 7074 696d 697a 652c  ric to optimize,
-00012f80: 2070 6f73 7369 626c 6520 7661 6c75 6573   possible values
-00012f90: 3a20 5b20 276c 6f73 7327 2c0a 2020 2020  : [ 'loss',.    
-00012fa0: 2020 2761 7665 7261 6765 5f6c 6f73 7327    'average_loss'
-00012fb0: 2c20 2772 6d73 6527 2c20 276d 6165 272c  , 'rmse', 'mae',
-00012fc0: 2027 6d71 6c27 2c20 2761 6363 7572 6163   'mql', 'accurac
-00012fd0: 7927 2c20 2761 7563 272c 2027 7072 6563  y', 'auc', 'prec
-00012fe0: 6973 696f 6e27 2c0a 2020 2020 2020 2772  ision',.      'r
-00012ff0: 6563 616c 6c27 5d2e 0a20 2020 2073 7475  ecall']..    stu
-00013000: 6479 5f73 7065 635f 6d65 7472 6963 5f67  dy_spec_metric_g
-00013010: 6f61 6c3a 204f 7074 696d 697a 6174 696f  oal: Optimizatio
-00013020: 6e20 676f 616c 206f 6620 7468 6520 6d65  n goal of the me
-00013030: 7472 6963 2c20 706f 7373 6962 6c65 2076  tric, possible v
-00013040: 616c 7565 733a 0a20 2020 2020 2022 4d41  alues:.      "MA
-00013050: 5849 4d49 5a45 222c 2022 4d49 4e49 4d49  XIMIZE", "MINIMI
-00013060: 5a45 222e 0a20 2020 2073 7475 6479 5f73  ZE"..    study_s
-00013070: 7065 635f 7061 7261 6d65 7465 7273 5f6f  pec_parameters_o
-00013080: 7665 7272 6964 653a 204c 6973 7420 6f66  verride: List of
-00013090: 2064 6963 7469 6f6e 6172 6965 7320 7265   dictionaries re
-000130a0: 7072 6573 656e 7469 6e67 2070 6172 616d  presenting param
-000130b0: 6574 6572 730a 2020 2020 2020 746f 206f  eters.      to o
-000130c0: 7074 696d 697a 652e 2054 6865 2064 6963  ptimize. The dic
-000130d0: 7469 6f6e 6172 7920 6b65 7920 6973 2074  tionary key is t
-000130e0: 6865 2070 6172 616d 6574 6572 5f69 642c  he parameter_id,
-000130f0: 2077 6869 6368 2069 7320 7061 7373 6564   which is passed
-00013100: 2074 6f0a 2020 2020 2020 7472 6169 6e69   to.      traini
-00013110: 6e67 206a 6f62 2061 7320 6120 636f 6d6d  ng job as a comm
-00013120: 616e 6420 6c69 6e65 2061 7267 756d 656e  and line argumen
-00013130: 742c 2061 6e64 2074 6865 2064 6963 7469  t, and the dicti
-00013140: 6f6e 6172 7920 7661 6c75 6520 6973 2074  onary value is t
-00013150: 6865 0a20 2020 2020 2070 6172 616d 6574  he.      paramet
-00013160: 6572 2073 7065 6369 6669 6361 7469 6f6e  er specification
-00013170: 206f 6620 7468 6520 6d65 7472 6963 2e0a   of the metric..
-00013180: 2020 2020 6d61 785f 7472 6961 6c5f 636f      max_trial_co
-00013190: 756e 743a 2054 6865 2064 6573 6972 6564  unt: The desired
-000131a0: 2074 6f74 616c 206e 756d 6265 7220 6f66   total number of
-000131b0: 2074 7269 616c 732e 0a20 2020 2070 6172   trials..    par
-000131c0: 616c 6c65 6c5f 7472 6961 6c5f 636f 756e  allel_trial_coun
-000131d0: 743a 2054 6865 2064 6573 6972 6564 206e  t: The desired n
-000131e0: 756d 6265 7220 6f66 2074 7269 616c 7320  umber of trials 
-000131f0: 746f 2072 756e 2069 6e20 7061 7261 6c6c  to run in parall
-00013200: 656c 2e0a 2020 2020 616c 676f 7269 7468  el..    algorith
-00013210: 6d3a 2041 6c67 6f72 6974 686d 2074 6f20  m: Algorithm to 
-00013220: 7472 6169 6e2e 204f 6e65 206f 6620 2274  train. One of "t
-00013230: 6162 6e65 7422 2061 6e64 2022 7769 6465  abnet" and "wide
-00013240: 5f61 6e64 5f64 6565 7022 2e0a 2020 2020  _and_deep"..    
-00013250: 656e 6162 6c65 5f70 726f 6669 6c65 723a  enable_profiler:
-00013260: 2045 6e61 626c 6573 2070 726f 6669 6c69   Enables profili
-00013270: 6e67 2061 6e64 2073 6176 6573 2061 2074  ng and saves a t
-00013280: 7261 6365 2064 7572 696e 6720 6576 616c  race during eval
-00013290: 7561 7469 6f6e 2e0a 2020 2020 7365 6564  uation..    seed
-000132a0: 3a20 5365 6564 2074 6f20 6265 2075 7365  : Seed to be use
-000132b0: 6420 666f 7220 7468 6973 2072 756e 2e0a  d for this run..
-000132c0: 2020 2020 6576 616c 5f73 7465 7073 3a20      eval_steps: 
-000132d0: 4e75 6d62 6572 206f 6620 7374 6570 7320  Number of steps 
-000132e0: 746f 2072 756e 2065 7661 6c75 6174 696f  to run evaluatio
-000132f0: 6e20 666f 722e 2049 6620 6e6f 7420 7370  n for. If not sp
-00013300: 6563 6966 6965 6420 6f72 0a20 2020 2020  ecified or.     
-00013310: 206e 6567 6174 6976 652c 2069 7420 6d65   negative, it me
-00013320: 616e 7320 7275 6e20 6576 616c 7561 7469  ans run evaluati
-00013330: 6f6e 206f 6e20 7468 6520 7768 6f6c 6520  on on the whole 
-00013340: 7661 6c69 6461 7469 6f6e 2064 6174 6173  validation datas
-00013350: 6574 2e20 4966 2073 6574 0a20 2020 2020  et. If set.     
-00013360: 2074 6f20 302c 2069 7420 6d65 616e 7320   to 0, it means 
-00013370: 7275 6e20 6576 616c 7561 7469 6f6e 2066  run evaluation f
-00013380: 6f72 2061 2066 6978 6564 206e 756d 6265  or a fixed numbe
-00013390: 7220 6f66 2073 616d 706c 6573 2e0a 2020  r of samples..  
-000133a0: 2020 6576 616c 5f66 7265 7175 656e 6379    eval_frequency
-000133b0: 5f73 6563 733a 2046 7265 7175 656e 6379  _secs: Frequency
-000133c0: 2061 7420 7768 6963 6820 6576 616c 7561   at which evalua
-000133d0: 7469 6f6e 2061 6e64 2063 6865 636b 706f  tion and checkpo
-000133e0: 696e 7469 6e67 2077 696c 6c0a 2020 2020  inting will.    
-000133f0: 2020 7461 6b65 2070 6c61 6365 2e0a 2020    take place..  
-00013400: 2020 7472 616e 7366 6f72 6d5f 636f 6e66    transform_conf
-00013410: 6967 3a20 5061 7468 2074 6f20 7631 2054  ig: Path to v1 T
-00013420: 4620 7472 616e 7366 6f72 6d61 7469 6f6e  F transformation
-00013430: 2063 6f6e 6669 6775 7261 7469 6f6e 2e0a   configuration..
-00013440: 2020 2020 6461 7461 7365 745f 6c65 7665      dataset_leve
-00013450: 6c5f 6375 7374 6f6d 5f74 7261 6e73 666f  l_custom_transfo
-00013460: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
-00013470: 6f6e 733a 2044 6174 6173 6574 2d6c 6576  ons: Dataset-lev
-00013480: 656c 2063 7573 746f 6d0a 2020 2020 2020  el custom.      
-00013490: 7472 616e 7366 6f72 6d61 7469 6f6e 2064  transformation d
-000134a0: 6566 696e 6974 696f 6e73 2069 6e20 7374  efinitions in st
-000134b0: 7269 6e67 2066 6f72 6d61 742e 0a20 2020  ring format..   
-000134c0: 2064 6174 6173 6574 5f6c 6576 656c 5f74   dataset_level_t
-000134d0: 7261 6e73 666f 726d 6174 696f 6e73 3a20  ransformations: 
-000134e0: 4461 7461 7365 742d 6c65 7665 6c20 7472  Dataset-level tr
-000134f0: 616e 7366 6f72 6d61 7469 6f6e 2063 6f6e  ansformation con
-00013500: 6669 6775 7261 7469 6f6e 2069 6e0a 2020  figuration in.  
-00013510: 2020 2020 7374 7269 6e67 2066 6f72 6d61      string forma
-00013520: 742e 0a20 2020 2070 7265 6465 6669 6e65  t..    predefine
-00013530: 645f 7370 6c69 745f 6b65 793a 2050 7265  d_split_key: Pre
-00013540: 6465 6669 6e65 6420 7370 6c69 7420 6b65  defined split ke
-00013550: 792e 0a20 2020 2073 7472 6174 6966 6965  y..    stratifie
-00013560: 645f 7370 6c69 745f 6b65 793a 2053 7472  d_split_key: Str
-00013570: 6174 6966 6965 6420 7370 6c69 7420 6b65  atified split ke
-00013580: 792e 0a20 2020 2074 7261 696e 696e 675f  y..    training_
-00013590: 6672 6163 7469 6f6e 3a20 5472 6169 6e69  fraction: Traini
-000135a0: 6e67 2066 7261 6374 696f 6e2e 0a20 2020  ng fraction..   
-000135b0: 2076 616c 6964 6174 696f 6e5f 6672 6163   validation_frac
-000135c0: 7469 6f6e 3a20 5661 6c69 6461 7469 6f6e  tion: Validation
-000135d0: 2066 7261 6374 696f 6e2e 0a20 2020 2074   fraction..    t
-000135e0: 6573 745f 6672 6163 7469 6f6e 3a20 5465  est_fraction: Te
-000135f0: 7374 2066 7261 6374 696f 6e2e 0a20 2020  st fraction..   
-00013600: 2074 665f 7472 616e 7366 6f72 6d5f 6578   tf_transform_ex
-00013610: 6563 7574 696f 6e5f 656e 6769 6e65 3a20  ecution_engine: 
-00013620: 5468 6520 6578 6563 7574 696f 6e20 656e  The execution en
-00013630: 6769 6e65 2075 7365 6420 746f 2065 7865  gine used to exe
-00013640: 6375 7465 2054 462d 6261 7365 640a 2020  cute TF-based.  
-00013650: 2020 2020 7472 616e 7366 6f72 6d61 7469      transformati
-00013660: 6f6e 732e 0a20 2020 2074 665f 6175 746f  ons..    tf_auto
-00013670: 5f74 7261 6e73 666f 726d 5f66 6561 7475  _transform_featu
-00013680: 7265 733a 204c 6973 7420 6f66 2061 7574  res: List of aut
-00013690: 6f20 7472 616e 7366 6f72 6d20 6665 6174  o transform feat
-000136a0: 7572 6573 2069 6e20 7468 650a 2020 2020  ures in the.    
-000136b0: 2020 636f 6d6d 612d 7365 7061 7261 7465    comma-separate
-000136c0: 6420 7374 7269 6e67 2066 6f72 6d61 742e  d string format.
-000136d0: 0a20 2020 2074 665f 6375 7374 6f6d 5f74  .    tf_custom_t
-000136e0: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
-000136f0: 6669 6e69 7469 6f6e 733a 2054 4620 6375  finitions: TF cu
-00013700: 7374 6f6d 2074 7261 6e73 666f 726d 6174  stom transformat
-00013710: 696f 6e20 6465 6669 6e69 7469 6f6e 730a  ion definitions.
-00013720: 2020 2020 2020 696e 2073 7472 696e 6720        in string 
-00013730: 666f 726d 6174 2e0a 2020 2020 7466 5f74  format..    tf_t
-00013740: 7261 6e73 666f 726d 6174 696f 6e73 5f70  ransformations_p
-00013750: 6174 683a 2050 6174 6820 746f 2054 4620  ath: Path to TF 
-00013760: 7472 616e 7366 6f72 6d61 7469 6f6e 2063  transformation c
-00013770: 6f6e 6669 6775 7261 7469 6f6e 2e0a 2020  onfiguration..  
-00013780: 2020 6461 7461 5f73 6f75 7263 655f 6373    data_source_cs
-00013790: 765f 6669 6c65 6e61 6d65 733a 2054 6865  v_filenames: The
-000137a0: 2043 5356 2064 6174 6120 736f 7572 6365   CSV data source
-000137b0: 2e0a 2020 2020 6461 7461 5f73 6f75 7263  ..    data_sourc
-000137c0: 655f 6269 6771 7565 7279 5f74 6162 6c65  e_bigquery_table
-000137d0: 5f70 6174 683a 2054 6865 2042 6967 5175  _path: The BigQu
-000137e0: 6572 7920 6461 7461 2073 6f75 7263 652e  ery data source.
-000137f0: 0a20 2020 2062 6967 7175 6572 795f 7374  .    bigquery_st
-00013800: 6167 696e 675f 6675 6c6c 5f64 6174 6173  aging_full_datas
-00013810: 6574 5f69 643a 2054 6865 2042 6967 5175  et_id: The BigQu
-00013820: 6572 7920 7374 6167 696e 6720 6675 6c6c  ery staging full
-00013830: 2064 6174 6173 6574 2069 6420 666f 720a   dataset id for.
-00013840: 2020 2020 2020 7374 6f72 696e 6720 696e        storing in
-00013850: 7465 726d 6564 6961 7465 2074 6162 6c65  termediate table
-00013860: 732e 0a20 2020 2077 6569 6768 745f 636f  s..    weight_co
-00013870: 6c75 6d6e 3a20 5468 6520 7765 6967 6874  lumn: The weight
-00013880: 2063 6f6c 756d 6e20 6e61 6d65 2e0a 2020   column name..  
-00013890: 2020 6d61 785f 6661 696c 6564 5f74 7269    max_failed_tri
-000138a0: 616c 5f63 6f75 6e74 3a20 5468 6520 6e75  al_count: The nu
-000138b0: 6d62 6572 206f 6620 6661 696c 6564 2074  mber of failed t
-000138c0: 7269 616c 7320 7468 6174 206e 6565 6420  rials that need 
-000138d0: 746f 2062 6520 7365 656e 0a20 2020 2020  to be seen.     
-000138e0: 2062 6566 6f72 6520 6661 696c 696e 6720   before failing 
-000138f0: 7468 6520 4879 7065 7270 6172 616d 6574  the Hyperparamet
-00013900: 6572 5475 6e69 6e67 4a6f 622e 2049 6620  erTuningJob. If 
-00013910: 7365 7420 746f 2030 2c20 5665 7274 6578  set to 0, Vertex
-00013920: 2041 4920 6465 6369 6465 730a 2020 2020   AI decides.    
-00013930: 2020 686f 7720 6d61 6e79 2074 7269 616c    how many trial
-00013940: 7320 6d75 7374 2066 6169 6c20 6265 666f  s must fail befo
-00013950: 7265 2074 6865 2077 686f 6c65 206a 6f62  re the whole job
-00013960: 2066 6169 6c73 2e0a 2020 2020 7374 7564   fails..    stud
-00013970: 795f 7370 6563 5f61 6c67 6f72 6974 686d  y_spec_algorithm
-00013980: 3a20 5468 6520 7365 6172 6368 2061 6c67  : The search alg
-00013990: 6f72 6974 686d 2073 7065 6369 6669 6564  orithm specified
-000139a0: 2066 6f72 2074 6865 2073 7475 6479 2e20   for the study. 
-000139b0: 4f6e 6520 6f66 0a20 2020 2020 2022 414c  One of.      "AL
-000139c0: 474f 5249 5448 4d5f 554e 5350 4543 4946  GORITHM_UNSPECIF
-000139d0: 4945 4422 2c20 2247 5249 445f 5345 4152  IED", "GRID_SEAR
-000139e0: 4348 222c 206f 7220 2252 414e 444f 4d5f  CH", or "RANDOM_
-000139f0: 5345 4152 4348 222e 0a20 2020 2073 7475  SEARCH"..    stu
-00013a00: 6479 5f73 7065 635f 6d65 6173 7572 656d  dy_spec_measurem
-00013a10: 656e 745f 7365 6c65 6374 696f 6e5f 7479  ent_selection_ty
-00013a20: 7065 3a20 5768 6963 6820 6d65 6173 7572  pe: Which measur
-00013a30: 656d 656e 7420 746f 2075 7365 2069 662f  ement to use if/
-00013a40: 7768 656e 2074 6865 0a20 2020 2020 2073  when the.      s
-00013a50: 6572 7669 6365 2061 7574 6f6d 6174 6963  ervice automatic
-00013a60: 616c 6c79 2073 656c 6563 7473 2074 6865  ally selects the
-00013a70: 2066 696e 616c 206d 6561 7375 7265 6d65   final measureme
-00013a80: 6e74 2066 726f 6d20 7072 6576 696f 7573  nt from previous
-00013a90: 6c79 0a20 2020 2020 2072 6570 6f72 7465  ly.      reporte
-00013aa0: 6420 696e 7465 726d 6564 6961 7465 206d  d intermediate m
-00013ab0: 6561 7375 7265 6d65 6e74 732e 204f 6e65  easurements. One
-00013ac0: 206f 6620 2242 4553 545f 4d45 4153 5552   of "BEST_MEASUR
-00013ad0: 454d 454e 5422 206f 720a 2020 2020 2020  EMENT" or.      
-00013ae0: 224c 4153 545f 4d45 4153 5552 454d 454e  "LAST_MEASUREMEN
-00013af0: 5422 2e0a 2020 2020 7472 616e 7366 6f72  T"..    transfor
-00013b00: 6d5f 6461 7461 666c 6f77 5f6d 6163 6869  m_dataflow_machi
-00013b10: 6e65 5f74 7970 653a 2054 6865 2064 6174  ne_type: The dat
-00013b20: 6166 6c6f 7720 6d61 6368 696e 6520 7479  aflow machine ty
-00013b30: 7065 2066 6f72 2074 7261 6e73 666f 726d  pe for transform
-00013b40: 0a20 2020 2020 2063 6f6d 706f 6e65 6e74  .      component
-00013b50: 2e0a 2020 2020 7472 616e 7366 6f72 6d5f  ..    transform_
-00013b60: 6461 7461 666c 6f77 5f6d 6178 5f6e 756d  dataflow_max_num
-00013b70: 5f77 6f72 6b65 7273 3a20 5468 6520 6d61  _workers: The ma
-00013b80: 7820 6e75 6d62 6572 206f 6620 4461 7461  x number of Data
-00013b90: 666c 6f77 2077 6f72 6b65 7273 2066 6f72  flow workers for
-00013ba0: 0a20 2020 2020 2074 7261 6e73 666f 726d  .      transform
-00013bb0: 2063 6f6d 706f 6e65 6e74 2e0a 2020 2020   component..    
-00013bc0: 7472 616e 7366 6f72 6d5f 6461 7461 666c  transform_datafl
-00013bd0: 6f77 5f64 6973 6b5f 7369 7a65 5f67 623a  ow_disk_size_gb:
-00013be0: 2044 6174 6166 6c6f 7720 776f 726b 6572   Dataflow worker
-00013bf0: 2773 2064 6973 6b20 7369 7a65 2069 6e20  's disk size in 
-00013c00: 4742 2066 6f72 0a20 2020 2020 2074 7261  GB for.      tra
-00013c10: 6e73 666f 726d 2063 6f6d 706f 6e65 6e74  nsform component
-00013c20: 2e0a 2020 2020 776f 726b 6572 5f70 6f6f  ..    worker_poo
-00013c30: 6c5f 7370 6563 735f 6f76 6572 7269 6465  l_specs_override
-00013c40: 3a20 5468 6520 6469 6374 696f 6e61 7279  : The dictionary
-00013c50: 2066 6f72 206f 7665 7272 6964 696e 6720   for overriding 
-00013c60: 7472 6169 6e69 6e67 2061 6e64 0a20 2020  training and.   
-00013c70: 2020 2065 7661 6c75 6174 696f 6e20 776f     evaluation wo
-00013c80: 726b 6572 2070 6f6f 6c20 7370 6563 732e  rker pool specs.
-00013c90: 2054 6865 2064 6963 7469 6f6e 6172 7920   The dictionary 
-00013ca0: 7368 6f75 6c64 2062 6520 6f66 2066 6f72  should be of for
-00013cb0: 6d61 740a 2020 2020 2020 2020 2020 6874  mat.          ht
-00013cc0: 7470 733a 2f2f 6769 7468 7562 2e63 6f6d  tps://github.com
-00013cd0: 2f67 6f6f 676c 6561 7069 732f 676f 6f67  /googleapis/goog
-00013ce0: 6c65 6170 6973 2f62 6c6f 622f 3465 3833  leapis/blob/4e83
-00013cf0: 3663 3763 3235 3765 3365 3230 6231 6465  6c7c257e3e20b1de
-00013d00: 3134 6434 3730 3939 3361 3262 3166 3437  14d470993a2b1f47
-00013d10: 3336 6138 2f67 6f6f 676c 652f 636c 6f75  36a8/google/clou
-00013d20: 642f 6169 706c 6174 666f 726d 2f76 3162  d/aiplatform/v1b
-00013d30: 6574 6131 2f63 7573 746f 6d5f 6a6f 622e  eta1/custom_job.
-00013d40: 7072 6f74 6f23 4c31 3732 2e0a 2020 2020  proto#L172..    
-00013d50: 7275 6e5f 6576 616c 7561 7469 6f6e 3a20  run_evaluation: 
-00013d60: 5768 6574 6865 7220 746f 2072 756e 2065  Whether to run e
-00013d70: 7661 6c75 6174 696f 6e20 7374 6570 7320  valuation steps 
-00013d80: 6475 7269 6e67 2074 7261 696e 696e 672e  during training.
-00013d90: 0a20 2020 2065 7661 6c75 6174 696f 6e5f  .    evaluation_
-00013da0: 6261 7463 685f 7072 6564 6963 745f 6d61  batch_predict_ma
-00013db0: 6368 696e 655f 7479 7065 3a20 5468 6520  chine_type: The 
-00013dc0: 7072 6564 6963 7469 6f6e 2073 6572 7665  prediction serve
-00013dd0: 7220 6d61 6368 696e 6520 7479 7065 0a20  r machine type. 
-00013de0: 2020 2020 2066 6f72 2062 6174 6368 2070       for batch p
-00013df0: 7265 6469 6374 2063 6f6d 706f 6e65 6e74  redict component
-00013e00: 7320 6475 7269 6e67 2065 7661 6c75 6174  s during evaluat
-00013e10: 696f 6e2e 0a20 2020 2065 7661 6c75 6174  ion..    evaluat
-00013e20: 696f 6e5f 6261 7463 685f 7072 6564 6963  ion_batch_predic
-00013e30: 745f 7374 6172 7469 6e67 5f72 6570 6c69  t_starting_repli
-00013e40: 6361 5f63 6f75 6e74 3a20 5468 6520 696e  ca_count: The in
-00013e50: 6974 6961 6c20 6e75 6d62 6572 206f 660a  itial number of.
-00013e60: 2020 2020 2020 7072 6564 6963 7469 6f6e        prediction
-00013e70: 2073 6572 7665 7220 666f 7220 6261 7463   server for batc
-00013e80: 6820 7072 6564 6963 7420 636f 6d70 6f6e  h predict compon
-00013e90: 656e 7473 2064 7572 696e 6720 6576 616c  ents during eval
-00013ea0: 7561 7469 6f6e 2e0a 2020 2020 6576 616c  uation..    eval
-00013eb0: 7561 7469 6f6e 5f62 6174 6368 5f70 7265  uation_batch_pre
-00013ec0: 6469 6374 5f6d 6178 5f72 6570 6c69 6361  dict_max_replica
-00013ed0: 5f63 6f75 6e74 3a20 5468 6520 6d61 7820  _count: The max 
-00013ee0: 6e75 6d62 6572 206f 6620 7072 6564 6963  number of predic
-00013ef0: 7469 6f6e 0a20 2020 2020 2073 6572 7665  tion.      serve
-00013f00: 7220 666f 7220 6261 7463 6820 7072 6564  r for batch pred
-00013f10: 6963 7420 636f 6d70 6f6e 656e 7473 2064  ict components d
-00013f20: 7572 696e 6720 6576 616c 7561 7469 6f6e  uring evaluation
-00013f30: 2e0a 2020 2020 6576 616c 7561 7469 6f6e  ..    evaluation
-00013f40: 5f64 6174 6166 6c6f 775f 6d61 6368 696e  _dataflow_machin
-00013f50: 655f 7479 7065 3a20 5468 6520 6461 7461  e_type: The data
-00013f60: 666c 6f77 206d 6163 6869 6e65 2074 7970  flow machine typ
-00013f70: 6520 666f 7220 6576 616c 7561 7469 6f6e  e for evaluation
-00013f80: 0a20 2020 2020 2063 6f6d 706f 6e65 6e74  .      component
-00013f90: 732e 0a20 2020 2065 7661 6c75 6174 696f  s..    evaluatio
-00013fa0: 6e5f 6461 7461 666c 6f77 5f73 7461 7274  n_dataflow_start
-00013fb0: 696e 675f 6e75 6d5f 776f 726b 6572 733a  ing_num_workers:
-00013fc0: 2054 6865 2069 6e69 7469 616c 206e 756d   The initial num
-00013fd0: 6265 7220 6f66 2044 6174 6166 6c6f 770a  ber of Dataflow.
-00013fe0: 2020 2020 2020 776f 726b 6572 7320 666f        workers fo
-00013ff0: 7220 6576 616c 7561 7469 6f6e 2063 6f6d  r evaluation com
-00014000: 706f 6e65 6e74 732e 0a20 2020 2065 7661  ponents..    eva
-00014010: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
-00014020: 5f6d 6178 5f6e 756d 5f77 6f72 6b65 7273  _max_num_workers
-00014030: 3a20 5468 6520 6d61 7820 6e75 6d62 6572  : The max number
-00014040: 206f 6620 4461 7461 666c 6f77 2077 6f72   of Dataflow wor
-00014050: 6b65 7273 2066 6f72 0a20 2020 2020 2065  kers for.      e
-00014060: 7661 6c75 6174 696f 6e20 636f 6d70 6f6e  valuation compon
-00014070: 656e 7473 2e0a 2020 2020 6576 616c 7561  ents..    evalua
-00014080: 7469 6f6e 5f64 6174 6166 6c6f 775f 6469  tion_dataflow_di
-00014090: 736b 5f73 697a 655f 6762 3a20 4461 7461  sk_size_gb: Data
-000140a0: 666c 6f77 2077 6f72 6b65 7227 7320 6469  flow worker's di
-000140b0: 736b 2073 697a 6520 696e 2047 4220 666f  sk size in GB fo
-000140c0: 720a 2020 2020 2020 6576 616c 7561 7469  r.      evaluati
-000140d0: 6f6e 2063 6f6d 706f 6e65 6e74 732e 0a20  on components.. 
-000140e0: 2020 2064 6174 6166 6c6f 775f 7365 7276     dataflow_serv
-000140f0: 6963 655f 6163 636f 756e 743a 2043 7573  ice_account: Cus
-00014100: 746f 6d20 7365 7276 6963 6520 6163 636f  tom service acco
-00014110: 756e 7420 746f 2072 756e 2064 6174 6166  unt to run dataf
-00014120: 6c6f 7720 6a6f 6273 2e0a 2020 2020 6461  low jobs..    da
-00014130: 7461 666c 6f77 5f73 7562 6e65 7477 6f72  taflow_subnetwor
-00014140: 6b3a 2044 6174 6166 6c6f 7727 7320 6675  k: Dataflow's fu
-00014150: 6c6c 7920 7175 616c 6966 6965 6420 7375  lly qualified su
-00014160: 626e 6574 776f 726b 206e 616d 652c 2077  bnetwork name, w
-00014170: 6865 6e20 656d 7074 790a 2020 2020 2020  hen empty.      
-00014180: 7468 6520 6465 6661 756c 7420 7375 626e  the default subn
-00014190: 6574 776f 726b 2077 696c 6c20 6265 2075  etwork will be u
-000141a0: 7365 642e 2045 7861 6d70 6c65 3a0a 2020  sed. Example:.  
-000141b0: 2020 2020 2020 6874 7470 733a 2f2f 636c        https://cl
-000141c0: 6f75 642e 676f 6f67 6c65 2e63 6f6d 2f64  oud.google.com/d
-000141d0: 6174 6166 6c6f 772f 646f 6373 2f67 7569  ataflow/docs/gui
-000141e0: 6465 732f 7370 6563 6966 7969 6e67 2d6e  des/specifying-n
-000141f0: 6574 776f 726b 7323 6578 616d 706c 655f  etworks#example_
-00014200: 6e65 7477 6f72 6b5f 616e 645f 7375 626e  network_and_subn
-00014210: 6574 776f 726b 5f73 7065 6369 6669 6361  etwork_specifica
-00014220: 7469 6f6e 730a 2020 2020 6461 7461 666c  tions.    datafl
-00014230: 6f77 5f75 7365 5f70 7562 6c69 635f 6970  ow_use_public_ip
-00014240: 733a 2053 7065 6369 6669 6573 2077 6865  s: Specifies whe
-00014250: 7468 6572 2044 6174 6166 6c6f 7720 776f  ther Dataflow wo
-00014260: 726b 6572 7320 7573 6520 7075 626c 6963  rkers use public
-00014270: 2049 500a 2020 2020 2020 6164 6472 6573   IP.      addres
-00014280: 7365 732e 0a20 2020 2065 6e63 7279 7074  ses..    encrypt
-00014290: 696f 6e5f 7370 6563 5f6b 6579 5f6e 616d  ion_spec_key_nam
-000142a0: 653a 2054 6865 204b 4d53 206b 6579 206e  e: The KMS key n
-000142b0: 616d 652e 0a0a 2020 5265 7475 726e 733a  ame...  Returns:
-000142c0: 0a20 2020 2054 7570 6c65 206f 6620 7069  .    Tuple of pi
-000142d0: 7065 6c69 6e65 5f64 6566 696e 6974 696f  peline_definitio
-000142e0: 6e5f 7061 7468 2061 6e64 2070 6172 616d  n_path and param
-000142f0: 6574 6572 5f76 616c 7565 732e 0a20 2022  eter_values..  "
-00014300: 2222 0a20 2077 6172 6e69 6e67 732e 7761  "".  warnings.wa
-00014310: 726e 280a 2020 2020 2020 2754 6869 7320  rn(.      'This 
-00014320: 6d65 7468 6f64 2069 7320 6465 7072 6563  method is deprec
-00014330: 6174 6564 2e20 506c 6561 7365 2075 7365  ated. Please use
-00014340: 270a 2020 2020 2020 2720 6765 745f 7461  '.      ' get_ta
-00014350: 626e 6574 5f68 7970 6572 7061 7261 6d65  bnet_hyperparame
-00014360: 7465 725f 7475 6e69 6e67 5f6a 6f62 5f70  ter_tuning_job_p
-00014370: 6970 656c 696e 655f 616e 645f 7061 7261  ipeline_and_para
-00014380: 6d65 7465 7273 206f 7227 0a20 2020 2020  meters or'.     
-00014390: 2027 2067 6574 5f77 6964 655f 616e 645f   ' get_wide_and_
-000143a0: 6465 6570 5f68 7970 6572 7061 7261 6d65  deep_hyperparame
-000143b0: 7465 725f 7475 6e69 6e67 5f6a 6f62 5f70  ter_tuning_job_p
-000143c0: 6970 656c 696e 655f 616e 645f 7061 7261  ipeline_and_para
-000143d0: 6d65 7465 7273 270a 2020 2020 2020 2720  meters'.      ' 
-000143e0: 696e 7374 6561 642e 270a 2020 290a 0a20  instead.'.  ).. 
-000143f0: 2069 6620 616c 676f 7269 7468 6d20 3d3d   if algorithm ==
-00014400: 2027 7461 626e 6574 273a 0a20 2020 2072   'tabnet':.    r
-00014410: 6574 7572 6e20 6765 745f 7461 626e 6574  eturn get_tabnet
-00014420: 5f68 7970 6572 7061 7261 6d65 7465 725f  _hyperparameter_
-00014430: 7475 6e69 6e67 5f6a 6f62 5f70 6970 656c  tuning_job_pipel
-00014440: 696e 655f 616e 645f 7061 7261 6d65 7465  ine_and_paramete
-00014450: 7273 280a 2020 2020 2020 2020 7072 6f6a  rs(.        proj
-00014460: 6563 743d 7072 6f6a 6563 742c 0a20 2020  ect=project,.   
-00014470: 2020 2020 206c 6f63 6174 696f 6e3d 6c6f       location=lo
-00014480: 6361 7469 6f6e 2c0a 2020 2020 2020 2020  cation,.        
-00014490: 726f 6f74 5f64 6972 3d72 6f6f 745f 6469  root_dir=root_di
-000144a0: 722c 0a20 2020 2020 2020 2074 6172 6765  r,.        targe
-000144b0: 745f 636f 6c75 6d6e 3d74 6172 6765 745f  t_column=target_
-000144c0: 636f 6c75 6d6e 2c0a 2020 2020 2020 2020  column,.        
-000144d0: 7072 6564 6963 7469 6f6e 5f74 7970 653d  prediction_type=
-000144e0: 7072 6564 6963 7469 6f6e 5f74 7970 652c  prediction_type,
-000144f0: 0a20 2020 2020 2020 2073 7475 6479 5f73  .        study_s
-00014500: 7065 635f 6d65 7472 6963 5f69 643d 7374  pec_metric_id=st
-00014510: 7564 795f 7370 6563 5f6d 6574 7269 635f  udy_spec_metric_
-00014520: 6964 2c0a 2020 2020 2020 2020 7374 7564  id,.        stud
-00014530: 795f 7370 6563 5f6d 6574 7269 635f 676f  y_spec_metric_go
-00014540: 616c 3d73 7475 6479 5f73 7065 635f 6d65  al=study_spec_me
-00014550: 7472 6963 5f67 6f61 6c2c 0a20 2020 2020  tric_goal,.     
-00014560: 2020 2073 7475 6479 5f73 7065 635f 7061     study_spec_pa
-00014570: 7261 6d65 7465 7273 5f6f 7665 7272 6964  rameters_overrid
-00014580: 653d 7374 7564 795f 7370 6563 5f70 6172  e=study_spec_par
-00014590: 616d 6574 6572 735f 6f76 6572 7269 6465  ameters_override
-000145a0: 2c0a 2020 2020 2020 2020 6d61 785f 7472  ,.        max_tr
-000145b0: 6961 6c5f 636f 756e 743d 6d61 785f 7472  ial_count=max_tr
-000145c0: 6961 6c5f 636f 756e 742c 0a20 2020 2020  ial_count,.     
-000145d0: 2020 2070 6172 616c 6c65 6c5f 7472 6961     parallel_tria
-000145e0: 6c5f 636f 756e 743d 7061 7261 6c6c 656c  l_count=parallel
-000145f0: 5f74 7269 616c 5f63 6f75 6e74 2c0a 2020  _trial_count,.  
-00014600: 2020 2020 2020 7472 616e 7366 6f72 6d5f        transform_
-00014610: 636f 6e66 6967 3d74 7261 6e73 666f 726d  config=transform
-00014620: 5f63 6f6e 6669 672c 0a20 2020 2020 2020  _config,.       
-00014630: 2064 6174 6173 6574 5f6c 6576 656c 5f63   dataset_level_c
-00014640: 7573 746f 6d5f 7472 616e 7366 6f72 6d61  ustom_transforma
-00014650: 7469 6f6e 5f64 6566 696e 6974 696f 6e73  tion_definitions
-00014660: 3d64 6174 6173 6574 5f6c 6576 656c 5f63  =dataset_level_c
-00014670: 7573 746f 6d5f 7472 616e 7366 6f72 6d61  ustom_transforma
-00014680: 7469 6f6e 5f64 6566 696e 6974 696f 6e73  tion_definitions
-00014690: 2c0a 2020 2020 2020 2020 6461 7461 7365  ,.        datase
-000146a0: 745f 6c65 7665 6c5f 7472 616e 7366 6f72  t_level_transfor
-000146b0: 6d61 7469 6f6e 733d 6461 7461 7365 745f  mations=dataset_
-000146c0: 6c65 7665 6c5f 7472 616e 7366 6f72 6d61  level_transforma
-000146d0: 7469 6f6e 732c 0a20 2020 2020 2020 2070  tions,.        p
-000146e0: 7265 6465 6669 6e65 645f 7370 6c69 745f  redefined_split_
-000146f0: 6b65 793d 7072 6564 6566 696e 6564 5f73  key=predefined_s
-00014700: 706c 6974 5f6b 6579 2c0a 2020 2020 2020  plit_key,.      
-00014710: 2020 7374 7261 7469 6669 6564 5f73 706c    stratified_spl
-00014720: 6974 5f6b 6579 3d73 7472 6174 6966 6965  it_key=stratifie
-00014730: 645f 7370 6c69 745f 6b65 792c 0a20 2020  d_split_key,.   
-00014740: 2020 2020 2074 7261 696e 696e 675f 6672       training_fr
-00014750: 6163 7469 6f6e 3d74 7261 696e 696e 675f  action=training_
-00014760: 6672 6163 7469 6f6e 2c0a 2020 2020 2020  fraction,.      
-00014770: 2020 7661 6c69 6461 7469 6f6e 5f66 7261    validation_fra
-00014780: 6374 696f 6e3d 7661 6c69 6461 7469 6f6e  ction=validation
-00014790: 5f66 7261 6374 696f 6e2c 0a20 2020 2020  _fraction,.     
-000147a0: 2020 2074 6573 745f 6672 6163 7469 6f6e     test_fraction
-000147b0: 3d74 6573 745f 6672 6163 7469 6f6e 2c0a  =test_fraction,.
-000147c0: 2020 2020 2020 2020 7466 5f74 7261 6e73          tf_trans
-000147d0: 666f 726d 5f65 7865 6375 7469 6f6e 5f65  form_execution_e
-000147e0: 6e67 696e 653d 7466 5f74 7261 6e73 666f  ngine=tf_transfo
-000147f0: 726d 5f65 7865 6375 7469 6f6e 5f65 6e67  rm_execution_eng
-00014800: 696e 652c 0a20 2020 2020 2020 2074 665f  ine,.        tf_
-00014810: 6175 746f 5f74 7261 6e73 666f 726d 5f66  auto_transform_f
-00014820: 6561 7475 7265 733d 7466 5f61 7574 6f5f  eatures=tf_auto_
-00014830: 7472 616e 7366 6f72 6d5f 6665 6174 7572  transform_featur
-00014840: 6573 2c0a 2020 2020 2020 2020 7466 5f63  es,.        tf_c
-00014850: 7573 746f 6d5f 7472 616e 7366 6f72 6d61  ustom_transforma
-00014860: 7469 6f6e 5f64 6566 696e 6974 696f 6e73  tion_definitions
-00014870: 3d74 665f 6375 7374 6f6d 5f74 7261 6e73  =tf_custom_trans
-00014880: 666f 726d 6174 696f 6e5f 6465 6669 6e69  formation_defini
-00014890: 7469 6f6e 732c 0a20 2020 2020 2020 2074  tions,.        t
-000148a0: 665f 7472 616e 7366 6f72 6d61 7469 6f6e  f_transformation
-000148b0: 735f 7061 7468 3d74 665f 7472 616e 7366  s_path=tf_transf
-000148c0: 6f72 6d61 7469 6f6e 735f 7061 7468 2c0a  ormations_path,.
-000148d0: 2020 2020 2020 2020 656e 6162 6c65 5f70          enable_p
-000148e0: 726f 6669 6c65 723d 656e 6162 6c65 5f70  rofiler=enable_p
-000148f0: 726f 6669 6c65 722c 0a20 2020 2020 2020  rofiler,.       
-00014900: 2073 6565 643d 7365 6564 2c0a 2020 2020   seed=seed,.    
-00014910: 2020 2020 6576 616c 5f73 7465 7073 3d65      eval_steps=e
-00014920: 7661 6c5f 7374 6570 732c 0a20 2020 2020  val_steps,.     
-00014930: 2020 2065 7661 6c5f 6672 6571 7565 6e63     eval_frequenc
-00014940: 795f 7365 6373 3d65 7661 6c5f 6672 6571  y_secs=eval_freq
-00014950: 7565 6e63 795f 7365 6373 2c0a 2020 2020  uency_secs,.    
-00014960: 2020 2020 6461 7461 5f73 6f75 7263 655f      data_source_
-00014970: 6373 765f 6669 6c65 6e61 6d65 733d 6461  csv_filenames=da
-00014980: 7461 5f73 6f75 7263 655f 6373 765f 6669  ta_source_csv_fi
-00014990: 6c65 6e61 6d65 732c 0a20 2020 2020 2020  lenames,.       
-000149a0: 2064 6174 615f 736f 7572 6365 5f62 6967   data_source_big
-000149b0: 7175 6572 795f 7461 626c 655f 7061 7468  query_table_path
-000149c0: 3d64 6174 615f 736f 7572 6365 5f62 6967  =data_source_big
-000149d0: 7175 6572 795f 7461 626c 655f 7061 7468  query_table_path
-000149e0: 2c0a 2020 2020 2020 2020 6269 6771 7565  ,.        bigque
-000149f0: 7279 5f73 7461 6769 6e67 5f66 756c 6c5f  ry_staging_full_
-00014a00: 6461 7461 7365 745f 6964 3d62 6967 7175  dataset_id=bigqu
-00014a10: 6572 795f 7374 6167 696e 675f 6675 6c6c  ery_staging_full
-00014a20: 5f64 6174 6173 6574 5f69 642c 0a20 2020  _dataset_id,.   
-00014a30: 2020 2020 2077 6569 6768 745f 636f 6c75       weight_colu
-00014a40: 6d6e 3d77 6569 6768 745f 636f 6c75 6d6e  mn=weight_column
-00014a50: 2c0a 2020 2020 2020 2020 6d61 785f 6661  ,.        max_fa
-00014a60: 696c 6564 5f74 7269 616c 5f63 6f75 6e74  iled_trial_count
-00014a70: 3d6d 6178 5f66 6169 6c65 645f 7472 6961  =max_failed_tria
-00014a80: 6c5f 636f 756e 742c 0a20 2020 2020 2020  l_count,.       
-00014a90: 2073 7475 6479 5f73 7065 635f 616c 676f   study_spec_algo
-00014aa0: 7269 7468 6d3d 7374 7564 795f 7370 6563  rithm=study_spec
-00014ab0: 5f61 6c67 6f72 6974 686d 2c0a 2020 2020  _algorithm,.    
-00014ac0: 2020 2020 7374 7564 795f 7370 6563 5f6d      study_spec_m
-00014ad0: 6561 7375 7265 6d65 6e74 5f73 656c 6563  easurement_selec
-00014ae0: 7469 6f6e 5f74 7970 653d 7374 7564 795f  tion_type=study_
-00014af0: 7370 6563 5f6d 6561 7375 7265 6d65 6e74  spec_measurement
-00014b00: 5f73 656c 6563 7469 6f6e 5f74 7970 652c  _selection_type,
-00014b10: 0a20 2020 2020 2020 2074 7261 6e73 666f  .        transfo
-00014b20: 726d 5f64 6174 6166 6c6f 775f 6d61 6368  rm_dataflow_mach
-00014b30: 696e 655f 7479 7065 3d74 7261 6e73 666f  ine_type=transfo
-00014b40: 726d 5f64 6174 6166 6c6f 775f 6d61 6368  rm_dataflow_mach
-00014b50: 696e 655f 7479 7065 2c0a 2020 2020 2020  ine_type,.      
-00014b60: 2020 7472 616e 7366 6f72 6d5f 6461 7461    transform_data
-00014b70: 666c 6f77 5f6d 6178 5f6e 756d 5f77 6f72  flow_max_num_wor
-00014b80: 6b65 7273 3d74 7261 6e73 666f 726d 5f64  kers=transform_d
-00014b90: 6174 6166 6c6f 775f 6d61 785f 6e75 6d5f  ataflow_max_num_
-00014ba0: 776f 726b 6572 732c 0a20 2020 2020 2020  workers,.       
-00014bb0: 2074 7261 6e73 666f 726d 5f64 6174 6166   transform_dataf
-00014bc0: 6c6f 775f 6469 736b 5f73 697a 655f 6762  low_disk_size_gb
-00014bd0: 3d74 7261 6e73 666f 726d 5f64 6174 6166  =transform_dataf
-00014be0: 6c6f 775f 6469 736b 5f73 697a 655f 6762  low_disk_size_gb
-00014bf0: 2c0a 2020 2020 2020 2020 776f 726b 6572  ,.        worker
-00014c00: 5f70 6f6f 6c5f 7370 6563 735f 6f76 6572  _pool_specs_over
-00014c10: 7269 6465 3d77 6f72 6b65 725f 706f 6f6c  ride=worker_pool
-00014c20: 5f73 7065 6373 5f6f 7665 7272 6964 652c  _specs_override,
-00014c30: 0a20 2020 2020 2020 2072 756e 5f65 7661  .        run_eva
-00014c40: 6c75 6174 696f 6e3d 7275 6e5f 6576 616c  luation=run_eval
-00014c50: 7561 7469 6f6e 2c0a 2020 2020 2020 2020  uation,.        
-00014c60: 6576 616c 7561 7469 6f6e 5f62 6174 6368  evaluation_batch
-00014c70: 5f70 7265 6469 6374 5f6d 6163 6869 6e65  _predict_machine
-00014c80: 5f74 7970 653d 6576 616c 7561 7469 6f6e  _type=evaluation
-00014c90: 5f62 6174 6368 5f70 7265 6469 6374 5f6d  _batch_predict_m
-00014ca0: 6163 6869 6e65 5f74 7970 652c 0a20 2020  achine_type,.   
-00014cb0: 2020 2020 2065 7661 6c75 6174 696f 6e5f       evaluation_
-00014cc0: 6261 7463 685f 7072 6564 6963 745f 7374  batch_predict_st
-00014cd0: 6172 7469 6e67 5f72 6570 6c69 6361 5f63  arting_replica_c
-00014ce0: 6f75 6e74 3d65 7661 6c75 6174 696f 6e5f  ount=evaluation_
-00014cf0: 6261 7463 685f 7072 6564 6963 745f 7374  batch_predict_st
-00014d00: 6172 7469 6e67 5f72 6570 6c69 6361 5f63  arting_replica_c
-00014d10: 6f75 6e74 2c0a 2020 2020 2020 2020 6576  ount,.        ev
-00014d20: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
-00014d30: 7265 6469 6374 5f6d 6178 5f72 6570 6c69  redict_max_repli
-00014d40: 6361 5f63 6f75 6e74 3d65 7661 6c75 6174  ca_count=evaluat
-00014d50: 696f 6e5f 6261 7463 685f 7072 6564 6963  ion_batch_predic
-00014d60: 745f 6d61 785f 7265 706c 6963 615f 636f  t_max_replica_co
-00014d70: 756e 742c 0a20 2020 2020 2020 2065 7661  unt,.        eva
-00014d80: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
-00014d90: 5f6d 6163 6869 6e65 5f74 7970 653d 6576  _machine_type=ev
-00014da0: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
-00014db0: 775f 6d61 6368 696e 655f 7479 7065 2c0a  w_machine_type,.
-00014dc0: 2020 2020 2020 2020 6576 616c 7561 7469          evaluati
-00014dd0: 6f6e 5f64 6174 6166 6c6f 775f 6469 736b  on_dataflow_disk
-00014de0: 5f73 697a 655f 6762 3d65 7661 6c75 6174  _size_gb=evaluat
-00014df0: 696f 6e5f 6461 7461 666c 6f77 5f64 6973  ion_dataflow_dis
-00014e00: 6b5f 7369 7a65 5f67 622c 0a20 2020 2020  k_size_gb,.     
-00014e10: 2020 2065 7661 6c75 6174 696f 6e5f 6461     evaluation_da
-00014e20: 7461 666c 6f77 5f73 7461 7274 696e 675f  taflow_starting_
-00014e30: 6e75 6d5f 776f 726b 6572 733d 6576 616c  num_workers=eval
-00014e40: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
-00014e50: 7374 6172 7469 6e67 5f6e 756d 5f77 6f72  starting_num_wor
-00014e60: 6b65 7273 2c0a 2020 2020 2020 2020 6576  kers,.        ev
-00014e70: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
-00014e80: 775f 6d61 785f 6e75 6d5f 776f 726b 6572  w_max_num_worker
-00014e90: 733d 6576 616c 7561 7469 6f6e 5f64 6174  s=evaluation_dat
-00014ea0: 6166 6c6f 775f 6d61 785f 6e75 6d5f 776f  aflow_max_num_wo
-00014eb0: 726b 6572 732c 0a20 2020 2020 2020 2064  rkers,.        d
-00014ec0: 6174 6166 6c6f 775f 7365 7276 6963 655f  ataflow_service_
-00014ed0: 6163 636f 756e 743d 6461 7461 666c 6f77  account=dataflow
-00014ee0: 5f73 6572 7669 6365 5f61 6363 6f75 6e74  _service_account
-00014ef0: 2c0a 2020 2020 2020 2020 6461 7461 666c  ,.        datafl
-00014f00: 6f77 5f73 7562 6e65 7477 6f72 6b3d 6461  ow_subnetwork=da
-00014f10: 7461 666c 6f77 5f73 7562 6e65 7477 6f72  taflow_subnetwor
-00014f20: 6b2c 0a20 2020 2020 2020 2064 6174 6166  k,.        dataf
-00014f30: 6c6f 775f 7573 655f 7075 626c 6963 5f69  low_use_public_i
-00014f40: 7073 3d64 6174 6166 6c6f 775f 7573 655f  ps=dataflow_use_
-00014f50: 7075 626c 6963 5f69 7073 2c0a 2020 2020  public_ips,.    
-00014f60: 2020 2020 656e 6372 7970 7469 6f6e 5f73      encryption_s
-00014f70: 7065 635f 6b65 795f 6e61 6d65 3d65 6e63  pec_key_name=enc
-00014f80: 7279 7074 696f 6e5f 7370 6563 5f6b 6579  ryption_spec_key
-00014f90: 5f6e 616d 652c 0a20 2020 2029 0a20 2065  _name,.    ).  e
-00014fa0: 6c69 6620 616c 676f 7269 7468 6d20 3d3d  lif algorithm ==
-00014fb0: 2027 7769 6465 5f61 6e64 5f64 6565 7027   'wide_and_deep'
-00014fc0: 3a0a 2020 2020 7265 7475 726e 2067 6574  :.    return get
-00014fd0: 5f77 6964 655f 616e 645f 6465 6570 5f68  _wide_and_deep_h
-00014fe0: 7970 6572 7061 7261 6d65 7465 725f 7475  yperparameter_tu
-00014ff0: 6e69 6e67 5f6a 6f62 5f70 6970 656c 696e  ning_job_pipelin
-00015000: 655f 616e 645f 7061 7261 6d65 7465 7273  e_and_parameters
-00015010: 280a 2020 2020 2020 2020 7072 6f6a 6563  (.        projec
-00015020: 743d 7072 6f6a 6563 742c 0a20 2020 2020  t=project,.     
-00015030: 2020 206c 6f63 6174 696f 6e3d 6c6f 6361     location=loca
-00015040: 7469 6f6e 2c0a 2020 2020 2020 2020 726f  tion,.        ro
-00015050: 6f74 5f64 6972 3d72 6f6f 745f 6469 722c  ot_dir=root_dir,
-00015060: 0a20 2020 2020 2020 2074 6172 6765 745f  .        target_
-00015070: 636f 6c75 6d6e 3d74 6172 6765 745f 636f  column=target_co
-00015080: 6c75 6d6e 2c0a 2020 2020 2020 2020 7072  lumn,.        pr
-00015090: 6564 6963 7469 6f6e 5f74 7970 653d 7072  ediction_type=pr
-000150a0: 6564 6963 7469 6f6e 5f74 7970 652c 0a20  ediction_type,. 
-000150b0: 2020 2020 2020 2073 7475 6479 5f73 7065         study_spe
-000150c0: 635f 6d65 7472 6963 5f69 643d 7374 7564  c_metric_id=stud
-000150d0: 795f 7370 6563 5f6d 6574 7269 635f 6964  y_spec_metric_id
-000150e0: 2c0a 2020 2020 2020 2020 7374 7564 795f  ,.        study_
-000150f0: 7370 6563 5f6d 6574 7269 635f 676f 616c  spec_metric_goal
-00015100: 3d73 7475 6479 5f73 7065 635f 6d65 7472  =study_spec_metr
-00015110: 6963 5f67 6f61 6c2c 0a20 2020 2020 2020  ic_goal,.       
-00015120: 2073 7475 6479 5f73 7065 635f 7061 7261   study_spec_para
-00015130: 6d65 7465 7273 5f6f 7665 7272 6964 653d  meters_override=
-00015140: 7374 7564 795f 7370 6563 5f70 6172 616d  study_spec_param
-00015150: 6574 6572 735f 6f76 6572 7269 6465 2c0a  eters_override,.
-00015160: 2020 2020 2020 2020 6d61 785f 7472 6961          max_tria
-00015170: 6c5f 636f 756e 743d 6d61 785f 7472 6961  l_count=max_tria
-00015180: 6c5f 636f 756e 742c 0a20 2020 2020 2020  l_count,.       
-00015190: 2070 6172 616c 6c65 6c5f 7472 6961 6c5f   parallel_trial_
-000151a0: 636f 756e 743d 7061 7261 6c6c 656c 5f74  count=parallel_t
-000151b0: 7269 616c 5f63 6f75 6e74 2c0a 2020 2020  rial_count,.    
-000151c0: 2020 2020 7472 616e 7366 6f72 6d5f 636f      transform_co
-000151d0: 6e66 6967 3d74 7261 6e73 666f 726d 5f63  nfig=transform_c
-000151e0: 6f6e 6669 672c 0a20 2020 2020 2020 2064  onfig,.        d
-000151f0: 6174 6173 6574 5f6c 6576 656c 5f63 7573  ataset_level_cus
-00015200: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
-00015210: 6f6e 5f64 6566 696e 6974 696f 6e73 3d64  on_definitions=d
-00015220: 6174 6173 6574 5f6c 6576 656c 5f63 7573  ataset_level_cus
-00015230: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
-00015240: 6f6e 5f64 6566 696e 6974 696f 6e73 2c0a  on_definitions,.
-00015250: 2020 2020 2020 2020 6461 7461 7365 745f          dataset_
-00015260: 6c65 7665 6c5f 7472 616e 7366 6f72 6d61  level_transforma
-00015270: 7469 6f6e 733d 6461 7461 7365 745f 6c65  tions=dataset_le
-00015280: 7665 6c5f 7472 616e 7366 6f72 6d61 7469  vel_transformati
-00015290: 6f6e 732c 0a20 2020 2020 2020 2070 7265  ons,.        pre
-000152a0: 6465 6669 6e65 645f 7370 6c69 745f 6b65  defined_split_ke
-000152b0: 793d 7072 6564 6566 696e 6564 5f73 706c  y=predefined_spl
-000152c0: 6974 5f6b 6579 2c0a 2020 2020 2020 2020  it_key,.        
-000152d0: 7374 7261 7469 6669 6564 5f73 706c 6974  stratified_split
-000152e0: 5f6b 6579 3d73 7472 6174 6966 6965 645f  _key=stratified_
-000152f0: 7370 6c69 745f 6b65 792c 0a20 2020 2020  split_key,.     
-00015300: 2020 2074 7261 696e 696e 675f 6672 6163     training_frac
-00015310: 7469 6f6e 3d74 7261 696e 696e 675f 6672  tion=training_fr
-00015320: 6163 7469 6f6e 2c0a 2020 2020 2020 2020  action,.        
-00015330: 7661 6c69 6461 7469 6f6e 5f66 7261 6374  validation_fract
-00015340: 696f 6e3d 7661 6c69 6461 7469 6f6e 5f66  ion=validation_f
-00015350: 7261 6374 696f 6e2c 0a20 2020 2020 2020  raction,.       
-00015360: 2074 6573 745f 6672 6163 7469 6f6e 3d74   test_fraction=t
-00015370: 6573 745f 6672 6163 7469 6f6e 2c0a 2020  est_fraction,.  
-00015380: 2020 2020 2020 7466 5f74 7261 6e73 666f        tf_transfo
-00015390: 726d 5f65 7865 6375 7469 6f6e 5f65 6e67  rm_execution_eng
-000153a0: 696e 653d 7466 5f74 7261 6e73 666f 726d  ine=tf_transform
-000153b0: 5f65 7865 6375 7469 6f6e 5f65 6e67 696e  _execution_engin
-000153c0: 652c 0a20 2020 2020 2020 2074 665f 6175  e,.        tf_au
-000153d0: 746f 5f74 7261 6e73 666f 726d 5f66 6561  to_transform_fea
-000153e0: 7475 7265 733d 7466 5f61 7574 6f5f 7472  tures=tf_auto_tr
-000153f0: 616e 7366 6f72 6d5f 6665 6174 7572 6573  ansform_features
-00015400: 2c0a 2020 2020 2020 2020 7466 5f63 7573  ,.        tf_cus
-00015410: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
-00015420: 6f6e 5f64 6566 696e 6974 696f 6e73 3d74  on_definitions=t
-00015430: 665f 6375 7374 6f6d 5f74 7261 6e73 666f  f_custom_transfo
-00015440: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
-00015450: 6f6e 732c 0a20 2020 2020 2020 2074 665f  ons,.        tf_
-00015460: 7472 616e 7366 6f72 6d61 7469 6f6e 735f  transformations_
-00015470: 7061 7468 3d74 665f 7472 616e 7366 6f72  path=tf_transfor
-00015480: 6d61 7469 6f6e 735f 7061 7468 2c0a 2020  mations_path,.  
-00015490: 2020 2020 2020 656e 6162 6c65 5f70 726f        enable_pro
-000154a0: 6669 6c65 723d 656e 6162 6c65 5f70 726f  filer=enable_pro
-000154b0: 6669 6c65 722c 0a20 2020 2020 2020 2073  filer,.        s
-000154c0: 6565 643d 7365 6564 2c0a 2020 2020 2020  eed=seed,.      
-000154d0: 2020 6576 616c 5f73 7465 7073 3d65 7661    eval_steps=eva
-000154e0: 6c5f 7374 6570 732c 0a20 2020 2020 2020  l_steps,.       
-000154f0: 2065 7661 6c5f 6672 6571 7565 6e63 795f   eval_frequency_
-00015500: 7365 6373 3d65 7661 6c5f 6672 6571 7565  secs=eval_freque
-00015510: 6e63 795f 7365 6373 2c0a 2020 2020 2020  ncy_secs,.      
-00015520: 2020 6461 7461 5f73 6f75 7263 655f 6373    data_source_cs
-00015530: 765f 6669 6c65 6e61 6d65 733d 6461 7461  v_filenames=data
-00015540: 5f73 6f75 7263 655f 6373 765f 6669 6c65  _source_csv_file
-00015550: 6e61 6d65 732c 0a20 2020 2020 2020 2064  names,.        d
-00015560: 6174 615f 736f 7572 6365 5f62 6967 7175  ata_source_bigqu
-00015570: 6572 795f 7461 626c 655f 7061 7468 3d64  ery_table_path=d
-00015580: 6174 615f 736f 7572 6365 5f62 6967 7175  ata_source_bigqu
-00015590: 6572 795f 7461 626c 655f 7061 7468 2c0a  ery_table_path,.
-000155a0: 2020 2020 2020 2020 6269 6771 7565 7279          bigquery
-000155b0: 5f73 7461 6769 6e67 5f66 756c 6c5f 6461  _staging_full_da
-000155c0: 7461 7365 745f 6964 3d62 6967 7175 6572  taset_id=bigquer
-000155d0: 795f 7374 6167 696e 675f 6675 6c6c 5f64  y_staging_full_d
-000155e0: 6174 6173 6574 5f69 642c 0a20 2020 2020  ataset_id,.     
-000155f0: 2020 2077 6569 6768 745f 636f 6c75 6d6e     weight_column
-00015600: 3d77 6569 6768 745f 636f 6c75 6d6e 2c0a  =weight_column,.
-00015610: 2020 2020 2020 2020 6d61 785f 6661 696c          max_fail
-00015620: 6564 5f74 7269 616c 5f63 6f75 6e74 3d6d  ed_trial_count=m
-00015630: 6178 5f66 6169 6c65 645f 7472 6961 6c5f  ax_failed_trial_
-00015640: 636f 756e 742c 0a20 2020 2020 2020 2073  count,.        s
-00015650: 7475 6479 5f73 7065 635f 616c 676f 7269  tudy_spec_algori
-00015660: 7468 6d3d 7374 7564 795f 7370 6563 5f61  thm=study_spec_a
-00015670: 6c67 6f72 6974 686d 2c0a 2020 2020 2020  lgorithm,.      
-00015680: 2020 7374 7564 795f 7370 6563 5f6d 6561    study_spec_mea
-00015690: 7375 7265 6d65 6e74 5f73 656c 6563 7469  surement_selecti
-000156a0: 6f6e 5f74 7970 653d 7374 7564 795f 7370  on_type=study_sp
-000156b0: 6563 5f6d 6561 7375 7265 6d65 6e74 5f73  ec_measurement_s
-000156c0: 656c 6563 7469 6f6e 5f74 7970 652c 0a20  election_type,. 
-000156d0: 2020 2020 2020 2074 7261 6e73 666f 726d         transform
-000156e0: 5f64 6174 6166 6c6f 775f 6d61 6368 696e  _dataflow_machin
-000156f0: 655f 7479 7065 3d74 7261 6e73 666f 726d  e_type=transform
-00015700: 5f64 6174 6166 6c6f 775f 6d61 6368 696e  _dataflow_machin
-00015710: 655f 7479 7065 2c0a 2020 2020 2020 2020  e_type,.        
-00015720: 7472 616e 7366 6f72 6d5f 6461 7461 666c  transform_datafl
-00015730: 6f77 5f6d 6178 5f6e 756d 5f77 6f72 6b65  ow_max_num_worke
-00015740: 7273 3d74 7261 6e73 666f 726d 5f64 6174  rs=transform_dat
-00015750: 6166 6c6f 775f 6d61 785f 6e75 6d5f 776f  aflow_max_num_wo
-00015760: 726b 6572 732c 0a20 2020 2020 2020 2074  rkers,.        t
-00015770: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
-00015780: 775f 6469 736b 5f73 697a 655f 6762 3d74  w_disk_size_gb=t
-00015790: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
-000157a0: 775f 6469 736b 5f73 697a 655f 6762 2c0a  w_disk_size_gb,.
-000157b0: 2020 2020 2020 2020 776f 726b 6572 5f70          worker_p
-000157c0: 6f6f 6c5f 7370 6563 735f 6f76 6572 7269  ool_specs_overri
-000157d0: 6465 3d77 6f72 6b65 725f 706f 6f6c 5f73  de=worker_pool_s
-000157e0: 7065 6373 5f6f 7665 7272 6964 652c 0a20  pecs_override,. 
-000157f0: 2020 2020 2020 2072 756e 5f65 7661 6c75         run_evalu
-00015800: 6174 696f 6e3d 7275 6e5f 6576 616c 7561  ation=run_evalua
-00015810: 7469 6f6e 2c0a 2020 2020 2020 2020 6576  tion,.        ev
-00015820: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
-00015830: 7265 6469 6374 5f6d 6163 6869 6e65 5f74  redict_machine_t
-00015840: 7970 653d 6576 616c 7561 7469 6f6e 5f62  ype=evaluation_b
-00015850: 6174 6368 5f70 7265 6469 6374 5f6d 6163  atch_predict_mac
-00015860: 6869 6e65 5f74 7970 652c 0a20 2020 2020  hine_type,.     
-00015870: 2020 2065 7661 6c75 6174 696f 6e5f 6261     evaluation_ba
-00015880: 7463 685f 7072 6564 6963 745f 7374 6172  tch_predict_star
-00015890: 7469 6e67 5f72 6570 6c69 6361 5f63 6f75  ting_replica_cou
-000158a0: 6e74 3d65 7661 6c75 6174 696f 6e5f 6261  nt=evaluation_ba
-000158b0: 7463 685f 7072 6564 6963 745f 7374 6172  tch_predict_star
-000158c0: 7469 6e67 5f72 6570 6c69 6361 5f63 6f75  ting_replica_cou
-000158d0: 6e74 2c0a 2020 2020 2020 2020 6576 616c  nt,.        eval
-000158e0: 7561 7469 6f6e 5f62 6174 6368 5f70 7265  uation_batch_pre
-000158f0: 6469 6374 5f6d 6178 5f72 6570 6c69 6361  dict_max_replica
-00015900: 5f63 6f75 6e74 3d65 7661 6c75 6174 696f  _count=evaluatio
-00015910: 6e5f 6261 7463 685f 7072 6564 6963 745f  n_batch_predict_
-00015920: 6d61 785f 7265 706c 6963 615f 636f 756e  max_replica_coun
-00015930: 742c 0a20 2020 2020 2020 2065 7661 6c75  t,.        evalu
-00015940: 6174 696f 6e5f 6461 7461 666c 6f77 5f6d  ation_dataflow_m
-00015950: 6163 6869 6e65 5f74 7970 653d 6576 616c  achine_type=eval
-00015960: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
-00015970: 6d61 6368 696e 655f 7479 7065 2c0a 2020  machine_type,.  
-00015980: 2020 2020 2020 6576 616c 7561 7469 6f6e        evaluation
-00015990: 5f64 6174 6166 6c6f 775f 6469 736b 5f73  _dataflow_disk_s
-000159a0: 697a 655f 6762 3d65 7661 6c75 6174 696f  ize_gb=evaluatio
-000159b0: 6e5f 6461 7461 666c 6f77 5f64 6973 6b5f  n_dataflow_disk_
-000159c0: 7369 7a65 5f67 622c 0a20 2020 2020 2020  size_gb,.       
-000159d0: 2065 7661 6c75 6174 696f 6e5f 6461 7461   evaluation_data
-000159e0: 666c 6f77 5f73 7461 7274 696e 675f 6e75  flow_starting_nu
-000159f0: 6d5f 776f 726b 6572 733d 6576 616c 7561  m_workers=evalua
-00015a00: 7469 6f6e 5f64 6174 6166 6c6f 775f 7374  tion_dataflow_st
-00015a10: 6172 7469 6e67 5f6e 756d 5f77 6f72 6b65  arting_num_worke
-00015a20: 7273 2c0a 2020 2020 2020 2020 6576 616c  rs,.        eval
-00015a30: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
-00015a40: 6d61 785f 6e75 6d5f 776f 726b 6572 733d  max_num_workers=
-00015a50: 6576 616c 7561 7469 6f6e 5f64 6174 6166  evaluation_dataf
-00015a60: 6c6f 775f 6d61 785f 6e75 6d5f 776f 726b  low_max_num_work
-00015a70: 6572 732c 0a20 2020 2020 2020 2064 6174  ers,.        dat
-00015a80: 6166 6c6f 775f 7365 7276 6963 655f 6163  aflow_service_ac
-00015a90: 636f 756e 743d 6461 7461 666c 6f77 5f73  count=dataflow_s
-00015aa0: 6572 7669 6365 5f61 6363 6f75 6e74 2c0a  ervice_account,.
-00015ab0: 2020 2020 2020 2020 6461 7461 666c 6f77          dataflow
-00015ac0: 5f73 7562 6e65 7477 6f72 6b3d 6461 7461  _subnetwork=data
-00015ad0: 666c 6f77 5f73 7562 6e65 7477 6f72 6b2c  flow_subnetwork,
-00015ae0: 0a20 2020 2020 2020 2064 6174 6166 6c6f  .        dataflo
-00015af0: 775f 7573 655f 7075 626c 6963 5f69 7073  w_use_public_ips
-00015b00: 3d64 6174 6166 6c6f 775f 7573 655f 7075  =dataflow_use_pu
-00015b10: 626c 6963 5f69 7073 2c0a 2020 2020 2020  blic_ips,.      
-00015b20: 2020 656e 6372 7970 7469 6f6e 5f73 7065    encryption_spe
-00015b30: 635f 6b65 795f 6e61 6d65 3d65 6e63 7279  c_key_name=encry
-00015b40: 7074 696f 6e5f 7370 6563 5f6b 6579 5f6e  ption_spec_key_n
-00015b50: 616d 652c 0a20 2020 2029 0a20 2065 6c73  ame,.    ).  els
-00015b60: 653a 0a20 2020 2072 6169 7365 2056 616c  e:.    raise Val
-00015b70: 7565 4572 726f 7228 0a20 2020 2020 2020  ueError(.       
-00015b80: 2027 496e 7661 6c69 6420 616c 676f 7269   'Invalid algori
-00015b90: 7468 6d20 7072 6f76 6964 6564 2e20 5375  thm provided. Su
-00015ba0: 7070 6f72 7465 6420 7661 6c75 6573 2061  pported values a
-00015bb0: 7265 2022 7461 626e 6574 2220 616e 6427  re "tabnet" and'
-00015bc0: 0a20 2020 2020 2020 2027 2022 7769 6465  .        ' "wide
-00015bd0: 5f61 6e64 5f64 6565 7022 2e27 0a20 2020  _and_deep".'.   
-00015be0: 2029 0a0a 0a64 6566 2067 6574 5f74 6162   )...def get_tab
-00015bf0: 6e65 745f 6879 7065 7270 6172 616d 6574  net_hyperparamet
-00015c00: 6572 5f74 756e 696e 675f 6a6f 625f 7069  er_tuning_job_pi
-00015c10: 7065 6c69 6e65 5f61 6e64 5f70 6172 616d  peline_and_param
-00015c20: 6574 6572 7328 0a20 2020 2070 726f 6a65  eters(.    proje
-00015c30: 6374 3a20 7374 722c 0a20 2020 206c 6f63  ct: str,.    loc
-00015c40: 6174 696f 6e3a 2073 7472 2c0a 2020 2020  ation: str,.    
-00015c50: 726f 6f74 5f64 6972 3a20 7374 722c 0a20  root_dir: str,. 
-00015c60: 2020 2074 6172 6765 745f 636f 6c75 6d6e     target_column
-00015c70: 3a20 7374 722c 0a20 2020 2070 7265 6469  : str,.    predi
-00015c80: 6374 696f 6e5f 7479 7065 3a20 7374 722c  ction_type: str,
-00015c90: 0a20 2020 2073 7475 6479 5f73 7065 635f  .    study_spec_
-00015ca0: 6d65 7472 6963 5f69 643a 2073 7472 2c0a  metric_id: str,.
-00015cb0: 2020 2020 7374 7564 795f 7370 6563 5f6d      study_spec_m
-00015cc0: 6574 7269 635f 676f 616c 3a20 7374 722c  etric_goal: str,
-00015cd0: 0a20 2020 2073 7475 6479 5f73 7065 635f  .    study_spec_
-00015ce0: 7061 7261 6d65 7465 7273 5f6f 7665 7272  parameters_overr
-00015cf0: 6964 653a 204c 6973 745b 4469 6374 5b73  ide: List[Dict[s
-00015d00: 7472 2c20 416e 795d 5d2c 0a20 2020 206d  tr, Any]],.    m
-00015d10: 6178 5f74 7269 616c 5f63 6f75 6e74 3a20  ax_trial_count: 
-00015d20: 696e 742c 0a20 2020 2070 6172 616c 6c65  int,.    paralle
-00015d30: 6c5f 7472 6961 6c5f 636f 756e 743a 2069  l_trial_count: i
-00015d40: 6e74 2c0a 2020 2020 7472 616e 7366 6f72  nt,.    transfor
-00015d50: 6d5f 636f 6e66 6967 3a20 4f70 7469 6f6e  m_config: Option
-00015d60: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
-00015d70: 2020 2020 6461 7461 7365 745f 6c65 7665      dataset_leve
-00015d80: 6c5f 6375 7374 6f6d 5f74 7261 6e73 666f  l_custom_transfo
-00015d90: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
-00015da0: 6f6e 733a 204f 7074 696f 6e61 6c5b 0a20  ons: Optional[. 
-00015db0: 2020 2020 2020 204c 6973 745b 4469 6374         List[Dict
-00015dc0: 5b73 7472 2c20 416e 795d 5d0a 2020 2020  [str, Any]].    
-00015dd0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6461  ] = None,.    da
-00015de0: 7461 7365 745f 6c65 7665 6c5f 7472 616e  taset_level_tran
-00015df0: 7366 6f72 6d61 7469 6f6e 733a 204f 7074  sformations: Opt
-00015e00: 696f 6e61 6c5b 4c69 7374 5b44 6963 745b  ional[List[Dict[
-00015e10: 7374 722c 2041 6e79 5d5d 5d20 3d20 4e6f  str, Any]]] = No
-00015e20: 6e65 2c0a 2020 2020 7275 6e5f 6665 6174  ne,.    run_feat
-00015e30: 7572 655f 7365 6c65 6374 696f 6e3a 2062  ure_selection: b
-00015e40: 6f6f 6c20 3d20 4661 6c73 652c 0a20 2020  ool = False,.   
-00015e50: 2066 6561 7475 7265 5f73 656c 6563 7469   feature_selecti
-00015e60: 6f6e 5f61 6c67 6f72 6974 686d 3a20 4f70  on_algorithm: Op
-00015e70: 7469 6f6e 616c 5b73 7472 5d20 3d20 4e6f  tional[str] = No
-00015e80: 6e65 2c0a 2020 2020 6d61 7465 7269 616c  ne,.    material
-00015e90: 697a 6564 5f65 7861 6d70 6c65 735f 666f  ized_examples_fo
-00015ea0: 726d 6174 3a20 4f70 7469 6f6e 616c 5b73  rmat: Optional[s
-00015eb0: 7472 5d20 3d20 4e6f 6e65 2c0a 2020 2020  tr] = None,.    
-00015ec0: 6d61 785f 7365 6c65 6374 6564 5f66 6561  max_selected_fea
-00015ed0: 7475 7265 733a 204f 7074 696f 6e61 6c5b  tures: Optional[
-00015ee0: 696e 745d 203d 204e 6f6e 652c 0a20 2020  int] = None,.   
-00015ef0: 2070 7265 6465 6669 6e65 645f 7370 6c69   predefined_spli
-00015f00: 745f 6b65 793a 204f 7074 696f 6e61 6c5b  t_key: Optional[
-00015f10: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
-00015f20: 2073 7472 6174 6966 6965 645f 7370 6c69   stratified_spli
-00015f30: 745f 6b65 793a 204f 7074 696f 6e61 6c5b  t_key: Optional[
-00015f40: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
-00015f50: 2074 7261 696e 696e 675f 6672 6163 7469   training_fracti
-00015f60: 6f6e 3a20 4f70 7469 6f6e 616c 5b66 6c6f  on: Optional[flo
-00015f70: 6174 5d20 3d20 4e6f 6e65 2c0a 2020 2020  at] = None,.    
-00015f80: 7661 6c69 6461 7469 6f6e 5f66 7261 6374  validation_fract
-00015f90: 696f 6e3a 204f 7074 696f 6e61 6c5b 666c  ion: Optional[fl
-00015fa0: 6f61 745d 203d 204e 6f6e 652c 0a20 2020  oat] = None,.   
-00015fb0: 2074 6573 745f 6672 6163 7469 6f6e 3a20   test_fraction: 
-00015fc0: 4f70 7469 6f6e 616c 5b66 6c6f 6174 5d20  Optional[float] 
-00015fd0: 3d20 4e6f 6e65 2c0a 2020 2020 7466 5f74  = None,.    tf_t
-00015fe0: 7261 6e73 666f 726d 5f65 7865 6375 7469  ransform_executi
-00015ff0: 6f6e 5f65 6e67 696e 653a 204f 7074 696f  on_engine: Optio
-00016000: 6e61 6c5b 7374 725d 203d 204e 6f6e 652c  nal[str] = None,
-00016010: 0a20 2020 2074 665f 6175 746f 5f74 7261  .    tf_auto_tra
-00016020: 6e73 666f 726d 5f66 6561 7475 7265 733a  nsform_features:
-00016030: 204f 7074 696f 6e61 6c5b 0a20 2020 2020   Optional[.     
-00016040: 2020 2055 6e69 6f6e 5b4c 6973 745b 7374     Union[List[st
-00016050: 725d 2c20 4469 6374 5b73 7472 2c20 4c69  r], Dict[str, Li
-00016060: 7374 5b73 7472 5d5d 5d0a 2020 2020 5d20  st[str]]].    ] 
-00016070: 3d20 4e6f 6e65 2c0a 2020 2020 7466 5f63  = None,.    tf_c
-00016080: 7573 746f 6d5f 7472 616e 7366 6f72 6d61  ustom_transforma
-00016090: 7469 6f6e 5f64 6566 696e 6974 696f 6e73  tion_definitions
-000160a0: 3a20 4f70 7469 6f6e 616c 5b4c 6973 745b  : Optional[List[
-000160b0: 4469 6374 5b73 7472 2c20 416e 795d 5d5d  Dict[str, Any]]]
-000160c0: 203d 204e 6f6e 652c 0a20 2020 2074 665f   = None,.    tf_
-000160d0: 7472 616e 7366 6f72 6d61 7469 6f6e 735f  transformations_
-000160e0: 7061 7468 3a20 4f70 7469 6f6e 616c 5b73  path: Optional[s
-000160f0: 7472 5d20 3d20 4e6f 6e65 2c0a 2020 2020  tr] = None,.    
-00016100: 656e 6162 6c65 5f70 726f 6669 6c65 723a  enable_profiler:
-00016110: 2062 6f6f 6c20 3d20 4661 6c73 652c 0a20   bool = False,. 
-00016120: 2020 2063 6163 6865 5f64 6174 613a 2073     cache_data: s
-00016130: 7472 203d 2027 6175 746f 272c 0a20 2020  tr = 'auto',.   
-00016140: 2073 6565 643a 2069 6e74 203d 2031 2c0a   seed: int = 1,.
-00016150: 2020 2020 6576 616c 5f73 7465 7073 3a20      eval_steps: 
-00016160: 696e 7420 3d20 302c 0a20 2020 2065 7661  int = 0,.    eva
-00016170: 6c5f 6672 6571 7565 6e63 795f 7365 6373  l_frequency_secs
-00016180: 3a20 696e 7420 3d20 3630 302c 0a20 2020  : int = 600,.   
-00016190: 2064 6174 615f 736f 7572 6365 5f63 7376   data_source_csv
-000161a0: 5f66 696c 656e 616d 6573 3a20 4f70 7469  _filenames: Opti
-000161b0: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
-000161c0: 2c0a 2020 2020 6461 7461 5f73 6f75 7263  ,.    data_sourc
-000161d0: 655f 6269 6771 7565 7279 5f74 6162 6c65  e_bigquery_table
-000161e0: 5f70 6174 683a 204f 7074 696f 6e61 6c5b  _path: Optional[
-000161f0: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
-00016200: 2062 6967 7175 6572 795f 7374 6167 696e   bigquery_stagin
-00016210: 675f 6675 6c6c 5f64 6174 6173 6574 5f69  g_full_dataset_i
-00016220: 643a 204f 7074 696f 6e61 6c5b 7374 725d  d: Optional[str]
-00016230: 203d 204e 6f6e 652c 0a20 2020 2077 6569   = None,.    wei
-00016240: 6768 745f 636f 6c75 6d6e 3a20 7374 7220  ght_column: str 
-00016250: 3d20 2727 2c0a 2020 2020 6d61 785f 6661  = '',.    max_fa
-00016260: 696c 6564 5f74 7269 616c 5f63 6f75 6e74  iled_trial_count
-00016270: 3a20 696e 7420 3d20 302c 0a20 2020 2073  : int = 0,.    s
-00016280: 7475 6479 5f73 7065 635f 616c 676f 7269  tudy_spec_algori
-00016290: 7468 6d3a 2073 7472 203d 2027 414c 474f  thm: str = 'ALGO
-000162a0: 5249 5448 4d5f 554e 5350 4543 4946 4945  RITHM_UNSPECIFIE
-000162b0: 4427 2c0a 2020 2020 7374 7564 795f 7370  D',.    study_sp
-000162c0: 6563 5f6d 6561 7375 7265 6d65 6e74 5f73  ec_measurement_s
-000162d0: 656c 6563 7469 6f6e 5f74 7970 653a 2073  election_type: s
-000162e0: 7472 203d 2027 4245 5354 5f4d 4541 5355  tr = 'BEST_MEASU
-000162f0: 5245 4d45 4e54 272c 0a20 2020 2074 7261  REMENT',.    tra
-00016300: 6e73 666f 726d 5f64 6174 6166 6c6f 775f  nsform_dataflow_
-00016310: 6d61 6368 696e 655f 7479 7065 3a20 7374  machine_type: st
-00016320: 7220 3d20 276e 312d 7374 616e 6461 7264  r = 'n1-standard
-00016330: 2d31 3627 2c0a 2020 2020 7472 616e 7366  -16',.    transf
-00016340: 6f72 6d5f 6461 7461 666c 6f77 5f6d 6178  orm_dataflow_max
-00016350: 5f6e 756d 5f77 6f72 6b65 7273 3a20 696e  _num_workers: in
-00016360: 7420 3d20 3235 2c0a 2020 2020 7472 616e  t = 25,.    tran
-00016370: 7366 6f72 6d5f 6461 7461 666c 6f77 5f64  sform_dataflow_d
-00016380: 6973 6b5f 7369 7a65 5f67 623a 2069 6e74  isk_size_gb: int
-00016390: 203d 2034 302c 0a20 2020 2077 6f72 6b65   = 40,.    worke
-000163a0: 725f 706f 6f6c 5f73 7065 6373 5f6f 7665  r_pool_specs_ove
-000163b0: 7272 6964 653a 204f 7074 696f 6e61 6c5b  rride: Optional[
-000163c0: 4469 6374 5b73 7472 2c20 416e 795d 5d20  Dict[str, Any]] 
-000163d0: 3d20 4e6f 6e65 2c0a 2020 2020 7275 6e5f  = None,.    run_
-000163e0: 6576 616c 7561 7469 6f6e 3a20 626f 6f6c  evaluation: bool
-000163f0: 203d 2054 7275 652c 0a20 2020 2065 7661   = True,.    eva
-00016400: 6c75 6174 696f 6e5f 6261 7463 685f 7072  luation_batch_pr
-00016410: 6564 6963 745f 6d61 6368 696e 655f 7479  edict_machine_ty
-00016420: 7065 3a20 7374 7220 3d20 5f45 5641 4c55  pe: str = _EVALU
-00016430: 4154 494f 4e5f 4241 5443 485f 5052 4544  ATION_BATCH_PRED
-00016440: 4943 545f 4d41 4348 494e 455f 5459 5045  ICT_MACHINE_TYPE
-00016450: 2c0a 2020 2020 6576 616c 7561 7469 6f6e  ,.    evaluation
-00016460: 5f62 6174 6368 5f70 7265 6469 6374 5f73  _batch_predict_s
-00016470: 7461 7274 696e 675f 7265 706c 6963 615f  tarting_replica_
-00016480: 636f 756e 743a 2069 6e74 203d 205f 4556  count: int = _EV
-00016490: 414c 5541 5449 4f4e 5f42 4154 4348 5f50  ALUATION_BATCH_P
-000164a0: 5245 4449 4354 5f53 5441 5254 494e 475f  REDICT_STARTING_
-000164b0: 5245 504c 4943 415f 434f 554e 542c 0a20  REPLICA_COUNT,. 
-000164c0: 2020 2065 7661 6c75 6174 696f 6e5f 6261     evaluation_ba
-000164d0: 7463 685f 7072 6564 6963 745f 6d61 785f  tch_predict_max_
-000164e0: 7265 706c 6963 615f 636f 756e 743a 2069  replica_count: i
-000164f0: 6e74 203d 205f 4556 414c 5541 5449 4f4e  nt = _EVALUATION
-00016500: 5f42 4154 4348 5f50 5245 4449 4354 5f4d  _BATCH_PREDICT_M
-00016510: 4158 5f52 4550 4c49 4341 5f43 4f55 4e54  AX_REPLICA_COUNT
-00016520: 2c0a 2020 2020 6576 616c 7561 7469 6f6e  ,.    evaluation
-00016530: 5f64 6174 6166 6c6f 775f 6d61 6368 696e  _dataflow_machin
-00016540: 655f 7479 7065 3a20 7374 7220 3d20 5f45  e_type: str = _E
-00016550: 5641 4c55 4154 494f 4e5f 4441 5441 464c  VALUATION_DATAFL
-00016560: 4f57 5f4d 4143 4849 4e45 5f54 5950 452c  OW_MACHINE_TYPE,
-00016570: 0a20 2020 2065 7661 6c75 6174 696f 6e5f  .    evaluation_
-00016580: 6461 7461 666c 6f77 5f73 7461 7274 696e  dataflow_startin
-00016590: 675f 6e75 6d5f 776f 726b 6572 733a 2069  g_num_workers: i
-000165a0: 6e74 203d 205f 4556 414c 5541 5449 4f4e  nt = _EVALUATION
-000165b0: 5f44 4154 4146 4c4f 575f 5354 4152 5449  _DATAFLOW_STARTI
-000165c0: 4e47 5f4e 554d 5f57 4f52 4b45 5253 2c0a  NG_NUM_WORKERS,.
-000165d0: 2020 2020 6576 616c 7561 7469 6f6e 5f64      evaluation_d
-000165e0: 6174 6166 6c6f 775f 6d61 785f 6e75 6d5f  ataflow_max_num_
-000165f0: 776f 726b 6572 733a 2069 6e74 203d 205f  workers: int = _
-00016600: 4556 414c 5541 5449 4f4e 5f44 4154 4146  EVALUATION_DATAF
-00016610: 4c4f 575f 4d41 585f 4e55 4d5f 574f 524b  LOW_MAX_NUM_WORK
-00016620: 4552 532c 0a20 2020 2065 7661 6c75 6174  ERS,.    evaluat
-00016630: 696f 6e5f 6461 7461 666c 6f77 5f64 6973  ion_dataflow_dis
-00016640: 6b5f 7369 7a65 5f67 623a 2069 6e74 203d  k_size_gb: int =
-00016650: 205f 4556 414c 5541 5449 4f4e 5f44 4154   _EVALUATION_DAT
-00016660: 4146 4c4f 575f 4449 534b 5f53 495a 455f  AFLOW_DISK_SIZE_
-00016670: 4742 2c0a 2020 2020 6461 7461 666c 6f77  GB,.    dataflow
-00016680: 5f73 6572 7669 6365 5f61 6363 6f75 6e74  _service_account
-00016690: 3a20 7374 7220 3d20 2727 2c0a 2020 2020  : str = '',.    
-000166a0: 6461 7461 666c 6f77 5f73 7562 6e65 7477  dataflow_subnetw
-000166b0: 6f72 6b3a 2073 7472 203d 2027 272c 0a20  ork: str = '',. 
-000166c0: 2020 2064 6174 6166 6c6f 775f 7573 655f     dataflow_use_
-000166d0: 7075 626c 6963 5f69 7073 3a20 626f 6f6c  public_ips: bool
-000166e0: 203d 2054 7275 652c 0a20 2020 2065 6e63   = True,.    enc
-000166f0: 7279 7074 696f 6e5f 7370 6563 5f6b 6579  ryption_spec_key
-00016700: 5f6e 616d 653a 2073 7472 203d 2027 272c  _name: str = '',
-00016710: 0a29 202d 3e20 5475 706c 655b 7374 722c  .) -> Tuple[str,
-00016720: 2044 6963 745b 7374 722c 2041 6e79 5d5d   Dict[str, Any]]
-00016730: 3a0a 2020 2222 2247 6574 2074 6865 2054  :.  """Get the T
-00016740: 6162 4e65 7420 4879 7065 7270 6172 616d  abNet Hyperparam
-00016750: 6574 6572 5475 6e69 6e67 4a6f 6220 7069  eterTuningJob pi
-00016760: 7065 6c69 6e65 2e0a 0a20 2041 7267 733a  peline...  Args:
-00016770: 0a20 2020 2070 726f 6a65 6374 3a20 5468  .    project: Th
-00016780: 6520 4743 5020 7072 6f6a 6563 7420 7468  e GCP project th
-00016790: 6174 2072 756e 7320 7468 6520 7069 7065  at runs the pipe
-000167a0: 6c69 6e65 2063 6f6d 706f 6e65 6e74 732e  line components.
-000167b0: 0a20 2020 206c 6f63 6174 696f 6e3a 2054  .    location: T
-000167c0: 6865 2047 4350 2072 6567 696f 6e20 7468  he GCP region th
-000167d0: 6174 2072 756e 7320 7468 6520 7069 7065  at runs the pipe
-000167e0: 6c69 6e65 2063 6f6d 706f 6e65 6e74 732e  line components.
-000167f0: 0a20 2020 2072 6f6f 745f 6469 723a 2054  .    root_dir: T
-00016800: 6865 2072 6f6f 7420 4743 5320 6469 7265  he root GCS dire
-00016810: 6374 6f72 7920 666f 7220 7468 6520 7069  ctory for the pi
-00016820: 7065 6c69 6e65 2063 6f6d 706f 6e65 6e74  peline component
-00016830: 732e 0a20 2020 2074 6172 6765 745f 636f  s..    target_co
-00016840: 6c75 6d6e 3a20 5468 6520 7461 7267 6574  lumn: The target
-00016850: 2063 6f6c 756d 6e20 6e61 6d65 2e0a 2020   column name..  
-00016860: 2020 7072 6564 6963 7469 6f6e 5f74 7970    prediction_typ
-00016870: 653a 2054 6865 2074 7970 6520 6f66 2070  e: The type of p
-00016880: 7265 6469 6374 696f 6e20 7468 6520 6d6f  rediction the mo
-00016890: 6465 6c20 6973 2074 6f20 7072 6f64 7563  del is to produc
-000168a0: 652e 0a20 2020 2020 2022 636c 6173 7369  e..      "classi
-000168b0: 6669 6361 7469 6f6e 2220 6f72 2022 7265  fication" or "re
-000168c0: 6772 6573 7369 6f6e 222e 0a20 2020 2073  gression"..    s
-000168d0: 7475 6479 5f73 7065 635f 6d65 7472 6963  tudy_spec_metric
-000168e0: 5f69 643a 204d 6574 7269 6320 746f 206f  _id: Metric to o
-000168f0: 7074 696d 697a 652c 2070 6f73 7369 626c  ptimize, possibl
-00016900: 6520 7661 6c75 6573 3a20 5b20 276c 6f73  e values: [ 'los
-00016910: 7327 2c0a 2020 2020 2020 2761 7665 7261  s',.      'avera
-00016920: 6765 5f6c 6f73 7327 2c20 2772 6d73 6527  ge_loss', 'rmse'
-00016930: 2c20 276d 6165 272c 2027 6d71 6c27 2c20  , 'mae', 'mql', 
-00016940: 2761 6363 7572 6163 7927 2c20 2761 7563  'accuracy', 'auc
-00016950: 272c 2027 7072 6563 6973 696f 6e27 2c0a  ', 'precision',.
-00016960: 2020 2020 2020 2772 6563 616c 6c27 5d2e        'recall'].
-00016970: 0a20 2020 2073 7475 6479 5f73 7065 635f  .    study_spec_
-00016980: 6d65 7472 6963 5f67 6f61 6c3a 204f 7074  metric_goal: Opt
-00016990: 696d 697a 6174 696f 6e20 676f 616c 206f  imization goal o
-000169a0: 6620 7468 6520 6d65 7472 6963 2c20 706f  f the metric, po
-000169b0: 7373 6962 6c65 2076 616c 7565 733a 0a20  ssible values:. 
-000169c0: 2020 2020 2022 4d41 5849 4d49 5a45 222c       "MAXIMIZE",
-000169d0: 2022 4d49 4e49 4d49 5a45 222e 0a20 2020   "MINIMIZE"..   
-000169e0: 2073 7475 6479 5f73 7065 635f 7061 7261   study_spec_para
-000169f0: 6d65 7465 7273 5f6f 7665 7272 6964 653a  meters_override:
-00016a00: 204c 6973 7420 6f66 2064 6963 7469 6f6e   List of diction
-00016a10: 6172 6965 7320 7265 7072 6573 656e 7469  aries representi
-00016a20: 6e67 2070 6172 616d 6574 6572 730a 2020  ng parameters.  
-00016a30: 2020 2020 746f 206f 7074 696d 697a 652e      to optimize.
-00016a40: 2054 6865 2064 6963 7469 6f6e 6172 7920   The dictionary 
-00016a50: 6b65 7920 6973 2074 6865 2070 6172 616d  key is the param
-00016a60: 6574 6572 5f69 642c 2077 6869 6368 2069  eter_id, which i
-00016a70: 7320 7061 7373 6564 2074 6f0a 2020 2020  s passed to.    
-00016a80: 2020 7472 6169 6e69 6e67 206a 6f62 2061    training job a
-00016a90: 7320 6120 636f 6d6d 616e 6420 6c69 6e65  s a command line
-00016aa0: 2061 7267 756d 656e 742c 2061 6e64 2074   argument, and t
-00016ab0: 6865 2064 6963 7469 6f6e 6172 7920 7661  he dictionary va
-00016ac0: 6c75 6520 6973 2074 6865 0a20 2020 2020  lue is the.     
-00016ad0: 2070 6172 616d 6574 6572 2073 7065 6369   parameter speci
-00016ae0: 6669 6361 7469 6f6e 206f 6620 7468 6520  fication of the 
-00016af0: 6d65 7472 6963 2e0a 2020 2020 6d61 785f  metric..    max_
-00016b00: 7472 6961 6c5f 636f 756e 743a 2054 6865  trial_count: The
-00016b10: 2064 6573 6972 6564 2074 6f74 616c 206e   desired total n
-00016b20: 756d 6265 7220 6f66 2074 7269 616c 732e  umber of trials.
-00016b30: 0a20 2020 2070 6172 616c 6c65 6c5f 7472  .    parallel_tr
-00016b40: 6961 6c5f 636f 756e 743a 2054 6865 2064  ial_count: The d
-00016b50: 6573 6972 6564 206e 756d 6265 7220 6f66  esired number of
-00016b60: 2074 7269 616c 7320 746f 2072 756e 2069   trials to run i
-00016b70: 6e20 7061 7261 6c6c 656c 2e0a 2020 2020  n parallel..    
-00016b80: 7472 616e 7366 6f72 6d5f 636f 6e66 6967  transform_config
-00016b90: 3a20 5061 7468 2074 6f20 7631 2054 4620  : Path to v1 TF 
-00016ba0: 7472 616e 7366 6f72 6d61 7469 6f6e 2063  transformation c
-00016bb0: 6f6e 6669 6775 7261 7469 6f6e 2e0a 2020  onfiguration..  
-00016bc0: 2020 6461 7461 7365 745f 6c65 7665 6c5f    dataset_level_
-00016bd0: 6375 7374 6f6d 5f74 7261 6e73 666f 726d  custom_transform
-00016be0: 6174 696f 6e5f 6465 6669 6e69 7469 6f6e  ation_definition
-00016bf0: 733a 2044 6174 6173 6574 2d6c 6576 656c  s: Dataset-level
-00016c00: 2063 7573 746f 6d0a 2020 2020 2020 7472   custom.      tr
-00016c10: 616e 7366 6f72 6d61 7469 6f6e 2064 6566  ansformation def
-00016c20: 696e 6974 696f 6e73 2069 6e20 7374 7269  initions in stri
-00016c30: 6e67 2066 6f72 6d61 742e 0a20 2020 2064  ng format..    d
-00016c40: 6174 6173 6574 5f6c 6576 656c 5f74 7261  ataset_level_tra
-00016c50: 6e73 666f 726d 6174 696f 6e73 3a20 4461  nsformations: Da
-00016c60: 7461 7365 742d 6c65 7665 6c20 7472 616e  taset-level tran
-00016c70: 7366 6f72 6d61 7469 6f6e 2063 6f6e 6669  sformation confi
-00016c80: 6775 7261 7469 6f6e 2069 6e0a 2020 2020  guration in.    
-00016c90: 2020 7374 7269 6e67 2066 6f72 6d61 742e    string format.
-00016ca0: 0a20 2020 2072 756e 5f66 6561 7475 7265  .    run_feature
-00016cb0: 5f73 656c 6563 7469 6f6e 3a20 5768 6574  _selection: Whet
-00016cc0: 6865 7220 746f 2065 6e61 626c 6520 6665  her to enable fe
-00016cd0: 6174 7572 6520 7365 6c65 6374 696f 6e2e  ature selection.
-00016ce0: 0a20 2020 2066 6561 7475 7265 5f73 656c  .    feature_sel
-00016cf0: 6563 7469 6f6e 5f61 6c67 6f72 6974 686d  ection_algorithm
-00016d00: 3a20 4665 6174 7572 6520 7365 6c65 6374  : Feature select
-00016d10: 696f 6e20 616c 676f 7269 7468 6d2e 0a20  ion algorithm.. 
-00016d20: 2020 206d 6174 6572 6961 6c69 7a65 645f     materialized_
-00016d30: 6578 616d 706c 6573 5f66 6f72 6d61 743a  examples_format:
-00016d40: 2054 6865 2066 6f72 6d61 7420 666f 7220   The format for 
-00016d50: 7468 6520 6d61 7465 7269 616c 697a 6564  the materialized
-00016d60: 2065 7861 6d70 6c65 732e 0a20 2020 206d   examples..    m
-00016d70: 6178 5f73 656c 6563 7465 645f 6665 6174  ax_selected_feat
-00016d80: 7572 6573 3a20 4d61 7869 6d75 6d20 6e75  ures: Maximum nu
-00016d90: 6d62 6572 206f 6620 6665 6174 7572 6573  mber of features
-00016da0: 2074 6f20 7365 6c65 6374 2e0a 2020 2020   to select..    
-00016db0: 7072 6564 6566 696e 6564 5f73 706c 6974  predefined_split
-00016dc0: 5f6b 6579 3a20 5072 6564 6566 696e 6564  _key: Predefined
-00016dd0: 2073 706c 6974 206b 6579 2e0a 2020 2020   split key..    
-00016de0: 7374 7261 7469 6669 6564 5f73 706c 6974  stratified_split
-00016df0: 5f6b 6579 3a20 5374 7261 7469 6669 6564  _key: Stratified
-00016e00: 2073 706c 6974 206b 6579 2e0a 2020 2020   split key..    
-00016e10: 7472 6169 6e69 6e67 5f66 7261 6374 696f  training_fractio
-00016e20: 6e3a 2054 7261 696e 696e 6720 6672 6163  n: Training frac
-00016e30: 7469 6f6e 2e0a 2020 2020 7661 6c69 6461  tion..    valida
-00016e40: 7469 6f6e 5f66 7261 6374 696f 6e3a 2056  tion_fraction: V
-00016e50: 616c 6964 6174 696f 6e20 6672 6163 7469  alidation fracti
-00016e60: 6f6e 2e0a 2020 2020 7465 7374 5f66 7261  on..    test_fra
-00016e70: 6374 696f 6e3a 2054 6573 7420 6672 6163  ction: Test frac
-00016e80: 7469 6f6e 2e0a 2020 2020 7466 5f74 7261  tion..    tf_tra
-00016e90: 6e73 666f 726d 5f65 7865 6375 7469 6f6e  nsform_execution
-00016ea0: 5f65 6e67 696e 653a 2054 6865 2065 7865  _engine: The exe
-00016eb0: 6375 7469 6f6e 2065 6e67 696e 6520 7573  cution engine us
-00016ec0: 6564 2074 6f20 6578 6563 7574 6520 5446  ed to execute TF
-00016ed0: 2d62 6173 6564 0a20 2020 2020 2074 7261  -based.      tra
-00016ee0: 6e73 666f 726d 6174 696f 6e73 2e0a 2020  nsformations..  
-00016ef0: 2020 7466 5f61 7574 6f5f 7472 616e 7366    tf_auto_transf
-00016f00: 6f72 6d5f 6665 6174 7572 6573 3a20 4c69  orm_features: Li
-00016f10: 7374 206f 6620 6175 746f 2074 7261 6e73  st of auto trans
-00016f20: 666f 726d 2066 6561 7475 7265 7320 696e  form features in
-00016f30: 2074 6865 0a20 2020 2020 2063 6f6d 6d61   the.      comma
-00016f40: 2d73 6570 6172 6174 6564 2073 7472 696e  -separated strin
-00016f50: 6720 666f 726d 6174 2e0a 2020 2020 7466  g format..    tf
-00016f60: 5f63 7573 746f 6d5f 7472 616e 7366 6f72  _custom_transfor
-00016f70: 6d61 7469 6f6e 5f64 6566 696e 6974 696f  mation_definitio
-00016f80: 6e73 3a20 5446 2063 7573 746f 6d20 7472  ns: TF custom tr
-00016f90: 616e 7366 6f72 6d61 7469 6f6e 2064 6566  ansformation def
-00016fa0: 696e 6974 696f 6e73 0a20 2020 2020 2069  initions.      i
-00016fb0: 6e20 7374 7269 6e67 2066 6f72 6d61 742e  n string format.
-00016fc0: 0a20 2020 2074 665f 7472 616e 7366 6f72  .    tf_transfor
-00016fd0: 6d61 7469 6f6e 735f 7061 7468 3a20 5061  mations_path: Pa
-00016fe0: 7468 2074 6f20 5446 2074 7261 6e73 666f  th to TF transfo
-00016ff0: 726d 6174 696f 6e20 636f 6e66 6967 7572  rmation configur
-00017000: 6174 696f 6e2e 0a20 2020 2065 6e61 626c  ation..    enabl
-00017010: 655f 7072 6f66 696c 6572 3a20 456e 6162  e_profiler: Enab
-00017020: 6c65 7320 7072 6f66 696c 696e 6720 616e  les profiling an
-00017030: 6420 7361 7665 7320 6120 7472 6163 6520  d saves a trace 
-00017040: 6475 7269 6e67 2065 7661 6c75 6174 696f  during evaluatio
-00017050: 6e2e 0a20 2020 2063 6163 6865 5f64 6174  n..    cache_dat
-00017060: 613a 2057 6865 7468 6572 2074 6f20 6361  a: Whether to ca
-00017070: 6368 6520 6461 7461 206f 7220 6e6f 742e  che data or not.
-00017080: 2049 6620 7365 7420 746f 2027 6175 746f   If set to 'auto
-00017090: 272c 2063 6163 6869 6e67 2069 730a 2020  ', caching is.  
-000170a0: 2020 2020 6465 7465 726d 696e 6564 2062      determined b
-000170b0: 6173 6564 206f 6e20 7468 6520 6461 7461  ased on the data
-000170c0: 7365 7420 7369 7a65 2e0a 2020 2020 7365  set size..    se
-000170d0: 6564 3a20 5365 6564 2074 6f20 6265 2075  ed: Seed to be u
-000170e0: 7365 6420 666f 7220 7468 6973 2072 756e  sed for this run
-000170f0: 2e0a 2020 2020 6576 616c 5f73 7465 7073  ..    eval_steps
-00017100: 3a20 4e75 6d62 6572 206f 6620 7374 6570  : Number of step
-00017110: 7320 746f 2072 756e 2065 7661 6c75 6174  s to run evaluat
-00017120: 696f 6e20 666f 722e 2049 6620 6e6f 7420  ion for. If not 
-00017130: 7370 6563 6966 6965 6420 6f72 0a20 2020  specified or.   
-00017140: 2020 206e 6567 6174 6976 652c 2069 7420     negative, it 
-00017150: 6d65 616e 7320 7275 6e20 6576 616c 7561  means run evalua
-00017160: 7469 6f6e 206f 6e20 7468 6520 7768 6f6c  tion on the whol
-00017170: 6520 7661 6c69 6461 7469 6f6e 2064 6174  e validation dat
-00017180: 6173 6574 2e20 4966 2073 6574 0a20 2020  aset. If set.   
-00017190: 2020 2074 6f20 302c 2069 7420 6d65 616e     to 0, it mean
-000171a0: 7320 7275 6e20 6576 616c 7561 7469 6f6e  s run evaluation
-000171b0: 2066 6f72 2061 2066 6978 6564 206e 756d   for a fixed num
-000171c0: 6265 7220 6f66 2073 616d 706c 6573 2e0a  ber of samples..
-000171d0: 2020 2020 6576 616c 5f66 7265 7175 656e      eval_frequen
-000171e0: 6379 5f73 6563 733a 2046 7265 7175 656e  cy_secs: Frequen
-000171f0: 6379 2061 7420 7768 6963 6820 6576 616c  cy at which eval
-00017200: 7561 7469 6f6e 2061 6e64 2063 6865 636b  uation and check
-00017210: 706f 696e 7469 6e67 2077 696c 6c0a 2020  pointing will.  
-00017220: 2020 2020 7461 6b65 2070 6c61 6365 2e0a      take place..
-00017230: 2020 2020 6461 7461 5f73 6f75 7263 655f      data_source_
-00017240: 6373 765f 6669 6c65 6e61 6d65 733a 2054  csv_filenames: T
-00017250: 6865 2043 5356 2064 6174 6120 736f 7572  he CSV data sour
-00017260: 6365 2e0a 2020 2020 6461 7461 5f73 6f75  ce..    data_sou
-00017270: 7263 655f 6269 6771 7565 7279 5f74 6162  rce_bigquery_tab
-00017280: 6c65 5f70 6174 683a 2054 6865 2042 6967  le_path: The Big
-00017290: 5175 6572 7920 6461 7461 2073 6f75 7263  Query data sourc
-000172a0: 652e 0a20 2020 2062 6967 7175 6572 795f  e..    bigquery_
-000172b0: 7374 6167 696e 675f 6675 6c6c 5f64 6174  staging_full_dat
-000172c0: 6173 6574 5f69 643a 2054 6865 2042 6967  aset_id: The Big
-000172d0: 5175 6572 7920 7374 6167 696e 6720 6675  Query staging fu
-000172e0: 6c6c 2064 6174 6173 6574 2069 6420 666f  ll dataset id fo
-000172f0: 720a 2020 2020 2020 7374 6f72 696e 6720  r.      storing 
-00017300: 696e 7465 726d 6564 6961 7465 2074 6162  intermediate tab
-00017310: 6c65 732e 0a20 2020 2077 6569 6768 745f  les..    weight_
-00017320: 636f 6c75 6d6e 3a20 5468 6520 7765 6967  column: The weig
-00017330: 6874 2063 6f6c 756d 6e20 6e61 6d65 2e0a  ht column name..
-00017340: 2020 2020 6d61 785f 6661 696c 6564 5f74      max_failed_t
-00017350: 7269 616c 5f63 6f75 6e74 3a20 5468 6520  rial_count: The 
-00017360: 6e75 6d62 6572 206f 6620 6661 696c 6564  number of failed
-00017370: 2074 7269 616c 7320 7468 6174 206e 6565   trials that nee
-00017380: 6420 746f 2062 6520 7365 656e 0a20 2020  d to be seen.   
-00017390: 2020 2062 6566 6f72 6520 6661 696c 696e     before failin
-000173a0: 6720 7468 6520 4879 7065 7270 6172 616d  g the Hyperparam
-000173b0: 6574 6572 5475 6e69 6e67 4a6f 622e 2049  eterTuningJob. I
-000173c0: 6620 7365 7420 746f 2030 2c20 5665 7274  f set to 0, Vert
-000173d0: 6578 2041 4920 6465 6369 6465 730a 2020  ex AI decides.  
-000173e0: 2020 2020 686f 7720 6d61 6e79 2074 7269      how many tri
-000173f0: 616c 7320 6d75 7374 2066 6169 6c20 6265  als must fail be
-00017400: 666f 7265 2074 6865 2077 686f 6c65 206a  fore the whole j
-00017410: 6f62 2066 6169 6c73 2e0a 2020 2020 7374  ob fails..    st
-00017420: 7564 795f 7370 6563 5f61 6c67 6f72 6974  udy_spec_algorit
-00017430: 686d 3a20 5468 6520 7365 6172 6368 2061  hm: The search a
-00017440: 6c67 6f72 6974 686d 2073 7065 6369 6669  lgorithm specifi
-00017450: 6564 2066 6f72 2074 6865 2073 7475 6479  ed for the study
-00017460: 2e20 4f6e 6520 6f66 0a20 2020 2020 2022  . One of.      "
-00017470: 414c 474f 5249 5448 4d5f 554e 5350 4543  ALGORITHM_UNSPEC
-00017480: 4946 4945 4422 2c20 2247 5249 445f 5345  IFIED", "GRID_SE
-00017490: 4152 4348 222c 206f 7220 2252 414e 444f  ARCH", or "RANDO
-000174a0: 4d5f 5345 4152 4348 222e 0a20 2020 2073  M_SEARCH"..    s
-000174b0: 7475 6479 5f73 7065 635f 6d65 6173 7572  tudy_spec_measur
-000174c0: 656d 656e 745f 7365 6c65 6374 696f 6e5f  ement_selection_
-000174d0: 7479 7065 3a20 5768 6963 6820 6d65 6173  type: Which meas
-000174e0: 7572 656d 656e 7420 746f 2075 7365 2069  urement to use i
-000174f0: 662f 7768 656e 2074 6865 0a20 2020 2020  f/when the.     
-00017500: 2073 6572 7669 6365 2061 7574 6f6d 6174   service automat
-00017510: 6963 616c 6c79 2073 656c 6563 7473 2074  ically selects t
-00017520: 6865 2066 696e 616c 206d 6561 7375 7265  he final measure
-00017530: 6d65 6e74 2066 726f 6d20 7072 6576 696f  ment from previo
-00017540: 7573 6c79 0a20 2020 2020 2072 6570 6f72  usly.      repor
-00017550: 7465 6420 696e 7465 726d 6564 6961 7465  ted intermediate
-00017560: 206d 6561 7375 7265 6d65 6e74 732e 204f   measurements. O
-00017570: 6e65 206f 6620 2242 4553 545f 4d45 4153  ne of "BEST_MEAS
-00017580: 5552 454d 454e 5422 206f 720a 2020 2020  UREMENT" or.    
-00017590: 2020 224c 4153 545f 4d45 4153 5552 454d    "LAST_MEASUREM
-000175a0: 454e 5422 2e0a 2020 2020 7472 616e 7366  ENT"..    transf
-000175b0: 6f72 6d5f 6461 7461 666c 6f77 5f6d 6163  orm_dataflow_mac
-000175c0: 6869 6e65 5f74 7970 653a 2054 6865 2064  hine_type: The d
-000175d0: 6174 6166 6c6f 7720 6d61 6368 696e 6520  ataflow machine 
-000175e0: 7479 7065 2066 6f72 2074 7261 6e73 666f  type for transfo
-000175f0: 726d 0a20 2020 2020 2063 6f6d 706f 6e65  rm.      compone
-00017600: 6e74 2e0a 2020 2020 7472 616e 7366 6f72  nt..    transfor
-00017610: 6d5f 6461 7461 666c 6f77 5f6d 6178 5f6e  m_dataflow_max_n
-00017620: 756d 5f77 6f72 6b65 7273 3a20 5468 6520  um_workers: The 
-00017630: 6d61 7820 6e75 6d62 6572 206f 6620 4461  max number of Da
-00017640: 7461 666c 6f77 2077 6f72 6b65 7273 2066  taflow workers f
-00017650: 6f72 0a20 2020 2020 2074 7261 6e73 666f  or.      transfo
-00017660: 726d 2063 6f6d 706f 6e65 6e74 2e0a 2020  rm component..  
-00017670: 2020 7472 616e 7366 6f72 6d5f 6461 7461    transform_data
-00017680: 666c 6f77 5f64 6973 6b5f 7369 7a65 5f67  flow_disk_size_g
-00017690: 623a 2044 6174 6166 6c6f 7720 776f 726b  b: Dataflow work
-000176a0: 6572 2773 2064 6973 6b20 7369 7a65 2069  er's disk size i
-000176b0: 6e20 4742 2066 6f72 0a20 2020 2020 2074  n GB for.      t
-000176c0: 7261 6e73 666f 726d 2063 6f6d 706f 6e65  ransform compone
-000176d0: 6e74 2e0a 2020 2020 776f 726b 6572 5f70  nt..    worker_p
-000176e0: 6f6f 6c5f 7370 6563 735f 6f76 6572 7269  ool_specs_overri
-000176f0: 6465 3a20 5468 6520 6469 6374 696f 6e61  de: The dictiona
-00017700: 7279 2066 6f72 206f 7665 7272 6964 696e  ry for overridin
-00017710: 6720 7472 6169 6e69 6e67 2061 6e64 0a20  g training and. 
-00017720: 2020 2020 2065 7661 6c75 6174 696f 6e20       evaluation 
-00017730: 776f 726b 6572 2070 6f6f 6c20 7370 6563  worker pool spec
-00017740: 732e 2054 6865 2064 6963 7469 6f6e 6172  s. The dictionar
-00017750: 7920 7368 6f75 6c64 2062 6520 6f66 2066  y should be of f
-00017760: 6f72 6d61 740a 2020 2020 2020 2020 2020  ormat.          
-00017770: 6874 7470 733a 2f2f 6769 7468 7562 2e63  https://github.c
-00017780: 6f6d 2f67 6f6f 676c 6561 7069 732f 676f  om/googleapis/go
-00017790: 6f67 6c65 6170 6973 2f62 6c6f 622f 3465  ogleapis/blob/4e
-000177a0: 3833 3663 3763 3235 3765 3365 3230 6231  836c7c257e3e20b1
-000177b0: 6465 3134 6434 3730 3939 3361 3262 3166  de14d470993a2b1f
-000177c0: 3437 3336 6138 2f67 6f6f 676c 652f 636c  4736a8/google/cl
-000177d0: 6f75 642f 6169 706c 6174 666f 726d 2f76  oud/aiplatform/v
-000177e0: 3162 6574 6131 2f63 7573 746f 6d5f 6a6f  1beta1/custom_jo
-000177f0: 622e 7072 6f74 6f23 4c31 3732 2e0a 2020  b.proto#L172..  
-00017800: 2020 7275 6e5f 6576 616c 7561 7469 6f6e    run_evaluation
-00017810: 3a20 5768 6574 6865 7220 746f 2072 756e  : Whether to run
-00017820: 2065 7661 6c75 6174 696f 6e20 7374 6570   evaluation step
-00017830: 7320 6475 7269 6e67 2074 7261 696e 696e  s during trainin
-00017840: 672e 0a20 2020 2065 7661 6c75 6174 696f  g..    evaluatio
-00017850: 6e5f 6261 7463 685f 7072 6564 6963 745f  n_batch_predict_
-00017860: 6d61 6368 696e 655f 7479 7065 3a20 5468  machine_type: Th
-00017870: 6520 7072 6564 6963 7469 6f6e 2073 6572  e prediction ser
-00017880: 7665 7220 6d61 6368 696e 6520 7479 7065  ver machine type
-00017890: 0a20 2020 2020 2066 6f72 2062 6174 6368  .      for batch
-000178a0: 2070 7265 6469 6374 2063 6f6d 706f 6e65   predict compone
-000178b0: 6e74 7320 6475 7269 6e67 2065 7661 6c75  nts during evalu
-000178c0: 6174 696f 6e2e 0a20 2020 2065 7661 6c75  ation..    evalu
-000178d0: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
-000178e0: 6963 745f 7374 6172 7469 6e67 5f72 6570  ict_starting_rep
-000178f0: 6c69 6361 5f63 6f75 6e74 3a20 5468 6520  lica_count: The 
-00017900: 696e 6974 6961 6c20 6e75 6d62 6572 206f  initial number o
-00017910: 660a 2020 2020 2020 7072 6564 6963 7469  f.      predicti
-00017920: 6f6e 2073 6572 7665 7220 666f 7220 6261  on server for ba
-00017930: 7463 6820 7072 6564 6963 7420 636f 6d70  tch predict comp
-00017940: 6f6e 656e 7473 2064 7572 696e 6720 6576  onents during ev
-00017950: 616c 7561 7469 6f6e 2e0a 2020 2020 6576  aluation..    ev
-00017960: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
-00017970: 7265 6469 6374 5f6d 6178 5f72 6570 6c69  redict_max_repli
-00017980: 6361 5f63 6f75 6e74 3a20 5468 6520 6d61  ca_count: The ma
-00017990: 7820 6e75 6d62 6572 206f 6620 7072 6564  x number of pred
-000179a0: 6963 7469 6f6e 0a20 2020 2020 2073 6572  iction.      ser
-000179b0: 7665 7220 666f 7220 6261 7463 6820 7072  ver for batch pr
-000179c0: 6564 6963 7420 636f 6d70 6f6e 656e 7473  edict components
-000179d0: 2064 7572 696e 6720 6576 616c 7561 7469   during evaluati
-000179e0: 6f6e 2e0a 2020 2020 6576 616c 7561 7469  on..    evaluati
-000179f0: 6f6e 5f64 6174 6166 6c6f 775f 6d61 6368  on_dataflow_mach
-00017a00: 696e 655f 7479 7065 3a20 5468 6520 6461  ine_type: The da
-00017a10: 7461 666c 6f77 206d 6163 6869 6e65 2074  taflow machine t
-00017a20: 7970 6520 666f 7220 6576 616c 7561 7469  ype for evaluati
-00017a30: 6f6e 0a20 2020 2020 2063 6f6d 706f 6e65  on.      compone
-00017a40: 6e74 732e 0a20 2020 2065 7661 6c75 6174  nts..    evaluat
-00017a50: 696f 6e5f 6461 7461 666c 6f77 5f73 7461  ion_dataflow_sta
-00017a60: 7274 696e 675f 6e75 6d5f 776f 726b 6572  rting_num_worker
-00017a70: 733a 2054 6865 2069 6e69 7469 616c 206e  s: The initial n
-00017a80: 756d 6265 7220 6f66 2044 6174 6166 6c6f  umber of Dataflo
-00017a90: 770a 2020 2020 2020 776f 726b 6572 7320  w.      workers 
-00017aa0: 666f 7220 6576 616c 7561 7469 6f6e 2063  for evaluation c
-00017ab0: 6f6d 706f 6e65 6e74 732e 0a20 2020 2065  omponents..    e
-00017ac0: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
-00017ad0: 6f77 5f6d 6178 5f6e 756d 5f77 6f72 6b65  ow_max_num_worke
-00017ae0: 7273 3a20 5468 6520 6d61 7820 6e75 6d62  rs: The max numb
-00017af0: 6572 206f 6620 4461 7461 666c 6f77 2077  er of Dataflow w
-00017b00: 6f72 6b65 7273 2066 6f72 0a20 2020 2020  orkers for.     
-00017b10: 2065 7661 6c75 6174 696f 6e20 636f 6d70   evaluation comp
-00017b20: 6f6e 656e 7473 2e0a 2020 2020 6576 616c  onents..    eval
-00017b30: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
-00017b40: 6469 736b 5f73 697a 655f 6762 3a20 4461  disk_size_gb: Da
-00017b50: 7461 666c 6f77 2077 6f72 6b65 7227 7320  taflow worker's 
-00017b60: 6469 736b 2073 697a 6520 696e 2047 4220  disk size in GB 
-00017b70: 666f 720a 2020 2020 2020 6576 616c 7561  for.      evalua
-00017b80: 7469 6f6e 2063 6f6d 706f 6e65 6e74 732e  tion components.
-00017b90: 0a20 2020 2064 6174 6166 6c6f 775f 7365  .    dataflow_se
-00017ba0: 7276 6963 655f 6163 636f 756e 743a 2043  rvice_account: C
-00017bb0: 7573 746f 6d20 7365 7276 6963 6520 6163  ustom service ac
-00017bc0: 636f 756e 7420 746f 2072 756e 2064 6174  count to run dat
-00017bd0: 6166 6c6f 7720 6a6f 6273 2e0a 2020 2020  aflow jobs..    
-00017be0: 6461 7461 666c 6f77 5f73 7562 6e65 7477  dataflow_subnetw
-00017bf0: 6f72 6b3a 2044 6174 6166 6c6f 7727 7320  ork: Dataflow's 
-00017c00: 6675 6c6c 7920 7175 616c 6966 6965 6420  fully qualified 
-00017c10: 7375 626e 6574 776f 726b 206e 616d 652c  subnetwork name,
-00017c20: 2077 6865 6e20 656d 7074 790a 2020 2020   when empty.    
-00017c30: 2020 7468 6520 6465 6661 756c 7420 7375    the default su
-00017c40: 626e 6574 776f 726b 2077 696c 6c20 6265  bnetwork will be
-00017c50: 2075 7365 642e 2045 7861 6d70 6c65 3a0a   used. Example:.
-00017c60: 2020 2020 2020 2020 6874 7470 733a 2f2f          https://
-00017c70: 636c 6f75 642e 676f 6f67 6c65 2e63 6f6d  cloud.google.com
-00017c80: 2f64 6174 6166 6c6f 772f 646f 6373 2f67  /dataflow/docs/g
-00017c90: 7569 6465 732f 7370 6563 6966 7969 6e67  uides/specifying
-00017ca0: 2d6e 6574 776f 726b 7323 6578 616d 706c  -networks#exampl
-00017cb0: 655f 6e65 7477 6f72 6b5f 616e 645f 7375  e_network_and_su
-00017cc0: 626e 6574 776f 726b 5f73 7065 6369 6669  bnetwork_specifi
-00017cd0: 6361 7469 6f6e 730a 2020 2020 6461 7461  cations.    data
-00017ce0: 666c 6f77 5f75 7365 5f70 7562 6c69 635f  flow_use_public_
-00017cf0: 6970 733a 2053 7065 6369 6669 6573 2077  ips: Specifies w
-00017d00: 6865 7468 6572 2044 6174 6166 6c6f 7720  hether Dataflow 
-00017d10: 776f 726b 6572 7320 7573 6520 7075 626c  workers use publ
-00017d20: 6963 2049 500a 2020 2020 2020 6164 6472  ic IP.      addr
-00017d30: 6573 7365 732e 0a20 2020 2065 6e63 7279  esses..    encry
-00017d40: 7074 696f 6e5f 7370 6563 5f6b 6579 5f6e  ption_spec_key_n
-00017d50: 616d 653a 2054 6865 204b 4d53 206b 6579  ame: The KMS key
-00017d60: 206e 616d 652e 0a0a 2020 5265 7475 726e   name...  Return
-00017d70: 733a 0a20 2020 2054 7570 6c65 206f 6620  s:.    Tuple of 
-00017d80: 7069 7065 6c69 6e65 5f64 6566 696e 6974  pipeline_definit
-00017d90: 696f 6e5f 7061 7468 2061 6e64 2070 6172  ion_path and par
-00017da0: 616d 6574 6572 5f76 616c 7565 732e 0a20  ameter_values.. 
-00017db0: 2022 2222 0a20 2069 6620 6973 696e 7374   """.  if isinst
-00017dc0: 616e 6365 2874 665f 6175 746f 5f74 7261  ance(tf_auto_tra
-00017dd0: 6e73 666f 726d 5f66 6561 7475 7265 732c  nsform_features,
-00017de0: 206c 6973 7429 3a0a 2020 2020 7466 5f61   list):.    tf_a
-00017df0: 7574 6f5f 7472 616e 7366 6f72 6d5f 6665  uto_transform_fe
-00017e00: 6174 7572 6573 203d 207b 2761 7574 6f27  atures = {'auto'
-00017e10: 3a20 7466 5f61 7574 6f5f 7472 616e 7366  : tf_auto_transf
-00017e20: 6f72 6d5f 6665 6174 7572 6573 7d0a 0a20  orm_features}.. 
-00017e30: 2069 6620 7472 616e 7366 6f72 6d5f 636f   if transform_co
-00017e40: 6e66 6967 2061 6e64 2074 665f 7472 616e  nfig and tf_tran
-00017e50: 7366 6f72 6d61 7469 6f6e 735f 7061 7468  sformations_path
-00017e60: 3a0a 2020 2020 7261 6973 6520 5661 6c75  :.    raise Valu
-00017e70: 6545 7272 6f72 280a 2020 2020 2020 2020  eError(.        
-00017e80: 274f 6e6c 7920 6f6e 6520 6f66 2074 7261  'Only one of tra
-00017e90: 6e73 666f 726d 5f63 6f6e 6669 6720 616e  nsform_config an
-00017ea0: 6420 7466 5f74 7261 6e73 666f 726d 6174  d tf_transformat
-00017eb0: 696f 6e73 5f70 6174 6820 6361 6e20 270a  ions_path can '.
-00017ec0: 2020 2020 2020 2020 2762 6520 7370 6563          'be spec
-00017ed0: 6966 6965 642e 270a 2020 2020 290a 0a20  ified.'.    ).. 
-00017ee0: 2065 6c69 6620 7472 616e 7366 6f72 6d5f   elif transform_
-00017ef0: 636f 6e66 6967 3a0a 2020 2020 7761 726e  config:.    warn
-00017f00: 696e 6773 2e77 6172 6e28 0a20 2020 2020  ings.warn(.     
-00017f10: 2020 2027 7472 616e 7366 6f72 6d5f 636f     'transform_co
-00017f20: 6e66 6967 2070 6172 616d 6574 6572 2069  nfig parameter i
-00017f30: 7320 6465 7072 6563 6174 6564 2e20 270a  s deprecated. '.
-00017f40: 2020 2020 2020 2020 2750 6c65 6173 6520          'Please 
-00017f50: 7573 6520 7468 6520 666c 6174 7465 6e65  use the flattene
-00017f60: 6420 7472 616e 7366 6f72 6d20 636f 6e66  d transform conf
-00017f70: 6967 2061 7267 756d 656e 7473 2069 6e73  ig arguments ins
-00017f80: 7465 6164 2e27 0a20 2020 2029 0a20 2020  tead.'.    ).   
-00017f90: 2074 665f 7472 616e 7366 6f72 6d61 7469   tf_transformati
-00017fa0: 6f6e 735f 7061 7468 203d 2074 7261 6e73  ons_path = trans
-00017fb0: 666f 726d 5f63 6f6e 6669 670a 0a20 2069  form_config..  i
-00017fc0: 6620 6e6f 7420 776f 726b 6572 5f70 6f6f  f not worker_poo
-00017fd0: 6c5f 7370 6563 735f 6f76 6572 7269 6465  l_specs_override
-00017fe0: 3a0a 2020 2020 776f 726b 6572 5f70 6f6f  :.    worker_poo
-00017ff0: 6c5f 7370 6563 735f 6f76 6572 7269 6465  l_specs_override
-00018000: 203d 205b 5d0a 0a20 2070 6172 616d 6574   = []..  paramet
-00018010: 6572 5f76 616c 7565 7320 3d20 7b0a 2020  er_values = {.  
-00018020: 2020 2020 2770 726f 6a65 6374 273a 2070      'project': p
-00018030: 726f 6a65 6374 2c0a 2020 2020 2020 276c  roject,.      'l
-00018040: 6f63 6174 696f 6e27 3a20 6c6f 6361 7469  ocation': locati
-00018050: 6f6e 2c0a 2020 2020 2020 2772 6f6f 745f  on,.      'root_
-00018060: 6469 7227 3a20 726f 6f74 5f64 6972 2c0a  dir': root_dir,.
-00018070: 2020 2020 2020 2774 6172 6765 745f 636f        'target_co
-00018080: 6c75 6d6e 273a 2074 6172 6765 745f 636f  lumn': target_co
-00018090: 6c75 6d6e 2c0a 2020 2020 2020 2770 7265  lumn,.      'pre
-000180a0: 6469 6374 696f 6e5f 7479 7065 273a 2070  diction_type': p
-000180b0: 7265 6469 6374 696f 6e5f 7479 7065 2c0a  rediction_type,.
-000180c0: 2020 2020 2020 2773 7475 6479 5f73 7065        'study_spe
-000180d0: 635f 6d65 7472 6963 5f69 6427 3a20 7374  c_metric_id': st
-000180e0: 7564 795f 7370 6563 5f6d 6574 7269 635f  udy_spec_metric_
-000180f0: 6964 2c0a 2020 2020 2020 2773 7475 6479  id,.      'study
-00018100: 5f73 7065 635f 6d65 7472 6963 5f67 6f61  _spec_metric_goa
-00018110: 6c27 3a20 7374 7564 795f 7370 6563 5f6d  l': study_spec_m
-00018120: 6574 7269 635f 676f 616c 2c0a 2020 2020  etric_goal,.    
-00018130: 2020 2773 7475 6479 5f73 7065 635f 7061    'study_spec_pa
-00018140: 7261 6d65 7465 7273 5f6f 7665 7272 6964  rameters_overrid
-00018150: 6527 3a20 7374 7564 795f 7370 6563 5f70  e': study_spec_p
-00018160: 6172 616d 6574 6572 735f 6f76 6572 7269  arameters_overri
-00018170: 6465 2c0a 2020 2020 2020 276d 6178 5f74  de,.      'max_t
-00018180: 7269 616c 5f63 6f75 6e74 273a 206d 6178  rial_count': max
-00018190: 5f74 7269 616c 5f63 6f75 6e74 2c0a 2020  _trial_count,.  
-000181a0: 2020 2020 2770 6172 616c 6c65 6c5f 7472      'parallel_tr
-000181b0: 6961 6c5f 636f 756e 7427 3a20 7061 7261  ial_count': para
-000181c0: 6c6c 656c 5f74 7269 616c 5f63 6f75 6e74  llel_trial_count
-000181d0: 2c0a 2020 2020 2020 2765 6e61 626c 655f  ,.      'enable_
-000181e0: 7072 6f66 696c 6572 273a 2065 6e61 626c  profiler': enabl
-000181f0: 655f 7072 6f66 696c 6572 2c0a 2020 2020  e_profiler,.    
-00018200: 2020 2763 6163 6865 5f64 6174 6127 3a20    'cache_data': 
-00018210: 6361 6368 655f 6461 7461 2c0a 2020 2020  cache_data,.    
-00018220: 2020 2773 6565 6427 3a20 7365 6564 2c0a    'seed': seed,.
-00018230: 2020 2020 2020 2765 7661 6c5f 7374 6570        'eval_step
-00018240: 7327 3a20 6576 616c 5f73 7465 7073 2c0a  s': eval_steps,.
-00018250: 2020 2020 2020 2765 7661 6c5f 6672 6571        'eval_freq
-00018260: 7565 6e63 795f 7365 6373 273a 2065 7661  uency_secs': eva
-00018270: 6c5f 6672 6571 7565 6e63 795f 7365 6373  l_frequency_secs
-00018280: 2c0a 2020 2020 2020 2777 6569 6768 745f  ,.      'weight_
-00018290: 636f 6c75 6d6e 273a 2077 6569 6768 745f  column': weight_
-000182a0: 636f 6c75 6d6e 2c0a 2020 2020 2020 276d  column,.      'm
-000182b0: 6178 5f66 6169 6c65 645f 7472 6961 6c5f  ax_failed_trial_
-000182c0: 636f 756e 7427 3a20 6d61 785f 6661 696c  count': max_fail
-000182d0: 6564 5f74 7269 616c 5f63 6f75 6e74 2c0a  ed_trial_count,.
-000182e0: 2020 2020 2020 2773 7475 6479 5f73 7065        'study_spe
-000182f0: 635f 616c 676f 7269 7468 6d27 3a20 7374  c_algorithm': st
-00018300: 7564 795f 7370 6563 5f61 6c67 6f72 6974  udy_spec_algorit
-00018310: 686d 2c0a 2020 2020 2020 2773 7475 6479  hm,.      'study
-00018320: 5f73 7065 635f 6d65 6173 7572 656d 656e  _spec_measuremen
-00018330: 745f 7365 6c65 6374 696f 6e5f 7479 7065  t_selection_type
-00018340: 273a 2028 0a20 2020 2020 2020 2020 2073  ': (.          s
-00018350: 7475 6479 5f73 7065 635f 6d65 6173 7572  tudy_spec_measur
-00018360: 656d 656e 745f 7365 6c65 6374 696f 6e5f  ement_selection_
-00018370: 7479 7065 0a20 2020 2020 2029 2c0a 2020  type.      ),.  
-00018380: 2020 2020 2774 7261 6e73 666f 726d 5f64      'transform_d
-00018390: 6174 6166 6c6f 775f 6d61 6368 696e 655f  ataflow_machine_
-000183a0: 7479 7065 273a 2074 7261 6e73 666f 726d  type': transform
-000183b0: 5f64 6174 6166 6c6f 775f 6d61 6368 696e  _dataflow_machin
-000183c0: 655f 7479 7065 2c0a 2020 2020 2020 2774  e_type,.      't
-000183d0: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
-000183e0: 775f 6d61 785f 6e75 6d5f 776f 726b 6572  w_max_num_worker
-000183f0: 7327 3a20 7472 616e 7366 6f72 6d5f 6461  s': transform_da
-00018400: 7461 666c 6f77 5f6d 6178 5f6e 756d 5f77  taflow_max_num_w
-00018410: 6f72 6b65 7273 2c0a 2020 2020 2020 2774  orkers,.      't
-00018420: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
-00018430: 775f 6469 736b 5f73 697a 655f 6762 273a  w_disk_size_gb':
-00018440: 2074 7261 6e73 666f 726d 5f64 6174 6166   transform_dataf
-00018450: 6c6f 775f 6469 736b 5f73 697a 655f 6762  low_disk_size_gb
-00018460: 2c0a 2020 2020 2020 2777 6f72 6b65 725f  ,.      'worker_
-00018470: 706f 6f6c 5f73 7065 6373 5f6f 7665 7272  pool_specs_overr
-00018480: 6964 6527 3a20 776f 726b 6572 5f70 6f6f  ide': worker_poo
-00018490: 6c5f 7370 6563 735f 6f76 6572 7269 6465  l_specs_override
-000184a0: 2c0a 2020 2020 2020 2772 756e 5f65 7661  ,.      'run_eva
-000184b0: 6c75 6174 696f 6e27 3a20 7275 6e5f 6576  luation': run_ev
-000184c0: 616c 7561 7469 6f6e 2c0a 2020 2020 2020  aluation,.      
-000184d0: 2765 7661 6c75 6174 696f 6e5f 6261 7463  'evaluation_batc
-000184e0: 685f 7072 6564 6963 745f 6d61 6368 696e  h_predict_machin
-000184f0: 655f 7479 7065 273a 2028 0a20 2020 2020  e_type': (.     
-00018500: 2020 2020 2065 7661 6c75 6174 696f 6e5f       evaluation_
-00018510: 6261 7463 685f 7072 6564 6963 745f 6d61  batch_predict_ma
-00018520: 6368 696e 655f 7479 7065 0a20 2020 2020  chine_type.     
-00018530: 2029 2c0a 2020 2020 2020 2765 7661 6c75   ),.      'evalu
-00018540: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
-00018550: 6963 745f 7374 6172 7469 6e67 5f72 6570  ict_starting_rep
-00018560: 6c69 6361 5f63 6f75 6e74 273a 2028 0a20  lica_count': (. 
-00018570: 2020 2020 2020 2020 2065 7661 6c75 6174           evaluat
-00018580: 696f 6e5f 6261 7463 685f 7072 6564 6963  ion_batch_predic
-00018590: 745f 7374 6172 7469 6e67 5f72 6570 6c69  t_starting_repli
-000185a0: 6361 5f63 6f75 6e74 0a20 2020 2020 2029  ca_count.      )
-000185b0: 2c0a 2020 2020 2020 2765 7661 6c75 6174  ,.      'evaluat
-000185c0: 696f 6e5f 6261 7463 685f 7072 6564 6963  ion_batch_predic
-000185d0: 745f 6d61 785f 7265 706c 6963 615f 636f  t_max_replica_co
-000185e0: 756e 7427 3a20 280a 2020 2020 2020 2020  unt': (.        
-000185f0: 2020 6576 616c 7561 7469 6f6e 5f62 6174    evaluation_bat
-00018600: 6368 5f70 7265 6469 6374 5f6d 6178 5f72  ch_predict_max_r
-00018610: 6570 6c69 6361 5f63 6f75 6e74 0a20 2020  eplica_count.   
-00018620: 2020 2029 2c0a 2020 2020 2020 2765 7661     ),.      'eva
-00018630: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
-00018640: 5f6d 6163 6869 6e65 5f74 7970 6527 3a20  _machine_type': 
-00018650: 6576 616c 7561 7469 6f6e 5f64 6174 6166  evaluation_dataf
-00018660: 6c6f 775f 6d61 6368 696e 655f 7479 7065  low_machine_type
-00018670: 2c0a 2020 2020 2020 2765 7661 6c75 6174  ,.      'evaluat
-00018680: 696f 6e5f 6461 7461 666c 6f77 5f73 7461  ion_dataflow_sta
-00018690: 7274 696e 675f 6e75 6d5f 776f 726b 6572  rting_num_worker
-000186a0: 7327 3a20 280a 2020 2020 2020 2020 2020  s': (.          
-000186b0: 6576 616c 7561 7469 6f6e 5f64 6174 6166  evaluation_dataf
-000186c0: 6c6f 775f 7374 6172 7469 6e67 5f6e 756d  low_starting_num
-000186d0: 5f77 6f72 6b65 7273 0a20 2020 2020 2029  _workers.      )
-000186e0: 2c0a 2020 2020 2020 2765 7661 6c75 6174  ,.      'evaluat
-000186f0: 696f 6e5f 6461 7461 666c 6f77 5f6d 6178  ion_dataflow_max
-00018700: 5f6e 756d 5f77 6f72 6b65 7273 273a 2028  _num_workers': (
-00018710: 0a20 2020 2020 2020 2020 2065 7661 6c75  .          evalu
-00018720: 6174 696f 6e5f 6461 7461 666c 6f77 5f6d  ation_dataflow_m
-00018730: 6178 5f6e 756d 5f77 6f72 6b65 7273 0a20  ax_num_workers. 
-00018740: 2020 2020 2029 2c0a 2020 2020 2020 2765       ),.      'e
-00018750: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
-00018760: 6f77 5f64 6973 6b5f 7369 7a65 5f67 6227  ow_disk_size_gb'
-00018770: 3a20 6576 616c 7561 7469 6f6e 5f64 6174  : evaluation_dat
-00018780: 6166 6c6f 775f 6469 736b 5f73 697a 655f  aflow_disk_size_
-00018790: 6762 2c0a 2020 2020 2020 2764 6174 6166  gb,.      'dataf
-000187a0: 6c6f 775f 7365 7276 6963 655f 6163 636f  low_service_acco
-000187b0: 756e 7427 3a20 6461 7461 666c 6f77 5f73  unt': dataflow_s
-000187c0: 6572 7669 6365 5f61 6363 6f75 6e74 2c0a  ervice_account,.
-000187d0: 2020 2020 2020 2764 6174 6166 6c6f 775f        'dataflow_
-000187e0: 7375 626e 6574 776f 726b 273a 2064 6174  subnetwork': dat
-000187f0: 6166 6c6f 775f 7375 626e 6574 776f 726b  aflow_subnetwork
-00018800: 2c0a 2020 2020 2020 2764 6174 6166 6c6f  ,.      'dataflo
-00018810: 775f 7573 655f 7075 626c 6963 5f69 7073  w_use_public_ips
-00018820: 273a 2064 6174 6166 6c6f 775f 7573 655f  ': dataflow_use_
-00018830: 7075 626c 6963 5f69 7073 2c0a 2020 2020  public_ips,.    
-00018840: 2020 2765 6e63 7279 7074 696f 6e5f 7370    'encryption_sp
-00018850: 6563 5f6b 6579 5f6e 616d 6527 3a20 656e  ec_key_name': en
-00018860: 6372 7970 7469 6f6e 5f73 7065 635f 6b65  cryption_spec_ke
-00018870: 795f 6e61 6d65 2c0a 2020 7d0a 0a20 2066  y_name,.  }..  f
-00018880: 7465 5f70 6172 616d 7320 3d20 7b0a 2020  te_params = {.  
-00018890: 2020 2020 2764 6174 6173 6574 5f6c 6576      'dataset_lev
-000188a0: 656c 5f63 7573 746f 6d5f 7472 616e 7366  el_custom_transf
-000188b0: 6f72 6d61 7469 6f6e 5f64 6566 696e 6974  ormation_definit
-000188c0: 696f 6e73 273a 2028 0a20 2020 2020 2020  ions': (.       
-000188d0: 2020 2064 6174 6173 6574 5f6c 6576 656c     dataset_level
-000188e0: 5f63 7573 746f 6d5f 7472 616e 7366 6f72  _custom_transfor
-000188f0: 6d61 7469 6f6e 5f64 6566 696e 6974 696f  mation_definitio
-00018900: 6e73 0a20 2020 2020 2020 2020 2069 6620  ns.          if 
-00018910: 6461 7461 7365 745f 6c65 7665 6c5f 6375  dataset_level_cu
-00018920: 7374 6f6d 5f74 7261 6e73 666f 726d 6174  stom_transformat
-00018930: 696f 6e5f 6465 6669 6e69 7469 6f6e 730a  ion_definitions.
-00018940: 2020 2020 2020 2020 2020 656c 7365 205b            else [
-00018950: 5d0a 2020 2020 2020 292c 0a20 2020 2020  ].      ),.     
-00018960: 2027 6461 7461 7365 745f 6c65 7665 6c5f   'dataset_level_
-00018970: 7472 616e 7366 6f72 6d61 7469 6f6e 7327  transformations'
-00018980: 3a20 280a 2020 2020 2020 2020 2020 6461  : (.          da
-00018990: 7461 7365 745f 6c65 7665 6c5f 7472 616e  taset_level_tran
-000189a0: 7366 6f72 6d61 7469 6f6e 7320 6966 2064  sformations if d
-000189b0: 6174 6173 6574 5f6c 6576 656c 5f74 7261  ataset_level_tra
-000189c0: 6e73 666f 726d 6174 696f 6e73 2065 6c73  nsformations els
-000189d0: 6520 5b5d 0a20 2020 2020 2029 2c0a 2020  e [].      ),.  
-000189e0: 2020 2020 2772 756e 5f66 6561 7475 7265      'run_feature
-000189f0: 5f73 656c 6563 7469 6f6e 273a 2072 756e  _selection': run
-00018a00: 5f66 6561 7475 7265 5f73 656c 6563 7469  _feature_selecti
-00018a10: 6f6e 2c0a 2020 2020 2020 2766 6561 7475  on,.      'featu
-00018a20: 7265 5f73 656c 6563 7469 6f6e 5f61 6c67  re_selection_alg
-00018a30: 6f72 6974 686d 273a 2066 6561 7475 7265  orithm': feature
-00018a40: 5f73 656c 6563 7469 6f6e 5f61 6c67 6f72  _selection_algor
-00018a50: 6974 686d 2c0a 2020 2020 2020 276d 6178  ithm,.      'max
-00018a60: 5f73 656c 6563 7465 645f 6665 6174 7572  _selected_featur
-00018a70: 6573 273a 206d 6178 5f73 656c 6563 7465  es': max_selecte
-00018a80: 645f 6665 6174 7572 6573 2c0a 2020 2020  d_features,.    
-00018a90: 2020 2770 7265 6465 6669 6e65 645f 7370    'predefined_sp
-00018aa0: 6c69 745f 6b65 7927 3a20 7072 6564 6566  lit_key': predef
-00018ab0: 696e 6564 5f73 706c 6974 5f6b 6579 2c0a  ined_split_key,.
-00018ac0: 2020 2020 2020 2773 7472 6174 6966 6965        'stratifie
-00018ad0: 645f 7370 6c69 745f 6b65 7927 3a20 7374  d_split_key': st
-00018ae0: 7261 7469 6669 6564 5f73 706c 6974 5f6b  ratified_split_k
-00018af0: 6579 2c0a 2020 2020 2020 2774 7261 696e  ey,.      'train
-00018b00: 696e 675f 6672 6163 7469 6f6e 273a 2074  ing_fraction': t
-00018b10: 7261 696e 696e 675f 6672 6163 7469 6f6e  raining_fraction
-00018b20: 2c0a 2020 2020 2020 2776 616c 6964 6174  ,.      'validat
-00018b30: 696f 6e5f 6672 6163 7469 6f6e 273a 2076  ion_fraction': v
-00018b40: 616c 6964 6174 696f 6e5f 6672 6163 7469  alidation_fracti
-00018b50: 6f6e 2c0a 2020 2020 2020 2774 6573 745f  on,.      'test_
-00018b60: 6672 6163 7469 6f6e 273a 2074 6573 745f  fraction': test_
-00018b70: 6672 6163 7469 6f6e 2c0a 2020 2020 2020  fraction,.      
-00018b80: 2774 665f 6175 746f 5f74 7261 6e73 666f  'tf_auto_transfo
-00018b90: 726d 5f66 6561 7475 7265 7327 3a20 280a  rm_features': (.
-00018ba0: 2020 2020 2020 2020 2020 7466 5f61 7574            tf_aut
-00018bb0: 6f5f 7472 616e 7366 6f72 6d5f 6665 6174  o_transform_feat
-00018bc0: 7572 6573 2069 6620 7466 5f61 7574 6f5f  ures if tf_auto_
-00018bd0: 7472 616e 7366 6f72 6d5f 6665 6174 7572  transform_featur
-00018be0: 6573 2065 6c73 6520 7b7d 0a20 2020 2020  es else {}.     
-00018bf0: 2029 2c0a 2020 2020 2020 2774 665f 6375   ),.      'tf_cu
-00018c00: 7374 6f6d 5f74 7261 6e73 666f 726d 6174  stom_transformat
-00018c10: 696f 6e5f 6465 6669 6e69 7469 6f6e 7327  ion_definitions'
-00018c20: 3a20 280a 2020 2020 2020 2020 2020 7466  : (.          tf
-00018c30: 5f63 7573 746f 6d5f 7472 616e 7366 6f72  _custom_transfor
-00018c40: 6d61 7469 6f6e 5f64 6566 696e 6974 696f  mation_definitio
-00018c50: 6e73 0a20 2020 2020 2020 2020 2069 6620  ns.          if 
-00018c60: 7466 5f63 7573 746f 6d5f 7472 616e 7366  tf_custom_transf
-00018c70: 6f72 6d61 7469 6f6e 5f64 6566 696e 6974  ormation_definit
-00018c80: 696f 6e73 0a20 2020 2020 2020 2020 2065  ions.          e
-00018c90: 6c73 6520 5b5d 0a20 2020 2020 2029 2c0a  lse [].      ),.
-00018ca0: 2020 2020 2020 2774 665f 7472 616e 7366        'tf_transf
-00018cb0: 6f72 6d61 7469 6f6e 735f 7061 7468 273a  ormations_path':
-00018cc0: 2074 665f 7472 616e 7366 6f72 6d61 7469   tf_transformati
-00018cd0: 6f6e 735f 7061 7468 2c0a 2020 2020 2020  ons_path,.      
-00018ce0: 276d 6174 6572 6961 6c69 7a65 645f 6578  'materialized_ex
-00018cf0: 616d 706c 6573 5f66 6f72 6d61 7427 3a20  amples_format': 
-00018d00: 280a 2020 2020 2020 2020 2020 6d61 7465  (.          mate
-00018d10: 7269 616c 697a 6564 5f65 7861 6d70 6c65  rialized_example
-00018d20: 735f 666f 726d 6174 0a20 2020 2020 2020  s_format.       
-00018d30: 2020 2069 6620 6d61 7465 7269 616c 697a     if materializ
-00018d40: 6564 5f65 7861 6d70 6c65 735f 666f 726d  ed_examples_form
-00018d50: 6174 0a20 2020 2020 2020 2020 2065 6c73  at.          els
-00018d60: 6520 2774 6672 6563 6f72 6473 5f67 7a69  e 'tfrecords_gzi
-00018d70: 7027 0a20 2020 2020 2029 2c0a 2020 2020  p'.      ),.    
-00018d80: 2020 2774 665f 7472 616e 7366 6f72 6d5f    'tf_transform_
-00018d90: 6578 6563 7574 696f 6e5f 656e 6769 6e65  execution_engine
-00018da0: 273a 2028 0a20 2020 2020 2020 2020 2074  ': (.          t
-00018db0: 665f 7472 616e 7366 6f72 6d5f 6578 6563  f_transform_exec
-00018dc0: 7574 696f 6e5f 656e 6769 6e65 0a20 2020  ution_engine.   
-00018dd0: 2020 2020 2020 2069 6620 7466 5f74 7261         if tf_tra
-00018de0: 6e73 666f 726d 5f65 7865 6375 7469 6f6e  nsform_execution
-00018df0: 5f65 6e67 696e 650a 2020 2020 2020 2020  _engine.        
-00018e00: 2020 656c 7365 2027 6461 7461 666c 6f77    else 'dataflow
-00018e10: 270a 2020 2020 2020 292c 0a20 207d 0a20  '.      ),.  }. 
-00018e20: 205f 7570 6461 7465 5f70 6172 616d 6574   _update_paramet
-00018e30: 6572 7328 7061 7261 6d65 7465 725f 7661  ers(parameter_va
-00018e40: 6c75 6573 2c20 6674 655f 7061 7261 6d73  lues, fte_params
-00018e50: 290a 0a20 2064 6174 615f 736f 7572 6365  )..  data_source
-00018e60: 5f61 6e64 5f73 706c 6974 5f70 6172 616d  _and_split_param
-00018e70: 6574 6572 7320 3d20 7b0a 2020 2020 2020  eters = {.      
-00018e80: 2764 6174 615f 736f 7572 6365 5f63 7376  'data_source_csv
-00018e90: 5f66 696c 656e 616d 6573 273a 2064 6174  _filenames': dat
-00018ea0: 615f 736f 7572 6365 5f63 7376 5f66 696c  a_source_csv_fil
-00018eb0: 656e 616d 6573 2c0a 2020 2020 2020 2764  enames,.      'd
-00018ec0: 6174 615f 736f 7572 6365 5f62 6967 7175  ata_source_bigqu
-00018ed0: 6572 795f 7461 626c 655f 7061 7468 273a  ery_table_path':
-00018ee0: 2064 6174 615f 736f 7572 6365 5f62 6967   data_source_big
-00018ef0: 7175 6572 795f 7461 626c 655f 7061 7468  query_table_path
-00018f00: 2c0a 2020 2020 2020 2762 6967 7175 6572  ,.      'bigquer
-00018f10: 795f 7374 6167 696e 675f 6675 6c6c 5f64  y_staging_full_d
-00018f20: 6174 6173 6574 5f69 6427 3a20 6269 6771  ataset_id': bigq
-00018f30: 7565 7279 5f73 7461 6769 6e67 5f66 756c  uery_staging_ful
-00018f40: 6c5f 6461 7461 7365 745f 6964 2c0a 2020  l_dataset_id,.  
-00018f50: 7d0a 2020 5f75 7064 6174 655f 7061 7261  }.  _update_para
-00018f60: 6d65 7465 7273 2870 6172 616d 6574 6572  meters(parameter
-00018f70: 5f76 616c 7565 732c 2064 6174 615f 736f  _values, data_so
-00018f80: 7572 6365 5f61 6e64 5f73 706c 6974 5f70  urce_and_split_p
-00018f90: 6172 616d 6574 6572 7329 0a0a 2020 7069  arameters)..  pi
-00018fa0: 7065 6c69 6e65 5f64 6566 696e 6974 696f  peline_definitio
-00018fb0: 6e5f 7061 7468 203d 206f 732e 7061 7468  n_path = os.path
-00018fc0: 2e6a 6f69 6e28 0a20 2020 2020 2070 6174  .join(.      pat
-00018fd0: 686c 6962 2e50 6174 6828 5f5f 6669 6c65  hlib.Path(__file
-00018fe0: 5f5f 292e 7061 7265 6e74 2e72 6573 6f6c  __).parent.resol
-00018ff0: 7665 2829 2c0a 2020 2020 2020 2774 6162  ve(),.      'tab
-00019000: 6e65 745f 6879 7065 7270 6172 616d 6574  net_hyperparamet
-00019010: 6572 5f74 756e 696e 675f 6a6f 625f 7069  er_tuning_job_pi
-00019020: 7065 6c69 6e65 2e79 616d 6c27 2c0a 2020  peline.yaml',.  
-00019030: 290a 0a20 2072 6574 7572 6e20 7069 7065  )..  return pipe
-00019040: 6c69 6e65 5f64 6566 696e 6974 696f 6e5f  line_definition_
-00019050: 7061 7468 2c20 7061 7261 6d65 7465 725f  path, parameter_
-00019060: 7661 6c75 6573 0a0a 0a64 6566 2067 6574  values...def get
-00019070: 5f77 6964 655f 616e 645f 6465 6570 5f68  _wide_and_deep_h
-00019080: 7970 6572 7061 7261 6d65 7465 725f 7475  yperparameter_tu
-00019090: 6e69 6e67 5f6a 6f62 5f70 6970 656c 696e  ning_job_pipelin
-000190a0: 655f 616e 645f 7061 7261 6d65 7465 7273  e_and_parameters
-000190b0: 280a 2020 2020 7072 6f6a 6563 743a 2073  (.    project: s
-000190c0: 7472 2c0a 2020 2020 6c6f 6361 7469 6f6e  tr,.    location
-000190d0: 3a20 7374 722c 0a20 2020 2072 6f6f 745f  : str,.    root_
-000190e0: 6469 723a 2073 7472 2c0a 2020 2020 7461  dir: str,.    ta
-000190f0: 7267 6574 5f63 6f6c 756d 6e3a 2073 7472  rget_column: str
-00019100: 2c0a 2020 2020 7072 6564 6963 7469 6f6e  ,.    prediction
-00019110: 5f74 7970 653a 2073 7472 2c0a 2020 2020  _type: str,.    
-00019120: 7374 7564 795f 7370 6563 5f6d 6574 7269  study_spec_metri
-00019130: 635f 6964 3a20 7374 722c 0a20 2020 2073  c_id: str,.    s
-00019140: 7475 6479 5f73 7065 635f 6d65 7472 6963  tudy_spec_metric
-00019150: 5f67 6f61 6c3a 2073 7472 2c0a 2020 2020  _goal: str,.    
-00019160: 7374 7564 795f 7370 6563 5f70 6172 616d  study_spec_param
-00019170: 6574 6572 735f 6f76 6572 7269 6465 3a20  eters_override: 
-00019180: 4c69 7374 5b44 6963 745b 7374 722c 2041  List[Dict[str, A
-00019190: 6e79 5d5d 2c0a 2020 2020 6d61 785f 7472  ny]],.    max_tr
-000191a0: 6961 6c5f 636f 756e 743a 2069 6e74 2c0a  ial_count: int,.
-000191b0: 2020 2020 7061 7261 6c6c 656c 5f74 7269      parallel_tri
-000191c0: 616c 5f63 6f75 6e74 3a20 696e 742c 0a20  al_count: int,. 
-000191d0: 2020 2074 7261 6e73 666f 726d 5f63 6f6e     transform_con
-000191e0: 6669 673a 204f 7074 696f 6e61 6c5b 7374  fig: Optional[st
-000191f0: 725d 203d 204e 6f6e 652c 0a20 2020 2064  r] = None,.    d
-00019200: 6174 6173 6574 5f6c 6576 656c 5f63 7573  ataset_level_cus
-00019210: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
-00019220: 6f6e 5f64 6566 696e 6974 696f 6e73 3a20  on_definitions: 
-00019230: 4f70 7469 6f6e 616c 5b0a 2020 2020 2020  Optional[.      
-00019240: 2020 4c69 7374 5b44 6963 745b 7374 722c    List[Dict[str,
-00019250: 2041 6e79 5d5d 0a20 2020 205d 203d 204e   Any]].    ] = N
-00019260: 6f6e 652c 0a20 2020 2064 6174 6173 6574  one,.    dataset
-00019270: 5f6c 6576 656c 5f74 7261 6e73 666f 726d  _level_transform
-00019280: 6174 696f 6e73 3a20 4f70 7469 6f6e 616c  ations: Optional
-00019290: 5b4c 6973 745b 4469 6374 5b73 7472 2c20  [List[Dict[str, 
-000192a0: 416e 795d 5d5d 203d 204e 6f6e 652c 0a20  Any]]] = None,. 
-000192b0: 2020 2072 756e 5f66 6561 7475 7265 5f73     run_feature_s
-000192c0: 656c 6563 7469 6f6e 3a20 626f 6f6c 203d  election: bool =
-000192d0: 2046 616c 7365 2c0a 2020 2020 6665 6174   False,.    feat
-000192e0: 7572 655f 7365 6c65 6374 696f 6e5f 616c  ure_selection_al
-000192f0: 676f 7269 7468 6d3a 204f 7074 696f 6e61  gorithm: Optiona
-00019300: 6c5b 7374 725d 203d 204e 6f6e 652c 0a20  l[str] = None,. 
-00019310: 2020 206d 6174 6572 6961 6c69 7a65 645f     materialized_
-00019320: 6578 616d 706c 6573 5f66 6f72 6d61 743a  examples_format:
-00019330: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
-00019340: 204e 6f6e 652c 0a20 2020 206d 6178 5f73   None,.    max_s
-00019350: 656c 6563 7465 645f 6665 6174 7572 6573  elected_features
-00019360: 3a20 4f70 7469 6f6e 616c 5b69 6e74 5d20  : Optional[int] 
-00019370: 3d20 4e6f 6e65 2c0a 2020 2020 7072 6564  = None,.    pred
-00019380: 6566 696e 6564 5f73 706c 6974 5f6b 6579  efined_split_key
-00019390: 3a20 4f70 7469 6f6e 616c 5b73 7472 5d20  : Optional[str] 
-000193a0: 3d20 4e6f 6e65 2c0a 2020 2020 7374 7261  = None,.    stra
-000193b0: 7469 6669 6564 5f73 706c 6974 5f6b 6579  tified_split_key
-000193c0: 3a20 4f70 7469 6f6e 616c 5b73 7472 5d20  : Optional[str] 
-000193d0: 3d20 4e6f 6e65 2c0a 2020 2020 7472 6169  = None,.    trai
-000193e0: 6e69 6e67 5f66 7261 6374 696f 6e3a 204f  ning_fraction: O
-000193f0: 7074 696f 6e61 6c5b 666c 6f61 745d 203d  ptional[float] =
-00019400: 204e 6f6e 652c 0a20 2020 2076 616c 6964   None,.    valid
-00019410: 6174 696f 6e5f 6672 6163 7469 6f6e 3a20  ation_fraction: 
-00019420: 4f70 7469 6f6e 616c 5b66 6c6f 6174 5d20  Optional[float] 
-00019430: 3d20 4e6f 6e65 2c0a 2020 2020 7465 7374  = None,.    test
-00019440: 5f66 7261 6374 696f 6e3a 204f 7074 696f  _fraction: Optio
-00019450: 6e61 6c5b 666c 6f61 745d 203d 204e 6f6e  nal[float] = Non
-00019460: 652c 0a20 2020 2074 665f 7472 616e 7366  e,.    tf_transf
-00019470: 6f72 6d5f 6578 6563 7574 696f 6e5f 656e  orm_execution_en
-00019480: 6769 6e65 3a20 4f70 7469 6f6e 616c 5b73  gine: Optional[s
-00019490: 7472 5d20 3d20 4e6f 6e65 2c0a 2020 2020  tr] = None,.    
-000194a0: 7466 5f61 7574 6f5f 7472 616e 7366 6f72  tf_auto_transfor
-000194b0: 6d5f 6665 6174 7572 6573 3a20 4f70 7469  m_features: Opti
-000194c0: 6f6e 616c 5b0a 2020 2020 2020 2020 556e  onal[.        Un
-000194d0: 696f 6e5b 4c69 7374 5b73 7472 5d2c 2044  ion[List[str], D
-000194e0: 6963 745b 7374 722c 204c 6973 745b 7374  ict[str, List[st
-000194f0: 725d 5d5d 0a20 2020 205d 203d 204e 6f6e  r]]].    ] = Non
-00019500: 652c 0a20 2020 2074 665f 6375 7374 6f6d  e,.    tf_custom
-00019510: 5f74 7261 6e73 666f 726d 6174 696f 6e5f  _transformation_
-00019520: 6465 6669 6e69 7469 6f6e 733a 204f 7074  definitions: Opt
-00019530: 696f 6e61 6c5b 4c69 7374 5b44 6963 745b  ional[List[Dict[
-00019540: 7374 722c 2041 6e79 5d5d 5d20 3d20 4e6f  str, Any]]] = No
-00019550: 6e65 2c0a 2020 2020 7466 5f74 7261 6e73  ne,.    tf_trans
-00019560: 666f 726d 6174 696f 6e73 5f70 6174 683a  formations_path:
-00019570: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
-00019580: 204e 6f6e 652c 0a20 2020 2065 6e61 626c   None,.    enabl
-00019590: 655f 7072 6f66 696c 6572 3a20 626f 6f6c  e_profiler: bool
-000195a0: 203d 2046 616c 7365 2c0a 2020 2020 6361   = False,.    ca
-000195b0: 6368 655f 6461 7461 3a20 7374 7220 3d20  che_data: str = 
-000195c0: 2761 7574 6f27 2c0a 2020 2020 7365 6564  'auto',.    seed
-000195d0: 3a20 696e 7420 3d20 312c 0a20 2020 2065  : int = 1,.    e
-000195e0: 7661 6c5f 7374 6570 733a 2069 6e74 203d  val_steps: int =
-000195f0: 2030 2c0a 2020 2020 6576 616c 5f66 7265   0,.    eval_fre
-00019600: 7175 656e 6379 5f73 6563 733a 2069 6e74  quency_secs: int
-00019610: 203d 2036 3030 2c0a 2020 2020 6461 7461   = 600,.    data
-00019620: 5f73 6f75 7263 655f 6373 765f 6669 6c65  _source_csv_file
-00019630: 6e61 6d65 733a 204f 7074 696f 6e61 6c5b  names: Optional[
-00019640: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
-00019650: 2064 6174 615f 736f 7572 6365 5f62 6967   data_source_big
-00019660: 7175 6572 795f 7461 626c 655f 7061 7468  query_table_path
-00019670: 3a20 4f70 7469 6f6e 616c 5b73 7472 5d20  : Optional[str] 
-00019680: 3d20 4e6f 6e65 2c0a 2020 2020 6269 6771  = None,.    bigq
-00019690: 7565 7279 5f73 7461 6769 6e67 5f66 756c  uery_staging_ful
-000196a0: 6c5f 6461 7461 7365 745f 6964 3a20 4f70  l_dataset_id: Op
-000196b0: 7469 6f6e 616c 5b73 7472 5d20 3d20 4e6f  tional[str] = No
-000196c0: 6e65 2c0a 2020 2020 7765 6967 6874 5f63  ne,.    weight_c
-000196d0: 6f6c 756d 6e3a 2073 7472 203d 2027 272c  olumn: str = '',
-000196e0: 0a20 2020 206d 6178 5f66 6169 6c65 645f  .    max_failed_
-000196f0: 7472 6961 6c5f 636f 756e 743a 2069 6e74  trial_count: int
-00019700: 203d 2030 2c0a 2020 2020 7374 7564 795f   = 0,.    study_
-00019710: 7370 6563 5f61 6c67 6f72 6974 686d 3a20  spec_algorithm: 
-00019720: 7374 7220 3d20 2741 4c47 4f52 4954 484d  str = 'ALGORITHM
-00019730: 5f55 4e53 5045 4349 4649 4544 272c 0a20  _UNSPECIFIED',. 
-00019740: 2020 2073 7475 6479 5f73 7065 635f 6d65     study_spec_me
-00019750: 6173 7572 656d 656e 745f 7365 6c65 6374  asurement_select
-00019760: 696f 6e5f 7479 7065 3a20 7374 7220 3d20  ion_type: str = 
-00019770: 2742 4553 545f 4d45 4153 5552 454d 454e  'BEST_MEASUREMEN
-00019780: 5427 2c0a 2020 2020 7472 616e 7366 6f72  T',.    transfor
-00019790: 6d5f 6461 7461 666c 6f77 5f6d 6163 6869  m_dataflow_machi
-000197a0: 6e65 5f74 7970 653a 2073 7472 203d 2027  ne_type: str = '
-000197b0: 6e31 2d73 7461 6e64 6172 642d 3136 272c  n1-standard-16',
-000197c0: 0a20 2020 2074 7261 6e73 666f 726d 5f64  .    transform_d
-000197d0: 6174 6166 6c6f 775f 6d61 785f 6e75 6d5f  ataflow_max_num_
-000197e0: 776f 726b 6572 733a 2069 6e74 203d 2032  workers: int = 2
-000197f0: 352c 0a20 2020 2074 7261 6e73 666f 726d  5,.    transform
-00019800: 5f64 6174 6166 6c6f 775f 6469 736b 5f73  _dataflow_disk_s
-00019810: 697a 655f 6762 3a20 696e 7420 3d20 3430  ize_gb: int = 40
-00019820: 2c0a 2020 2020 776f 726b 6572 5f70 6f6f  ,.    worker_poo
-00019830: 6c5f 7370 6563 735f 6f76 6572 7269 6465  l_specs_override
-00019840: 3a20 4f70 7469 6f6e 616c 5b44 6963 745b  : Optional[Dict[
-00019850: 7374 722c 2041 6e79 5d5d 203d 204e 6f6e  str, Any]] = Non
-00019860: 652c 0a20 2020 2072 756e 5f65 7661 6c75  e,.    run_evalu
-00019870: 6174 696f 6e3a 2062 6f6f 6c20 3d20 5472  ation: bool = Tr
-00019880: 7565 2c0a 2020 2020 6576 616c 7561 7469  ue,.    evaluati
-00019890: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
-000198a0: 5f6d 6163 6869 6e65 5f74 7970 653a 2073  _machine_type: s
-000198b0: 7472 203d 205f 4556 414c 5541 5449 4f4e  tr = _EVALUATION
-000198c0: 5f42 4154 4348 5f50 5245 4449 4354 5f4d  _BATCH_PREDICT_M
-000198d0: 4143 4849 4e45 5f54 5950 452c 0a20 2020  ACHINE_TYPE,.   
-000198e0: 2065 7661 6c75 6174 696f 6e5f 6261 7463   evaluation_batc
-000198f0: 685f 7072 6564 6963 745f 7374 6172 7469  h_predict_starti
-00019900: 6e67 5f72 6570 6c69 6361 5f63 6f75 6e74  ng_replica_count
-00019910: 3a20 696e 7420 3d20 5f45 5641 4c55 4154  : int = _EVALUAT
-00019920: 494f 4e5f 4241 5443 485f 5052 4544 4943  ION_BATCH_PREDIC
-00019930: 545f 5354 4152 5449 4e47 5f52 4550 4c49  T_STARTING_REPLI
-00019940: 4341 5f43 4f55 4e54 2c0a 2020 2020 6576  CA_COUNT,.    ev
-00019950: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
-00019960: 7265 6469 6374 5f6d 6178 5f72 6570 6c69  redict_max_repli
-00019970: 6361 5f63 6f75 6e74 3a20 696e 7420 3d20  ca_count: int = 
-00019980: 5f45 5641 4c55 4154 494f 4e5f 4241 5443  _EVALUATION_BATC
-00019990: 485f 5052 4544 4943 545f 4d41 585f 5245  H_PREDICT_MAX_RE
-000199a0: 504c 4943 415f 434f 554e 542c 0a20 2020  PLICA_COUNT,.   
-000199b0: 2065 7661 6c75 6174 696f 6e5f 6461 7461   evaluation_data
-000199c0: 666c 6f77 5f6d 6163 6869 6e65 5f74 7970  flow_machine_typ
-000199d0: 653a 2073 7472 203d 205f 4556 414c 5541  e: str = _EVALUA
-000199e0: 5449 4f4e 5f44 4154 4146 4c4f 575f 4d41  TION_DATAFLOW_MA
-000199f0: 4348 494e 455f 5459 5045 2c0a 2020 2020  CHINE_TYPE,.    
-00019a00: 6576 616c 7561 7469 6f6e 5f64 6174 6166  evaluation_dataf
-00019a10: 6c6f 775f 7374 6172 7469 6e67 5f6e 756d  low_starting_num
-00019a20: 5f77 6f72 6b65 7273 3a20 696e 7420 3d20  _workers: int = 
-00019a30: 5f45 5641 4c55 4154 494f 4e5f 4441 5441  _EVALUATION_DATA
-00019a40: 464c 4f57 5f53 5441 5254 494e 475f 4e55  FLOW_STARTING_NU
-00019a50: 4d5f 574f 524b 4552 532c 0a20 2020 2065  M_WORKERS,.    e
-00019a60: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
-00019a70: 6f77 5f6d 6178 5f6e 756d 5f77 6f72 6b65  ow_max_num_worke
-00019a80: 7273 3a20 696e 7420 3d20 5f45 5641 4c55  rs: int = _EVALU
-00019a90: 4154 494f 4e5f 4441 5441 464c 4f57 5f4d  ATION_DATAFLOW_M
-00019aa0: 4158 5f4e 554d 5f57 4f52 4b45 5253 2c0a  AX_NUM_WORKERS,.
-00019ab0: 2020 2020 6576 616c 7561 7469 6f6e 5f64      evaluation_d
-00019ac0: 6174 6166 6c6f 775f 6469 736b 5f73 697a  ataflow_disk_siz
-00019ad0: 655f 6762 3a20 696e 7420 3d20 5f45 5641  e_gb: int = _EVA
-00019ae0: 4c55 4154 494f 4e5f 4441 5441 464c 4f57  LUATION_DATAFLOW
-00019af0: 5f44 4953 4b5f 5349 5a45 5f47 422c 0a20  _DISK_SIZE_GB,. 
-00019b00: 2020 2064 6174 6166 6c6f 775f 7365 7276     dataflow_serv
-00019b10: 6963 655f 6163 636f 756e 743a 2073 7472  ice_account: str
-00019b20: 203d 2027 272c 0a20 2020 2064 6174 6166   = '',.    dataf
-00019b30: 6c6f 775f 7375 626e 6574 776f 726b 3a20  low_subnetwork: 
-00019b40: 7374 7220 3d20 2727 2c0a 2020 2020 6461  str = '',.    da
-00019b50: 7461 666c 6f77 5f75 7365 5f70 7562 6c69  taflow_use_publi
-00019b60: 635f 6970 733a 2062 6f6f 6c20 3d20 5472  c_ips: bool = Tr
-00019b70: 7565 2c0a 2020 2020 656e 6372 7970 7469  ue,.    encrypti
-00019b80: 6f6e 5f73 7065 635f 6b65 795f 6e61 6d65  on_spec_key_name
-00019b90: 3a20 7374 7220 3d20 2727 2c0a 2920 2d3e  : str = '',.) ->
-00019ba0: 2054 7570 6c65 5b73 7472 2c20 4469 6374   Tuple[str, Dict
-00019bb0: 5b73 7472 2c20 416e 795d 5d3a 0a20 2022  [str, Any]]:.  "
-00019bc0: 2222 4765 7420 7468 6520 5769 6465 2026  ""Get the Wide &
-00019bd0: 2044 6565 7020 616c 676f 7269 7468 6d20   Deep algorithm 
-00019be0: 4879 7065 7270 6172 616d 6574 6572 5475  HyperparameterTu
-00019bf0: 6e69 6e67 4a6f 6220 7069 7065 6c69 6e65  ningJob pipeline
-00019c00: 2e0a 0a20 2041 7267 733a 0a20 2020 2070  ...  Args:.    p
-00019c10: 726f 6a65 6374 3a20 5468 6520 4743 5020  roject: The GCP 
-00019c20: 7072 6f6a 6563 7420 7468 6174 2072 756e  project that run
-00019c30: 7320 7468 6520 7069 7065 6c69 6e65 2063  s the pipeline c
-00019c40: 6f6d 706f 6e65 6e74 732e 0a20 2020 206c  omponents..    l
-00019c50: 6f63 6174 696f 6e3a 2054 6865 2047 4350  ocation: The GCP
-00019c60: 2072 6567 696f 6e20 7468 6174 2072 756e   region that run
-00019c70: 7320 7468 6520 7069 7065 6c69 6e65 2063  s the pipeline c
-00019c80: 6f6d 706f 6e65 6e74 732e 0a20 2020 2072  omponents..    r
-00019c90: 6f6f 745f 6469 723a 2054 6865 2072 6f6f  oot_dir: The roo
-00019ca0: 7420 4743 5320 6469 7265 6374 6f72 7920  t GCS directory 
-00019cb0: 666f 7220 7468 6520 7069 7065 6c69 6e65  for the pipeline
-00019cc0: 2063 6f6d 706f 6e65 6e74 732e 0a20 2020   components..   
-00019cd0: 2074 6172 6765 745f 636f 6c75 6d6e 3a20   target_column: 
-00019ce0: 5468 6520 7461 7267 6574 2063 6f6c 756d  The target colum
-00019cf0: 6e20 6e61 6d65 2e0a 2020 2020 7072 6564  n name..    pred
-00019d00: 6963 7469 6f6e 5f74 7970 653a 2054 6865  iction_type: The
-00019d10: 2074 7970 6520 6f66 2070 7265 6469 6374   type of predict
-00019d20: 696f 6e20 7468 6520 6d6f 6465 6c20 6973  ion the model is
-00019d30: 2074 6f20 7072 6f64 7563 652e 0a20 2020   to produce..   
-00019d40: 2020 2022 636c 6173 7369 6669 6361 7469     "classificati
-00019d50: 6f6e 2220 6f72 2022 7265 6772 6573 7369  on" or "regressi
-00019d60: 6f6e 222e 0a20 2020 2073 7475 6479 5f73  on"..    study_s
-00019d70: 7065 635f 6d65 7472 6963 5f69 643a 204d  pec_metric_id: M
-00019d80: 6574 7269 6320 746f 206f 7074 696d 697a  etric to optimiz
-00019d90: 652c 2070 6f73 7369 626c 6520 7661 6c75  e, possible valu
-00019da0: 6573 3a20 5b20 276c 6f73 7327 2c0a 2020  es: [ 'loss',.  
-00019db0: 2020 2020 2761 7665 7261 6765 5f6c 6f73      'average_los
-00019dc0: 7327 2c20 2772 6d73 6527 2c20 276d 6165  s', 'rmse', 'mae
-00019dd0: 272c 2027 6d71 6c27 2c20 2761 6363 7572  ', 'mql', 'accur
-00019de0: 6163 7927 2c20 2761 7563 272c 2027 7072  acy', 'auc', 'pr
-00019df0: 6563 6973 696f 6e27 2c0a 2020 2020 2020  ecision',.      
-00019e00: 2772 6563 616c 6c27 5d2e 0a20 2020 2073  'recall']..    s
-00019e10: 7475 6479 5f73 7065 635f 6d65 7472 6963  tudy_spec_metric
-00019e20: 5f67 6f61 6c3a 204f 7074 696d 697a 6174  _goal: Optimizat
-00019e30: 696f 6e20 676f 616c 206f 6620 7468 6520  ion goal of the 
-00019e40: 6d65 7472 6963 2c20 706f 7373 6962 6c65  metric, possible
-00019e50: 2076 616c 7565 733a 0a20 2020 2020 2022   values:.      "
-00019e60: 4d41 5849 4d49 5a45 222c 2022 4d49 4e49  MAXIMIZE", "MINI
-00019e70: 4d49 5a45 222e 0a20 2020 2073 7475 6479  MIZE"..    study
-00019e80: 5f73 7065 635f 7061 7261 6d65 7465 7273  _spec_parameters
-00019e90: 5f6f 7665 7272 6964 653a 204c 6973 7420  _override: List 
-00019ea0: 6f66 2064 6963 7469 6f6e 6172 6965 7320  of dictionaries 
-00019eb0: 7265 7072 6573 656e 7469 6e67 2070 6172  representing par
-00019ec0: 616d 6574 6572 730a 2020 2020 2020 746f  ameters.      to
-00019ed0: 206f 7074 696d 697a 652e 2054 6865 2064   optimize. The d
-00019ee0: 6963 7469 6f6e 6172 7920 6b65 7920 6973  ictionary key is
-00019ef0: 2074 6865 2070 6172 616d 6574 6572 5f69   the parameter_i
-00019f00: 642c 2077 6869 6368 2069 7320 7061 7373  d, which is pass
-00019f10: 6564 2074 6f0a 2020 2020 2020 7472 6169  ed to.      trai
-00019f20: 6e69 6e67 206a 6f62 2061 7320 6120 636f  ning job as a co
-00019f30: 6d6d 616e 6420 6c69 6e65 2061 7267 756d  mmand line argum
-00019f40: 656e 742c 2061 6e64 2074 6865 2064 6963  ent, and the dic
-00019f50: 7469 6f6e 6172 7920 7661 6c75 6520 6973  tionary value is
-00019f60: 2074 6865 0a20 2020 2020 2070 6172 616d   the.      param
-00019f70: 6574 6572 2073 7065 6369 6669 6361 7469  eter specificati
-00019f80: 6f6e 206f 6620 7468 6520 6d65 7472 6963  on of the metric
-00019f90: 2e0a 2020 2020 6d61 785f 7472 6961 6c5f  ..    max_trial_
-00019fa0: 636f 756e 743a 2054 6865 2064 6573 6972  count: The desir
-00019fb0: 6564 2074 6f74 616c 206e 756d 6265 7220  ed total number 
-00019fc0: 6f66 2074 7269 616c 732e 0a20 2020 2070  of trials..    p
-00019fd0: 6172 616c 6c65 6c5f 7472 6961 6c5f 636f  arallel_trial_co
-00019fe0: 756e 743a 2054 6865 2064 6573 6972 6564  unt: The desired
-00019ff0: 206e 756d 6265 7220 6f66 2074 7269 616c   number of trial
-0001a000: 7320 746f 2072 756e 2069 6e20 7061 7261  s to run in para
-0001a010: 6c6c 656c 2e0a 2020 2020 7472 616e 7366  llel..    transf
-0001a020: 6f72 6d5f 636f 6e66 6967 3a20 5061 7468  orm_config: Path
-0001a030: 2074 6f20 7631 2054 4620 7472 616e 7366   to v1 TF transf
-0001a040: 6f72 6d61 7469 6f6e 2063 6f6e 6669 6775  ormation configu
-0001a050: 7261 7469 6f6e 2e0a 2020 2020 6461 7461  ration..    data
-0001a060: 7365 745f 6c65 7665 6c5f 6375 7374 6f6d  set_level_custom
-0001a070: 5f74 7261 6e73 666f 726d 6174 696f 6e5f  _transformation_
-0001a080: 6465 6669 6e69 7469 6f6e 733a 2044 6174  definitions: Dat
-0001a090: 6173 6574 2d6c 6576 656c 2063 7573 746f  aset-level custo
-0001a0a0: 6d0a 2020 2020 2020 7472 616e 7366 6f72  m.      transfor
-0001a0b0: 6d61 7469 6f6e 2064 6566 696e 6974 696f  mation definitio
-0001a0c0: 6e73 2069 6e20 7374 7269 6e67 2066 6f72  ns in string for
-0001a0d0: 6d61 742e 0a20 2020 2064 6174 6173 6574  mat..    dataset
-0001a0e0: 5f6c 6576 656c 5f74 7261 6e73 666f 726d  _level_transform
-0001a0f0: 6174 696f 6e73 3a20 4461 7461 7365 742d  ations: Dataset-
-0001a100: 6c65 7665 6c20 7472 616e 7366 6f72 6d61  level transforma
-0001a110: 7469 6f6e 2063 6f6e 6669 6775 7261 7469  tion configurati
-0001a120: 6f6e 2069 6e0a 2020 2020 2020 7374 7269  on in.      stri
-0001a130: 6e67 2066 6f72 6d61 742e 0a20 2020 2072  ng format..    r
-0001a140: 756e 5f66 6561 7475 7265 5f73 656c 6563  un_feature_selec
-0001a150: 7469 6f6e 3a20 5768 6574 6865 7220 746f  tion: Whether to
-0001a160: 2065 6e61 626c 6520 6665 6174 7572 6520   enable feature 
-0001a170: 7365 6c65 6374 696f 6e2e 0a20 2020 2066  selection..    f
-0001a180: 6561 7475 7265 5f73 656c 6563 7469 6f6e  eature_selection
-0001a190: 5f61 6c67 6f72 6974 686d 3a20 4665 6174  _algorithm: Feat
-0001a1a0: 7572 6520 7365 6c65 6374 696f 6e20 616c  ure selection al
-0001a1b0: 676f 7269 7468 6d2e 0a20 2020 206d 6174  gorithm..    mat
-0001a1c0: 6572 6961 6c69 7a65 645f 6578 616d 706c  erialized_exampl
-0001a1d0: 6573 5f66 6f72 6d61 743a 2054 6865 2066  es_format: The f
-0001a1e0: 6f72 6d61 7420 666f 7220 7468 6520 6d61  ormat for the ma
-0001a1f0: 7465 7269 616c 697a 6564 2065 7861 6d70  terialized examp
-0001a200: 6c65 732e 0a20 2020 206d 6178 5f73 656c  les..    max_sel
-0001a210: 6563 7465 645f 6665 6174 7572 6573 3a20  ected_features: 
-0001a220: 4d61 7869 6d75 6d20 6e75 6d62 6572 206f  Maximum number o
-0001a230: 6620 6665 6174 7572 6573 2074 6f20 7365  f features to se
-0001a240: 6c65 6374 2e0a 2020 2020 7072 6564 6566  lect..    predef
-0001a250: 696e 6564 5f73 706c 6974 5f6b 6579 3a20  ined_split_key: 
-0001a260: 5072 6564 6566 696e 6564 2073 706c 6974  Predefined split
-0001a270: 206b 6579 2e0a 2020 2020 7374 7261 7469   key..    strati
-0001a280: 6669 6564 5f73 706c 6974 5f6b 6579 3a20  fied_split_key: 
-0001a290: 5374 7261 7469 6669 6564 2073 706c 6974  Stratified split
-0001a2a0: 206b 6579 2e0a 2020 2020 7472 6169 6e69   key..    traini
-0001a2b0: 6e67 5f66 7261 6374 696f 6e3a 2054 7261  ng_fraction: Tra
-0001a2c0: 696e 696e 6720 6672 6163 7469 6f6e 2e0a  ining fraction..
-0001a2d0: 2020 2020 7661 6c69 6461 7469 6f6e 5f66      validation_f
-0001a2e0: 7261 6374 696f 6e3a 2056 616c 6964 6174  raction: Validat
-0001a2f0: 696f 6e20 6672 6163 7469 6f6e 2e0a 2020  ion fraction..  
-0001a300: 2020 7465 7374 5f66 7261 6374 696f 6e3a    test_fraction:
-0001a310: 2054 6573 7420 6672 6163 7469 6f6e 2e0a   Test fraction..
-0001a320: 2020 2020 7466 5f74 7261 6e73 666f 726d      tf_transform
-0001a330: 5f65 7865 6375 7469 6f6e 5f65 6e67 696e  _execution_engin
-0001a340: 653a 2054 6865 2065 7865 6375 7469 6f6e  e: The execution
-0001a350: 2065 6e67 696e 6520 7573 6564 2074 6f20   engine used to 
-0001a360: 6578 6563 7574 6520 5446 2d62 6173 6564  execute TF-based
-0001a370: 0a20 2020 2020 2074 7261 6e73 666f 726d  .      transform
-0001a380: 6174 696f 6e73 2e0a 2020 2020 7466 5f61  ations..    tf_a
-0001a390: 7574 6f5f 7472 616e 7366 6f72 6d5f 6665  uto_transform_fe
-0001a3a0: 6174 7572 6573 3a20 4c69 7374 206f 6620  atures: List of 
-0001a3b0: 6175 746f 2074 7261 6e73 666f 726d 2066  auto transform f
-0001a3c0: 6561 7475 7265 7320 696e 2074 6865 0a20  eatures in the. 
-0001a3d0: 2020 2020 2063 6f6d 6d61 2d73 6570 6172       comma-separ
-0001a3e0: 6174 6564 2073 7472 696e 6720 666f 726d  ated string form
-0001a3f0: 6174 2e0a 2020 2020 7466 5f63 7573 746f  at..    tf_custo
-0001a400: 6d5f 7472 616e 7366 6f72 6d61 7469 6f6e  m_transformation
-0001a410: 5f64 6566 696e 6974 696f 6e73 3a20 5446  _definitions: TF
-0001a420: 2063 7573 746f 6d20 7472 616e 7366 6f72   custom transfor
-0001a430: 6d61 7469 6f6e 2064 6566 696e 6974 696f  mation definitio
-0001a440: 6e73 0a20 2020 2020 2069 6e20 7374 7269  ns.      in stri
-0001a450: 6e67 2066 6f72 6d61 742e 0a20 2020 2074  ng format..    t
-0001a460: 665f 7472 616e 7366 6f72 6d61 7469 6f6e  f_transformation
-0001a470: 735f 7061 7468 3a20 5061 7468 2074 6f20  s_path: Path to 
-0001a480: 5446 2074 7261 6e73 666f 726d 6174 696f  TF transformatio
-0001a490: 6e20 636f 6e66 6967 7572 6174 696f 6e2e  n configuration.
-0001a4a0: 0a20 2020 2065 6e61 626c 655f 7072 6f66  .    enable_prof
-0001a4b0: 696c 6572 3a20 456e 6162 6c65 7320 7072  iler: Enables pr
-0001a4c0: 6f66 696c 696e 6720 616e 6420 7361 7665  ofiling and save
-0001a4d0: 7320 6120 7472 6163 6520 6475 7269 6e67  s a trace during
-0001a4e0: 2065 7661 6c75 6174 696f 6e2e 0a20 2020   evaluation..   
-0001a4f0: 2063 6163 6865 5f64 6174 613a 2057 6865   cache_data: Whe
-0001a500: 7468 6572 2074 6f20 6361 6368 6520 6461  ther to cache da
-0001a510: 7461 206f 7220 6e6f 742e 2049 6620 7365  ta or not. If se
-0001a520: 7420 746f 2027 6175 746f 272c 2063 6163  t to 'auto', cac
-0001a530: 6869 6e67 2069 730a 2020 2020 2020 6465  hing is.      de
-0001a540: 7465 726d 696e 6564 2062 6173 6564 206f  termined based o
-0001a550: 6e20 7468 6520 6461 7461 7365 7420 7369  n the dataset si
-0001a560: 7a65 2e0a 2020 2020 7365 6564 3a20 5365  ze..    seed: Se
-0001a570: 6564 2074 6f20 6265 2075 7365 6420 666f  ed to be used fo
-0001a580: 7220 7468 6973 2072 756e 2e0a 2020 2020  r this run..    
-0001a590: 6576 616c 5f73 7465 7073 3a20 4e75 6d62  eval_steps: Numb
-0001a5a0: 6572 206f 6620 7374 6570 7320 746f 2072  er of steps to r
-0001a5b0: 756e 2065 7661 6c75 6174 696f 6e20 666f  un evaluation fo
-0001a5c0: 722e 2049 6620 6e6f 7420 7370 6563 6966  r. If not specif
-0001a5d0: 6965 6420 6f72 0a20 2020 2020 206e 6567  ied or.      neg
-0001a5e0: 6174 6976 652c 2069 7420 6d65 616e 7320  ative, it means 
-0001a5f0: 7275 6e20 6576 616c 7561 7469 6f6e 206f  run evaluation o
-0001a600: 6e20 7468 6520 7768 6f6c 6520 7661 6c69  n the whole vali
-0001a610: 6461 7469 6f6e 2064 6174 6173 6574 2e20  dation dataset. 
-0001a620: 4966 2073 6574 0a20 2020 2020 2074 6f20  If set.      to 
-0001a630: 302c 2069 7420 6d65 616e 7320 7275 6e20  0, it means run 
-0001a640: 6576 616c 7561 7469 6f6e 2066 6f72 2061  evaluation for a
-0001a650: 2066 6978 6564 206e 756d 6265 7220 6f66   fixed number of
-0001a660: 2073 616d 706c 6573 2e0a 2020 2020 6576   samples..    ev
-0001a670: 616c 5f66 7265 7175 656e 6379 5f73 6563  al_frequency_sec
-0001a680: 733a 2046 7265 7175 656e 6379 2061 7420  s: Frequency at 
-0001a690: 7768 6963 6820 6576 616c 7561 7469 6f6e  which evaluation
-0001a6a0: 2061 6e64 2063 6865 636b 706f 696e 7469   and checkpointi
-0001a6b0: 6e67 2077 696c 6c0a 2020 2020 2020 7461  ng will.      ta
-0001a6c0: 6b65 2070 6c61 6365 2e0a 2020 2020 6461  ke place..    da
-0001a6d0: 7461 5f73 6f75 7263 655f 6373 765f 6669  ta_source_csv_fi
-0001a6e0: 6c65 6e61 6d65 733a 2054 6865 2043 5356  lenames: The CSV
-0001a6f0: 2064 6174 6120 736f 7572 6365 2e0a 2020   data source..  
-0001a700: 2020 6461 7461 5f73 6f75 7263 655f 6269    data_source_bi
-0001a710: 6771 7565 7279 5f74 6162 6c65 5f70 6174  gquery_table_pat
-0001a720: 683a 2054 6865 2042 6967 5175 6572 7920  h: The BigQuery 
-0001a730: 6461 7461 2073 6f75 7263 652e 0a20 2020  data source..   
-0001a740: 2062 6967 7175 6572 795f 7374 6167 696e   bigquery_stagin
-0001a750: 675f 6675 6c6c 5f64 6174 6173 6574 5f69  g_full_dataset_i
-0001a760: 643a 2054 6865 2042 6967 5175 6572 7920  d: The BigQuery 
-0001a770: 7374 6167 696e 6720 6675 6c6c 2064 6174  staging full dat
-0001a780: 6173 6574 2069 6420 666f 720a 2020 2020  aset id for.    
-0001a790: 2020 7374 6f72 696e 6720 696e 7465 726d    storing interm
-0001a7a0: 6564 6961 7465 2074 6162 6c65 732e 0a20  ediate tables.. 
-0001a7b0: 2020 2077 6569 6768 745f 636f 6c75 6d6e     weight_column
-0001a7c0: 3a20 5468 6520 7765 6967 6874 2063 6f6c  : The weight col
-0001a7d0: 756d 6e20 6e61 6d65 2e0a 2020 2020 6d61  umn name..    ma
-0001a7e0: 785f 6661 696c 6564 5f74 7269 616c 5f63  x_failed_trial_c
-0001a7f0: 6f75 6e74 3a20 5468 6520 6e75 6d62 6572  ount: The number
-0001a800: 206f 6620 6661 696c 6564 2074 7269 616c   of failed trial
-0001a810: 7320 7468 6174 206e 6565 6420 746f 2062  s that need to b
-0001a820: 6520 7365 656e 0a20 2020 2020 2062 6566  e seen.      bef
-0001a830: 6f72 6520 6661 696c 696e 6720 7468 6520  ore failing the 
-0001a840: 4879 7065 7270 6172 616d 6574 6572 5475  HyperparameterTu
-0001a850: 6e69 6e67 4a6f 622e 2049 6620 7365 7420  ningJob. If set 
-0001a860: 746f 2030 2c20 5665 7274 6578 2041 4920  to 0, Vertex AI 
-0001a870: 6465 6369 6465 730a 2020 2020 2020 686f  decides.      ho
-0001a880: 7720 6d61 6e79 2074 7269 616c 7320 6d75  w many trials mu
-0001a890: 7374 2066 6169 6c20 6265 666f 7265 2074  st fail before t
-0001a8a0: 6865 2077 686f 6c65 206a 6f62 2066 6169  he whole job fai
-0001a8b0: 6c73 2e0a 2020 2020 7374 7564 795f 7370  ls..    study_sp
-0001a8c0: 6563 5f61 6c67 6f72 6974 686d 3a20 5468  ec_algorithm: Th
-0001a8d0: 6520 7365 6172 6368 2061 6c67 6f72 6974  e search algorit
-0001a8e0: 686d 2073 7065 6369 6669 6564 2066 6f72  hm specified for
-0001a8f0: 2074 6865 2073 7475 6479 2e20 4f6e 6520   the study. One 
-0001a900: 6f66 0a20 2020 2020 2022 414c 474f 5249  of.      "ALGORI
-0001a910: 5448 4d5f 554e 5350 4543 4946 4945 4422  THM_UNSPECIFIED"
-0001a920: 2c20 2247 5249 445f 5345 4152 4348 222c  , "GRID_SEARCH",
-0001a930: 206f 7220 2252 414e 444f 4d5f 5345 4152   or "RANDOM_SEAR
-0001a940: 4348 222e 0a20 2020 2073 7475 6479 5f73  CH"..    study_s
-0001a950: 7065 635f 6d65 6173 7572 656d 656e 745f  pec_measurement_
-0001a960: 7365 6c65 6374 696f 6e5f 7479 7065 3a20  selection_type: 
-0001a970: 5768 6963 6820 6d65 6173 7572 656d 656e  Which measuremen
-0001a980: 7420 746f 2075 7365 2069 662f 7768 656e  t to use if/when
-0001a990: 2074 6865 0a20 2020 2020 2073 6572 7669   the.      servi
-0001a9a0: 6365 2061 7574 6f6d 6174 6963 616c 6c79  ce automatically
-0001a9b0: 2073 656c 6563 7473 2074 6865 2066 696e   selects the fin
-0001a9c0: 616c 206d 6561 7375 7265 6d65 6e74 2066  al measurement f
-0001a9d0: 726f 6d20 7072 6576 696f 7573 6c79 0a20  rom previously. 
-0001a9e0: 2020 2020 2072 6570 6f72 7465 6420 696e       reported in
-0001a9f0: 7465 726d 6564 6961 7465 206d 6561 7375  termediate measu
-0001aa00: 7265 6d65 6e74 732e 204f 6e65 206f 6620  rements. One of 
-0001aa10: 2242 4553 545f 4d45 4153 5552 454d 454e  "BEST_MEASUREMEN
-0001aa20: 5422 206f 720a 2020 2020 2020 224c 4153  T" or.      "LAS
-0001aa30: 545f 4d45 4153 5552 454d 454e 5422 2e0a  T_MEASUREMENT"..
-0001aa40: 2020 2020 7472 616e 7366 6f72 6d5f 6461      transform_da
-0001aa50: 7461 666c 6f77 5f6d 6163 6869 6e65 5f74  taflow_machine_t
-0001aa60: 7970 653a 2054 6865 2064 6174 6166 6c6f  ype: The dataflo
-0001aa70: 7720 6d61 6368 696e 6520 7479 7065 2066  w machine type f
-0001aa80: 6f72 2074 7261 6e73 666f 726d 0a20 2020  or transform.   
-0001aa90: 2020 2063 6f6d 706f 6e65 6e74 2e0a 2020     component..  
-0001aaa0: 2020 7472 616e 7366 6f72 6d5f 6461 7461    transform_data
-0001aab0: 666c 6f77 5f6d 6178 5f6e 756d 5f77 6f72  flow_max_num_wor
-0001aac0: 6b65 7273 3a20 5468 6520 6d61 7820 6e75  kers: The max nu
-0001aad0: 6d62 6572 206f 6620 4461 7461 666c 6f77  mber of Dataflow
-0001aae0: 2077 6f72 6b65 7273 2066 6f72 0a20 2020   workers for.   
-0001aaf0: 2020 2074 7261 6e73 666f 726d 2063 6f6d     transform com
-0001ab00: 706f 6e65 6e74 2e0a 2020 2020 7472 616e  ponent..    tran
-0001ab10: 7366 6f72 6d5f 6461 7461 666c 6f77 5f64  sform_dataflow_d
-0001ab20: 6973 6b5f 7369 7a65 5f67 623a 2044 6174  isk_size_gb: Dat
-0001ab30: 6166 6c6f 7720 776f 726b 6572 2773 2064  aflow worker's d
-0001ab40: 6973 6b20 7369 7a65 2069 6e20 4742 2066  isk size in GB f
-0001ab50: 6f72 0a20 2020 2020 2074 7261 6e73 666f  or.      transfo
-0001ab60: 726d 2063 6f6d 706f 6e65 6e74 2e0a 2020  rm component..  
-0001ab70: 2020 776f 726b 6572 5f70 6f6f 6c5f 7370    worker_pool_sp
-0001ab80: 6563 735f 6f76 6572 7269 6465 3a20 5468  ecs_override: Th
-0001ab90: 6520 6469 6374 696f 6e61 7279 2066 6f72  e dictionary for
-0001aba0: 206f 7665 7272 6964 696e 6720 7472 6169   overriding trai
-0001abb0: 6e69 6e67 2061 6e64 0a20 2020 2020 2065  ning and.      e
-0001abc0: 7661 6c75 6174 696f 6e20 776f 726b 6572  valuation worker
-0001abd0: 2070 6f6f 6c20 7370 6563 732e 2054 6865   pool specs. The
-0001abe0: 2064 6963 7469 6f6e 6172 7920 7368 6f75   dictionary shou
-0001abf0: 6c64 2062 6520 6f66 2066 6f72 6d61 740a  ld be of format.
-0001ac00: 2020 2020 2020 2020 2020 6874 7470 733a            https:
-0001ac10: 2f2f 6769 7468 7562 2e63 6f6d 2f67 6f6f  //github.com/goo
-0001ac20: 676c 6561 7069 732f 676f 6f67 6c65 6170  gleapis/googleap
-0001ac30: 6973 2f62 6c6f 622f 3465 3833 3663 3763  is/blob/4e836c7c
-0001ac40: 3235 3765 3365 3230 6231 6465 3134 6434  257e3e20b1de14d4
-0001ac50: 3730 3939 3361 3262 3166 3437 3336 6138  70993a2b1f4736a8
-0001ac60: 2f67 6f6f 676c 652f 636c 6f75 642f 6169  /google/cloud/ai
-0001ac70: 706c 6174 666f 726d 2f76 3162 6574 6131  platform/v1beta1
-0001ac80: 2f63 7573 746f 6d5f 6a6f 622e 7072 6f74  /custom_job.prot
-0001ac90: 6f23 4c31 3732 2e0a 2020 2020 7275 6e5f  o#L172..    run_
-0001aca0: 6576 616c 7561 7469 6f6e 3a20 5768 6574  evaluation: Whet
-0001acb0: 6865 7220 746f 2072 756e 2065 7661 6c75  her to run evalu
-0001acc0: 6174 696f 6e20 7374 6570 7320 6475 7269  ation steps duri
-0001acd0: 6e67 2074 7261 696e 696e 672e 0a20 2020  ng training..   
-0001ace0: 2065 7661 6c75 6174 696f 6e5f 6261 7463   evaluation_batc
-0001acf0: 685f 7072 6564 6963 745f 6d61 6368 696e  h_predict_machin
-0001ad00: 655f 7479 7065 3a20 5468 6520 7072 6564  e_type: The pred
-0001ad10: 6963 7469 6f6e 2073 6572 7665 7220 6d61  iction server ma
-0001ad20: 6368 696e 6520 7479 7065 0a20 2020 2020  chine type.     
-0001ad30: 2066 6f72 2062 6174 6368 2070 7265 6469   for batch predi
-0001ad40: 6374 2063 6f6d 706f 6e65 6e74 7320 6475  ct components du
-0001ad50: 7269 6e67 2065 7661 6c75 6174 696f 6e2e  ring evaluation.
-0001ad60: 0a20 2020 2065 7661 6c75 6174 696f 6e5f  .    evaluation_
-0001ad70: 6261 7463 685f 7072 6564 6963 745f 7374  batch_predict_st
-0001ad80: 6172 7469 6e67 5f72 6570 6c69 6361 5f63  arting_replica_c
-0001ad90: 6f75 6e74 3a20 5468 6520 696e 6974 6961  ount: The initia
-0001ada0: 6c20 6e75 6d62 6572 206f 660a 2020 2020  l number of.    
-0001adb0: 2020 7072 6564 6963 7469 6f6e 2073 6572    prediction ser
-0001adc0: 7665 7220 666f 7220 6261 7463 6820 7072  ver for batch pr
-0001add0: 6564 6963 7420 636f 6d70 6f6e 656e 7473  edict components
-0001ade0: 2064 7572 696e 6720 6576 616c 7561 7469   during evaluati
-0001adf0: 6f6e 2e0a 2020 2020 6576 616c 7561 7469  on..    evaluati
-0001ae00: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
-0001ae10: 5f6d 6178 5f72 6570 6c69 6361 5f63 6f75  _max_replica_cou
-0001ae20: 6e74 3a20 5468 6520 6d61 7820 6e75 6d62  nt: The max numb
-0001ae30: 6572 206f 6620 7072 6564 6963 7469 6f6e  er of prediction
-0001ae40: 0a20 2020 2020 2073 6572 7665 7220 666f  .      server fo
-0001ae50: 7220 6261 7463 6820 7072 6564 6963 7420  r batch predict 
-0001ae60: 636f 6d70 6f6e 656e 7473 2064 7572 696e  components durin
-0001ae70: 6720 6576 616c 7561 7469 6f6e 2e0a 2020  g evaluation..  
-0001ae80: 2020 6576 616c 7561 7469 6f6e 5f64 6174    evaluation_dat
-0001ae90: 6166 6c6f 775f 6d61 6368 696e 655f 7479  aflow_machine_ty
-0001aea0: 7065 3a20 5468 6520 6461 7461 666c 6f77  pe: The dataflow
-0001aeb0: 206d 6163 6869 6e65 2074 7970 6520 666f   machine type fo
-0001aec0: 7220 6576 616c 7561 7469 6f6e 0a20 2020  r evaluation.   
-0001aed0: 2020 2063 6f6d 706f 6e65 6e74 732e 0a20     components.. 
-0001aee0: 2020 2065 7661 6c75 6174 696f 6e5f 6461     evaluation_da
-0001aef0: 7461 666c 6f77 5f73 7461 7274 696e 675f  taflow_starting_
-0001af00: 6e75 6d5f 776f 726b 6572 733a 2054 6865  num_workers: The
-0001af10: 2069 6e69 7469 616c 206e 756d 6265 7220   initial number 
-0001af20: 6f66 2044 6174 6166 6c6f 770a 2020 2020  of Dataflow.    
-0001af30: 2020 776f 726b 6572 7320 666f 7220 6576    workers for ev
-0001af40: 616c 7561 7469 6f6e 2063 6f6d 706f 6e65  aluation compone
-0001af50: 6e74 732e 0a20 2020 2065 7661 6c75 6174  nts..    evaluat
-0001af60: 696f 6e5f 6461 7461 666c 6f77 5f6d 6178  ion_dataflow_max
-0001af70: 5f6e 756d 5f77 6f72 6b65 7273 3a20 5468  _num_workers: Th
-0001af80: 6520 6d61 7820 6e75 6d62 6572 206f 6620  e max number of 
-0001af90: 4461 7461 666c 6f77 2077 6f72 6b65 7273  Dataflow workers
-0001afa0: 2066 6f72 0a20 2020 2020 2065 7661 6c75   for.      evalu
-0001afb0: 6174 696f 6e20 636f 6d70 6f6e 656e 7473  ation components
-0001afc0: 2e0a 2020 2020 6576 616c 7561 7469 6f6e  ..    evaluation
-0001afd0: 5f64 6174 6166 6c6f 775f 6469 736b 5f73  _dataflow_disk_s
-0001afe0: 697a 655f 6762 3a20 4461 7461 666c 6f77  ize_gb: Dataflow
-0001aff0: 2077 6f72 6b65 7227 7320 6469 736b 2073   worker's disk s
-0001b000: 697a 6520 696e 2047 4220 666f 720a 2020  ize in GB for.  
-0001b010: 2020 2020 6576 616c 7561 7469 6f6e 2063      evaluation c
-0001b020: 6f6d 706f 6e65 6e74 732e 0a20 2020 2064  omponents..    d
-0001b030: 6174 6166 6c6f 775f 7365 7276 6963 655f  ataflow_service_
-0001b040: 6163 636f 756e 743a 2043 7573 746f 6d20  account: Custom 
-0001b050: 7365 7276 6963 6520 6163 636f 756e 7420  service account 
-0001b060: 746f 2072 756e 2064 6174 6166 6c6f 7720  to run dataflow 
-0001b070: 6a6f 6273 2e0a 2020 2020 6461 7461 666c  jobs..    datafl
-0001b080: 6f77 5f73 7562 6e65 7477 6f72 6b3a 2044  ow_subnetwork: D
-0001b090: 6174 6166 6c6f 7727 7320 6675 6c6c 7920  ataflow's fully 
-0001b0a0: 7175 616c 6966 6965 6420 7375 626e 6574  qualified subnet
-0001b0b0: 776f 726b 206e 616d 652c 2077 6865 6e20  work name, when 
-0001b0c0: 656d 7074 790a 2020 2020 2020 7468 6520  empty.      the 
-0001b0d0: 6465 6661 756c 7420 7375 626e 6574 776f  default subnetwo
-0001b0e0: 726b 2077 696c 6c20 6265 2075 7365 642e  rk will be used.
-0001b0f0: 2045 7861 6d70 6c65 3a0a 2020 2020 2020   Example:.      
-0001b100: 2020 6874 7470 733a 2f2f 636c 6f75 642e    https://cloud.
-0001b110: 676f 6f67 6c65 2e63 6f6d 2f64 6174 6166  google.com/dataf
-0001b120: 6c6f 772f 646f 6373 2f67 7569 6465 732f  low/docs/guides/
-0001b130: 7370 6563 6966 7969 6e67 2d6e 6574 776f  specifying-netwo
-0001b140: 726b 7323 6578 616d 706c 655f 6e65 7477  rks#example_netw
-0001b150: 6f72 6b5f 616e 645f 7375 626e 6574 776f  ork_and_subnetwo
-0001b160: 726b 5f73 7065 6369 6669 6361 7469 6f6e  rk_specification
-0001b170: 730a 2020 2020 6461 7461 666c 6f77 5f75  s.    dataflow_u
-0001b180: 7365 5f70 7562 6c69 635f 6970 733a 2053  se_public_ips: S
-0001b190: 7065 6369 6669 6573 2077 6865 7468 6572  pecifies whether
-0001b1a0: 2044 6174 6166 6c6f 7720 776f 726b 6572   Dataflow worker
-0001b1b0: 7320 7573 6520 7075 626c 6963 2049 500a  s use public IP.
-0001b1c0: 2020 2020 2020 6164 6472 6573 7365 732e        addresses.
-0001b1d0: 0a20 2020 2065 6e63 7279 7074 696f 6e5f  .    encryption_
-0001b1e0: 7370 6563 5f6b 6579 5f6e 616d 653a 2054  spec_key_name: T
-0001b1f0: 6865 204b 4d53 206b 6579 206e 616d 652e  he KMS key name.
-0001b200: 0a0a 2020 5265 7475 726e 733a 0a20 2020  ..  Returns:.   
-0001b210: 2054 7570 6c65 206f 6620 7069 7065 6c69   Tuple of pipeli
-0001b220: 6e65 5f64 6566 696e 6974 696f 6e5f 7061  ne_definition_pa
-0001b230: 7468 2061 6e64 2070 6172 616d 6574 6572  th and parameter
-0001b240: 5f76 616c 7565 732e 0a20 2022 2222 0a20  _values..  """. 
-0001b250: 2069 6620 6973 696e 7374 616e 6365 2874   if isinstance(t
-0001b260: 665f 6175 746f 5f74 7261 6e73 666f 726d  f_auto_transform
-0001b270: 5f66 6561 7475 7265 732c 206c 6973 7429  _features, list)
-0001b280: 3a0a 2020 2020 7466 5f61 7574 6f5f 7472  :.    tf_auto_tr
-0001b290: 616e 7366 6f72 6d5f 6665 6174 7572 6573  ansform_features
-0001b2a0: 203d 207b 2761 7574 6f27 3a20 7466 5f61   = {'auto': tf_a
-0001b2b0: 7574 6f5f 7472 616e 7366 6f72 6d5f 6665  uto_transform_fe
-0001b2c0: 6174 7572 6573 7d0a 0a20 2069 6620 7472  atures}..  if tr
-0001b2d0: 616e 7366 6f72 6d5f 636f 6e66 6967 2061  ansform_config a
-0001b2e0: 6e64 2074 665f 7472 616e 7366 6f72 6d61  nd tf_transforma
-0001b2f0: 7469 6f6e 735f 7061 7468 3a0a 2020 2020  tions_path:.    
-0001b300: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-0001b310: 280a 2020 2020 2020 2020 274f 6e6c 7920  (.        'Only 
-0001b320: 6f6e 6520 6f66 2074 7261 6e73 666f 726d  one of transform
-0001b330: 5f63 6f6e 6669 6720 616e 6420 7466 5f74  _config and tf_t
-0001b340: 7261 6e73 666f 726d 6174 696f 6e73 5f70  ransformations_p
-0001b350: 6174 6820 6361 6e20 270a 2020 2020 2020  ath can '.      
-0001b360: 2020 2762 6520 7370 6563 6966 6965 642e    'be specified.
-0001b370: 270a 2020 2020 290a 0a20 2065 6c69 6620  '.    )..  elif 
-0001b380: 7472 616e 7366 6f72 6d5f 636f 6e66 6967  transform_config
-0001b390: 3a0a 2020 2020 7761 726e 696e 6773 2e77  :.    warnings.w
-0001b3a0: 6172 6e28 0a20 2020 2020 2020 2027 7472  arn(.        'tr
-0001b3b0: 616e 7366 6f72 6d5f 636f 6e66 6967 2070  ansform_config p
-0001b3c0: 6172 616d 6574 6572 2069 7320 6465 7072  arameter is depr
-0001b3d0: 6563 6174 6564 2e20 270a 2020 2020 2020  ecated. '.      
-0001b3e0: 2020 2750 6c65 6173 6520 7573 6520 7468    'Please use th
-0001b3f0: 6520 666c 6174 7465 6e65 6420 7472 616e  e flattened tran
-0001b400: 7366 6f72 6d20 636f 6e66 6967 2061 7267  sform config arg
-0001b410: 756d 656e 7473 2069 6e73 7465 6164 2e27  uments instead.'
-0001b420: 0a20 2020 2029 0a20 2020 2074 665f 7472  .    ).    tf_tr
-0001b430: 616e 7366 6f72 6d61 7469 6f6e 735f 7061  ansformations_pa
-0001b440: 7468 203d 2074 7261 6e73 666f 726d 5f63  th = transform_c
-0001b450: 6f6e 6669 670a 0a20 2069 6620 6e6f 7420  onfig..  if not 
-0001b460: 776f 726b 6572 5f70 6f6f 6c5f 7370 6563  worker_pool_spec
-0001b470: 735f 6f76 6572 7269 6465 3a0a 2020 2020  s_override:.    
-0001b480: 776f 726b 6572 5f70 6f6f 6c5f 7370 6563  worker_pool_spec
-0001b490: 735f 6f76 6572 7269 6465 203d 205b 5d0a  s_override = [].
-0001b4a0: 0a20 2070 6172 616d 6574 6572 5f76 616c  .  parameter_val
-0001b4b0: 7565 7320 3d20 7b0a 2020 2020 2020 2770  ues = {.      'p
-0001b4c0: 726f 6a65 6374 273a 2070 726f 6a65 6374  roject': project
-0001b4d0: 2c0a 2020 2020 2020 276c 6f63 6174 696f  ,.      'locatio
-0001b4e0: 6e27 3a20 6c6f 6361 7469 6f6e 2c0a 2020  n': location,.  
-0001b4f0: 2020 2020 2772 6f6f 745f 6469 7227 3a20      'root_dir': 
-0001b500: 726f 6f74 5f64 6972 2c0a 2020 2020 2020  root_dir,.      
-0001b510: 2774 6172 6765 745f 636f 6c75 6d6e 273a  'target_column':
-0001b520: 2074 6172 6765 745f 636f 6c75 6d6e 2c0a   target_column,.
-0001b530: 2020 2020 2020 2770 7265 6469 6374 696f        'predictio
-0001b540: 6e5f 7479 7065 273a 2070 7265 6469 6374  n_type': predict
-0001b550: 696f 6e5f 7479 7065 2c0a 2020 2020 2020  ion_type,.      
-0001b560: 2773 7475 6479 5f73 7065 635f 6d65 7472  'study_spec_metr
-0001b570: 6963 5f69 6427 3a20 7374 7564 795f 7370  ic_id': study_sp
-0001b580: 6563 5f6d 6574 7269 635f 6964 2c0a 2020  ec_metric_id,.  
-0001b590: 2020 2020 2773 7475 6479 5f73 7065 635f      'study_spec_
-0001b5a0: 6d65 7472 6963 5f67 6f61 6c27 3a20 7374  metric_goal': st
-0001b5b0: 7564 795f 7370 6563 5f6d 6574 7269 635f  udy_spec_metric_
-0001b5c0: 676f 616c 2c0a 2020 2020 2020 2773 7475  goal,.      'stu
-0001b5d0: 6479 5f73 7065 635f 7061 7261 6d65 7465  dy_spec_paramete
-0001b5e0: 7273 5f6f 7665 7272 6964 6527 3a20 7374  rs_override': st
-0001b5f0: 7564 795f 7370 6563 5f70 6172 616d 6574  udy_spec_paramet
-0001b600: 6572 735f 6f76 6572 7269 6465 2c0a 2020  ers_override,.  
-0001b610: 2020 2020 276d 6178 5f74 7269 616c 5f63      'max_trial_c
-0001b620: 6f75 6e74 273a 206d 6178 5f74 7269 616c  ount': max_trial
-0001b630: 5f63 6f75 6e74 2c0a 2020 2020 2020 2770  _count,.      'p
-0001b640: 6172 616c 6c65 6c5f 7472 6961 6c5f 636f  arallel_trial_co
-0001b650: 756e 7427 3a20 7061 7261 6c6c 656c 5f74  unt': parallel_t
-0001b660: 7269 616c 5f63 6f75 6e74 2c0a 2020 2020  rial_count,.    
-0001b670: 2020 2765 6e61 626c 655f 7072 6f66 696c    'enable_profil
-0001b680: 6572 273a 2065 6e61 626c 655f 7072 6f66  er': enable_prof
-0001b690: 696c 6572 2c0a 2020 2020 2020 2763 6163  iler,.      'cac
-0001b6a0: 6865 5f64 6174 6127 3a20 6361 6368 655f  he_data': cache_
-0001b6b0: 6461 7461 2c0a 2020 2020 2020 2773 6565  data,.      'see
-0001b6c0: 6427 3a20 7365 6564 2c0a 2020 2020 2020  d': seed,.      
-0001b6d0: 2765 7661 6c5f 7374 6570 7327 3a20 6576  'eval_steps': ev
-0001b6e0: 616c 5f73 7465 7073 2c0a 2020 2020 2020  al_steps,.      
-0001b6f0: 2765 7661 6c5f 6672 6571 7565 6e63 795f  'eval_frequency_
-0001b700: 7365 6373 273a 2065 7661 6c5f 6672 6571  secs': eval_freq
-0001b710: 7565 6e63 795f 7365 6373 2c0a 2020 2020  uency_secs,.    
-0001b720: 2020 2777 6569 6768 745f 636f 6c75 6d6e    'weight_column
-0001b730: 273a 2077 6569 6768 745f 636f 6c75 6d6e  ': weight_column
-0001b740: 2c0a 2020 2020 2020 276d 6178 5f66 6169  ,.      'max_fai
-0001b750: 6c65 645f 7472 6961 6c5f 636f 756e 7427  led_trial_count'
-0001b760: 3a20 6d61 785f 6661 696c 6564 5f74 7269  : max_failed_tri
-0001b770: 616c 5f63 6f75 6e74 2c0a 2020 2020 2020  al_count,.      
-0001b780: 2773 7475 6479 5f73 7065 635f 616c 676f  'study_spec_algo
-0001b790: 7269 7468 6d27 3a20 7374 7564 795f 7370  rithm': study_sp
-0001b7a0: 6563 5f61 6c67 6f72 6974 686d 2c0a 2020  ec_algorithm,.  
-0001b7b0: 2020 2020 2773 7475 6479 5f73 7065 635f      'study_spec_
-0001b7c0: 6d65 6173 7572 656d 656e 745f 7365 6c65  measurement_sele
-0001b7d0: 6374 696f 6e5f 7479 7065 273a 2028 0a20  ction_type': (. 
-0001b7e0: 2020 2020 2020 2020 2073 7475 6479 5f73           study_s
-0001b7f0: 7065 635f 6d65 6173 7572 656d 656e 745f  pec_measurement_
-0001b800: 7365 6c65 6374 696f 6e5f 7479 7065 0a20  selection_type. 
-0001b810: 2020 2020 2029 2c0a 2020 2020 2020 2774       ),.      't
-0001b820: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
-0001b830: 775f 6d61 6368 696e 655f 7479 7065 273a  w_machine_type':
-0001b840: 2074 7261 6e73 666f 726d 5f64 6174 6166   transform_dataf
-0001b850: 6c6f 775f 6d61 6368 696e 655f 7479 7065  low_machine_type
-0001b860: 2c0a 2020 2020 2020 2774 7261 6e73 666f  ,.      'transfo
-0001b870: 726d 5f64 6174 6166 6c6f 775f 6d61 785f  rm_dataflow_max_
-0001b880: 6e75 6d5f 776f 726b 6572 7327 3a20 7472  num_workers': tr
-0001b890: 616e 7366 6f72 6d5f 6461 7461 666c 6f77  ansform_dataflow
-0001b8a0: 5f6d 6178 5f6e 756d 5f77 6f72 6b65 7273  _max_num_workers
-0001b8b0: 2c0a 2020 2020 2020 2774 7261 6e73 666f  ,.      'transfo
-0001b8c0: 726d 5f64 6174 6166 6c6f 775f 6469 736b  rm_dataflow_disk
-0001b8d0: 5f73 697a 655f 6762 273a 2074 7261 6e73  _size_gb': trans
-0001b8e0: 666f 726d 5f64 6174 6166 6c6f 775f 6469  form_dataflow_di
-0001b8f0: 736b 5f73 697a 655f 6762 2c0a 2020 2020  sk_size_gb,.    
-0001b900: 2020 2777 6f72 6b65 725f 706f 6f6c 5f73    'worker_pool_s
-0001b910: 7065 6373 5f6f 7665 7272 6964 6527 3a20  pecs_override': 
-0001b920: 776f 726b 6572 5f70 6f6f 6c5f 7370 6563  worker_pool_spec
-0001b930: 735f 6f76 6572 7269 6465 2c0a 2020 2020  s_override,.    
-0001b940: 2020 2772 756e 5f65 7661 6c75 6174 696f    'run_evaluatio
-0001b950: 6e27 3a20 7275 6e5f 6576 616c 7561 7469  n': run_evaluati
-0001b960: 6f6e 2c0a 2020 2020 2020 2765 7661 6c75  on,.      'evalu
-0001b970: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
-0001b980: 6963 745f 6d61 6368 696e 655f 7479 7065  ict_machine_type
-0001b990: 273a 2028 0a20 2020 2020 2020 2020 2065  ': (.          e
-0001b9a0: 7661 6c75 6174 696f 6e5f 6261 7463 685f  valuation_batch_
-0001b9b0: 7072 6564 6963 745f 6d61 6368 696e 655f  predict_machine_
-0001b9c0: 7479 7065 0a20 2020 2020 2029 2c0a 2020  type.      ),.  
-0001b9d0: 2020 2020 2765 7661 6c75 6174 696f 6e5f      'evaluation_
-0001b9e0: 6261 7463 685f 7072 6564 6963 745f 7374  batch_predict_st
-0001b9f0: 6172 7469 6e67 5f72 6570 6c69 6361 5f63  arting_replica_c
-0001ba00: 6f75 6e74 273a 2028 0a20 2020 2020 2020  ount': (.       
-0001ba10: 2020 2065 7661 6c75 6174 696f 6e5f 6261     evaluation_ba
-0001ba20: 7463 685f 7072 6564 6963 745f 7374 6172  tch_predict_star
-0001ba30: 7469 6e67 5f72 6570 6c69 6361 5f63 6f75  ting_replica_cou
-0001ba40: 6e74 0a20 2020 2020 2029 2c0a 2020 2020  nt.      ),.    
-0001ba50: 2020 2765 7661 6c75 6174 696f 6e5f 6261    'evaluation_ba
-0001ba60: 7463 685f 7072 6564 6963 745f 6d61 785f  tch_predict_max_
-0001ba70: 7265 706c 6963 615f 636f 756e 7427 3a20  replica_count': 
-0001ba80: 280a 2020 2020 2020 2020 2020 6576 616c  (.          eval
-0001ba90: 7561 7469 6f6e 5f62 6174 6368 5f70 7265  uation_batch_pre
-0001baa0: 6469 6374 5f6d 6178 5f72 6570 6c69 6361  dict_max_replica
-0001bab0: 5f63 6f75 6e74 0a20 2020 2020 2029 2c0a  _count.      ),.
-0001bac0: 2020 2020 2020 2765 7661 6c75 6174 696f        'evaluatio
-0001bad0: 6e5f 6461 7461 666c 6f77 5f6d 6163 6869  n_dataflow_machi
-0001bae0: 6e65 5f74 7970 6527 3a20 6576 616c 7561  ne_type': evalua
-0001baf0: 7469 6f6e 5f64 6174 6166 6c6f 775f 6d61  tion_dataflow_ma
-0001bb00: 6368 696e 655f 7479 7065 2c0a 2020 2020  chine_type,.    
-0001bb10: 2020 2765 7661 6c75 6174 696f 6e5f 6461    'evaluation_da
-0001bb20: 7461 666c 6f77 5f73 7461 7274 696e 675f  taflow_starting_
-0001bb30: 6e75 6d5f 776f 726b 6572 7327 3a20 280a  num_workers': (.
-0001bb40: 2020 2020 2020 2020 2020 6576 616c 7561            evalua
-0001bb50: 7469 6f6e 5f64 6174 6166 6c6f 775f 7374  tion_dataflow_st
-0001bb60: 6172 7469 6e67 5f6e 756d 5f77 6f72 6b65  arting_num_worke
-0001bb70: 7273 0a20 2020 2020 2029 2c0a 2020 2020  rs.      ),.    
-0001bb80: 2020 2765 7661 6c75 6174 696f 6e5f 6461    'evaluation_da
-0001bb90: 7461 666c 6f77 5f6d 6178 5f6e 756d 5f77  taflow_max_num_w
-0001bba0: 6f72 6b65 7273 273a 2028 0a20 2020 2020  orkers': (.     
-0001bbb0: 2020 2020 2065 7661 6c75 6174 696f 6e5f       evaluation_
-0001bbc0: 6461 7461 666c 6f77 5f6d 6178 5f6e 756d  dataflow_max_num
-0001bbd0: 5f77 6f72 6b65 7273 0a20 2020 2020 2029  _workers.      )
-0001bbe0: 2c0a 2020 2020 2020 2765 7661 6c75 6174  ,.      'evaluat
-0001bbf0: 696f 6e5f 6461 7461 666c 6f77 5f64 6973  ion_dataflow_dis
-0001bc00: 6b5f 7369 7a65 5f67 6227 3a20 6576 616c  k_size_gb': eval
-0001bc10: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
-0001bc20: 6469 736b 5f73 697a 655f 6762 2c0a 2020  disk_size_gb,.  
-0001bc30: 2020 2020 2764 6174 6166 6c6f 775f 7365      'dataflow_se
-0001bc40: 7276 6963 655f 6163 636f 756e 7427 3a20  rvice_account': 
-0001bc50: 6461 7461 666c 6f77 5f73 6572 7669 6365  dataflow_service
-0001bc60: 5f61 6363 6f75 6e74 2c0a 2020 2020 2020  _account,.      
-0001bc70: 2764 6174 6166 6c6f 775f 7375 626e 6574  'dataflow_subnet
-0001bc80: 776f 726b 273a 2064 6174 6166 6c6f 775f  work': dataflow_
-0001bc90: 7375 626e 6574 776f 726b 2c0a 2020 2020  subnetwork,.    
-0001bca0: 2020 2764 6174 6166 6c6f 775f 7573 655f    'dataflow_use_
-0001bcb0: 7075 626c 6963 5f69 7073 273a 2064 6174  public_ips': dat
-0001bcc0: 6166 6c6f 775f 7573 655f 7075 626c 6963  aflow_use_public
-0001bcd0: 5f69 7073 2c0a 2020 2020 2020 2765 6e63  _ips,.      'enc
-0001bce0: 7279 7074 696f 6e5f 7370 6563 5f6b 6579  ryption_spec_key
-0001bcf0: 5f6e 616d 6527 3a20 656e 6372 7970 7469  _name': encrypti
-0001bd00: 6f6e 5f73 7065 635f 6b65 795f 6e61 6d65  on_spec_key_name
-0001bd10: 2c0a 2020 7d0a 0a20 2066 7465 5f70 6172  ,.  }..  fte_par
-0001bd20: 616d 7320 3d20 7b0a 2020 2020 2020 2764  ams = {.      'd
-0001bd30: 6174 6173 6574 5f6c 6576 656c 5f63 7573  ataset_level_cus
-0001bd40: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
-0001bd50: 6f6e 5f64 6566 696e 6974 696f 6e73 273a  on_definitions':
-0001bd60: 2028 0a20 2020 2020 2020 2020 2064 6174   (.          dat
-0001bd70: 6173 6574 5f6c 6576 656c 5f63 7573 746f  aset_level_custo
-0001bd80: 6d5f 7472 616e 7366 6f72 6d61 7469 6f6e  m_transformation
-0001bd90: 5f64 6566 696e 6974 696f 6e73 0a20 2020  _definitions.   
-0001bda0: 2020 2020 2020 2069 6620 6461 7461 7365         if datase
-0001bdb0: 745f 6c65 7665 6c5f 6375 7374 6f6d 5f74  t_level_custom_t
-0001bdc0: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
-0001bdd0: 6669 6e69 7469 6f6e 730a 2020 2020 2020  finitions.      
-0001bde0: 2020 2020 656c 7365 205b 5d0a 2020 2020      else [].    
-0001bdf0: 2020 292c 0a20 2020 2020 2027 6461 7461    ),.      'data
-0001be00: 7365 745f 6c65 7665 6c5f 7472 616e 7366  set_level_transf
-0001be10: 6f72 6d61 7469 6f6e 7327 3a20 280a 2020  ormations': (.  
-0001be20: 2020 2020 2020 2020 6461 7461 7365 745f          dataset_
-0001be30: 6c65 7665 6c5f 7472 616e 7366 6f72 6d61  level_transforma
-0001be40: 7469 6f6e 7320 6966 2064 6174 6173 6574  tions if dataset
-0001be50: 5f6c 6576 656c 5f74 7261 6e73 666f 726d  _level_transform
-0001be60: 6174 696f 6e73 2065 6c73 6520 5b5d 0a20  ations else []. 
-0001be70: 2020 2020 2029 2c0a 2020 2020 2020 2772       ),.      'r
-0001be80: 756e 5f66 6561 7475 7265 5f73 656c 6563  un_feature_selec
-0001be90: 7469 6f6e 273a 2072 756e 5f66 6561 7475  tion': run_featu
-0001bea0: 7265 5f73 656c 6563 7469 6f6e 2c0a 2020  re_selection,.  
-0001beb0: 2020 2020 2766 6561 7475 7265 5f73 656c      'feature_sel
-0001bec0: 6563 7469 6f6e 5f61 6c67 6f72 6974 686d  ection_algorithm
-0001bed0: 273a 2066 6561 7475 7265 5f73 656c 6563  ': feature_selec
-0001bee0: 7469 6f6e 5f61 6c67 6f72 6974 686d 2c0a  tion_algorithm,.
-0001bef0: 2020 2020 2020 276d 6178 5f73 656c 6563        'max_selec
-0001bf00: 7465 645f 6665 6174 7572 6573 273a 206d  ted_features': m
-0001bf10: 6178 5f73 656c 6563 7465 645f 6665 6174  ax_selected_feat
-0001bf20: 7572 6573 2c0a 2020 2020 2020 2770 7265  ures,.      'pre
-0001bf30: 6465 6669 6e65 645f 7370 6c69 745f 6b65  defined_split_ke
-0001bf40: 7927 3a20 7072 6564 6566 696e 6564 5f73  y': predefined_s
-0001bf50: 706c 6974 5f6b 6579 2c0a 2020 2020 2020  plit_key,.      
-0001bf60: 2773 7472 6174 6966 6965 645f 7370 6c69  'stratified_spli
-0001bf70: 745f 6b65 7927 3a20 7374 7261 7469 6669  t_key': stratifi
-0001bf80: 6564 5f73 706c 6974 5f6b 6579 2c0a 2020  ed_split_key,.  
-0001bf90: 2020 2020 2774 7261 696e 696e 675f 6672      'training_fr
-0001bfa0: 6163 7469 6f6e 273a 2074 7261 696e 696e  action': trainin
-0001bfb0: 675f 6672 6163 7469 6f6e 2c0a 2020 2020  g_fraction,.    
-0001bfc0: 2020 2776 616c 6964 6174 696f 6e5f 6672    'validation_fr
-0001bfd0: 6163 7469 6f6e 273a 2076 616c 6964 6174  action': validat
-0001bfe0: 696f 6e5f 6672 6163 7469 6f6e 2c0a 2020  ion_fraction,.  
-0001bff0: 2020 2020 2774 6573 745f 6672 6163 7469      'test_fracti
-0001c000: 6f6e 273a 2074 6573 745f 6672 6163 7469  on': test_fracti
-0001c010: 6f6e 2c0a 2020 2020 2020 2774 665f 6175  on,.      'tf_au
-0001c020: 746f 5f74 7261 6e73 666f 726d 5f66 6561  to_transform_fea
-0001c030: 7475 7265 7327 3a20 280a 2020 2020 2020  tures': (.      
-0001c040: 2020 2020 7466 5f61 7574 6f5f 7472 616e      tf_auto_tran
-0001c050: 7366 6f72 6d5f 6665 6174 7572 6573 2069  sform_features i
-0001c060: 6620 7466 5f61 7574 6f5f 7472 616e 7366  f tf_auto_transf
-0001c070: 6f72 6d5f 6665 6174 7572 6573 2065 6c73  orm_features els
-0001c080: 6520 7b7d 0a20 2020 2020 2029 2c0a 2020  e {}.      ),.  
-0001c090: 2020 2020 2774 665f 6375 7374 6f6d 5f74      'tf_custom_t
-0001c0a0: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
-0001c0b0: 6669 6e69 7469 6f6e 7327 3a20 280a 2020  finitions': (.  
-0001c0c0: 2020 2020 2020 2020 7466 5f63 7573 746f          tf_custo
-0001c0d0: 6d5f 7472 616e 7366 6f72 6d61 7469 6f6e  m_transformation
-0001c0e0: 5f64 6566 696e 6974 696f 6e73 0a20 2020  _definitions.   
-0001c0f0: 2020 2020 2020 2069 6620 7466 5f63 7573         if tf_cus
-0001c100: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
-0001c110: 6f6e 5f64 6566 696e 6974 696f 6e73 0a20  on_definitions. 
-0001c120: 2020 2020 2020 2020 2065 6c73 6520 5b5d           else []
-0001c130: 0a20 2020 2020 2029 2c0a 2020 2020 2020  .      ),.      
-0001c140: 2774 665f 7472 616e 7366 6f72 6d61 7469  'tf_transformati
-0001c150: 6f6e 735f 7061 7468 273a 2074 665f 7472  ons_path': tf_tr
-0001c160: 616e 7366 6f72 6d61 7469 6f6e 735f 7061  ansformations_pa
-0001c170: 7468 2c0a 2020 2020 2020 276d 6174 6572  th,.      'mater
-0001c180: 6961 6c69 7a65 645f 6578 616d 706c 6573  ialized_examples
-0001c190: 5f66 6f72 6d61 7427 3a20 280a 2020 2020  _format': (.    
-0001c1a0: 2020 2020 2020 6d61 7465 7269 616c 697a        materializ
-0001c1b0: 6564 5f65 7861 6d70 6c65 735f 666f 726d  ed_examples_form
-0001c1c0: 6174 0a20 2020 2020 2020 2020 2069 6620  at.          if 
-0001c1d0: 6d61 7465 7269 616c 697a 6564 5f65 7861  materialized_exa
-0001c1e0: 6d70 6c65 735f 666f 726d 6174 0a20 2020  mples_format.   
-0001c1f0: 2020 2020 2020 2065 6c73 6520 2774 6672         else 'tfr
-0001c200: 6563 6f72 6473 5f67 7a69 7027 0a20 2020  ecords_gzip'.   
-0001c210: 2020 2029 2c0a 2020 2020 2020 2774 665f     ),.      'tf_
-0001c220: 7472 616e 7366 6f72 6d5f 6578 6563 7574  transform_execut
-0001c230: 696f 6e5f 656e 6769 6e65 273a 2028 0a20  ion_engine': (. 
-0001c240: 2020 2020 2020 2020 2074 665f 7472 616e           tf_tran
-0001c250: 7366 6f72 6d5f 6578 6563 7574 696f 6e5f  sform_execution_
-0001c260: 656e 6769 6e65 0a20 2020 2020 2020 2020  engine.         
-0001c270: 2069 6620 7466 5f74 7261 6e73 666f 726d   if tf_transform
-0001c280: 5f65 7865 6375 7469 6f6e 5f65 6e67 696e  _execution_engin
-0001c290: 650a 2020 2020 2020 2020 2020 656c 7365  e.          else
-0001c2a0: 2027 6461 7461 666c 6f77 270a 2020 2020   'dataflow'.    
-0001c2b0: 2020 292c 0a20 207d 0a20 205f 7570 6461    ),.  }.  _upda
-0001c2c0: 7465 5f70 6172 616d 6574 6572 7328 7061  te_parameters(pa
-0001c2d0: 7261 6d65 7465 725f 7661 6c75 6573 2c20  rameter_values, 
-0001c2e0: 6674 655f 7061 7261 6d73 290a 0a20 2064  fte_params)..  d
-0001c2f0: 6174 615f 736f 7572 6365 5f61 6e64 5f73  ata_source_and_s
-0001c300: 706c 6974 5f70 6172 616d 6574 6572 7320  plit_parameters 
-0001c310: 3d20 7b0a 2020 2020 2020 2764 6174 615f  = {.      'data_
-0001c320: 736f 7572 6365 5f63 7376 5f66 696c 656e  source_csv_filen
-0001c330: 616d 6573 273a 2064 6174 615f 736f 7572  ames': data_sour
-0001c340: 6365 5f63 7376 5f66 696c 656e 616d 6573  ce_csv_filenames
-0001c350: 2c0a 2020 2020 2020 2764 6174 615f 736f  ,.      'data_so
-0001c360: 7572 6365 5f62 6967 7175 6572 795f 7461  urce_bigquery_ta
-0001c370: 626c 655f 7061 7468 273a 2064 6174 615f  ble_path': data_
-0001c380: 736f 7572 6365 5f62 6967 7175 6572 795f  source_bigquery_
-0001c390: 7461 626c 655f 7061 7468 2c0a 2020 2020  table_path,.    
-0001c3a0: 2020 2762 6967 7175 6572 795f 7374 6167    'bigquery_stag
-0001c3b0: 696e 675f 6675 6c6c 5f64 6174 6173 6574  ing_full_dataset
-0001c3c0: 5f69 6427 3a20 6269 6771 7565 7279 5f73  _id': bigquery_s
-0001c3d0: 7461 6769 6e67 5f66 756c 6c5f 6461 7461  taging_full_data
-0001c3e0: 7365 745f 6964 2c0a 2020 7d0a 2020 5f75  set_id,.  }.  _u
-0001c3f0: 7064 6174 655f 7061 7261 6d65 7465 7273  pdate_parameters
-0001c400: 2870 6172 616d 6574 6572 5f76 616c 7565  (parameter_value
-0001c410: 732c 2064 6174 615f 736f 7572 6365 5f61  s, data_source_a
-0001c420: 6e64 5f73 706c 6974 5f70 6172 616d 6574  nd_split_paramet
-0001c430: 6572 7329 0a0a 2020 7069 7065 6c69 6e65  ers)..  pipeline
-0001c440: 5f64 6566 696e 6974 696f 6e5f 7061 7468  _definition_path
-0001c450: 203d 206f 732e 7061 7468 2e6a 6f69 6e28   = os.path.join(
-0001c460: 0a20 2020 2020 2070 6174 686c 6962 2e50  .      pathlib.P
-0001c470: 6174 6828 5f5f 6669 6c65 5f5f 292e 7061  ath(__file__).pa
-0001c480: 7265 6e74 2e72 6573 6f6c 7665 2829 2c0a  rent.resolve(),.
-0001c490: 2020 2020 2020 2777 6964 655f 616e 645f        'wide_and_
-0001c4a0: 6465 6570 5f68 7970 6572 7061 7261 6d65  deep_hyperparame
-0001c4b0: 7465 725f 7475 6e69 6e67 5f6a 6f62 5f70  ter_tuning_job_p
-0001c4c0: 6970 656c 696e 652e 7961 6d6c 272c 0a20  ipeline.yaml',. 
-0001c4d0: 2029 0a0a 2020 7265 7475 726e 2070 6970   )..  return pip
-0001c4e0: 656c 696e 655f 6465 6669 6e69 7469 6f6e  eline_definition
-0001c4f0: 5f70 6174 682c 2070 6172 616d 6574 6572  _path, parameter
-0001c500: 5f76 616c 7565 730a 0a0a 6465 6620 6765  _values...def ge
-0001c510: 745f 7461 626e 6574 5f74 7261 696e 6572  t_tabnet_trainer
-0001c520: 5f70 6970 656c 696e 655f 616e 645f 7061  _pipeline_and_pa
-0001c530: 7261 6d65 7465 7273 280a 2020 2020 7072  rameters(.    pr
-0001c540: 6f6a 6563 743a 2073 7472 2c0a 2020 2020  oject: str,.    
-0001c550: 6c6f 6361 7469 6f6e 3a20 7374 722c 0a20  location: str,. 
-0001c560: 2020 2072 6f6f 745f 6469 723a 2073 7472     root_dir: str
-0001c570: 2c0a 2020 2020 7461 7267 6574 5f63 6f6c  ,.    target_col
-0001c580: 756d 6e3a 2073 7472 2c0a 2020 2020 7072  umn: str,.    pr
-0001c590: 6564 6963 7469 6f6e 5f74 7970 653a 2073  ediction_type: s
-0001c5a0: 7472 2c0a 2020 2020 6c65 6172 6e69 6e67  tr,.    learning
-0001c5b0: 5f72 6174 653a 2066 6c6f 6174 2c0a 2020  _rate: float,.  
-0001c5c0: 2020 7472 616e 7366 6f72 6d5f 636f 6e66    transform_conf
-0001c5d0: 6967 3a20 4f70 7469 6f6e 616c 5b73 7472  ig: Optional[str
-0001c5e0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6461  ] = None,.    da
-0001c5f0: 7461 7365 745f 6c65 7665 6c5f 6375 7374  taset_level_cust
-0001c600: 6f6d 5f74 7261 6e73 666f 726d 6174 696f  om_transformatio
-0001c610: 6e5f 6465 6669 6e69 7469 6f6e 733a 204f  n_definitions: O
-0001c620: 7074 696f 6e61 6c5b 0a20 2020 2020 2020  ptional[.       
-0001c630: 204c 6973 745b 4469 6374 5b73 7472 2c20   List[Dict[str, 
-0001c640: 416e 795d 5d0a 2020 2020 5d20 3d20 4e6f  Any]].    ] = No
-0001c650: 6e65 2c0a 2020 2020 6461 7461 7365 745f  ne,.    dataset_
-0001c660: 6c65 7665 6c5f 7472 616e 7366 6f72 6d61  level_transforma
-0001c670: 7469 6f6e 733a 204f 7074 696f 6e61 6c5b  tions: Optional[
-0001c680: 4c69 7374 5b44 6963 745b 7374 722c 2041  List[Dict[str, A
-0001c690: 6e79 5d5d 5d20 3d20 4e6f 6e65 2c0a 2020  ny]]] = None,.  
-0001c6a0: 2020 7275 6e5f 6665 6174 7572 655f 7365    run_feature_se
-0001c6b0: 6c65 6374 696f 6e3a 2062 6f6f 6c20 3d20  lection: bool = 
-0001c6c0: 4661 6c73 652c 0a20 2020 2066 6561 7475  False,.    featu
-0001c6d0: 7265 5f73 656c 6563 7469 6f6e 5f61 6c67  re_selection_alg
-0001c6e0: 6f72 6974 686d 3a20 4f70 7469 6f6e 616c  orithm: Optional
-0001c6f0: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 2020  [str] = None,.  
-0001c700: 2020 6d61 7465 7269 616c 697a 6564 5f65    materialized_e
-0001c710: 7861 6d70 6c65 735f 666f 726d 6174 3a20  xamples_format: 
-0001c720: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
-0001c730: 4e6f 6e65 2c0a 2020 2020 6d61 785f 7365  None,.    max_se
-0001c740: 6c65 6374 6564 5f66 6561 7475 7265 733a  lected_features:
-0001c750: 204f 7074 696f 6e61 6c5b 696e 745d 203d   Optional[int] =
-0001c760: 204e 6f6e 652c 0a20 2020 2070 7265 6465   None,.    prede
-0001c770: 6669 6e65 645f 7370 6c69 745f 6b65 793a  fined_split_key:
-0001c780: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
-0001c790: 204e 6f6e 652c 0a20 2020 2073 7472 6174   None,.    strat
-0001c7a0: 6966 6965 645f 7370 6c69 745f 6b65 793a  ified_split_key:
-0001c7b0: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
-0001c7c0: 204e 6f6e 652c 0a20 2020 2074 7261 696e   None,.    train
-0001c7d0: 696e 675f 6672 6163 7469 6f6e 3a20 4f70  ing_fraction: Op
-0001c7e0: 7469 6f6e 616c 5b66 6c6f 6174 5d20 3d20  tional[float] = 
-0001c7f0: 4e6f 6e65 2c0a 2020 2020 7661 6c69 6461  None,.    valida
-0001c800: 7469 6f6e 5f66 7261 6374 696f 6e3a 204f  tion_fraction: O
-0001c810: 7074 696f 6e61 6c5b 666c 6f61 745d 203d  ptional[float] =
-0001c820: 204e 6f6e 652c 0a20 2020 2074 6573 745f   None,.    test_
-0001c830: 6672 6163 7469 6f6e 3a20 4f70 7469 6f6e  fraction: Option
-0001c840: 616c 5b66 6c6f 6174 5d20 3d20 4e6f 6e65  al[float] = None
-0001c850: 2c0a 2020 2020 7466 5f74 7261 6e73 666f  ,.    tf_transfo
-0001c860: 726d 5f65 7865 6375 7469 6f6e 5f65 6e67  rm_execution_eng
-0001c870: 696e 653a 204f 7074 696f 6e61 6c5b 7374  ine: Optional[st
-0001c880: 725d 203d 204e 6f6e 652c 0a20 2020 2074  r] = None,.    t
-0001c890: 665f 6175 746f 5f74 7261 6e73 666f 726d  f_auto_transform
-0001c8a0: 5f66 6561 7475 7265 733a 204f 7074 696f  _features: Optio
-0001c8b0: 6e61 6c5b 0a20 2020 2020 2020 2055 6e69  nal[.        Uni
-0001c8c0: 6f6e 5b4c 6973 745b 7374 725d 2c20 4469  on[List[str], Di
-0001c8d0: 6374 5b73 7472 2c20 4c69 7374 5b73 7472  ct[str, List[str
-0001c8e0: 5d5d 5d0a 2020 2020 5d20 3d20 4e6f 6e65  ]]].    ] = None
-0001c8f0: 2c0a 2020 2020 7466 5f63 7573 746f 6d5f  ,.    tf_custom_
-0001c900: 7472 616e 7366 6f72 6d61 7469 6f6e 5f64  transformation_d
-0001c910: 6566 696e 6974 696f 6e73 3a20 4f70 7469  efinitions: Opti
-0001c920: 6f6e 616c 5b4c 6973 745b 4469 6374 5b73  onal[List[Dict[s
-0001c930: 7472 2c20 416e 795d 5d5d 203d 204e 6f6e  tr, Any]]] = Non
-0001c940: 652c 0a20 2020 2074 665f 7472 616e 7366  e,.    tf_transf
-0001c950: 6f72 6d61 7469 6f6e 735f 7061 7468 3a20  ormations_path: 
-0001c960: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
-0001c970: 4e6f 6e65 2c0a 2020 2020 6d61 785f 7374  None,.    max_st
-0001c980: 6570 733a 2069 6e74 203d 202d 312c 0a20  eps: int = -1,. 
-0001c990: 2020 206d 6178 5f74 7261 696e 5f73 6563     max_train_sec
-0001c9a0: 733a 2069 6e74 203d 202d 312c 0a20 2020  s: int = -1,.   
-0001c9b0: 206c 6172 6765 5f63 6174 6567 6f72 795f   large_category_
-0001c9c0: 6469 6d3a 2069 6e74 203d 2031 2c0a 2020  dim: int = 1,.  
-0001c9d0: 2020 6c61 7267 655f 6361 7465 676f 7279    large_category
-0001c9e0: 5f74 6872 6573 683a 2069 6e74 203d 2033  _thresh: int = 3
-0001c9f0: 3030 2c0a 2020 2020 7965 6f5f 6a6f 686e  00,.    yeo_john
-0001ca00: 736f 6e5f 7472 616e 7366 6f72 6d3a 2062  son_transform: b
-0001ca10: 6f6f 6c20 3d20 5472 7565 2c0a 2020 2020  ool = True,.    
-0001ca20: 6665 6174 7572 655f 6469 6d3a 2069 6e74  feature_dim: int
-0001ca30: 203d 2036 342c 0a20 2020 2066 6561 7475   = 64,.    featu
-0001ca40: 7265 5f64 696d 5f72 6174 696f 3a20 666c  re_dim_ratio: fl
-0001ca50: 6f61 7420 3d20 302e 352c 0a20 2020 206e  oat = 0.5,.    n
-0001ca60: 756d 5f64 6563 6973 696f 6e5f 7374 6570  um_decision_step
-0001ca70: 733a 2069 6e74 203d 2036 2c0a 2020 2020  s: int = 6,.    
-0001ca80: 7265 6c61 7861 7469 6f6e 5f66 6163 746f  relaxation_facto
-0001ca90: 723a 2066 6c6f 6174 203d 2031 2e35 2c0a  r: float = 1.5,.
-0001caa0: 2020 2020 6465 6361 795f 6576 6572 793a      decay_every:
-0001cab0: 2066 6c6f 6174 203d 2031 3030 2c0a 2020   float = 100,.  
-0001cac0: 2020 6465 6361 795f 7261 7465 3a20 666c    decay_rate: fl
-0001cad0: 6f61 7420 3d20 302e 3935 2c0a 2020 2020  oat = 0.95,.    
-0001cae0: 6772 6164 6965 6e74 5f74 6872 6573 683a  gradient_thresh:
-0001caf0: 2066 6c6f 6174 203d 2032 3030 302c 0a20   float = 2000,. 
-0001cb00: 2020 2073 7061 7273 6974 795f 6c6f 7373     sparsity_loss
-0001cb10: 5f77 6569 6768 743a 2066 6c6f 6174 203d  _weight: float =
-0001cb20: 2030 2e30 3030 3031 2c0a 2020 2020 6261   0.00001,.    ba
-0001cb30: 7463 685f 6d6f 6d65 6e74 756d 3a20 666c  tch_momentum: fl
-0001cb40: 6f61 7420 3d20 302e 3935 2c0a 2020 2020  oat = 0.95,.    
-0001cb50: 6261 7463 685f 7369 7a65 5f72 6174 696f  batch_size_ratio
-0001cb60: 3a20 666c 6f61 7420 3d20 302e 3235 2c0a  : float = 0.25,.
-0001cb70: 2020 2020 6e75 6d5f 7472 616e 7366 6f72      num_transfor
-0001cb80: 6d65 725f 6c61 7965 7273 3a20 696e 7420  mer_layers: int 
-0001cb90: 3d20 342c 0a20 2020 206e 756d 5f74 7261  = 4,.    num_tra
-0001cba0: 6e73 666f 726d 6572 5f6c 6179 6572 735f  nsformer_layers_
-0001cbb0: 7261 7469 6f3a 2066 6c6f 6174 203d 2030  ratio: float = 0
-0001cbc0: 2e32 352c 0a20 2020 2063 6c61 7373 5f77  .25,.    class_w
-0001cbd0: 6569 6768 743a 2066 6c6f 6174 203d 2031  eight: float = 1
-0001cbe0: 2e30 2c0a 2020 2020 6c6f 7373 5f66 756e  .0,.    loss_fun
-0001cbf0: 6374 696f 6e5f 7479 7065 3a20 7374 7220  ction_type: str 
-0001cc00: 3d20 2764 6566 6175 6c74 272c 0a20 2020  = 'default',.   
-0001cc10: 2061 6c70 6861 5f66 6f63 616c 5f6c 6f73   alpha_focal_los
-0001cc20: 733a 2066 6c6f 6174 203d 2030 2e32 352c  s: float = 0.25,
-0001cc30: 0a20 2020 2067 616d 6d61 5f66 6f63 616c  .    gamma_focal
-0001cc40: 5f6c 6f73 733a 2066 6c6f 6174 203d 2032  _loss: float = 2
-0001cc50: 2e30 2c0a 2020 2020 656e 6162 6c65 5f70  .0,.    enable_p
-0001cc60: 726f 6669 6c65 723a 2062 6f6f 6c20 3d20  rofiler: bool = 
-0001cc70: 4661 6c73 652c 0a20 2020 2063 6163 6865  False,.    cache
-0001cc80: 5f64 6174 613a 2073 7472 203d 2027 6175  _data: str = 'au
-0001cc90: 746f 272c 0a20 2020 2073 6565 643a 2069  to',.    seed: i
-0001cca0: 6e74 203d 2031 2c0a 2020 2020 6576 616c  nt = 1,.    eval
-0001ccb0: 5f73 7465 7073 3a20 696e 7420 3d20 302c  _steps: int = 0,
-0001ccc0: 0a20 2020 2062 6174 6368 5f73 697a 653a  .    batch_size:
-0001ccd0: 2069 6e74 203d 2031 3030 2c0a 2020 2020   int = 100,.    
-0001cce0: 6d65 6173 7572 656d 656e 745f 7365 6c65  measurement_sele
-0001ccf0: 6374 696f 6e5f 7479 7065 3a20 4f70 7469  ction_type: Opti
-0001cd00: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
-0001cd10: 2c0a 2020 2020 6f70 7469 6d69 7a61 7469  ,.    optimizati
-0001cd20: 6f6e 5f6d 6574 7269 633a 204f 7074 696f  on_metric: Optio
-0001cd30: 6e61 6c5b 7374 725d 203d 204e 6f6e 652c  nal[str] = None,
-0001cd40: 0a20 2020 2065 7661 6c5f 6672 6571 7565  .    eval_freque
-0001cd50: 6e63 795f 7365 6373 3a20 696e 7420 3d20  ncy_secs: int = 
-0001cd60: 3630 302c 0a20 2020 2064 6174 615f 736f  600,.    data_so
-0001cd70: 7572 6365 5f63 7376 5f66 696c 656e 616d  urce_csv_filenam
-0001cd80: 6573 3a20 4f70 7469 6f6e 616c 5b73 7472  es: Optional[str
-0001cd90: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6461  ] = None,.    da
-0001cda0: 7461 5f73 6f75 7263 655f 6269 6771 7565  ta_source_bigque
-0001cdb0: 7279 5f74 6162 6c65 5f70 6174 683a 204f  ry_table_path: O
-0001cdc0: 7074 696f 6e61 6c5b 7374 725d 203d 204e  ptional[str] = N
-0001cdd0: 6f6e 652c 0a20 2020 2062 6967 7175 6572  one,.    bigquer
-0001cde0: 795f 7374 6167 696e 675f 6675 6c6c 5f64  y_staging_full_d
-0001cdf0: 6174 6173 6574 5f69 643a 204f 7074 696f  ataset_id: Optio
-0001ce00: 6e61 6c5b 7374 725d 203d 204e 6f6e 652c  nal[str] = None,
-0001ce10: 0a20 2020 2077 6569 6768 745f 636f 6c75  .    weight_colu
-0001ce20: 6d6e 3a20 7374 7220 3d20 2727 2c0a 2020  mn: str = '',.  
-0001ce30: 2020 7472 616e 7366 6f72 6d5f 6461 7461    transform_data
-0001ce40: 666c 6f77 5f6d 6163 6869 6e65 5f74 7970  flow_machine_typ
-0001ce50: 653a 2073 7472 203d 2027 6e31 2d73 7461  e: str = 'n1-sta
-0001ce60: 6e64 6172 642d 3136 272c 0a20 2020 2074  ndard-16',.    t
-0001ce70: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
+0000f2c0: 7472 2c20 416e 795d 5d3a 0a20 2023 2066  tr, Any]]:.  # f
+0000f2d0: 6d74 3a20 6f66 660a 2020 2222 2247 6574  mt: off.  """Get
+0000f2e0: 2074 6865 2057 6964 6520 2620 4465 6570   the Wide & Deep
+0000f2f0: 2074 7261 696e 696e 6720 7069 7065 6c69   training pipeli
+0000f300: 6e65 2e0a 0a20 2041 7267 733a 0a20 2020  ne...  Args:.   
+0000f310: 2070 726f 6a65 6374 3a20 5468 6520 4743   project: The GC
+0000f320: 5020 7072 6f6a 6563 7420 7468 6174 2072  P project that r
+0000f330: 756e 7320 7468 6520 7069 7065 6c69 6e65  uns the pipeline
+0000f340: 2063 6f6d 706f 6e65 6e74 732e 0a20 2020   components..   
+0000f350: 206c 6f63 6174 696f 6e3a 2054 6865 2047   location: The G
+0000f360: 4350 2072 6567 696f 6e20 7468 6174 2072  CP region that r
+0000f370: 756e 7320 7468 6520 7069 7065 6c69 6e65  uns the pipeline
+0000f380: 2063 6f6d 706f 6e65 6e74 732e 0a20 2020   components..   
+0000f390: 2072 6f6f 745f 6469 723a 2054 6865 2072   root_dir: The r
+0000f3a0: 6f6f 7420 4743 5320 6469 7265 6374 6f72  oot GCS director
+0000f3b0: 7920 666f 7220 7468 6520 7069 7065 6c69  y for the pipeli
+0000f3c0: 6e65 2063 6f6d 706f 6e65 6e74 732e 0a20  ne components.. 
+0000f3d0: 2020 2074 6172 6765 745f 636f 6c75 6d6e     target_column
+0000f3e0: 3a20 5468 6520 7461 7267 6574 2063 6f6c  : The target col
+0000f3f0: 756d 6e20 6e61 6d65 2e0a 2020 2020 7072  umn name..    pr
+0000f400: 6564 6963 7469 6f6e 5f74 7970 653a 2054  ediction_type: T
+0000f410: 6865 2074 7970 6520 6f66 2070 7265 6469  he type of predi
+0000f420: 6374 696f 6e20 7468 6520 6d6f 6465 6c20  ction the model 
+0000f430: 6973 2074 6f20 7072 6f64 7563 652e 2027  is to produce. '
+0000f440: 636c 6173 7369 6669 6361 7469 6f6e 2720  classification' 
+0000f450: 6f72 2027 7265 6772 6573 7369 6f6e 272e  or 'regression'.
+0000f460: 0a20 2020 206c 6561 726e 696e 675f 7261  .    learning_ra
+0000f470: 7465 3a20 5468 6520 6c65 6172 6e69 6e67  te: The learning
+0000f480: 2072 6174 6520 7573 6564 2062 7920 7468   rate used by th
+0000f490: 6520 6c69 6e65 6172 206f 7074 696d 697a  e linear optimiz
+0000f4a0: 6572 2e0a 2020 2020 646e 6e5f 6c65 6172  er..    dnn_lear
+0000f4b0: 6e69 6e67 5f72 6174 653a 2054 6865 206c  ning_rate: The l
+0000f4c0: 6561 726e 696e 6720 7261 7465 2066 6f72  earning rate for
+0000f4d0: 2074 7261 696e 696e 6720 7468 6520 6465   training the de
+0000f4e0: 6570 2070 6172 7420 6f66 2074 6865 206d  ep part of the m
+0000f4f0: 6f64 656c 2e0a 2020 2020 7472 616e 7366  odel..    transf
+0000f500: 6f72 6d5f 636f 6e66 6967 3a20 5061 7468  orm_config: Path
+0000f510: 2074 6f20 7631 2054 4620 7472 616e 7366   to v1 TF transf
+0000f520: 6f72 6d61 7469 6f6e 2063 6f6e 6669 6775  ormation configu
+0000f530: 7261 7469 6f6e 2e0a 2020 2020 6461 7461  ration..    data
+0000f540: 7365 745f 6c65 7665 6c5f 6375 7374 6f6d  set_level_custom
+0000f550: 5f74 7261 6e73 666f 726d 6174 696f 6e5f  _transformation_
+0000f560: 6465 6669 6e69 7469 6f6e 733a 2044 6174  definitions: Dat
+0000f570: 6173 6574 2d6c 6576 656c 2063 7573 746f  aset-level custo
+0000f580: 6d20 7472 616e 7366 6f72 6d61 7469 6f6e  m transformation
+0000f590: 2064 6566 696e 6974 696f 6e73 2069 6e20   definitions in 
+0000f5a0: 7374 7269 6e67 2066 6f72 6d61 742e 0a20  string format.. 
+0000f5b0: 2020 2064 6174 6173 6574 5f6c 6576 656c     dataset_level
+0000f5c0: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
+0000f5d0: 3a20 4461 7461 7365 742d 6c65 7665 6c20  : Dataset-level 
+0000f5e0: 7472 616e 7366 6f72 6d61 7469 6f6e 2063  transformation c
+0000f5f0: 6f6e 6669 6775 7261 7469 6f6e 2069 6e20  onfiguration in 
+0000f600: 7374 7269 6e67 2066 6f72 6d61 742e 0a20  string format.. 
+0000f610: 2020 2072 756e 5f66 6561 7475 7265 5f73     run_feature_s
+0000f620: 656c 6563 7469 6f6e 3a20 5768 6574 6865  election: Whethe
+0000f630: 7220 746f 2065 6e61 626c 6520 6665 6174  r to enable feat
+0000f640: 7572 6520 7365 6c65 6374 696f 6e2e 0a20  ure selection.. 
+0000f650: 2020 2066 6561 7475 7265 5f73 656c 6563     feature_selec
+0000f660: 7469 6f6e 5f61 6c67 6f72 6974 686d 3a20  tion_algorithm: 
+0000f670: 4665 6174 7572 6520 7365 6c65 6374 696f  Feature selectio
+0000f680: 6e20 616c 676f 7269 7468 6d2e 0a20 2020  n algorithm..   
+0000f690: 206d 6174 6572 6961 6c69 7a65 645f 6578   materialized_ex
+0000f6a0: 616d 706c 6573 5f66 6f72 6d61 743a 2054  amples_format: T
+0000f6b0: 6865 2066 6f72 6d61 7420 666f 7220 7468  he format for th
+0000f6c0: 6520 6d61 7465 7269 616c 697a 6564 2065  e materialized e
+0000f6d0: 7861 6d70 6c65 732e 0a20 2020 206d 6178  xamples..    max
+0000f6e0: 5f73 656c 6563 7465 645f 6665 6174 7572  _selected_featur
+0000f6f0: 6573 3a20 4d61 7869 6d75 6d20 6e75 6d62  es: Maximum numb
+0000f700: 6572 206f 6620 6665 6174 7572 6573 2074  er of features t
+0000f710: 6f20 7365 6c65 6374 2e0a 2020 2020 7072  o select..    pr
+0000f720: 6564 6566 696e 6564 5f73 706c 6974 5f6b  edefined_split_k
+0000f730: 6579 3a20 5072 6564 6566 696e 6564 2073  ey: Predefined s
+0000f740: 706c 6974 206b 6579 2e0a 2020 2020 7374  plit key..    st
+0000f750: 7261 7469 6669 6564 5f73 706c 6974 5f6b  ratified_split_k
+0000f760: 6579 3a20 5374 7261 7469 6669 6564 2073  ey: Stratified s
+0000f770: 706c 6974 206b 6579 2e0a 2020 2020 7472  plit key..    tr
+0000f780: 6169 6e69 6e67 5f66 7261 6374 696f 6e3a  aining_fraction:
+0000f790: 2054 7261 696e 696e 6720 6672 6163 7469   Training fracti
+0000f7a0: 6f6e 2e0a 2020 2020 7661 6c69 6461 7469  on..    validati
+0000f7b0: 6f6e 5f66 7261 6374 696f 6e3a 2056 616c  on_fraction: Val
+0000f7c0: 6964 6174 696f 6e20 6672 6163 7469 6f6e  idation fraction
+0000f7d0: 2e0a 2020 2020 7465 7374 5f66 7261 6374  ..    test_fract
+0000f7e0: 696f 6e3a 2054 6573 7420 6672 6163 7469  ion: Test fracti
+0000f7f0: 6f6e 2e0a 2020 2020 7466 5f74 7261 6e73  on..    tf_trans
+0000f800: 666f 726d 5f65 7865 6375 7469 6f6e 5f65  form_execution_e
+0000f810: 6e67 696e 653a 2054 6865 2065 7865 6375  ngine: The execu
+0000f820: 7469 6f6e 2065 6e67 696e 6520 7573 6564  tion engine used
+0000f830: 2074 6f20 6578 6563 7574 6520 5446 2d62   to execute TF-b
+0000f840: 6173 6564 2074 7261 6e73 666f 726d 6174  ased transformat
+0000f850: 696f 6e73 2e0a 2020 2020 7466 5f61 7574  ions..    tf_aut
+0000f860: 6f5f 7472 616e 7366 6f72 6d5f 6665 6174  o_transform_feat
+0000f870: 7572 6573 3a20 4c69 7374 206f 6620 6175  ures: List of au
+0000f880: 746f 2074 7261 6e73 666f 726d 2066 6561  to transform fea
+0000f890: 7475 7265 7320 696e 2074 6865 2063 6f6d  tures in the com
+0000f8a0: 6d61 2d73 6570 6172 6174 6564 2073 7472  ma-separated str
+0000f8b0: 696e 6720 666f 726d 6174 2e0a 2020 2020  ing format..    
+0000f8c0: 7466 5f63 7573 746f 6d5f 7472 616e 7366  tf_custom_transf
+0000f8d0: 6f72 6d61 7469 6f6e 5f64 6566 696e 6974  ormation_definit
+0000f8e0: 696f 6e73 3a20 5446 2063 7573 746f 6d20  ions: TF custom 
+0000f8f0: 7472 616e 7366 6f72 6d61 7469 6f6e 2064  transformation d
+0000f900: 6566 696e 6974 696f 6e73 2069 6e20 7374  efinitions in st
+0000f910: 7269 6e67 2066 6f72 6d61 742e 0a20 2020  ring format..   
+0000f920: 2074 665f 7472 616e 7366 6f72 6d61 7469   tf_transformati
+0000f930: 6f6e 735f 7061 7468 3a20 5061 7468 2074  ons_path: Path t
+0000f940: 6f20 5446 2074 7261 6e73 666f 726d 6174  o TF transformat
+0000f950: 696f 6e20 636f 6e66 6967 7572 6174 696f  ion configuratio
+0000f960: 6e2e 0a20 2020 206f 7074 696d 697a 6572  n..    optimizer
+0000f970: 5f74 7970 653a 2054 6865 2074 7970 6520  _type: The type 
+0000f980: 6f66 206f 7074 696d 697a 6572 2074 6f20  of optimizer to 
+0000f990: 7573 652e 2043 686f 6963 6573 2061 7265  use. Choices are
+0000f9a0: 2022 6164 616d 222c 2022 6674 726c 2220   "adam", "ftrl" 
+0000f9b0: 616e 6420 2273 6764 2220 666f 7220 7468  and "sgd" for th
+0000f9c0: 6520 4164 616d 2c20 4654 524c 2c20 616e  e Adam, FTRL, an
+0000f9d0: 6420 4772 6164 6965 6e74 2044 6573 6365  d Gradient Desce
+0000f9e0: 6e74 204f 7074 696d 697a 6572 732c 2072  nt Optimizers, r
+0000f9f0: 6573 7065 6374 6976 656c 792e 0a20 2020  espectively..   
+0000fa00: 206d 6178 5f73 7465 7073 3a20 4e75 6d62   max_steps: Numb
+0000fa10: 6572 206f 6620 7374 6570 7320 746f 2072  er of steps to r
+0000fa20: 756e 2074 6865 2074 7261 696e 6572 2066  un the trainer f
+0000fa30: 6f72 2e0a 2020 2020 6d61 785f 7472 6169  or..    max_trai
+0000fa40: 6e5f 7365 6373 3a20 416d 6f75 6e74 206f  n_secs: Amount o
+0000fa50: 6620 7469 6d65 2069 6e20 7365 636f 6e64  f time in second
+0000fa60: 7320 746f 2072 756e 2074 6865 2074 7261  s to run the tra
+0000fa70: 696e 6572 2066 6f72 2e0a 2020 2020 6c31  iner for..    l1
+0000fa80: 5f72 6567 756c 6172 697a 6174 696f 6e5f  _regularization_
+0000fa90: 7374 7265 6e67 7468 3a20 4c31 2072 6567  strength: L1 reg
+0000faa0: 756c 6172 697a 6174 696f 6e20 7374 7265  ularization stre
+0000fab0: 6e67 7468 2066 6f72 206f 7074 696d 697a  ngth for optimiz
+0000fac0: 6572 5f74 7970 653d 2266 7472 6c22 2e0a  er_type="ftrl"..
+0000fad0: 2020 2020 6c32 5f72 6567 756c 6172 697a      l2_regulariz
+0000fae0: 6174 696f 6e5f 7374 7265 6e67 7468 3a20  ation_strength: 
+0000faf0: 4c32 2072 6567 756c 6172 697a 6174 696f  L2 regularizatio
+0000fb00: 6e20 7374 7265 6e67 7468 2066 6f72 206f  n strength for o
+0000fb10: 7074 696d 697a 6572 5f74 7970 653d 2266  ptimizer_type="f
+0000fb20: 7472 6c22 2e0a 2020 2020 6c32 5f73 6872  trl"..    l2_shr
+0000fb30: 696e 6b61 6765 5f72 6567 756c 6172 697a  inkage_regulariz
+0000fb40: 6174 696f 6e5f 7374 7265 6e67 7468 3a20  ation_strength: 
+0000fb50: 4c32 2073 6872 696e 6b61 6765 2072 6567  L2 shrinkage reg
+0000fb60: 756c 6172 697a 6174 696f 6e20 7374 7265  ularization stre
+0000fb70: 6e67 7468 2066 6f72 206f 7074 696d 697a  ngth for optimiz
+0000fb80: 6572 5f74 7970 653d 2266 7472 6c22 2e0a  er_type="ftrl"..
+0000fb90: 2020 2020 6265 7461 5f31 3a20 4265 7461      beta_1: Beta
+0000fba0: 2031 2076 616c 7565 2066 6f72 206f 7074   1 value for opt
+0000fbb0: 696d 697a 6572 5f74 7970 653d 2261 6461  imizer_type="ada
+0000fbc0: 6d22 2e0a 2020 2020 6265 7461 5f32 3a20  m"..    beta_2: 
+0000fbd0: 4265 7461 2032 2076 616c 7565 2066 6f72  Beta 2 value for
+0000fbe0: 206f 7074 696d 697a 6572 5f74 7970 653d   optimizer_type=
+0000fbf0: 2261 6461 6d22 2e0a 2020 2020 6869 6464  "adam"..    hidd
+0000fc00: 656e 5f75 6e69 7473 3a20 4869 6464 656e  en_units: Hidden
+0000fc10: 206c 6179 6572 2073 697a 6573 2074 6f20   layer sizes to 
+0000fc20: 7573 6520 666f 7220 444e 4e20 6665 6174  use for DNN feat
+0000fc30: 7572 6520 636f 6c75 6d6e 732c 2070 726f  ure columns, pro
+0000fc40: 7669 6465 6420 696e 2063 6f6d 6d61 2d73  vided in comma-s
+0000fc50: 6570 6172 6174 6564 206c 6179 6572 732e  eparated layers.
+0000fc60: 0a20 2020 2075 7365 5f77 6964 653a 2049  .    use_wide: I
+0000fc70: 6620 7365 7420 746f 2074 7275 652c 2074  f set to true, t
+0000fc80: 6865 2063 6174 6567 6f72 6963 616c 2063  he categorical c
+0000fc90: 6f6c 756d 6e73 2077 696c 6c20 6265 2075  olumns will be u
+0000fca0: 7365 6420 696e 2074 6865 2077 6964 6520  sed in the wide 
+0000fcb0: 7061 7274 206f 6620 7468 6520 444e 4e20  part of the DNN 
+0000fcc0: 6d6f 6465 6c2e 0a20 2020 2065 6d62 6564  model..    embed
+0000fcd0: 5f63 6174 6567 6f72 6965 733a 2049 6620  _categories: If 
+0000fce0: 7365 7420 746f 2074 7275 652c 2074 6865  set to true, the
+0000fcf0: 2063 6174 6567 6f72 6963 616c 2063 6f6c   categorical col
+0000fd00: 756d 6e73 2077 696c 6c20 6265 2075 7365  umns will be use
+0000fd10: 6420 656d 6265 6464 6564 2061 6e64 2075  d embedded and u
+0000fd20: 7365 6420 696e 2074 6865 2064 6565 7020  sed in the deep 
+0000fd30: 7061 7274 206f 6620 7468 6520 6d6f 6465  part of the mode
+0000fd40: 6c2e 2045 6d62 6564 6469 6e67 2073 697a  l. Embedding siz
+0000fd50: 6520 6973 2074 6865 2073 7175 6172 6520  e is the square 
+0000fd60: 726f 6f74 206f 6620 7468 6520 636f 6c75  root of the colu
+0000fd70: 6d6e 2063 6172 6469 6e61 6c69 7479 2e0a  mn cardinality..
+0000fd80: 2020 2020 646e 6e5f 6472 6f70 6f75 743a      dnn_dropout:
+0000fd90: 2054 6865 2070 726f 6261 6269 6c69 7479   The probability
+0000fda0: 2077 6520 7769 6c6c 2064 726f 7020 6f75   we will drop ou
+0000fdb0: 7420 6120 6769 7665 6e20 636f 6f72 6469  t a given coordi
+0000fdc0: 6e61 7465 2e0a 2020 2020 646e 6e5f 6f70  nate..    dnn_op
+0000fdd0: 7469 6d69 7a65 725f 7479 7065 3a20 5468  timizer_type: Th
+0000fde0: 6520 7479 7065 206f 6620 6f70 7469 6d69  e type of optimi
+0000fdf0: 7a65 7220 746f 2075 7365 2066 6f72 2074  zer to use for t
+0000fe00: 6865 2064 6565 7020 7061 7274 206f 6620  he deep part of 
+0000fe10: 7468 6520 6d6f 6465 6c2e 2043 686f 6963  the model. Choic
+0000fe20: 6573 2061 7265 2022 6164 616d 222c 2022  es are "adam", "
+0000fe30: 6674 726c 2220 616e 6420 2273 6764 222e  ftrl" and "sgd".
+0000fe40: 2066 6f72 2074 6865 2041 6461 6d2c 2046   for the Adam, F
+0000fe50: 5452 4c2c 2061 6e64 2047 7261 6469 656e  TRL, and Gradien
+0000fe60: 7420 4465 7363 656e 7420 4f70 7469 6d69  t Descent Optimi
+0000fe70: 7a65 7273 2c20 7265 7370 6563 7469 7665  zers, respective
+0000fe80: 6c79 2e0a 2020 2020 646e 6e5f 6c31 5f72  ly..    dnn_l1_r
+0000fe90: 6567 756c 6172 697a 6174 696f 6e5f 7374  egularization_st
+0000fea0: 7265 6e67 7468 3a20 4c31 2072 6567 756c  rength: L1 regul
+0000feb0: 6172 697a 6174 696f 6e20 7374 7265 6e67  arization streng
+0000fec0: 7468 2066 6f72 2064 6e6e 5f6f 7074 696d  th for dnn_optim
+0000fed0: 697a 6572 5f74 7970 653d 2266 7472 6c22  izer_type="ftrl"
+0000fee0: 2e0a 2020 2020 646e 6e5f 6c32 5f72 6567  ..    dnn_l2_reg
+0000fef0: 756c 6172 697a 6174 696f 6e5f 7374 7265  ularization_stre
+0000ff00: 6e67 7468 3a20 4c32 2072 6567 756c 6172  ngth: L2 regular
+0000ff10: 697a 6174 696f 6e20 7374 7265 6e67 7468  ization strength
+0000ff20: 2066 6f72 2064 6e6e 5f6f 7074 696d 697a   for dnn_optimiz
+0000ff30: 6572 5f74 7970 653d 2266 7472 6c22 2e0a  er_type="ftrl"..
+0000ff40: 2020 2020 646e 6e5f 6c32 5f73 6872 696e      dnn_l2_shrin
+0000ff50: 6b61 6765 5f72 6567 756c 6172 697a 6174  kage_regularizat
+0000ff60: 696f 6e5f 7374 7265 6e67 7468 3a20 4c32  ion_strength: L2
+0000ff70: 2073 6872 696e 6b61 6765 2072 6567 756c   shrinkage regul
+0000ff80: 6172 697a 6174 696f 6e20 7374 7265 6e67  arization streng
+0000ff90: 7468 2066 6f72 2064 6e6e 5f6f 7074 696d  th for dnn_optim
+0000ffa0: 697a 6572 5f74 7970 653d 2266 7472 6c22  izer_type="ftrl"
+0000ffb0: 2e0a 2020 2020 646e 6e5f 6265 7461 5f31  ..    dnn_beta_1
+0000ffc0: 3a20 4265 7461 2031 2076 616c 7565 2066  : Beta 1 value f
+0000ffd0: 6f72 2064 6e6e 5f6f 7074 696d 697a 6572  or dnn_optimizer
+0000ffe0: 5f74 7970 653d 2261 6461 6d22 2e0a 2020  _type="adam"..  
+0000fff0: 2020 646e 6e5f 6265 7461 5f32 3a20 4265    dnn_beta_2: Be
+00010000: 7461 2032 2076 616c 7565 2066 6f72 2064  ta 2 value for d
+00010010: 6e6e 5f6f 7074 696d 697a 6572 5f74 7970  nn_optimizer_typ
+00010020: 653d 2261 6461 6d22 2e0a 2020 2020 656e  e="adam"..    en
+00010030: 6162 6c65 5f70 726f 6669 6c65 723a 2045  able_profiler: E
+00010040: 6e61 626c 6573 2070 726f 6669 6c69 6e67  nables profiling
+00010050: 2061 6e64 2073 6176 6573 2061 2074 7261   and saves a tra
+00010060: 6365 2064 7572 696e 6720 6576 616c 7561  ce during evalua
+00010070: 7469 6f6e 2e0a 2020 2020 6361 6368 655f  tion..    cache_
+00010080: 6461 7461 3a20 5768 6574 6865 7220 746f  data: Whether to
+00010090: 2063 6163 6865 2064 6174 6120 6f72 206e   cache data or n
+000100a0: 6f74 2e20 4966 2073 6574 2074 6f20 2761  ot. If set to 'a
+000100b0: 7574 6f27 2c20 6361 6368 696e 6720 6973  uto', caching is
+000100c0: 2064 6574 6572 6d69 6e65 6420 6261 7365   determined base
+000100d0: 6420 6f6e 2074 6865 2064 6174 6173 6574  d on the dataset
+000100e0: 2073 697a 652e 0a20 2020 2073 6565 643a   size..    seed:
+000100f0: 2053 6565 6420 746f 2062 6520 7573 6564   Seed to be used
+00010100: 2066 6f72 2074 6869 7320 7275 6e2e 0a20   for this run.. 
+00010110: 2020 2065 7661 6c5f 7374 6570 733a 204e     eval_steps: N
+00010120: 756d 6265 7220 6f66 2073 7465 7073 2074  umber of steps t
+00010130: 6f20 7275 6e20 6576 616c 7561 7469 6f6e  o run evaluation
+00010140: 2066 6f72 2e20 4966 206e 6f74 2073 7065   for. If not spe
+00010150: 6369 6669 6564 206f 7220 6e65 6761 7469  cified or negati
+00010160: 7665 2c20 6974 206d 6561 6e73 2072 756e  ve, it means run
+00010170: 2065 7661 6c75 6174 696f 6e20 6f6e 2074   evaluation on t
+00010180: 6865 2077 686f 6c65 2076 616c 6964 6174  he whole validat
+00010190: 696f 6e20 6461 7461 7365 742e 2049 6620  ion dataset. If 
+000101a0: 7365 7420 746f 2030 2c20 6974 206d 6561  set to 0, it mea
+000101b0: 6e73 2072 756e 2065 7661 6c75 6174 696f  ns run evaluatio
+000101c0: 6e20 666f 7220 6120 6669 7865 6420 6e75  n for a fixed nu
+000101d0: 6d62 6572 206f 6620 7361 6d70 6c65 732e  mber of samples.
+000101e0: 0a20 2020 2062 6174 6368 5f73 697a 653a  .    batch_size:
+000101f0: 2042 6174 6368 2073 697a 6520 666f 7220   Batch size for 
+00010200: 7472 6169 6e69 6e67 2e0a 2020 2020 6d65  training..    me
+00010210: 6173 7572 656d 656e 745f 7365 6c65 6374  asurement_select
+00010220: 696f 6e5f 7479 7065 3a20 5768 6963 6820  ion_type: Which 
+00010230: 6d65 6173 7572 656d 656e 7420 746f 2075  measurement to u
+00010240: 7365 2069 662f 7768 656e 2074 6865 2073  se if/when the s
+00010250: 6572 7669 6365 2061 7574 6f6d 6174 6963  ervice automatic
+00010260: 616c 6c79 2073 656c 6563 7473 2074 6865  ally selects the
+00010270: 2066 696e 616c 206d 6561 7375 7265 6d65   final measureme
+00010280: 6e74 2066 726f 6d20 7072 6576 696f 7573  nt from previous
+00010290: 6c79 2072 6570 6f72 7465 6420 696e 7465  ly reported inte
+000102a0: 726d 6564 6961 7465 206d 6561 7375 7265  rmediate measure
+000102b0: 6d65 6e74 732e 204f 6e65 206f 6620 2242  ments. One of "B
+000102c0: 4553 545f 4d45 4153 5552 454d 454e 5422  EST_MEASUREMENT"
+000102d0: 206f 7220 224c 4153 545f 4d45 4153 5552   or "LAST_MEASUR
+000102e0: 454d 454e 5422 2e0a 2020 2020 6f70 7469  EMENT"..    opti
+000102f0: 6d69 7a61 7469 6f6e 5f6d 6574 7269 633a  mization_metric:
+00010300: 204f 7074 696d 697a 6174 696f 6e20 6d65   Optimization me
+00010310: 7472 6963 2075 7365 6420 666f 7220 606d  tric used for `m
+00010320: 6561 7375 7265 6d65 6e74 5f73 656c 6563  easurement_selec
+00010330: 7469 6f6e 5f74 7970 6560 2e20 4465 6661  tion_type`. Defa
+00010340: 756c 7420 6973 2022 726d 7365 2220 666f  ult is "rmse" fo
+00010350: 7220 7265 6772 6573 7369 6f6e 2061 6e64  r regression and
+00010360: 2022 6175 6322 2066 6f72 2063 6c61 7373   "auc" for class
+00010370: 6966 6963 6174 696f 6e2e 0a20 2020 2065  ification..    e
+00010380: 7661 6c5f 6672 6571 7565 6e63 795f 7365  val_frequency_se
+00010390: 6373 3a20 4672 6571 7565 6e63 7920 6174  cs: Frequency at
+000103a0: 2077 6869 6368 2065 7661 6c75 6174 696f   which evaluatio
+000103b0: 6e20 616e 6420 6368 6563 6b70 6f69 6e74  n and checkpoint
+000103c0: 696e 6720 7769 6c6c 2074 616b 6520 706c  ing will take pl
+000103d0: 6163 652e 0a20 2020 2064 6174 615f 736f  ace..    data_so
+000103e0: 7572 6365 5f63 7376 5f66 696c 656e 616d  urce_csv_filenam
+000103f0: 6573 3a20 5468 6520 4353 5620 6461 7461  es: The CSV data
+00010400: 2073 6f75 7263 652e 0a20 2020 2064 6174   source..    dat
+00010410: 615f 736f 7572 6365 5f62 6967 7175 6572  a_source_bigquer
+00010420: 795f 7461 626c 655f 7061 7468 3a20 5468  y_table_path: Th
+00010430: 6520 4269 6751 7565 7279 2064 6174 6120  e BigQuery data 
+00010440: 736f 7572 6365 2e0a 2020 2020 6269 6771  source..    bigq
+00010450: 7565 7279 5f73 7461 6769 6e67 5f66 756c  uery_staging_ful
+00010460: 6c5f 6461 7461 7365 745f 6964 3a20 5468  l_dataset_id: Th
+00010470: 6520 4269 6751 7565 7279 2073 7461 6769  e BigQuery stagi
+00010480: 6e67 2066 756c 6c20 6461 7461 7365 7420  ng full dataset 
+00010490: 6964 2066 6f72 2073 746f 7269 6e67 2069  id for storing i
+000104a0: 6e74 6572 6d65 6469 6174 6520 7461 626c  ntermediate tabl
+000104b0: 6573 2e0a 2020 2020 7765 6967 6874 5f63  es..    weight_c
+000104c0: 6f6c 756d 6e3a 2054 6865 2077 6569 6768  olumn: The weigh
+000104d0: 7420 636f 6c75 6d6e 206e 616d 652e 0a20  t column name.. 
+000104e0: 2020 2074 7261 6e73 666f 726d 5f64 6174     transform_dat
+000104f0: 6166 6c6f 775f 6d61 6368 696e 655f 7479  aflow_machine_ty
+00010500: 7065 3a20 5468 6520 6461 7461 666c 6f77  pe: The dataflow
+00010510: 206d 6163 6869 6e65 2074 7970 6520 666f   machine type fo
+00010520: 7220 7472 616e 7366 6f72 6d20 636f 6d70  r transform comp
+00010530: 6f6e 656e 742e 0a20 2020 2074 7261 6e73  onent..    trans
+00010540: 666f 726d 5f64 6174 6166 6c6f 775f 6d61  form_dataflow_ma
+00010550: 785f 6e75 6d5f 776f 726b 6572 733a 2054  x_num_workers: T
+00010560: 6865 206d 6178 206e 756d 6265 7220 6f66  he max number of
+00010570: 2044 6174 6166 6c6f 7720 776f 726b 6572   Dataflow worker
+00010580: 7320 666f 7220 7472 616e 7366 6f72 6d20  s for transform 
+00010590: 636f 6d70 6f6e 656e 742e 0a20 2020 2074  component..    t
+000105a0: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
+000105b0: 775f 6469 736b 5f73 697a 655f 6762 3a20  w_disk_size_gb: 
+000105c0: 4461 7461 666c 6f77 2077 6f72 6b65 7227  Dataflow worker'
+000105d0: 7320 6469 736b 2073 697a 6520 696e 2047  s disk size in G
+000105e0: 4220 666f 7220 7472 616e 7366 6f72 6d20  B for transform 
+000105f0: 636f 6d70 6f6e 656e 742e 0a20 2020 2077  component..    w
+00010600: 6f72 6b65 725f 706f 6f6c 5f73 7065 6373  orker_pool_specs
+00010610: 5f6f 7665 7272 6964 653a 2054 6865 2064  _override: The d
+00010620: 6963 7469 6f6e 6172 7920 666f 7220 6f76  ictionary for ov
+00010630: 6572 7269 6469 6e67 2074 7261 696e 696e  erriding trainin
+00010640: 6720 616e 6420 6576 616c 7561 7469 6f6e  g and evaluation
+00010650: 2077 6f72 6b65 7220 706f 6f6c 2073 7065   worker pool spe
+00010660: 6373 2e20 5468 6520 6469 6374 696f 6e61  cs. The dictiona
+00010670: 7279 2073 686f 756c 6420 6265 206f 6620  ry should be of 
+00010680: 666f 726d 6174 2068 7474 7073 3a2f 2f67  format https://g
+00010690: 6974 6875 622e 636f 6d2f 676f 6f67 6c65  ithub.com/google
+000106a0: 6170 6973 2f67 6f6f 676c 6561 7069 732f  apis/googleapis/
+000106b0: 626c 6f62 2f34 6538 3336 6337 6332 3537  blob/4e836c7c257
+000106c0: 6533 6532 3062 3164 6531 3464 3437 3039  e3e20b1de14d4709
+000106d0: 3933 6132 6231 6634 3733 3661 382f 676f  93a2b1f4736a8/go
+000106e0: 6f67 6c65 2f63 6c6f 7564 2f61 6970 6c61  ogle/cloud/aipla
+000106f0: 7466 6f72 6d2f 7631 6265 7461 312f 6375  tform/v1beta1/cu
+00010700: 7374 6f6d 5f6a 6f62 2e70 726f 746f 234c  stom_job.proto#L
+00010710: 3137 322e 0a20 2020 2072 756e 5f65 7661  172..    run_eva
+00010720: 6c75 6174 696f 6e3a 2057 6865 7468 6572  luation: Whether
+00010730: 2074 6f20 7275 6e20 6576 616c 7561 7469   to run evaluati
+00010740: 6f6e 2073 7465 7073 2064 7572 696e 6720  on steps during 
+00010750: 7472 6169 6e69 6e67 2e0a 2020 2020 6576  training..    ev
+00010760: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+00010770: 7265 6469 6374 5f6d 6163 6869 6e65 5f74  redict_machine_t
+00010780: 7970 653a 2054 6865 2070 7265 6469 6374  ype: The predict
+00010790: 696f 6e20 7365 7276 6572 206d 6163 6869  ion server machi
+000107a0: 6e65 2074 7970 6520 666f 7220 6261 7463  ne type for batc
+000107b0: 6820 7072 6564 6963 7420 636f 6d70 6f6e  h predict compon
+000107c0: 656e 7473 2064 7572 696e 6720 6576 616c  ents during eval
+000107d0: 7561 7469 6f6e 2e0a 2020 2020 6576 616c  uation..    eval
+000107e0: 7561 7469 6f6e 5f62 6174 6368 5f70 7265  uation_batch_pre
+000107f0: 6469 6374 5f73 7461 7274 696e 675f 7265  dict_starting_re
+00010800: 706c 6963 615f 636f 756e 743a 2054 6865  plica_count: The
+00010810: 2069 6e69 7469 616c 206e 756d 6265 7220   initial number 
+00010820: 6f66 2070 7265 6469 6374 696f 6e20 7365  of prediction se
+00010830: 7276 6572 2066 6f72 2062 6174 6368 2070  rver for batch p
+00010840: 7265 6469 6374 2063 6f6d 706f 6e65 6e74  redict component
+00010850: 7320 6475 7269 6e67 2065 7661 6c75 6174  s during evaluat
+00010860: 696f 6e2e 0a20 2020 2065 7661 6c75 6174  ion..    evaluat
+00010870: 696f 6e5f 6261 7463 685f 7072 6564 6963  ion_batch_predic
+00010880: 745f 6d61 785f 7265 706c 6963 615f 636f  t_max_replica_co
+00010890: 756e 743a 2054 6865 206d 6178 206e 756d  unt: The max num
+000108a0: 6265 7220 6f66 2070 7265 6469 6374 696f  ber of predictio
+000108b0: 6e20 7365 7276 6572 2066 6f72 2062 6174  n server for bat
+000108c0: 6368 2070 7265 6469 6374 2063 6f6d 706f  ch predict compo
+000108d0: 6e65 6e74 7320 6475 7269 6e67 2065 7661  nents during eva
+000108e0: 6c75 6174 696f 6e2e 0a20 2020 2065 7661  luation..    eva
+000108f0: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
+00010900: 5f6d 6163 6869 6e65 5f74 7970 653a 2054  _machine_type: T
+00010910: 6865 2064 6174 6166 6c6f 7720 6d61 6368  he dataflow mach
+00010920: 696e 6520 7479 7065 2066 6f72 2065 7661  ine type for eva
+00010930: 6c75 6174 696f 6e20 636f 6d70 6f6e 656e  luation componen
+00010940: 7473 2e0a 2020 2020 6576 616c 7561 7469  ts..    evaluati
+00010950: 6f6e 5f64 6174 6166 6c6f 775f 7374 6172  on_dataflow_star
+00010960: 7469 6e67 5f6e 756d 5f77 6f72 6b65 7273  ting_num_workers
+00010970: 3a20 5468 6520 696e 6974 6961 6c20 6e75  : The initial nu
+00010980: 6d62 6572 206f 6620 4461 7461 666c 6f77  mber of Dataflow
+00010990: 2077 6f72 6b65 7273 2066 6f72 2065 7661   workers for eva
+000109a0: 6c75 6174 696f 6e20 636f 6d70 6f6e 656e  luation componen
+000109b0: 7473 2e0a 2020 2020 6576 616c 7561 7469  ts..    evaluati
+000109c0: 6f6e 5f64 6174 6166 6c6f 775f 6d61 785f  on_dataflow_max_
+000109d0: 6e75 6d5f 776f 726b 6572 733a 2054 6865  num_workers: The
+000109e0: 206d 6178 206e 756d 6265 7220 6f66 2044   max number of D
+000109f0: 6174 6166 6c6f 7720 776f 726b 6572 7320  ataflow workers 
+00010a00: 666f 7220 6576 616c 7561 7469 6f6e 2063  for evaluation c
+00010a10: 6f6d 706f 6e65 6e74 732e 0a20 2020 2065  omponents..    e
+00010a20: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
+00010a30: 6f77 5f64 6973 6b5f 7369 7a65 5f67 623a  ow_disk_size_gb:
+00010a40: 2044 6174 6166 6c6f 7720 776f 726b 6572   Dataflow worker
+00010a50: 2773 2064 6973 6b20 7369 7a65 2069 6e20  's disk size in 
+00010a60: 4742 2066 6f72 2065 7661 6c75 6174 696f  GB for evaluatio
+00010a70: 6e20 636f 6d70 6f6e 656e 7473 2e0a 2020  n components..  
+00010a80: 2020 6461 7461 666c 6f77 5f73 6572 7669    dataflow_servi
+00010a90: 6365 5f61 6363 6f75 6e74 3a20 4375 7374  ce_account: Cust
+00010aa0: 6f6d 2073 6572 7669 6365 2061 6363 6f75  om service accou
+00010ab0: 6e74 2074 6f20 7275 6e20 6461 7461 666c  nt to run datafl
+00010ac0: 6f77 206a 6f62 732e 0a20 2020 2064 6174  ow jobs..    dat
+00010ad0: 6166 6c6f 775f 7375 626e 6574 776f 726b  aflow_subnetwork
+00010ae0: 3a20 4461 7461 666c 6f77 2773 2066 756c  : Dataflow's ful
+00010af0: 6c79 2071 7561 6c69 6669 6564 2073 7562  ly qualified sub
+00010b00: 6e65 7477 6f72 6b20 6e61 6d65 2c20 7768  network name, wh
+00010b10: 656e 2065 6d70 7479 2074 6865 2064 6566  en empty the def
+00010b20: 6175 6c74 2073 7562 6e65 7477 6f72 6b20  ault subnetwork 
+00010b30: 7769 6c6c 2062 6520 7573 6564 2e20 4578  will be used. Ex
+00010b40: 616d 706c 653a 2068 7474 7073 3a2f 2f63  ample: https://c
+00010b50: 6c6f 7564 2e67 6f6f 676c 652e 636f 6d2f  loud.google.com/
+00010b60: 6461 7461 666c 6f77 2f64 6f63 732f 6775  dataflow/docs/gu
+00010b70: 6964 6573 2f73 7065 6369 6679 696e 672d  ides/specifying-
+00010b80: 6e65 7477 6f72 6b73 2365 7861 6d70 6c65  networks#example
+00010b90: 5f6e 6574 776f 726b 5f61 6e64 5f73 7562  _network_and_sub
+00010ba0: 6e65 7477 6f72 6b5f 7370 6563 6966 6963  network_specific
+00010bb0: 6174 696f 6e73 0a20 2020 2064 6174 6166  ations.    dataf
+00010bc0: 6c6f 775f 7573 655f 7075 626c 6963 5f69  low_use_public_i
+00010bd0: 7073 3a20 5370 6563 6966 6965 7320 7768  ps: Specifies wh
+00010be0: 6574 6865 7220 4461 7461 666c 6f77 2077  ether Dataflow w
+00010bf0: 6f72 6b65 7273 2075 7365 2070 7562 6c69  orkers use publi
+00010c00: 6320 4950 2061 6464 7265 7373 6573 2e0a  c IP addresses..
+00010c10: 2020 2020 656e 6372 7970 7469 6f6e 5f73      encryption_s
+00010c20: 7065 635f 6b65 795f 6e61 6d65 3a20 5468  pec_key_name: Th
+00010c30: 6520 4b4d 5320 6b65 7920 6e61 6d65 2e0a  e KMS key name..
+00010c40: 0a20 2052 6574 7572 6e73 3a0a 2020 2020  .  Returns:.    
+00010c50: 5475 706c 6520 6f66 2070 6970 656c 696e  Tuple of pipelin
+00010c60: 655f 6465 6669 6e69 7469 6f6e 5f70 6174  e_definition_pat
+00010c70: 6820 616e 6420 7061 7261 6d65 7465 725f  h and parameter_
+00010c80: 7661 6c75 6573 2e0a 2020 2222 220a 2020  values..  """.  
+00010c90: 2320 666d 743a 206f 6e0a 2020 6966 2069  # fmt: on.  if i
+00010ca0: 7369 6e73 7461 6e63 6528 7466 5f61 7574  sinstance(tf_aut
+00010cb0: 6f5f 7472 616e 7366 6f72 6d5f 6665 6174  o_transform_feat
+00010cc0: 7572 6573 2c20 6c69 7374 293a 0a20 2020  ures, list):.   
+00010cd0: 2074 665f 6175 746f 5f74 7261 6e73 666f   tf_auto_transfo
+00010ce0: 726d 5f66 6561 7475 7265 7320 3d20 7b27  rm_features = {'
+00010cf0: 6175 746f 273a 2074 665f 6175 746f 5f74  auto': tf_auto_t
+00010d00: 7261 6e73 666f 726d 5f66 6561 7475 7265  ransform_feature
+00010d10: 737d 0a0a 2020 6966 2074 7261 6e73 666f  s}..  if transfo
+00010d20: 726d 5f63 6f6e 6669 6720 616e 6420 7466  rm_config and tf
+00010d30: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
+00010d40: 5f70 6174 683a 0a20 2020 2072 6169 7365  _path:.    raise
+00010d50: 2056 616c 7565 4572 726f 7228 0a20 2020   ValueError(.   
+00010d60: 2020 2020 2027 4f6e 6c79 206f 6e65 206f       'Only one o
+00010d70: 6620 7472 616e 7366 6f72 6d5f 636f 6e66  f transform_conf
+00010d80: 6967 2061 6e64 2074 665f 7472 616e 7366  ig and tf_transf
+00010d90: 6f72 6d61 7469 6f6e 735f 7061 7468 2063  ormations_path c
+00010da0: 616e 2027 0a20 2020 2020 2020 2027 6265  an '.        'be
+00010db0: 2073 7065 6369 6669 6564 2e27 0a20 2020   specified.'.   
+00010dc0: 2029 0a0a 2020 656c 6966 2074 7261 6e73   )..  elif trans
+00010dd0: 666f 726d 5f63 6f6e 6669 673a 0a20 2020  form_config:.   
+00010de0: 2077 6172 6e69 6e67 732e 7761 726e 280a   warnings.warn(.
+00010df0: 2020 2020 2020 2020 2774 7261 6e73 666f          'transfo
+00010e00: 726d 5f63 6f6e 6669 6720 7061 7261 6d65  rm_config parame
+00010e10: 7465 7220 6973 2064 6570 7265 6361 7465  ter is deprecate
+00010e20: 642e 2027 0a20 2020 2020 2020 2027 506c  d. '.        'Pl
+00010e30: 6561 7365 2075 7365 2074 6865 2066 6c61  ease use the fla
+00010e40: 7474 656e 6564 2074 7261 6e73 666f 726d  ttened transform
+00010e50: 2063 6f6e 6669 6720 6172 6775 6d65 6e74   config argument
+00010e60: 7320 696e 7374 6561 642e 270a 2020 2020  s instead.'.    
+00010e70: 290a 2020 2020 7466 5f74 7261 6e73 666f  ).    tf_transfo
+00010e80: 726d 6174 696f 6e73 5f70 6174 6820 3d20  rmations_path = 
+00010e90: 7472 616e 7366 6f72 6d5f 636f 6e66 6967  transform_config
+00010ea0: 0a0a 2020 6966 206e 6f74 2077 6f72 6b65  ..  if not worke
+00010eb0: 725f 706f 6f6c 5f73 7065 6373 5f6f 7665  r_pool_specs_ove
+00010ec0: 7272 6964 653a 0a20 2020 2077 6f72 6b65  rride:.    worke
+00010ed0: 725f 706f 6f6c 5f73 7065 6373 5f6f 7665  r_pool_specs_ove
+00010ee0: 7272 6964 6520 3d20 5b5d 0a0a 2020 7061  rride = []..  pa
+00010ef0: 7261 6d65 7465 725f 7661 6c75 6573 203d  rameter_values =
+00010f00: 207b 7d0a 2020 7472 6169 6e69 6e67 5f61   {}.  training_a
+00010f10: 6e64 5f65 7661 6c5f 7061 7261 6d65 7465  nd_eval_paramete
+00010f20: 7273 203d 207b 0a20 2020 2020 2027 7072  rs = {.      'pr
+00010f30: 6f6a 6563 7427 3a20 7072 6f6a 6563 742c  oject': project,
+00010f40: 0a20 2020 2020 2027 6c6f 6361 7469 6f6e  .      'location
+00010f50: 273a 206c 6f63 6174 696f 6e2c 0a20 2020  ': location,.   
+00010f60: 2020 2027 726f 6f74 5f64 6972 273a 2072     'root_dir': r
+00010f70: 6f6f 745f 6469 722c 0a20 2020 2020 2027  oot_dir,.      '
+00010f80: 7461 7267 6574 5f63 6f6c 756d 6e27 3a20  target_column': 
+00010f90: 7461 7267 6574 5f63 6f6c 756d 6e2c 0a20  target_column,. 
+00010fa0: 2020 2020 2027 7072 6564 6963 7469 6f6e       'prediction
+00010fb0: 5f74 7970 6527 3a20 7072 6564 6963 7469  _type': predicti
+00010fc0: 6f6e 5f74 7970 652c 0a20 2020 2020 2027  on_type,.      '
+00010fd0: 6c65 6172 6e69 6e67 5f72 6174 6527 3a20  learning_rate': 
+00010fe0: 6c65 6172 6e69 6e67 5f72 6174 652c 0a20  learning_rate,. 
+00010ff0: 2020 2020 2027 646e 6e5f 6c65 6172 6e69       'dnn_learni
+00011000: 6e67 5f72 6174 6527 3a20 646e 6e5f 6c65  ng_rate': dnn_le
+00011010: 6172 6e69 6e67 5f72 6174 652c 0a20 2020  arning_rate,.   
+00011020: 2020 2027 6f70 7469 6d69 7a65 725f 7479     'optimizer_ty
+00011030: 7065 273a 206f 7074 696d 697a 6572 5f74  pe': optimizer_t
+00011040: 7970 652c 0a20 2020 2020 2027 6d61 785f  ype,.      'max_
+00011050: 7374 6570 7327 3a20 6d61 785f 7374 6570  steps': max_step
+00011060: 732c 0a20 2020 2020 2027 6d61 785f 7472  s,.      'max_tr
+00011070: 6169 6e5f 7365 6373 273a 206d 6178 5f74  ain_secs': max_t
+00011080: 7261 696e 5f73 6563 732c 0a20 2020 2020  rain_secs,.     
+00011090: 2027 6c31 5f72 6567 756c 6172 697a 6174   'l1_regularizat
+000110a0: 696f 6e5f 7374 7265 6e67 7468 273a 206c  ion_strength': l
+000110b0: 315f 7265 6775 6c61 7269 7a61 7469 6f6e  1_regularization
+000110c0: 5f73 7472 656e 6774 682c 0a20 2020 2020  _strength,.     
+000110d0: 2027 6c32 5f72 6567 756c 6172 697a 6174   'l2_regularizat
+000110e0: 696f 6e5f 7374 7265 6e67 7468 273a 206c  ion_strength': l
+000110f0: 325f 7265 6775 6c61 7269 7a61 7469 6f6e  2_regularization
+00011100: 5f73 7472 656e 6774 682c 0a20 2020 2020  _strength,.     
+00011110: 2027 6c32 5f73 6872 696e 6b61 6765 5f72   'l2_shrinkage_r
+00011120: 6567 756c 6172 697a 6174 696f 6e5f 7374  egularization_st
+00011130: 7265 6e67 7468 273a 2028 0a20 2020 2020  rength': (.     
+00011140: 2020 2020 206c 325f 7368 7269 6e6b 6167       l2_shrinkag
+00011150: 655f 7265 6775 6c61 7269 7a61 7469 6f6e  e_regularization
+00011160: 5f73 7472 656e 6774 680a 2020 2020 2020  _strength.      
+00011170: 292c 0a20 2020 2020 2027 6265 7461 5f31  ),.      'beta_1
+00011180: 273a 2062 6574 615f 312c 0a20 2020 2020  ': beta_1,.     
+00011190: 2027 6265 7461 5f32 273a 2062 6574 615f   'beta_2': beta_
+000111a0: 322c 0a20 2020 2020 2027 6869 6464 656e  2,.      'hidden
+000111b0: 5f75 6e69 7473 273a 2068 6964 6465 6e5f  _units': hidden_
+000111c0: 756e 6974 732c 0a20 2020 2020 2027 7573  units,.      'us
+000111d0: 655f 7769 6465 273a 2075 7365 5f77 6964  e_wide': use_wid
+000111e0: 652c 0a20 2020 2020 2027 656d 6265 645f  e,.      'embed_
+000111f0: 6361 7465 676f 7269 6573 273a 2065 6d62  categories': emb
+00011200: 6564 5f63 6174 6567 6f72 6965 732c 0a20  ed_categories,. 
+00011210: 2020 2020 2027 646e 6e5f 6472 6f70 6f75       'dnn_dropou
+00011220: 7427 3a20 646e 6e5f 6472 6f70 6f75 742c  t': dnn_dropout,
+00011230: 0a20 2020 2020 2027 646e 6e5f 6f70 7469  .      'dnn_opti
+00011240: 6d69 7a65 725f 7479 7065 273a 2064 6e6e  mizer_type': dnn
+00011250: 5f6f 7074 696d 697a 6572 5f74 7970 652c  _optimizer_type,
+00011260: 0a20 2020 2020 2027 646e 6e5f 6c31 5f72  .      'dnn_l1_r
+00011270: 6567 756c 6172 697a 6174 696f 6e5f 7374  egularization_st
+00011280: 7265 6e67 7468 273a 2064 6e6e 5f6c 315f  rength': dnn_l1_
+00011290: 7265 6775 6c61 7269 7a61 7469 6f6e 5f73  regularization_s
+000112a0: 7472 656e 6774 682c 0a20 2020 2020 2027  trength,.      '
+000112b0: 646e 6e5f 6c32 5f72 6567 756c 6172 697a  dnn_l2_regulariz
+000112c0: 6174 696f 6e5f 7374 7265 6e67 7468 273a  ation_strength':
+000112d0: 2064 6e6e 5f6c 325f 7265 6775 6c61 7269   dnn_l2_regulari
+000112e0: 7a61 7469 6f6e 5f73 7472 656e 6774 682c  zation_strength,
+000112f0: 0a20 2020 2020 2027 646e 6e5f 6c32 5f73  .      'dnn_l2_s
+00011300: 6872 696e 6b61 6765 5f72 6567 756c 6172  hrinkage_regular
+00011310: 697a 6174 696f 6e5f 7374 7265 6e67 7468  ization_strength
+00011320: 273a 2028 0a20 2020 2020 2020 2020 2064  ': (.          d
+00011330: 6e6e 5f6c 325f 7368 7269 6e6b 6167 655f  nn_l2_shrinkage_
+00011340: 7265 6775 6c61 7269 7a61 7469 6f6e 5f73  regularization_s
+00011350: 7472 656e 6774 680a 2020 2020 2020 292c  trength.      ),
+00011360: 0a20 2020 2020 2027 646e 6e5f 6265 7461  .      'dnn_beta
+00011370: 5f31 273a 2064 6e6e 5f62 6574 615f 312c  _1': dnn_beta_1,
+00011380: 0a20 2020 2020 2027 646e 6e5f 6265 7461  .      'dnn_beta
+00011390: 5f32 273a 2064 6e6e 5f62 6574 615f 322c  _2': dnn_beta_2,
+000113a0: 0a20 2020 2020 2027 656e 6162 6c65 5f70  .      'enable_p
+000113b0: 726f 6669 6c65 7227 3a20 656e 6162 6c65  rofiler': enable
+000113c0: 5f70 726f 6669 6c65 722c 0a20 2020 2020  _profiler,.     
+000113d0: 2027 6361 6368 655f 6461 7461 273a 2063   'cache_data': c
+000113e0: 6163 6865 5f64 6174 612c 0a20 2020 2020  ache_data,.     
+000113f0: 2027 7365 6564 273a 2073 6565 642c 0a20   'seed': seed,. 
+00011400: 2020 2020 2027 6576 616c 5f73 7465 7073       'eval_steps
+00011410: 273a 2065 7661 6c5f 7374 6570 732c 0a20  ': eval_steps,. 
+00011420: 2020 2020 2027 6261 7463 685f 7369 7a65       'batch_size
+00011430: 273a 2062 6174 6368 5f73 697a 652c 0a20  ': batch_size,. 
+00011440: 2020 2020 2027 6d65 6173 7572 656d 656e       'measuremen
+00011450: 745f 7365 6c65 6374 696f 6e5f 7479 7065  t_selection_type
+00011460: 273a 206d 6561 7375 7265 6d65 6e74 5f73  ': measurement_s
+00011470: 656c 6563 7469 6f6e 5f74 7970 652c 0a20  election_type,. 
+00011480: 2020 2020 2027 6f70 7469 6d69 7a61 7469       'optimizati
+00011490: 6f6e 5f6d 6574 7269 6327 3a20 6f70 7469  on_metric': opti
+000114a0: 6d69 7a61 7469 6f6e 5f6d 6574 7269 632c  mization_metric,
+000114b0: 0a20 2020 2020 2027 6576 616c 5f66 7265  .      'eval_fre
+000114c0: 7175 656e 6379 5f73 6563 7327 3a20 6576  quency_secs': ev
+000114d0: 616c 5f66 7265 7175 656e 6379 5f73 6563  al_frequency_sec
+000114e0: 732c 0a20 2020 2020 2027 7765 6967 6874  s,.      'weight
+000114f0: 5f63 6f6c 756d 6e27 3a20 7765 6967 6874  _column': weight
+00011500: 5f63 6f6c 756d 6e2c 0a20 2020 2020 2027  _column,.      '
+00011510: 7472 616e 7366 6f72 6d5f 6461 7461 666c  transform_datafl
+00011520: 6f77 5f6d 6163 6869 6e65 5f74 7970 6527  ow_machine_type'
+00011530: 3a20 7472 616e 7366 6f72 6d5f 6461 7461  : transform_data
+00011540: 666c 6f77 5f6d 6163 6869 6e65 5f74 7970  flow_machine_typ
+00011550: 652c 0a20 2020 2020 2027 7472 616e 7366  e,.      'transf
+00011560: 6f72 6d5f 6461 7461 666c 6f77 5f6d 6178  orm_dataflow_max
+00011570: 5f6e 756d 5f77 6f72 6b65 7273 273a 2074  _num_workers': t
+00011580: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
+00011590: 775f 6d61 785f 6e75 6d5f 776f 726b 6572  w_max_num_worker
+000115a0: 732c 0a20 2020 2020 2027 7472 616e 7366  s,.      'transf
+000115b0: 6f72 6d5f 6461 7461 666c 6f77 5f64 6973  orm_dataflow_dis
+000115c0: 6b5f 7369 7a65 5f67 6227 3a20 7472 616e  k_size_gb': tran
+000115d0: 7366 6f72 6d5f 6461 7461 666c 6f77 5f64  sform_dataflow_d
+000115e0: 6973 6b5f 7369 7a65 5f67 622c 0a20 2020  isk_size_gb,.   
+000115f0: 2020 2027 776f 726b 6572 5f70 6f6f 6c5f     'worker_pool_
+00011600: 7370 6563 735f 6f76 6572 7269 6465 273a  specs_override':
+00011610: 2077 6f72 6b65 725f 706f 6f6c 5f73 7065   worker_pool_spe
+00011620: 6373 5f6f 7665 7272 6964 652c 0a20 2020  cs_override,.   
+00011630: 2020 2027 7275 6e5f 6576 616c 7561 7469     'run_evaluati
+00011640: 6f6e 273a 2072 756e 5f65 7661 6c75 6174  on': run_evaluat
+00011650: 696f 6e2c 0a20 2020 2020 2027 6576 616c  ion,.      'eval
+00011660: 7561 7469 6f6e 5f62 6174 6368 5f70 7265  uation_batch_pre
+00011670: 6469 6374 5f6d 6163 6869 6e65 5f74 7970  dict_machine_typ
+00011680: 6527 3a20 280a 2020 2020 2020 2020 2020  e': (.          
+00011690: 6576 616c 7561 7469 6f6e 5f62 6174 6368  evaluation_batch
+000116a0: 5f70 7265 6469 6374 5f6d 6163 6869 6e65  _predict_machine
+000116b0: 5f74 7970 650a 2020 2020 2020 292c 0a20  _type.      ),. 
+000116c0: 2020 2020 2027 6576 616c 7561 7469 6f6e       'evaluation
+000116d0: 5f62 6174 6368 5f70 7265 6469 6374 5f73  _batch_predict_s
+000116e0: 7461 7274 696e 675f 7265 706c 6963 615f  tarting_replica_
+000116f0: 636f 756e 7427 3a20 280a 2020 2020 2020  count': (.      
+00011700: 2020 2020 6576 616c 7561 7469 6f6e 5f62      evaluation_b
+00011710: 6174 6368 5f70 7265 6469 6374 5f73 7461  atch_predict_sta
+00011720: 7274 696e 675f 7265 706c 6963 615f 636f  rting_replica_co
+00011730: 756e 740a 2020 2020 2020 292c 0a20 2020  unt.      ),.   
+00011740: 2020 2027 6576 616c 7561 7469 6f6e 5f62     'evaluation_b
+00011750: 6174 6368 5f70 7265 6469 6374 5f6d 6178  atch_predict_max
+00011760: 5f72 6570 6c69 6361 5f63 6f75 6e74 273a  _replica_count':
+00011770: 2028 0a20 2020 2020 2020 2020 2065 7661   (.          eva
+00011780: 6c75 6174 696f 6e5f 6261 7463 685f 7072  luation_batch_pr
+00011790: 6564 6963 745f 6d61 785f 7265 706c 6963  edict_max_replic
+000117a0: 615f 636f 756e 740a 2020 2020 2020 292c  a_count.      ),
+000117b0: 0a20 2020 2020 2027 6576 616c 7561 7469  .      'evaluati
+000117c0: 6f6e 5f64 6174 6166 6c6f 775f 6d61 6368  on_dataflow_mach
+000117d0: 696e 655f 7479 7065 273a 2065 7661 6c75  ine_type': evalu
+000117e0: 6174 696f 6e5f 6461 7461 666c 6f77 5f6d  ation_dataflow_m
+000117f0: 6163 6869 6e65 5f74 7970 652c 0a20 2020  achine_type,.   
+00011800: 2020 2027 6576 616c 7561 7469 6f6e 5f64     'evaluation_d
+00011810: 6174 6166 6c6f 775f 7374 6172 7469 6e67  ataflow_starting
+00011820: 5f6e 756d 5f77 6f72 6b65 7273 273a 2028  _num_workers': (
+00011830: 0a20 2020 2020 2020 2020 2065 7661 6c75  .          evalu
+00011840: 6174 696f 6e5f 6461 7461 666c 6f77 5f73  ation_dataflow_s
+00011850: 7461 7274 696e 675f 6e75 6d5f 776f 726b  tarting_num_work
+00011860: 6572 730a 2020 2020 2020 292c 0a20 2020  ers.      ),.   
+00011870: 2020 2027 6576 616c 7561 7469 6f6e 5f64     'evaluation_d
+00011880: 6174 6166 6c6f 775f 6d61 785f 6e75 6d5f  ataflow_max_num_
+00011890: 776f 726b 6572 7327 3a20 280a 2020 2020  workers': (.    
+000118a0: 2020 2020 2020 6576 616c 7561 7469 6f6e        evaluation
+000118b0: 5f64 6174 6166 6c6f 775f 6d61 785f 6e75  _dataflow_max_nu
+000118c0: 6d5f 776f 726b 6572 730a 2020 2020 2020  m_workers.      
+000118d0: 292c 0a20 2020 2020 2027 6576 616c 7561  ),.      'evalua
+000118e0: 7469 6f6e 5f64 6174 6166 6c6f 775f 6469  tion_dataflow_di
+000118f0: 736b 5f73 697a 655f 6762 273a 2065 7661  sk_size_gb': eva
+00011900: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
+00011910: 5f64 6973 6b5f 7369 7a65 5f67 622c 0a20  _disk_size_gb,. 
+00011920: 2020 2020 2027 6461 7461 666c 6f77 5f73       'dataflow_s
+00011930: 6572 7669 6365 5f61 6363 6f75 6e74 273a  ervice_account':
+00011940: 2064 6174 6166 6c6f 775f 7365 7276 6963   dataflow_servic
+00011950: 655f 6163 636f 756e 742c 0a20 2020 2020  e_account,.     
+00011960: 2027 6461 7461 666c 6f77 5f73 7562 6e65   'dataflow_subne
+00011970: 7477 6f72 6b27 3a20 6461 7461 666c 6f77  twork': dataflow
+00011980: 5f73 7562 6e65 7477 6f72 6b2c 0a20 2020  _subnetwork,.   
+00011990: 2020 2027 6461 7461 666c 6f77 5f75 7365     'dataflow_use
+000119a0: 5f70 7562 6c69 635f 6970 7327 3a20 6461  _public_ips': da
+000119b0: 7461 666c 6f77 5f75 7365 5f70 7562 6c69  taflow_use_publi
+000119c0: 635f 6970 732c 0a20 2020 2020 2027 656e  c_ips,.      'en
+000119d0: 6372 7970 7469 6f6e 5f73 7065 635f 6b65  cryption_spec_ke
+000119e0: 795f 6e61 6d65 273a 2065 6e63 7279 7074  y_name': encrypt
+000119f0: 696f 6e5f 7370 6563 5f6b 6579 5f6e 616d  ion_spec_key_nam
+00011a00: 652c 0a20 207d 0a20 205f 7570 6461 7465  e,.  }.  _update
+00011a10: 5f70 6172 616d 6574 6572 7328 7061 7261  _parameters(para
+00011a20: 6d65 7465 725f 7661 6c75 6573 2c20 7472  meter_values, tr
+00011a30: 6169 6e69 6e67 5f61 6e64 5f65 7661 6c5f  aining_and_eval_
+00011a40: 7061 7261 6d65 7465 7273 290a 0a20 2066  parameters)..  f
+00011a50: 7465 5f70 6172 616d 7320 3d20 7b0a 2020  te_params = {.  
+00011a60: 2020 2020 2764 6174 6173 6574 5f6c 6576      'dataset_lev
+00011a70: 656c 5f63 7573 746f 6d5f 7472 616e 7366  el_custom_transf
+00011a80: 6f72 6d61 7469 6f6e 5f64 6566 696e 6974  ormation_definit
+00011a90: 696f 6e73 273a 2028 0a20 2020 2020 2020  ions': (.       
+00011aa0: 2020 2064 6174 6173 6574 5f6c 6576 656c     dataset_level
+00011ab0: 5f63 7573 746f 6d5f 7472 616e 7366 6f72  _custom_transfor
+00011ac0: 6d61 7469 6f6e 5f64 6566 696e 6974 696f  mation_definitio
+00011ad0: 6e73 0a20 2020 2020 2020 2020 2069 6620  ns.          if 
+00011ae0: 6461 7461 7365 745f 6c65 7665 6c5f 6375  dataset_level_cu
+00011af0: 7374 6f6d 5f74 7261 6e73 666f 726d 6174  stom_transformat
+00011b00: 696f 6e5f 6465 6669 6e69 7469 6f6e 730a  ion_definitions.
+00011b10: 2020 2020 2020 2020 2020 656c 7365 205b            else [
+00011b20: 5d0a 2020 2020 2020 292c 0a20 2020 2020  ].      ),.     
+00011b30: 2027 6461 7461 7365 745f 6c65 7665 6c5f   'dataset_level_
+00011b40: 7472 616e 7366 6f72 6d61 7469 6f6e 7327  transformations'
+00011b50: 3a20 280a 2020 2020 2020 2020 2020 6461  : (.          da
+00011b60: 7461 7365 745f 6c65 7665 6c5f 7472 616e  taset_level_tran
+00011b70: 7366 6f72 6d61 7469 6f6e 7320 6966 2064  sformations if d
+00011b80: 6174 6173 6574 5f6c 6576 656c 5f74 7261  ataset_level_tra
+00011b90: 6e73 666f 726d 6174 696f 6e73 2065 6c73  nsformations els
+00011ba0: 6520 5b5d 0a20 2020 2020 2029 2c0a 2020  e [].      ),.  
+00011bb0: 2020 2020 2772 756e 5f66 6561 7475 7265      'run_feature
+00011bc0: 5f73 656c 6563 7469 6f6e 273a 2072 756e  _selection': run
+00011bd0: 5f66 6561 7475 7265 5f73 656c 6563 7469  _feature_selecti
+00011be0: 6f6e 2c0a 2020 2020 2020 2766 6561 7475  on,.      'featu
+00011bf0: 7265 5f73 656c 6563 7469 6f6e 5f61 6c67  re_selection_alg
+00011c00: 6f72 6974 686d 273a 2066 6561 7475 7265  orithm': feature
+00011c10: 5f73 656c 6563 7469 6f6e 5f61 6c67 6f72  _selection_algor
+00011c20: 6974 686d 2c0a 2020 2020 2020 276d 6178  ithm,.      'max
+00011c30: 5f73 656c 6563 7465 645f 6665 6174 7572  _selected_featur
+00011c40: 6573 273a 206d 6178 5f73 656c 6563 7465  es': max_selecte
+00011c50: 645f 6665 6174 7572 6573 2c0a 2020 2020  d_features,.    
+00011c60: 2020 2770 7265 6465 6669 6e65 645f 7370    'predefined_sp
+00011c70: 6c69 745f 6b65 7927 3a20 7072 6564 6566  lit_key': predef
+00011c80: 696e 6564 5f73 706c 6974 5f6b 6579 2c0a  ined_split_key,.
+00011c90: 2020 2020 2020 2773 7472 6174 6966 6965        'stratifie
+00011ca0: 645f 7370 6c69 745f 6b65 7927 3a20 7374  d_split_key': st
+00011cb0: 7261 7469 6669 6564 5f73 706c 6974 5f6b  ratified_split_k
+00011cc0: 6579 2c0a 2020 2020 2020 2774 7261 696e  ey,.      'train
+00011cd0: 696e 675f 6672 6163 7469 6f6e 273a 2074  ing_fraction': t
+00011ce0: 7261 696e 696e 675f 6672 6163 7469 6f6e  raining_fraction
+00011cf0: 2c0a 2020 2020 2020 2776 616c 6964 6174  ,.      'validat
+00011d00: 696f 6e5f 6672 6163 7469 6f6e 273a 2076  ion_fraction': v
+00011d10: 616c 6964 6174 696f 6e5f 6672 6163 7469  alidation_fracti
+00011d20: 6f6e 2c0a 2020 2020 2020 2774 6573 745f  on,.      'test_
+00011d30: 6672 6163 7469 6f6e 273a 2074 6573 745f  fraction': test_
+00011d40: 6672 6163 7469 6f6e 2c0a 2020 2020 2020  fraction,.      
+00011d50: 2774 665f 6175 746f 5f74 7261 6e73 666f  'tf_auto_transfo
+00011d60: 726d 5f66 6561 7475 7265 7327 3a20 280a  rm_features': (.
+00011d70: 2020 2020 2020 2020 2020 7466 5f61 7574            tf_aut
+00011d80: 6f5f 7472 616e 7366 6f72 6d5f 6665 6174  o_transform_feat
+00011d90: 7572 6573 2069 6620 7466 5f61 7574 6f5f  ures if tf_auto_
+00011da0: 7472 616e 7366 6f72 6d5f 6665 6174 7572  transform_featur
+00011db0: 6573 2065 6c73 6520 7b7d 0a20 2020 2020  es else {}.     
+00011dc0: 2029 2c0a 2020 2020 2020 2774 665f 6375   ),.      'tf_cu
+00011dd0: 7374 6f6d 5f74 7261 6e73 666f 726d 6174  stom_transformat
+00011de0: 696f 6e5f 6465 6669 6e69 7469 6f6e 7327  ion_definitions'
+00011df0: 3a20 280a 2020 2020 2020 2020 2020 7466  : (.          tf
+00011e00: 5f63 7573 746f 6d5f 7472 616e 7366 6f72  _custom_transfor
+00011e10: 6d61 7469 6f6e 5f64 6566 696e 6974 696f  mation_definitio
+00011e20: 6e73 0a20 2020 2020 2020 2020 2069 6620  ns.          if 
+00011e30: 7466 5f63 7573 746f 6d5f 7472 616e 7366  tf_custom_transf
+00011e40: 6f72 6d61 7469 6f6e 5f64 6566 696e 6974  ormation_definit
+00011e50: 696f 6e73 0a20 2020 2020 2020 2020 2065  ions.          e
+00011e60: 6c73 6520 5b5d 0a20 2020 2020 2029 2c0a  lse [].      ),.
+00011e70: 2020 2020 2020 2774 665f 7472 616e 7366        'tf_transf
+00011e80: 6f72 6d61 7469 6f6e 735f 7061 7468 273a  ormations_path':
+00011e90: 2074 665f 7472 616e 7366 6f72 6d61 7469   tf_transformati
+00011ea0: 6f6e 735f 7061 7468 2c0a 2020 2020 2020  ons_path,.      
+00011eb0: 276d 6174 6572 6961 6c69 7a65 645f 6578  'materialized_ex
+00011ec0: 616d 706c 6573 5f66 6f72 6d61 7427 3a20  amples_format': 
+00011ed0: 280a 2020 2020 2020 2020 2020 6d61 7465  (.          mate
+00011ee0: 7269 616c 697a 6564 5f65 7861 6d70 6c65  rialized_example
+00011ef0: 735f 666f 726d 6174 0a20 2020 2020 2020  s_format.       
+00011f00: 2020 2069 6620 6d61 7465 7269 616c 697a     if materializ
+00011f10: 6564 5f65 7861 6d70 6c65 735f 666f 726d  ed_examples_form
+00011f20: 6174 0a20 2020 2020 2020 2020 2065 6c73  at.          els
+00011f30: 6520 2774 6672 6563 6f72 6473 5f67 7a69  e 'tfrecords_gzi
+00011f40: 7027 0a20 2020 2020 2029 2c0a 2020 2020  p'.      ),.    
+00011f50: 2020 2774 665f 7472 616e 7366 6f72 6d5f    'tf_transform_
+00011f60: 6578 6563 7574 696f 6e5f 656e 6769 6e65  execution_engine
+00011f70: 273a 2028 0a20 2020 2020 2020 2020 2074  ': (.          t
+00011f80: 665f 7472 616e 7366 6f72 6d5f 6578 6563  f_transform_exec
+00011f90: 7574 696f 6e5f 656e 6769 6e65 0a20 2020  ution_engine.   
+00011fa0: 2020 2020 2020 2069 6620 7466 5f74 7261         if tf_tra
+00011fb0: 6e73 666f 726d 5f65 7865 6375 7469 6f6e  nsform_execution
+00011fc0: 5f65 6e67 696e 650a 2020 2020 2020 2020  _engine.        
+00011fd0: 2020 656c 7365 2027 6461 7461 666c 6f77    else 'dataflow
+00011fe0: 270a 2020 2020 2020 292c 0a20 207d 0a20  '.      ),.  }. 
+00011ff0: 205f 7570 6461 7465 5f70 6172 616d 6574   _update_paramet
+00012000: 6572 7328 7061 7261 6d65 7465 725f 7661  ers(parameter_va
+00012010: 6c75 6573 2c20 6674 655f 7061 7261 6d73  lues, fte_params
+00012020: 290a 0a20 2064 6174 615f 736f 7572 6365  )..  data_source
+00012030: 5f61 6e64 5f73 706c 6974 5f70 6172 616d  _and_split_param
+00012040: 6574 6572 7320 3d20 7b0a 2020 2020 2020  eters = {.      
+00012050: 2764 6174 615f 736f 7572 6365 5f63 7376  'data_source_csv
+00012060: 5f66 696c 656e 616d 6573 273a 2064 6174  _filenames': dat
+00012070: 615f 736f 7572 6365 5f63 7376 5f66 696c  a_source_csv_fil
+00012080: 656e 616d 6573 2c0a 2020 2020 2020 2764  enames,.      'd
+00012090: 6174 615f 736f 7572 6365 5f62 6967 7175  ata_source_bigqu
+000120a0: 6572 795f 7461 626c 655f 7061 7468 273a  ery_table_path':
+000120b0: 2064 6174 615f 736f 7572 6365 5f62 6967   data_source_big
+000120c0: 7175 6572 795f 7461 626c 655f 7061 7468  query_table_path
+000120d0: 2c0a 2020 2020 2020 2762 6967 7175 6572  ,.      'bigquer
+000120e0: 795f 7374 6167 696e 675f 6675 6c6c 5f64  y_staging_full_d
+000120f0: 6174 6173 6574 5f69 6427 3a20 6269 6771  ataset_id': bigq
+00012100: 7565 7279 5f73 7461 6769 6e67 5f66 756c  uery_staging_ful
+00012110: 6c5f 6461 7461 7365 745f 6964 2c0a 2020  l_dataset_id,.  
+00012120: 7d0a 2020 5f75 7064 6174 655f 7061 7261  }.  _update_para
+00012130: 6d65 7465 7273 2870 6172 616d 6574 6572  meters(parameter
+00012140: 5f76 616c 7565 732c 2064 6174 615f 736f  _values, data_so
+00012150: 7572 6365 5f61 6e64 5f73 706c 6974 5f70  urce_and_split_p
+00012160: 6172 616d 6574 6572 7329 0a0a 2020 7069  arameters)..  pi
+00012170: 7065 6c69 6e65 5f64 6566 696e 6974 696f  peline_definitio
+00012180: 6e5f 7061 7468 203d 206f 732e 7061 7468  n_path = os.path
+00012190: 2e6a 6f69 6e28 0a20 2020 2020 2070 6174  .join(.      pat
+000121a0: 686c 6962 2e50 6174 6828 5f5f 6669 6c65  hlib.Path(__file
+000121b0: 5f5f 292e 7061 7265 6e74 2e72 6573 6f6c  __).parent.resol
+000121c0: 7665 2829 2c0a 2020 2020 2020 2777 6964  ve(),.      'wid
+000121d0: 655f 616e 645f 6465 6570 5f74 7261 696e  e_and_deep_train
+000121e0: 6572 5f70 6970 656c 696e 652e 7961 6d6c  er_pipeline.yaml
+000121f0: 272c 0a20 2029 0a0a 2020 7265 7475 726e  ',.  )..  return
+00012200: 2070 6970 656c 696e 655f 6465 6669 6e69   pipeline_defini
+00012210: 7469 6f6e 5f70 6174 682c 2070 6172 616d  tion_path, param
+00012220: 6574 6572 5f76 616c 7565 730a 0a0a 6465  eter_values...de
+00012230: 6620 6765 745f 6275 696c 7469 6e5f 616c  f get_builtin_al
+00012240: 676f 7269 7468 6d5f 6879 7065 7270 6172  gorithm_hyperpar
+00012250: 616d 6574 6572 5f74 756e 696e 675f 6a6f  ameter_tuning_jo
+00012260: 625f 7069 7065 6c69 6e65 5f61 6e64 5f70  b_pipeline_and_p
+00012270: 6172 616d 6574 6572 7328 0a20 2020 2070  arameters(.    p
+00012280: 726f 6a65 6374 3a20 7374 722c 0a20 2020  roject: str,.   
+00012290: 206c 6f63 6174 696f 6e3a 2073 7472 2c0a   location: str,.
+000122a0: 2020 2020 726f 6f74 5f64 6972 3a20 7374      root_dir: st
+000122b0: 722c 0a20 2020 2074 6172 6765 745f 636f  r,.    target_co
+000122c0: 6c75 6d6e 3a20 7374 722c 0a20 2020 2070  lumn: str,.    p
+000122d0: 7265 6469 6374 696f 6e5f 7479 7065 3a20  rediction_type: 
+000122e0: 7374 722c 0a20 2020 2073 7475 6479 5f73  str,.    study_s
+000122f0: 7065 635f 6d65 7472 6963 5f69 643a 2073  pec_metric_id: s
+00012300: 7472 2c0a 2020 2020 7374 7564 795f 7370  tr,.    study_sp
+00012310: 6563 5f6d 6574 7269 635f 676f 616c 3a20  ec_metric_goal: 
+00012320: 7374 722c 0a20 2020 2073 7475 6479 5f73  str,.    study_s
+00012330: 7065 635f 7061 7261 6d65 7465 7273 5f6f  pec_parameters_o
+00012340: 7665 7272 6964 653a 204c 6973 745b 4469  verride: List[Di
+00012350: 6374 5b73 7472 2c20 416e 795d 5d2c 0a20  ct[str, Any]],. 
+00012360: 2020 206d 6178 5f74 7269 616c 5f63 6f75     max_trial_cou
+00012370: 6e74 3a20 696e 742c 0a20 2020 2070 6172  nt: int,.    par
+00012380: 616c 6c65 6c5f 7472 6961 6c5f 636f 756e  allel_trial_coun
+00012390: 743a 2069 6e74 2c0a 2020 2020 616c 676f  t: int,.    algo
+000123a0: 7269 7468 6d3a 2073 7472 2c0a 2020 2020  rithm: str,.    
+000123b0: 656e 6162 6c65 5f70 726f 6669 6c65 723a  enable_profiler:
+000123c0: 2062 6f6f 6c20 3d20 4661 6c73 652c 0a20   bool = False,. 
+000123d0: 2020 2073 6565 643a 2069 6e74 203d 2031     seed: int = 1
+000123e0: 2c0a 2020 2020 6576 616c 5f73 7465 7073  ,.    eval_steps
+000123f0: 3a20 696e 7420 3d20 302c 0a20 2020 2065  : int = 0,.    e
+00012400: 7661 6c5f 6672 6571 7565 6e63 795f 7365  val_frequency_se
+00012410: 6373 3a20 696e 7420 3d20 3630 302c 0a20  cs: int = 600,. 
+00012420: 2020 2074 7261 6e73 666f 726d 5f63 6f6e     transform_con
+00012430: 6669 673a 204f 7074 696f 6e61 6c5b 7374  fig: Optional[st
+00012440: 725d 203d 204e 6f6e 652c 0a20 2020 2064  r] = None,.    d
+00012450: 6174 6173 6574 5f6c 6576 656c 5f63 7573  ataset_level_cus
+00012460: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
+00012470: 6f6e 5f64 6566 696e 6974 696f 6e73 3a20  on_definitions: 
+00012480: 4f70 7469 6f6e 616c 5b0a 2020 2020 2020  Optional[.      
+00012490: 2020 4c69 7374 5b44 6963 745b 7374 722c    List[Dict[str,
+000124a0: 2041 6e79 5d5d 0a20 2020 205d 203d 204e   Any]].    ] = N
+000124b0: 6f6e 652c 0a20 2020 2064 6174 6173 6574  one,.    dataset
+000124c0: 5f6c 6576 656c 5f74 7261 6e73 666f 726d  _level_transform
+000124d0: 6174 696f 6e73 3a20 4f70 7469 6f6e 616c  ations: Optional
+000124e0: 5b4c 6973 745b 4469 6374 5b73 7472 2c20  [List[Dict[str, 
+000124f0: 416e 795d 5d5d 203d 204e 6f6e 652c 0a20  Any]]] = None,. 
+00012500: 2020 2070 7265 6465 6669 6e65 645f 7370     predefined_sp
+00012510: 6c69 745f 6b65 793a 204f 7074 696f 6e61  lit_key: Optiona
+00012520: 6c5b 7374 725d 203d 204e 6f6e 652c 0a20  l[str] = None,. 
+00012530: 2020 2073 7472 6174 6966 6965 645f 7370     stratified_sp
+00012540: 6c69 745f 6b65 793a 204f 7074 696f 6e61  lit_key: Optiona
+00012550: 6c5b 7374 725d 203d 204e 6f6e 652c 0a20  l[str] = None,. 
+00012560: 2020 2074 7261 696e 696e 675f 6672 6163     training_frac
+00012570: 7469 6f6e 3a20 4f70 7469 6f6e 616c 5b66  tion: Optional[f
+00012580: 6c6f 6174 5d20 3d20 4e6f 6e65 2c0a 2020  loat] = None,.  
+00012590: 2020 7661 6c69 6461 7469 6f6e 5f66 7261    validation_fra
+000125a0: 6374 696f 6e3a 204f 7074 696f 6e61 6c5b  ction: Optional[
+000125b0: 666c 6f61 745d 203d 204e 6f6e 652c 0a20  float] = None,. 
+000125c0: 2020 2074 6573 745f 6672 6163 7469 6f6e     test_fraction
+000125d0: 3a20 4f70 7469 6f6e 616c 5b66 6c6f 6174  : Optional[float
+000125e0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7466  ] = None,.    tf
+000125f0: 5f74 7261 6e73 666f 726d 5f65 7865 6375  _transform_execu
+00012600: 7469 6f6e 5f65 6e67 696e 653a 204f 7074  tion_engine: Opt
+00012610: 696f 6e61 6c5b 7374 725d 203d 204e 6f6e  ional[str] = Non
+00012620: 652c 0a20 2020 2074 665f 6175 746f 5f74  e,.    tf_auto_t
+00012630: 7261 6e73 666f 726d 5f66 6561 7475 7265  ransform_feature
+00012640: 733a 204f 7074 696f 6e61 6c5b 0a20 2020  s: Optional[.   
+00012650: 2020 2020 2055 6e69 6f6e 5b4c 6973 745b       Union[List[
+00012660: 7374 725d 2c20 4469 6374 5b73 7472 2c20  str], Dict[str, 
+00012670: 4c69 7374 5b73 7472 5d5d 5d0a 2020 2020  List[str]]].    
+00012680: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7466  ] = None,.    tf
+00012690: 5f63 7573 746f 6d5f 7472 616e 7366 6f72  _custom_transfor
+000126a0: 6d61 7469 6f6e 5f64 6566 696e 6974 696f  mation_definitio
+000126b0: 6e73 3a20 4f70 7469 6f6e 616c 5b4c 6973  ns: Optional[Lis
+000126c0: 745b 4469 6374 5b73 7472 2c20 416e 795d  t[Dict[str, Any]
+000126d0: 5d5d 203d 204e 6f6e 652c 0a20 2020 2074  ]] = None,.    t
+000126e0: 665f 7472 616e 7366 6f72 6d61 7469 6f6e  f_transformation
+000126f0: 735f 7061 7468 3a20 4f70 7469 6f6e 616c  s_path: Optional
+00012700: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 2020  [str] = None,.  
+00012710: 2020 6461 7461 5f73 6f75 7263 655f 6373    data_source_cs
+00012720: 765f 6669 6c65 6e61 6d65 733a 204f 7074  v_filenames: Opt
+00012730: 696f 6e61 6c5b 7374 725d 203d 204e 6f6e  ional[str] = Non
+00012740: 652c 0a20 2020 2064 6174 615f 736f 7572  e,.    data_sour
+00012750: 6365 5f62 6967 7175 6572 795f 7461 626c  ce_bigquery_tabl
+00012760: 655f 7061 7468 3a20 4f70 7469 6f6e 616c  e_path: Optional
+00012770: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 2020  [str] = None,.  
+00012780: 2020 6269 6771 7565 7279 5f73 7461 6769    bigquery_stagi
+00012790: 6e67 5f66 756c 6c5f 6461 7461 7365 745f  ng_full_dataset_
+000127a0: 6964 3a20 4f70 7469 6f6e 616c 5b73 7472  id: Optional[str
+000127b0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7765  ] = None,.    we
+000127c0: 6967 6874 5f63 6f6c 756d 6e3a 2073 7472  ight_column: str
+000127d0: 203d 2027 272c 0a20 2020 206d 6178 5f66   = '',.    max_f
+000127e0: 6169 6c65 645f 7472 6961 6c5f 636f 756e  ailed_trial_coun
+000127f0: 743a 2069 6e74 203d 2030 2c0a 2020 2020  t: int = 0,.    
+00012800: 7374 7564 795f 7370 6563 5f61 6c67 6f72  study_spec_algor
+00012810: 6974 686d 3a20 7374 7220 3d20 2741 4c47  ithm: str = 'ALG
+00012820: 4f52 4954 484d 5f55 4e53 5045 4349 4649  ORITHM_UNSPECIFI
+00012830: 4544 272c 0a20 2020 2073 7475 6479 5f73  ED',.    study_s
+00012840: 7065 635f 6d65 6173 7572 656d 656e 745f  pec_measurement_
+00012850: 7365 6c65 6374 696f 6e5f 7479 7065 3a20  selection_type: 
+00012860: 7374 7220 3d20 2742 4553 545f 4d45 4153  str = 'BEST_MEAS
+00012870: 5552 454d 454e 5427 2c0a 2020 2020 7472  UREMENT',.    tr
+00012880: 616e 7366 6f72 6d5f 6461 7461 666c 6f77  ansform_dataflow
+00012890: 5f6d 6163 6869 6e65 5f74 7970 653a 2073  _machine_type: s
+000128a0: 7472 203d 2027 6e31 2d73 7461 6e64 6172  tr = 'n1-standar
+000128b0: 642d 3136 272c 0a20 2020 2074 7261 6e73  d-16',.    trans
+000128c0: 666f 726d 5f64 6174 6166 6c6f 775f 6d61  form_dataflow_ma
+000128d0: 785f 6e75 6d5f 776f 726b 6572 733a 2069  x_num_workers: i
+000128e0: 6e74 203d 2032 352c 0a20 2020 2074 7261  nt = 25,.    tra
+000128f0: 6e73 666f 726d 5f64 6174 6166 6c6f 775f  nsform_dataflow_
+00012900: 6469 736b 5f73 697a 655f 6762 3a20 696e  disk_size_gb: in
+00012910: 7420 3d20 3430 2c0a 2020 2020 776f 726b  t = 40,.    work
+00012920: 6572 5f70 6f6f 6c5f 7370 6563 735f 6f76  er_pool_specs_ov
+00012930: 6572 7269 6465 3a20 4f70 7469 6f6e 616c  erride: Optional
+00012940: 5b44 6963 745b 7374 722c 2041 6e79 5d5d  [Dict[str, Any]]
+00012950: 203d 204e 6f6e 652c 0a20 2020 2072 756e   = None,.    run
+00012960: 5f65 7661 6c75 6174 696f 6e3a 2062 6f6f  _evaluation: boo
+00012970: 6c20 3d20 5472 7565 2c0a 2020 2020 6576  l = True,.    ev
+00012980: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+00012990: 7265 6469 6374 5f6d 6163 6869 6e65 5f74  redict_machine_t
+000129a0: 7970 653a 2073 7472 203d 205f 4556 414c  ype: str = _EVAL
+000129b0: 5541 5449 4f4e 5f42 4154 4348 5f50 5245  UATION_BATCH_PRE
+000129c0: 4449 4354 5f4d 4143 4849 4e45 5f54 5950  DICT_MACHINE_TYP
+000129d0: 452c 0a20 2020 2065 7661 6c75 6174 696f  E,.    evaluatio
+000129e0: 6e5f 6261 7463 685f 7072 6564 6963 745f  n_batch_predict_
+000129f0: 7374 6172 7469 6e67 5f72 6570 6c69 6361  starting_replica
+00012a00: 5f63 6f75 6e74 3a20 696e 7420 3d20 5f45  _count: int = _E
+00012a10: 5641 4c55 4154 494f 4e5f 4241 5443 485f  VALUATION_BATCH_
+00012a20: 5052 4544 4943 545f 5354 4152 5449 4e47  PREDICT_STARTING
+00012a30: 5f52 4550 4c49 4341 5f43 4f55 4e54 2c0a  _REPLICA_COUNT,.
+00012a40: 2020 2020 6576 616c 7561 7469 6f6e 5f62      evaluation_b
+00012a50: 6174 6368 5f70 7265 6469 6374 5f6d 6178  atch_predict_max
+00012a60: 5f72 6570 6c69 6361 5f63 6f75 6e74 3a20  _replica_count: 
+00012a70: 696e 7420 3d20 5f45 5641 4c55 4154 494f  int = _EVALUATIO
+00012a80: 4e5f 4241 5443 485f 5052 4544 4943 545f  N_BATCH_PREDICT_
+00012a90: 4d41 585f 5245 504c 4943 415f 434f 554e  MAX_REPLICA_COUN
+00012aa0: 542c 0a20 2020 2065 7661 6c75 6174 696f  T,.    evaluatio
+00012ab0: 6e5f 6461 7461 666c 6f77 5f6d 6163 6869  n_dataflow_machi
+00012ac0: 6e65 5f74 7970 653a 2073 7472 203d 205f  ne_type: str = _
+00012ad0: 4556 414c 5541 5449 4f4e 5f44 4154 4146  EVALUATION_DATAF
+00012ae0: 4c4f 575f 4d41 4348 494e 455f 5459 5045  LOW_MACHINE_TYPE
+00012af0: 2c0a 2020 2020 6576 616c 7561 7469 6f6e  ,.    evaluation
+00012b00: 5f64 6174 6166 6c6f 775f 7374 6172 7469  _dataflow_starti
+00012b10: 6e67 5f6e 756d 5f77 6f72 6b65 7273 3a20  ng_num_workers: 
+00012b20: 696e 7420 3d20 5f45 5641 4c55 4154 494f  int = _EVALUATIO
+00012b30: 4e5f 4441 5441 464c 4f57 5f53 5441 5254  N_DATAFLOW_START
+00012b40: 494e 475f 4e55 4d5f 574f 524b 4552 532c  ING_NUM_WORKERS,
+00012b50: 0a20 2020 2065 7661 6c75 6174 696f 6e5f  .    evaluation_
+00012b60: 6461 7461 666c 6f77 5f6d 6178 5f6e 756d  dataflow_max_num
+00012b70: 5f77 6f72 6b65 7273 3a20 696e 7420 3d20  _workers: int = 
+00012b80: 5f45 5641 4c55 4154 494f 4e5f 4441 5441  _EVALUATION_DATA
+00012b90: 464c 4f57 5f4d 4158 5f4e 554d 5f57 4f52  FLOW_MAX_NUM_WOR
+00012ba0: 4b45 5253 2c0a 2020 2020 6576 616c 7561  KERS,.    evalua
+00012bb0: 7469 6f6e 5f64 6174 6166 6c6f 775f 6469  tion_dataflow_di
+00012bc0: 736b 5f73 697a 655f 6762 3a20 696e 7420  sk_size_gb: int 
+00012bd0: 3d20 5f45 5641 4c55 4154 494f 4e5f 4441  = _EVALUATION_DA
+00012be0: 5441 464c 4f57 5f44 4953 4b5f 5349 5a45  TAFLOW_DISK_SIZE
+00012bf0: 5f47 422c 0a20 2020 2064 6174 6166 6c6f  _GB,.    dataflo
+00012c00: 775f 7365 7276 6963 655f 6163 636f 756e  w_service_accoun
+00012c10: 743a 2073 7472 203d 2027 272c 0a20 2020  t: str = '',.   
+00012c20: 2064 6174 6166 6c6f 775f 7375 626e 6574   dataflow_subnet
+00012c30: 776f 726b 3a20 7374 7220 3d20 2727 2c0a  work: str = '',.
+00012c40: 2020 2020 6461 7461 666c 6f77 5f75 7365      dataflow_use
+00012c50: 5f70 7562 6c69 635f 6970 733a 2062 6f6f  _public_ips: boo
+00012c60: 6c20 3d20 5472 7565 2c0a 2020 2020 656e  l = True,.    en
+00012c70: 6372 7970 7469 6f6e 5f73 7065 635f 6b65  cryption_spec_ke
+00012c80: 795f 6e61 6d65 3a20 7374 7220 3d20 2727  y_name: str = ''
+00012c90: 2c0a 2920 2d3e 2054 7570 6c65 5b73 7472  ,.) -> Tuple[str
+00012ca0: 2c20 4469 6374 5b73 7472 2c20 416e 795d  , Dict[str, Any]
+00012cb0: 5d3a 0a20 2022 2222 4765 7420 7468 6520  ]:.  """Get the 
+00012cc0: 6275 696c 742d 696e 2061 6c67 6f72 6974  built-in algorit
+00012cd0: 686d 2048 7970 6572 7061 7261 6d65 7465  hm Hyperparamete
+00012ce0: 7254 756e 696e 674a 6f62 2070 6970 656c  rTuningJob pipel
+00012cf0: 696e 652e 0a0a 2020 4172 6773 3a0a 2020  ine...  Args:.  
+00012d00: 2020 7072 6f6a 6563 743a 2054 6865 2047    project: The G
+00012d10: 4350 2070 726f 6a65 6374 2074 6861 7420  CP project that 
+00012d20: 7275 6e73 2074 6865 2070 6970 656c 696e  runs the pipelin
+00012d30: 6520 636f 6d70 6f6e 656e 7473 2e0a 2020  e components..  
+00012d40: 2020 6c6f 6361 7469 6f6e 3a20 5468 6520    location: The 
+00012d50: 4743 5020 7265 6769 6f6e 2074 6861 7420  GCP region that 
+00012d60: 7275 6e73 2074 6865 2070 6970 656c 696e  runs the pipelin
+00012d70: 6520 636f 6d70 6f6e 656e 7473 2e0a 2020  e components..  
+00012d80: 2020 726f 6f74 5f64 6972 3a20 5468 6520    root_dir: The 
+00012d90: 726f 6f74 2047 4353 2064 6972 6563 746f  root GCS directo
+00012da0: 7279 2066 6f72 2074 6865 2070 6970 656c  ry for the pipel
+00012db0: 696e 6520 636f 6d70 6f6e 656e 7473 2e0a  ine components..
+00012dc0: 2020 2020 7461 7267 6574 5f63 6f6c 756d      target_colum
+00012dd0: 6e3a 2054 6865 2074 6172 6765 7420 636f  n: The target co
+00012de0: 6c75 6d6e 206e 616d 652e 0a20 2020 2070  lumn name..    p
+00012df0: 7265 6469 6374 696f 6e5f 7479 7065 3a20  rediction_type: 
+00012e00: 5468 6520 7479 7065 206f 6620 7072 6564  The type of pred
+00012e10: 6963 7469 6f6e 2074 6865 206d 6f64 656c  iction the model
+00012e20: 2069 7320 746f 2070 726f 6475 6365 2e0a   is to produce..
+00012e30: 2020 2020 2020 2263 6c61 7373 6966 6963        "classific
+00012e40: 6174 696f 6e22 206f 7220 2272 6567 7265  ation" or "regre
+00012e50: 7373 696f 6e22 2e0a 2020 2020 7374 7564  ssion"..    stud
+00012e60: 795f 7370 6563 5f6d 6574 7269 635f 6964  y_spec_metric_id
+00012e70: 3a20 4d65 7472 6963 2074 6f20 6f70 7469  : Metric to opti
+00012e80: 6d69 7a65 2c20 706f 7373 6962 6c65 2076  mize, possible v
+00012e90: 616c 7565 733a 205b 2027 6c6f 7373 272c  alues: [ 'loss',
+00012ea0: 0a20 2020 2020 2027 6176 6572 6167 655f  .      'average_
+00012eb0: 6c6f 7373 272c 2027 726d 7365 272c 2027  loss', 'rmse', '
+00012ec0: 6d61 6527 2c20 276d 716c 272c 2027 6163  mae', 'mql', 'ac
+00012ed0: 6375 7261 6379 272c 2027 6175 6327 2c20  curacy', 'auc', 
+00012ee0: 2770 7265 6369 7369 6f6e 272c 0a20 2020  'precision',.   
+00012ef0: 2020 2027 7265 6361 6c6c 275d 2e0a 2020     'recall']..  
+00012f00: 2020 7374 7564 795f 7370 6563 5f6d 6574    study_spec_met
+00012f10: 7269 635f 676f 616c 3a20 4f70 7469 6d69  ric_goal: Optimi
+00012f20: 7a61 7469 6f6e 2067 6f61 6c20 6f66 2074  zation goal of t
+00012f30: 6865 206d 6574 7269 632c 2070 6f73 7369  he metric, possi
+00012f40: 626c 6520 7661 6c75 6573 3a0a 2020 2020  ble values:.    
+00012f50: 2020 224d 4158 494d 495a 4522 2c20 224d    "MAXIMIZE", "M
+00012f60: 494e 494d 495a 4522 2e0a 2020 2020 7374  INIMIZE"..    st
+00012f70: 7564 795f 7370 6563 5f70 6172 616d 6574  udy_spec_paramet
+00012f80: 6572 735f 6f76 6572 7269 6465 3a20 4c69  ers_override: Li
+00012f90: 7374 206f 6620 6469 6374 696f 6e61 7269  st of dictionari
+00012fa0: 6573 2072 6570 7265 7365 6e74 696e 6720  es representing 
+00012fb0: 7061 7261 6d65 7465 7273 0a20 2020 2020  parameters.     
+00012fc0: 2074 6f20 6f70 7469 6d69 7a65 2e20 5468   to optimize. Th
+00012fd0: 6520 6469 6374 696f 6e61 7279 206b 6579  e dictionary key
+00012fe0: 2069 7320 7468 6520 7061 7261 6d65 7465   is the paramete
+00012ff0: 725f 6964 2c20 7768 6963 6820 6973 2070  r_id, which is p
+00013000: 6173 7365 6420 746f 0a20 2020 2020 2074  assed to.      t
+00013010: 7261 696e 696e 6720 6a6f 6220 6173 2061  raining job as a
+00013020: 2063 6f6d 6d61 6e64 206c 696e 6520 6172   command line ar
+00013030: 6775 6d65 6e74 2c20 616e 6420 7468 6520  gument, and the 
+00013040: 6469 6374 696f 6e61 7279 2076 616c 7565  dictionary value
+00013050: 2069 7320 7468 650a 2020 2020 2020 7061   is the.      pa
+00013060: 7261 6d65 7465 7220 7370 6563 6966 6963  rameter specific
+00013070: 6174 696f 6e20 6f66 2074 6865 206d 6574  ation of the met
+00013080: 7269 632e 0a20 2020 206d 6178 5f74 7269  ric..    max_tri
+00013090: 616c 5f63 6f75 6e74 3a20 5468 6520 6465  al_count: The de
+000130a0: 7369 7265 6420 746f 7461 6c20 6e75 6d62  sired total numb
+000130b0: 6572 206f 6620 7472 6961 6c73 2e0a 2020  er of trials..  
+000130c0: 2020 7061 7261 6c6c 656c 5f74 7269 616c    parallel_trial
+000130d0: 5f63 6f75 6e74 3a20 5468 6520 6465 7369  _count: The desi
+000130e0: 7265 6420 6e75 6d62 6572 206f 6620 7472  red number of tr
+000130f0: 6961 6c73 2074 6f20 7275 6e20 696e 2070  ials to run in p
+00013100: 6172 616c 6c65 6c2e 0a20 2020 2061 6c67  arallel..    alg
+00013110: 6f72 6974 686d 3a20 416c 676f 7269 7468  orithm: Algorith
+00013120: 6d20 746f 2074 7261 696e 2e20 4f6e 6520  m to train. One 
+00013130: 6f66 2022 7461 626e 6574 2220 616e 6420  of "tabnet" and 
+00013140: 2277 6964 655f 616e 645f 6465 6570 222e  "wide_and_deep".
+00013150: 0a20 2020 2065 6e61 626c 655f 7072 6f66  .    enable_prof
+00013160: 696c 6572 3a20 456e 6162 6c65 7320 7072  iler: Enables pr
+00013170: 6f66 696c 696e 6720 616e 6420 7361 7665  ofiling and save
+00013180: 7320 6120 7472 6163 6520 6475 7269 6e67  s a trace during
+00013190: 2065 7661 6c75 6174 696f 6e2e 0a20 2020   evaluation..   
+000131a0: 2073 6565 643a 2053 6565 6420 746f 2062   seed: Seed to b
+000131b0: 6520 7573 6564 2066 6f72 2074 6869 7320  e used for this 
+000131c0: 7275 6e2e 0a20 2020 2065 7661 6c5f 7374  run..    eval_st
+000131d0: 6570 733a 204e 756d 6265 7220 6f66 2073  eps: Number of s
+000131e0: 7465 7073 2074 6f20 7275 6e20 6576 616c  teps to run eval
+000131f0: 7561 7469 6f6e 2066 6f72 2e20 4966 206e  uation for. If n
+00013200: 6f74 2073 7065 6369 6669 6564 206f 720a  ot specified or.
+00013210: 2020 2020 2020 6e65 6761 7469 7665 2c20        negative, 
+00013220: 6974 206d 6561 6e73 2072 756e 2065 7661  it means run eva
+00013230: 6c75 6174 696f 6e20 6f6e 2074 6865 2077  luation on the w
+00013240: 686f 6c65 2076 616c 6964 6174 696f 6e20  hole validation 
+00013250: 6461 7461 7365 742e 2049 6620 7365 740a  dataset. If set.
+00013260: 2020 2020 2020 746f 2030 2c20 6974 206d        to 0, it m
+00013270: 6561 6e73 2072 756e 2065 7661 6c75 6174  eans run evaluat
+00013280: 696f 6e20 666f 7220 6120 6669 7865 6420  ion for a fixed 
+00013290: 6e75 6d62 6572 206f 6620 7361 6d70 6c65  number of sample
+000132a0: 732e 0a20 2020 2065 7661 6c5f 6672 6571  s..    eval_freq
+000132b0: 7565 6e63 795f 7365 6373 3a20 4672 6571  uency_secs: Freq
+000132c0: 7565 6e63 7920 6174 2077 6869 6368 2065  uency at which e
+000132d0: 7661 6c75 6174 696f 6e20 616e 6420 6368  valuation and ch
+000132e0: 6563 6b70 6f69 6e74 696e 6720 7769 6c6c  eckpointing will
+000132f0: 0a20 2020 2020 2074 616b 6520 706c 6163  .      take plac
+00013300: 652e 0a20 2020 2074 7261 6e73 666f 726d  e..    transform
+00013310: 5f63 6f6e 6669 673a 2050 6174 6820 746f  _config: Path to
+00013320: 2076 3120 5446 2074 7261 6e73 666f 726d   v1 TF transform
+00013330: 6174 696f 6e20 636f 6e66 6967 7572 6174  ation configurat
+00013340: 696f 6e2e 0a20 2020 2064 6174 6173 6574  ion..    dataset
+00013350: 5f6c 6576 656c 5f63 7573 746f 6d5f 7472  _level_custom_tr
+00013360: 616e 7366 6f72 6d61 7469 6f6e 5f64 6566  ansformation_def
+00013370: 696e 6974 696f 6e73 3a20 4461 7461 7365  initions: Datase
+00013380: 742d 6c65 7665 6c20 6375 7374 6f6d 0a20  t-level custom. 
+00013390: 2020 2020 2074 7261 6e73 666f 726d 6174       transformat
+000133a0: 696f 6e20 6465 6669 6e69 7469 6f6e 7320  ion definitions 
+000133b0: 696e 2073 7472 696e 6720 666f 726d 6174  in string format
+000133c0: 2e0a 2020 2020 6461 7461 7365 745f 6c65  ..    dataset_le
+000133d0: 7665 6c5f 7472 616e 7366 6f72 6d61 7469  vel_transformati
+000133e0: 6f6e 733a 2044 6174 6173 6574 2d6c 6576  ons: Dataset-lev
+000133f0: 656c 2074 7261 6e73 666f 726d 6174 696f  el transformatio
+00013400: 6e20 636f 6e66 6967 7572 6174 696f 6e20  n configuration 
+00013410: 696e 0a20 2020 2020 2073 7472 696e 6720  in.      string 
+00013420: 666f 726d 6174 2e0a 2020 2020 7072 6564  format..    pred
+00013430: 6566 696e 6564 5f73 706c 6974 5f6b 6579  efined_split_key
+00013440: 3a20 5072 6564 6566 696e 6564 2073 706c  : Predefined spl
+00013450: 6974 206b 6579 2e0a 2020 2020 7374 7261  it key..    stra
+00013460: 7469 6669 6564 5f73 706c 6974 5f6b 6579  tified_split_key
+00013470: 3a20 5374 7261 7469 6669 6564 2073 706c  : Stratified spl
+00013480: 6974 206b 6579 2e0a 2020 2020 7472 6169  it key..    trai
+00013490: 6e69 6e67 5f66 7261 6374 696f 6e3a 2054  ning_fraction: T
+000134a0: 7261 696e 696e 6720 6672 6163 7469 6f6e  raining fraction
+000134b0: 2e0a 2020 2020 7661 6c69 6461 7469 6f6e  ..    validation
+000134c0: 5f66 7261 6374 696f 6e3a 2056 616c 6964  _fraction: Valid
+000134d0: 6174 696f 6e20 6672 6163 7469 6f6e 2e0a  ation fraction..
+000134e0: 2020 2020 7465 7374 5f66 7261 6374 696f      test_fractio
+000134f0: 6e3a 2054 6573 7420 6672 6163 7469 6f6e  n: Test fraction
+00013500: 2e0a 2020 2020 7466 5f74 7261 6e73 666f  ..    tf_transfo
+00013510: 726d 5f65 7865 6375 7469 6f6e 5f65 6e67  rm_execution_eng
+00013520: 696e 653a 2054 6865 2065 7865 6375 7469  ine: The executi
+00013530: 6f6e 2065 6e67 696e 6520 7573 6564 2074  on engine used t
+00013540: 6f20 6578 6563 7574 6520 5446 2d62 6173  o execute TF-bas
+00013550: 6564 0a20 2020 2020 2074 7261 6e73 666f  ed.      transfo
+00013560: 726d 6174 696f 6e73 2e0a 2020 2020 7466  rmations..    tf
+00013570: 5f61 7574 6f5f 7472 616e 7366 6f72 6d5f  _auto_transform_
+00013580: 6665 6174 7572 6573 3a20 4c69 7374 206f  features: List o
+00013590: 6620 6175 746f 2074 7261 6e73 666f 726d  f auto transform
+000135a0: 2066 6561 7475 7265 7320 696e 2074 6865   features in the
+000135b0: 0a20 2020 2020 2063 6f6d 6d61 2d73 6570  .      comma-sep
+000135c0: 6172 6174 6564 2073 7472 696e 6720 666f  arated string fo
+000135d0: 726d 6174 2e0a 2020 2020 7466 5f63 7573  rmat..    tf_cus
+000135e0: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
+000135f0: 6f6e 5f64 6566 696e 6974 696f 6e73 3a20  on_definitions: 
+00013600: 5446 2063 7573 746f 6d20 7472 616e 7366  TF custom transf
+00013610: 6f72 6d61 7469 6f6e 2064 6566 696e 6974  ormation definit
+00013620: 696f 6e73 0a20 2020 2020 2069 6e20 7374  ions.      in st
+00013630: 7269 6e67 2066 6f72 6d61 742e 0a20 2020  ring format..   
+00013640: 2074 665f 7472 616e 7366 6f72 6d61 7469   tf_transformati
+00013650: 6f6e 735f 7061 7468 3a20 5061 7468 2074  ons_path: Path t
+00013660: 6f20 5446 2074 7261 6e73 666f 726d 6174  o TF transformat
+00013670: 696f 6e20 636f 6e66 6967 7572 6174 696f  ion configuratio
+00013680: 6e2e 0a20 2020 2064 6174 615f 736f 7572  n..    data_sour
+00013690: 6365 5f63 7376 5f66 696c 656e 616d 6573  ce_csv_filenames
+000136a0: 3a20 5468 6520 4353 5620 6461 7461 2073  : The CSV data s
+000136b0: 6f75 7263 652e 0a20 2020 2064 6174 615f  ource..    data_
+000136c0: 736f 7572 6365 5f62 6967 7175 6572 795f  source_bigquery_
+000136d0: 7461 626c 655f 7061 7468 3a20 5468 6520  table_path: The 
+000136e0: 4269 6751 7565 7279 2064 6174 6120 736f  BigQuery data so
+000136f0: 7572 6365 2e0a 2020 2020 6269 6771 7565  urce..    bigque
+00013700: 7279 5f73 7461 6769 6e67 5f66 756c 6c5f  ry_staging_full_
+00013710: 6461 7461 7365 745f 6964 3a20 5468 6520  dataset_id: The 
+00013720: 4269 6751 7565 7279 2073 7461 6769 6e67  BigQuery staging
+00013730: 2066 756c 6c20 6461 7461 7365 7420 6964   full dataset id
+00013740: 2066 6f72 0a20 2020 2020 2073 746f 7269   for.      stori
+00013750: 6e67 2069 6e74 6572 6d65 6469 6174 6520  ng intermediate 
+00013760: 7461 626c 6573 2e0a 2020 2020 7765 6967  tables..    weig
+00013770: 6874 5f63 6f6c 756d 6e3a 2054 6865 2077  ht_column: The w
+00013780: 6569 6768 7420 636f 6c75 6d6e 206e 616d  eight column nam
+00013790: 652e 0a20 2020 206d 6178 5f66 6169 6c65  e..    max_faile
+000137a0: 645f 7472 6961 6c5f 636f 756e 743a 2054  d_trial_count: T
+000137b0: 6865 206e 756d 6265 7220 6f66 2066 6169  he number of fai
+000137c0: 6c65 6420 7472 6961 6c73 2074 6861 7420  led trials that 
+000137d0: 6e65 6564 2074 6f20 6265 2073 6565 6e0a  need to be seen.
+000137e0: 2020 2020 2020 6265 666f 7265 2066 6169        before fai
+000137f0: 6c69 6e67 2074 6865 2048 7970 6572 7061  ling the Hyperpa
+00013800: 7261 6d65 7465 7254 756e 696e 674a 6f62  rameterTuningJob
+00013810: 2e20 4966 2073 6574 2074 6f20 302c 2056  . If set to 0, V
+00013820: 6572 7465 7820 4149 2064 6563 6964 6573  ertex AI decides
+00013830: 0a20 2020 2020 2068 6f77 206d 616e 7920  .      how many 
+00013840: 7472 6961 6c73 206d 7573 7420 6661 696c  trials must fail
+00013850: 2062 6566 6f72 6520 7468 6520 7768 6f6c   before the whol
+00013860: 6520 6a6f 6220 6661 696c 732e 0a20 2020  e job fails..   
+00013870: 2073 7475 6479 5f73 7065 635f 616c 676f   study_spec_algo
+00013880: 7269 7468 6d3a 2054 6865 2073 6561 7263  rithm: The searc
+00013890: 6820 616c 676f 7269 7468 6d20 7370 6563  h algorithm spec
+000138a0: 6966 6965 6420 666f 7220 7468 6520 7374  ified for the st
+000138b0: 7564 792e 204f 6e65 206f 660a 2020 2020  udy. One of.    
+000138c0: 2020 2241 4c47 4f52 4954 484d 5f55 4e53    "ALGORITHM_UNS
+000138d0: 5045 4349 4649 4544 222c 2022 4752 4944  PECIFIED", "GRID
+000138e0: 5f53 4541 5243 4822 2c20 6f72 2022 5241  _SEARCH", or "RA
+000138f0: 4e44 4f4d 5f53 4541 5243 4822 2e0a 2020  NDOM_SEARCH"..  
+00013900: 2020 7374 7564 795f 7370 6563 5f6d 6561    study_spec_mea
+00013910: 7375 7265 6d65 6e74 5f73 656c 6563 7469  surement_selecti
+00013920: 6f6e 5f74 7970 653a 2057 6869 6368 206d  on_type: Which m
+00013930: 6561 7375 7265 6d65 6e74 2074 6f20 7573  easurement to us
+00013940: 6520 6966 2f77 6865 6e20 7468 650a 2020  e if/when the.  
+00013950: 2020 2020 7365 7276 6963 6520 6175 746f      service auto
+00013960: 6d61 7469 6361 6c6c 7920 7365 6c65 6374  matically select
+00013970: 7320 7468 6520 6669 6e61 6c20 6d65 6173  s the final meas
+00013980: 7572 656d 656e 7420 6672 6f6d 2070 7265  urement from pre
+00013990: 7669 6f75 736c 790a 2020 2020 2020 7265  viously.      re
+000139a0: 706f 7274 6564 2069 6e74 6572 6d65 6469  ported intermedi
+000139b0: 6174 6520 6d65 6173 7572 656d 656e 7473  ate measurements
+000139c0: 2e20 4f6e 6520 6f66 2022 4245 5354 5f4d  . One of "BEST_M
+000139d0: 4541 5355 5245 4d45 4e54 2220 6f72 0a20  EASUREMENT" or. 
+000139e0: 2020 2020 2022 4c41 5354 5f4d 4541 5355       "LAST_MEASU
+000139f0: 5245 4d45 4e54 222e 0a20 2020 2074 7261  REMENT"..    tra
+00013a00: 6e73 666f 726d 5f64 6174 6166 6c6f 775f  nsform_dataflow_
+00013a10: 6d61 6368 696e 655f 7479 7065 3a20 5468  machine_type: Th
+00013a20: 6520 6461 7461 666c 6f77 206d 6163 6869  e dataflow machi
+00013a30: 6e65 2074 7970 6520 666f 7220 7472 616e  ne type for tran
+00013a40: 7366 6f72 6d0a 2020 2020 2020 636f 6d70  sform.      comp
+00013a50: 6f6e 656e 742e 0a20 2020 2074 7261 6e73  onent..    trans
+00013a60: 666f 726d 5f64 6174 6166 6c6f 775f 6d61  form_dataflow_ma
+00013a70: 785f 6e75 6d5f 776f 726b 6572 733a 2054  x_num_workers: T
+00013a80: 6865 206d 6178 206e 756d 6265 7220 6f66  he max number of
+00013a90: 2044 6174 6166 6c6f 7720 776f 726b 6572   Dataflow worker
+00013aa0: 7320 666f 720a 2020 2020 2020 7472 616e  s for.      tran
+00013ab0: 7366 6f72 6d20 636f 6d70 6f6e 656e 742e  sform component.
+00013ac0: 0a20 2020 2074 7261 6e73 666f 726d 5f64  .    transform_d
+00013ad0: 6174 6166 6c6f 775f 6469 736b 5f73 697a  ataflow_disk_siz
+00013ae0: 655f 6762 3a20 4461 7461 666c 6f77 2077  e_gb: Dataflow w
+00013af0: 6f72 6b65 7227 7320 6469 736b 2073 697a  orker's disk siz
+00013b00: 6520 696e 2047 4220 666f 720a 2020 2020  e in GB for.    
+00013b10: 2020 7472 616e 7366 6f72 6d20 636f 6d70    transform comp
+00013b20: 6f6e 656e 742e 0a20 2020 2077 6f72 6b65  onent..    worke
+00013b30: 725f 706f 6f6c 5f73 7065 6373 5f6f 7665  r_pool_specs_ove
+00013b40: 7272 6964 653a 2054 6865 2064 6963 7469  rride: The dicti
+00013b50: 6f6e 6172 7920 666f 7220 6f76 6572 7269  onary for overri
+00013b60: 6469 6e67 2074 7261 696e 696e 6720 616e  ding training an
+00013b70: 640a 2020 2020 2020 6576 616c 7561 7469  d.      evaluati
+00013b80: 6f6e 2077 6f72 6b65 7220 706f 6f6c 2073  on worker pool s
+00013b90: 7065 6373 2e20 5468 6520 6469 6374 696f  pecs. The dictio
+00013ba0: 6e61 7279 2073 686f 756c 6420 6265 206f  nary should be o
+00013bb0: 6620 666f 726d 6174 0a20 2020 2020 2020  f format.       
+00013bc0: 2020 2068 7474 7073 3a2f 2f67 6974 6875     https://githu
+00013bd0: 622e 636f 6d2f 676f 6f67 6c65 6170 6973  b.com/googleapis
+00013be0: 2f67 6f6f 676c 6561 7069 732f 626c 6f62  /googleapis/blob
+00013bf0: 2f34 6538 3336 6337 6332 3537 6533 6532  /4e836c7c257e3e2
+00013c00: 3062 3164 6531 3464 3437 3039 3933 6132  0b1de14d470993a2
+00013c10: 6231 6634 3733 3661 382f 676f 6f67 6c65  b1f4736a8/google
+00013c20: 2f63 6c6f 7564 2f61 6970 6c61 7466 6f72  /cloud/aiplatfor
+00013c30: 6d2f 7631 6265 7461 312f 6375 7374 6f6d  m/v1beta1/custom
+00013c40: 5f6a 6f62 2e70 726f 746f 234c 3137 322e  _job.proto#L172.
+00013c50: 0a20 2020 2072 756e 5f65 7661 6c75 6174  .    run_evaluat
+00013c60: 696f 6e3a 2057 6865 7468 6572 2074 6f20  ion: Whether to 
+00013c70: 7275 6e20 6576 616c 7561 7469 6f6e 2073  run evaluation s
+00013c80: 7465 7073 2064 7572 696e 6720 7472 6169  teps during trai
+00013c90: 6e69 6e67 2e0a 2020 2020 6576 616c 7561  ning..    evalua
+00013ca0: 7469 6f6e 5f62 6174 6368 5f70 7265 6469  tion_batch_predi
+00013cb0: 6374 5f6d 6163 6869 6e65 5f74 7970 653a  ct_machine_type:
+00013cc0: 2054 6865 2070 7265 6469 6374 696f 6e20   The prediction 
+00013cd0: 7365 7276 6572 206d 6163 6869 6e65 2074  server machine t
+00013ce0: 7970 650a 2020 2020 2020 666f 7220 6261  ype.      for ba
+00013cf0: 7463 6820 7072 6564 6963 7420 636f 6d70  tch predict comp
+00013d00: 6f6e 656e 7473 2064 7572 696e 6720 6576  onents during ev
+00013d10: 616c 7561 7469 6f6e 2e0a 2020 2020 6576  aluation..    ev
+00013d20: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+00013d30: 7265 6469 6374 5f73 7461 7274 696e 675f  redict_starting_
+00013d40: 7265 706c 6963 615f 636f 756e 743a 2054  replica_count: T
+00013d50: 6865 2069 6e69 7469 616c 206e 756d 6265  he initial numbe
+00013d60: 7220 6f66 0a20 2020 2020 2070 7265 6469  r of.      predi
+00013d70: 6374 696f 6e20 7365 7276 6572 2066 6f72  ction server for
+00013d80: 2062 6174 6368 2070 7265 6469 6374 2063   batch predict c
+00013d90: 6f6d 706f 6e65 6e74 7320 6475 7269 6e67  omponents during
+00013da0: 2065 7661 6c75 6174 696f 6e2e 0a20 2020   evaluation..   
+00013db0: 2065 7661 6c75 6174 696f 6e5f 6261 7463   evaluation_batc
+00013dc0: 685f 7072 6564 6963 745f 6d61 785f 7265  h_predict_max_re
+00013dd0: 706c 6963 615f 636f 756e 743a 2054 6865  plica_count: The
+00013de0: 206d 6178 206e 756d 6265 7220 6f66 2070   max number of p
+00013df0: 7265 6469 6374 696f 6e0a 2020 2020 2020  rediction.      
+00013e00: 7365 7276 6572 2066 6f72 2062 6174 6368  server for batch
+00013e10: 2070 7265 6469 6374 2063 6f6d 706f 6e65   predict compone
+00013e20: 6e74 7320 6475 7269 6e67 2065 7661 6c75  nts during evalu
+00013e30: 6174 696f 6e2e 0a20 2020 2065 7661 6c75  ation..    evalu
+00013e40: 6174 696f 6e5f 6461 7461 666c 6f77 5f6d  ation_dataflow_m
+00013e50: 6163 6869 6e65 5f74 7970 653a 2054 6865  achine_type: The
+00013e60: 2064 6174 6166 6c6f 7720 6d61 6368 696e   dataflow machin
+00013e70: 6520 7479 7065 2066 6f72 2065 7661 6c75  e type for evalu
+00013e80: 6174 696f 6e0a 2020 2020 2020 636f 6d70  ation.      comp
+00013e90: 6f6e 656e 7473 2e0a 2020 2020 6576 616c  onents..    eval
+00013ea0: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
+00013eb0: 7374 6172 7469 6e67 5f6e 756d 5f77 6f72  starting_num_wor
+00013ec0: 6b65 7273 3a20 5468 6520 696e 6974 6961  kers: The initia
+00013ed0: 6c20 6e75 6d62 6572 206f 6620 4461 7461  l number of Data
+00013ee0: 666c 6f77 0a20 2020 2020 2077 6f72 6b65  flow.      worke
+00013ef0: 7273 2066 6f72 2065 7661 6c75 6174 696f  rs for evaluatio
+00013f00: 6e20 636f 6d70 6f6e 656e 7473 2e0a 2020  n components..  
+00013f10: 2020 6576 616c 7561 7469 6f6e 5f64 6174    evaluation_dat
+00013f20: 6166 6c6f 775f 6d61 785f 6e75 6d5f 776f  aflow_max_num_wo
+00013f30: 726b 6572 733a 2054 6865 206d 6178 206e  rkers: The max n
+00013f40: 756d 6265 7220 6f66 2044 6174 6166 6c6f  umber of Dataflo
+00013f50: 7720 776f 726b 6572 7320 666f 720a 2020  w workers for.  
+00013f60: 2020 2020 6576 616c 7561 7469 6f6e 2063      evaluation c
+00013f70: 6f6d 706f 6e65 6e74 732e 0a20 2020 2065  omponents..    e
+00013f80: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
+00013f90: 6f77 5f64 6973 6b5f 7369 7a65 5f67 623a  ow_disk_size_gb:
+00013fa0: 2044 6174 6166 6c6f 7720 776f 726b 6572   Dataflow worker
+00013fb0: 2773 2064 6973 6b20 7369 7a65 2069 6e20  's disk size in 
+00013fc0: 4742 2066 6f72 0a20 2020 2020 2065 7661  GB for.      eva
+00013fd0: 6c75 6174 696f 6e20 636f 6d70 6f6e 656e  luation componen
+00013fe0: 7473 2e0a 2020 2020 6461 7461 666c 6f77  ts..    dataflow
+00013ff0: 5f73 6572 7669 6365 5f61 6363 6f75 6e74  _service_account
+00014000: 3a20 4375 7374 6f6d 2073 6572 7669 6365  : Custom service
+00014010: 2061 6363 6f75 6e74 2074 6f20 7275 6e20   account to run 
+00014020: 6461 7461 666c 6f77 206a 6f62 732e 0a20  dataflow jobs.. 
+00014030: 2020 2064 6174 6166 6c6f 775f 7375 626e     dataflow_subn
+00014040: 6574 776f 726b 3a20 4461 7461 666c 6f77  etwork: Dataflow
+00014050: 2773 2066 756c 6c79 2071 7561 6c69 6669  's fully qualifi
+00014060: 6564 2073 7562 6e65 7477 6f72 6b20 6e61  ed subnetwork na
+00014070: 6d65 2c20 7768 656e 2065 6d70 7479 0a20  me, when empty. 
+00014080: 2020 2020 2074 6865 2064 6566 6175 6c74       the default
+00014090: 2073 7562 6e65 7477 6f72 6b20 7769 6c6c   subnetwork will
+000140a0: 2062 6520 7573 6564 2e20 4578 616d 706c   be used. Exampl
+000140b0: 653a 0a20 2020 2020 2020 2068 7474 7073  e:.        https
+000140c0: 3a2f 2f63 6c6f 7564 2e67 6f6f 676c 652e  ://cloud.google.
+000140d0: 636f 6d2f 6461 7461 666c 6f77 2f64 6f63  com/dataflow/doc
+000140e0: 732f 6775 6964 6573 2f73 7065 6369 6679  s/guides/specify
+000140f0: 696e 672d 6e65 7477 6f72 6b73 2365 7861  ing-networks#exa
+00014100: 6d70 6c65 5f6e 6574 776f 726b 5f61 6e64  mple_network_and
+00014110: 5f73 7562 6e65 7477 6f72 6b5f 7370 6563  _subnetwork_spec
+00014120: 6966 6963 6174 696f 6e73 0a20 2020 2064  ifications.    d
+00014130: 6174 6166 6c6f 775f 7573 655f 7075 626c  ataflow_use_publ
+00014140: 6963 5f69 7073 3a20 5370 6563 6966 6965  ic_ips: Specifie
+00014150: 7320 7768 6574 6865 7220 4461 7461 666c  s whether Datafl
+00014160: 6f77 2077 6f72 6b65 7273 2075 7365 2070  ow workers use p
+00014170: 7562 6c69 6320 4950 0a20 2020 2020 2061  ublic IP.      a
+00014180: 6464 7265 7373 6573 2e0a 2020 2020 656e  ddresses..    en
+00014190: 6372 7970 7469 6f6e 5f73 7065 635f 6b65  cryption_spec_ke
+000141a0: 795f 6e61 6d65 3a20 5468 6520 4b4d 5320  y_name: The KMS 
+000141b0: 6b65 7920 6e61 6d65 2e0a 0a20 2052 6574  key name...  Ret
+000141c0: 7572 6e73 3a0a 2020 2020 5475 706c 6520  urns:.    Tuple 
+000141d0: 6f66 2070 6970 656c 696e 655f 6465 6669  of pipeline_defi
+000141e0: 6e69 7469 6f6e 5f70 6174 6820 616e 6420  nition_path and 
+000141f0: 7061 7261 6d65 7465 725f 7661 6c75 6573  parameter_values
+00014200: 2e0a 2020 2222 220a 2020 7761 726e 696e  ..  """.  warnin
+00014210: 6773 2e77 6172 6e28 0a20 2020 2020 2027  gs.warn(.      '
+00014220: 5468 6973 206d 6574 686f 6420 6973 2064  This method is d
+00014230: 6570 7265 6361 7465 642e 2050 6c65 6173  eprecated. Pleas
+00014240: 6520 7573 6527 0a20 2020 2020 2027 2067  e use'.      ' g
+00014250: 6574 5f74 6162 6e65 745f 6879 7065 7270  et_tabnet_hyperp
+00014260: 6172 616d 6574 6572 5f74 756e 696e 675f  arameter_tuning_
+00014270: 6a6f 625f 7069 7065 6c69 6e65 5f61 6e64  job_pipeline_and
+00014280: 5f70 6172 616d 6574 6572 7320 6f72 270a  _parameters or'.
+00014290: 2020 2020 2020 2720 6765 745f 7769 6465        ' get_wide
+000142a0: 5f61 6e64 5f64 6565 705f 6879 7065 7270  _and_deep_hyperp
+000142b0: 6172 616d 6574 6572 5f74 756e 696e 675f  arameter_tuning_
+000142c0: 6a6f 625f 7069 7065 6c69 6e65 5f61 6e64  job_pipeline_and
+000142d0: 5f70 6172 616d 6574 6572 7327 0a20 2020  _parameters'.   
+000142e0: 2020 2027 2069 6e73 7465 6164 2e27 0a20     ' instead.'. 
+000142f0: 2029 0a0a 2020 6966 2061 6c67 6f72 6974   )..  if algorit
+00014300: 686d 203d 3d20 2774 6162 6e65 7427 3a0a  hm == 'tabnet':.
+00014310: 2020 2020 7265 7475 726e 2067 6574 5f74      return get_t
+00014320: 6162 6e65 745f 6879 7065 7270 6172 616d  abnet_hyperparam
+00014330: 6574 6572 5f74 756e 696e 675f 6a6f 625f  eter_tuning_job_
+00014340: 7069 7065 6c69 6e65 5f61 6e64 5f70 6172  pipeline_and_par
+00014350: 616d 6574 6572 7328 0a20 2020 2020 2020  ameters(.       
+00014360: 2070 726f 6a65 6374 3d70 726f 6a65 6374   project=project
+00014370: 2c0a 2020 2020 2020 2020 6c6f 6361 7469  ,.        locati
+00014380: 6f6e 3d6c 6f63 6174 696f 6e2c 0a20 2020  on=location,.   
+00014390: 2020 2020 2072 6f6f 745f 6469 723d 726f       root_dir=ro
+000143a0: 6f74 5f64 6972 2c0a 2020 2020 2020 2020  ot_dir,.        
+000143b0: 7461 7267 6574 5f63 6f6c 756d 6e3d 7461  target_column=ta
+000143c0: 7267 6574 5f63 6f6c 756d 6e2c 0a20 2020  rget_column,.   
+000143d0: 2020 2020 2070 7265 6469 6374 696f 6e5f       prediction_
+000143e0: 7479 7065 3d70 7265 6469 6374 696f 6e5f  type=prediction_
+000143f0: 7479 7065 2c0a 2020 2020 2020 2020 7374  type,.        st
+00014400: 7564 795f 7370 6563 5f6d 6574 7269 635f  udy_spec_metric_
+00014410: 6964 3d73 7475 6479 5f73 7065 635f 6d65  id=study_spec_me
+00014420: 7472 6963 5f69 642c 0a20 2020 2020 2020  tric_id,.       
+00014430: 2073 7475 6479 5f73 7065 635f 6d65 7472   study_spec_metr
+00014440: 6963 5f67 6f61 6c3d 7374 7564 795f 7370  ic_goal=study_sp
+00014450: 6563 5f6d 6574 7269 635f 676f 616c 2c0a  ec_metric_goal,.
+00014460: 2020 2020 2020 2020 7374 7564 795f 7370          study_sp
+00014470: 6563 5f70 6172 616d 6574 6572 735f 6f76  ec_parameters_ov
+00014480: 6572 7269 6465 3d73 7475 6479 5f73 7065  erride=study_spe
+00014490: 635f 7061 7261 6d65 7465 7273 5f6f 7665  c_parameters_ove
+000144a0: 7272 6964 652c 0a20 2020 2020 2020 206d  rride,.        m
+000144b0: 6178 5f74 7269 616c 5f63 6f75 6e74 3d6d  ax_trial_count=m
+000144c0: 6178 5f74 7269 616c 5f63 6f75 6e74 2c0a  ax_trial_count,.
+000144d0: 2020 2020 2020 2020 7061 7261 6c6c 656c          parallel
+000144e0: 5f74 7269 616c 5f63 6f75 6e74 3d70 6172  _trial_count=par
+000144f0: 616c 6c65 6c5f 7472 6961 6c5f 636f 756e  allel_trial_coun
+00014500: 742c 0a20 2020 2020 2020 2074 7261 6e73  t,.        trans
+00014510: 666f 726d 5f63 6f6e 6669 673d 7472 616e  form_config=tran
+00014520: 7366 6f72 6d5f 636f 6e66 6967 2c0a 2020  sform_config,.  
+00014530: 2020 2020 2020 6461 7461 7365 745f 6c65        dataset_le
+00014540: 7665 6c5f 6375 7374 6f6d 5f74 7261 6e73  vel_custom_trans
+00014550: 666f 726d 6174 696f 6e5f 6465 6669 6e69  formation_defini
+00014560: 7469 6f6e 733d 6461 7461 7365 745f 6c65  tions=dataset_le
+00014570: 7665 6c5f 6375 7374 6f6d 5f74 7261 6e73  vel_custom_trans
+00014580: 666f 726d 6174 696f 6e5f 6465 6669 6e69  formation_defini
+00014590: 7469 6f6e 732c 0a20 2020 2020 2020 2064  tions,.        d
+000145a0: 6174 6173 6574 5f6c 6576 656c 5f74 7261  ataset_level_tra
+000145b0: 6e73 666f 726d 6174 696f 6e73 3d64 6174  nsformations=dat
+000145c0: 6173 6574 5f6c 6576 656c 5f74 7261 6e73  aset_level_trans
+000145d0: 666f 726d 6174 696f 6e73 2c0a 2020 2020  formations,.    
+000145e0: 2020 2020 7072 6564 6566 696e 6564 5f73      predefined_s
+000145f0: 706c 6974 5f6b 6579 3d70 7265 6465 6669  plit_key=predefi
+00014600: 6e65 645f 7370 6c69 745f 6b65 792c 0a20  ned_split_key,. 
+00014610: 2020 2020 2020 2073 7472 6174 6966 6965         stratifie
+00014620: 645f 7370 6c69 745f 6b65 793d 7374 7261  d_split_key=stra
+00014630: 7469 6669 6564 5f73 706c 6974 5f6b 6579  tified_split_key
+00014640: 2c0a 2020 2020 2020 2020 7472 6169 6e69  ,.        traini
+00014650: 6e67 5f66 7261 6374 696f 6e3d 7472 6169  ng_fraction=trai
+00014660: 6e69 6e67 5f66 7261 6374 696f 6e2c 0a20  ning_fraction,. 
+00014670: 2020 2020 2020 2076 616c 6964 6174 696f         validatio
+00014680: 6e5f 6672 6163 7469 6f6e 3d76 616c 6964  n_fraction=valid
+00014690: 6174 696f 6e5f 6672 6163 7469 6f6e 2c0a  ation_fraction,.
+000146a0: 2020 2020 2020 2020 7465 7374 5f66 7261          test_fra
+000146b0: 6374 696f 6e3d 7465 7374 5f66 7261 6374  ction=test_fract
+000146c0: 696f 6e2c 0a20 2020 2020 2020 2074 665f  ion,.        tf_
+000146d0: 7472 616e 7366 6f72 6d5f 6578 6563 7574  transform_execut
+000146e0: 696f 6e5f 656e 6769 6e65 3d74 665f 7472  ion_engine=tf_tr
+000146f0: 616e 7366 6f72 6d5f 6578 6563 7574 696f  ansform_executio
+00014700: 6e5f 656e 6769 6e65 2c0a 2020 2020 2020  n_engine,.      
+00014710: 2020 7466 5f61 7574 6f5f 7472 616e 7366    tf_auto_transf
+00014720: 6f72 6d5f 6665 6174 7572 6573 3d74 665f  orm_features=tf_
+00014730: 6175 746f 5f74 7261 6e73 666f 726d 5f66  auto_transform_f
+00014740: 6561 7475 7265 732c 0a20 2020 2020 2020  eatures,.       
+00014750: 2074 665f 6375 7374 6f6d 5f74 7261 6e73   tf_custom_trans
+00014760: 666f 726d 6174 696f 6e5f 6465 6669 6e69  formation_defini
+00014770: 7469 6f6e 733d 7466 5f63 7573 746f 6d5f  tions=tf_custom_
+00014780: 7472 616e 7366 6f72 6d61 7469 6f6e 5f64  transformation_d
+00014790: 6566 696e 6974 696f 6e73 2c0a 2020 2020  efinitions,.    
+000147a0: 2020 2020 7466 5f74 7261 6e73 666f 726d      tf_transform
+000147b0: 6174 696f 6e73 5f70 6174 683d 7466 5f74  ations_path=tf_t
+000147c0: 7261 6e73 666f 726d 6174 696f 6e73 5f70  ransformations_p
+000147d0: 6174 682c 0a20 2020 2020 2020 2065 6e61  ath,.        ena
+000147e0: 626c 655f 7072 6f66 696c 6572 3d65 6e61  ble_profiler=ena
+000147f0: 626c 655f 7072 6f66 696c 6572 2c0a 2020  ble_profiler,.  
+00014800: 2020 2020 2020 7365 6564 3d73 6565 642c        seed=seed,
+00014810: 0a20 2020 2020 2020 2065 7661 6c5f 7374  .        eval_st
+00014820: 6570 733d 6576 616c 5f73 7465 7073 2c0a  eps=eval_steps,.
+00014830: 2020 2020 2020 2020 6576 616c 5f66 7265          eval_fre
+00014840: 7175 656e 6379 5f73 6563 733d 6576 616c  quency_secs=eval
+00014850: 5f66 7265 7175 656e 6379 5f73 6563 732c  _frequency_secs,
+00014860: 0a20 2020 2020 2020 2064 6174 615f 736f  .        data_so
+00014870: 7572 6365 5f63 7376 5f66 696c 656e 616d  urce_csv_filenam
+00014880: 6573 3d64 6174 615f 736f 7572 6365 5f63  es=data_source_c
+00014890: 7376 5f66 696c 656e 616d 6573 2c0a 2020  sv_filenames,.  
+000148a0: 2020 2020 2020 6461 7461 5f73 6f75 7263        data_sourc
+000148b0: 655f 6269 6771 7565 7279 5f74 6162 6c65  e_bigquery_table
+000148c0: 5f70 6174 683d 6461 7461 5f73 6f75 7263  _path=data_sourc
+000148d0: 655f 6269 6771 7565 7279 5f74 6162 6c65  e_bigquery_table
+000148e0: 5f70 6174 682c 0a20 2020 2020 2020 2062  _path,.        b
+000148f0: 6967 7175 6572 795f 7374 6167 696e 675f  igquery_staging_
+00014900: 6675 6c6c 5f64 6174 6173 6574 5f69 643d  full_dataset_id=
+00014910: 6269 6771 7565 7279 5f73 7461 6769 6e67  bigquery_staging
+00014920: 5f66 756c 6c5f 6461 7461 7365 745f 6964  _full_dataset_id
+00014930: 2c0a 2020 2020 2020 2020 7765 6967 6874  ,.        weight
+00014940: 5f63 6f6c 756d 6e3d 7765 6967 6874 5f63  _column=weight_c
+00014950: 6f6c 756d 6e2c 0a20 2020 2020 2020 206d  olumn,.        m
+00014960: 6178 5f66 6169 6c65 645f 7472 6961 6c5f  ax_failed_trial_
+00014970: 636f 756e 743d 6d61 785f 6661 696c 6564  count=max_failed
+00014980: 5f74 7269 616c 5f63 6f75 6e74 2c0a 2020  _trial_count,.  
+00014990: 2020 2020 2020 7374 7564 795f 7370 6563        study_spec
+000149a0: 5f61 6c67 6f72 6974 686d 3d73 7475 6479  _algorithm=study
+000149b0: 5f73 7065 635f 616c 676f 7269 7468 6d2c  _spec_algorithm,
+000149c0: 0a20 2020 2020 2020 2073 7475 6479 5f73  .        study_s
+000149d0: 7065 635f 6d65 6173 7572 656d 656e 745f  pec_measurement_
+000149e0: 7365 6c65 6374 696f 6e5f 7479 7065 3d73  selection_type=s
+000149f0: 7475 6479 5f73 7065 635f 6d65 6173 7572  tudy_spec_measur
+00014a00: 656d 656e 745f 7365 6c65 6374 696f 6e5f  ement_selection_
+00014a10: 7479 7065 2c0a 2020 2020 2020 2020 7472  type,.        tr
+00014a20: 616e 7366 6f72 6d5f 6461 7461 666c 6f77  ansform_dataflow
+00014a30: 5f6d 6163 6869 6e65 5f74 7970 653d 7472  _machine_type=tr
+00014a40: 616e 7366 6f72 6d5f 6461 7461 666c 6f77  ansform_dataflow
+00014a50: 5f6d 6163 6869 6e65 5f74 7970 652c 0a20  _machine_type,. 
+00014a60: 2020 2020 2020 2074 7261 6e73 666f 726d         transform
+00014a70: 5f64 6174 6166 6c6f 775f 6d61 785f 6e75  _dataflow_max_nu
+00014a80: 6d5f 776f 726b 6572 733d 7472 616e 7366  m_workers=transf
+00014a90: 6f72 6d5f 6461 7461 666c 6f77 5f6d 6178  orm_dataflow_max
+00014aa0: 5f6e 756d 5f77 6f72 6b65 7273 2c0a 2020  _num_workers,.  
+00014ab0: 2020 2020 2020 7472 616e 7366 6f72 6d5f        transform_
+00014ac0: 6461 7461 666c 6f77 5f64 6973 6b5f 7369  dataflow_disk_si
+00014ad0: 7a65 5f67 623d 7472 616e 7366 6f72 6d5f  ze_gb=transform_
+00014ae0: 6461 7461 666c 6f77 5f64 6973 6b5f 7369  dataflow_disk_si
+00014af0: 7a65 5f67 622c 0a20 2020 2020 2020 2077  ze_gb,.        w
+00014b00: 6f72 6b65 725f 706f 6f6c 5f73 7065 6373  orker_pool_specs
+00014b10: 5f6f 7665 7272 6964 653d 776f 726b 6572  _override=worker
+00014b20: 5f70 6f6f 6c5f 7370 6563 735f 6f76 6572  _pool_specs_over
+00014b30: 7269 6465 2c0a 2020 2020 2020 2020 7275  ride,.        ru
+00014b40: 6e5f 6576 616c 7561 7469 6f6e 3d72 756e  n_evaluation=run
+00014b50: 5f65 7661 6c75 6174 696f 6e2c 0a20 2020  _evaluation,.   
+00014b60: 2020 2020 2065 7661 6c75 6174 696f 6e5f       evaluation_
+00014b70: 6261 7463 685f 7072 6564 6963 745f 6d61  batch_predict_ma
+00014b80: 6368 696e 655f 7479 7065 3d65 7661 6c75  chine_type=evalu
+00014b90: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
+00014ba0: 6963 745f 6d61 6368 696e 655f 7479 7065  ict_machine_type
+00014bb0: 2c0a 2020 2020 2020 2020 6576 616c 7561  ,.        evalua
+00014bc0: 7469 6f6e 5f62 6174 6368 5f70 7265 6469  tion_batch_predi
+00014bd0: 6374 5f73 7461 7274 696e 675f 7265 706c  ct_starting_repl
+00014be0: 6963 615f 636f 756e 743d 6576 616c 7561  ica_count=evalua
+00014bf0: 7469 6f6e 5f62 6174 6368 5f70 7265 6469  tion_batch_predi
+00014c00: 6374 5f73 7461 7274 696e 675f 7265 706c  ct_starting_repl
+00014c10: 6963 615f 636f 756e 742c 0a20 2020 2020  ica_count,.     
+00014c20: 2020 2065 7661 6c75 6174 696f 6e5f 6261     evaluation_ba
+00014c30: 7463 685f 7072 6564 6963 745f 6d61 785f  tch_predict_max_
+00014c40: 7265 706c 6963 615f 636f 756e 743d 6576  replica_count=ev
+00014c50: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+00014c60: 7265 6469 6374 5f6d 6178 5f72 6570 6c69  redict_max_repli
+00014c70: 6361 5f63 6f75 6e74 2c0a 2020 2020 2020  ca_count,.      
+00014c80: 2020 6576 616c 7561 7469 6f6e 5f64 6174    evaluation_dat
+00014c90: 6166 6c6f 775f 6d61 6368 696e 655f 7479  aflow_machine_ty
+00014ca0: 7065 3d65 7661 6c75 6174 696f 6e5f 6461  pe=evaluation_da
+00014cb0: 7461 666c 6f77 5f6d 6163 6869 6e65 5f74  taflow_machine_t
+00014cc0: 7970 652c 0a20 2020 2020 2020 2065 7661  ype,.        eva
+00014cd0: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
+00014ce0: 5f64 6973 6b5f 7369 7a65 5f67 623d 6576  _disk_size_gb=ev
+00014cf0: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
+00014d00: 775f 6469 736b 5f73 697a 655f 6762 2c0a  w_disk_size_gb,.
+00014d10: 2020 2020 2020 2020 6576 616c 7561 7469          evaluati
+00014d20: 6f6e 5f64 6174 6166 6c6f 775f 7374 6172  on_dataflow_star
+00014d30: 7469 6e67 5f6e 756d 5f77 6f72 6b65 7273  ting_num_workers
+00014d40: 3d65 7661 6c75 6174 696f 6e5f 6461 7461  =evaluation_data
+00014d50: 666c 6f77 5f73 7461 7274 696e 675f 6e75  flow_starting_nu
+00014d60: 6d5f 776f 726b 6572 732c 0a20 2020 2020  m_workers,.     
+00014d70: 2020 2065 7661 6c75 6174 696f 6e5f 6461     evaluation_da
+00014d80: 7461 666c 6f77 5f6d 6178 5f6e 756d 5f77  taflow_max_num_w
+00014d90: 6f72 6b65 7273 3d65 7661 6c75 6174 696f  orkers=evaluatio
+00014da0: 6e5f 6461 7461 666c 6f77 5f6d 6178 5f6e  n_dataflow_max_n
+00014db0: 756d 5f77 6f72 6b65 7273 2c0a 2020 2020  um_workers,.    
+00014dc0: 2020 2020 6461 7461 666c 6f77 5f73 6572      dataflow_ser
+00014dd0: 7669 6365 5f61 6363 6f75 6e74 3d64 6174  vice_account=dat
+00014de0: 6166 6c6f 775f 7365 7276 6963 655f 6163  aflow_service_ac
+00014df0: 636f 756e 742c 0a20 2020 2020 2020 2064  count,.        d
+00014e00: 6174 6166 6c6f 775f 7375 626e 6574 776f  ataflow_subnetwo
+00014e10: 726b 3d64 6174 6166 6c6f 775f 7375 626e  rk=dataflow_subn
+00014e20: 6574 776f 726b 2c0a 2020 2020 2020 2020  etwork,.        
+00014e30: 6461 7461 666c 6f77 5f75 7365 5f70 7562  dataflow_use_pub
+00014e40: 6c69 635f 6970 733d 6461 7461 666c 6f77  lic_ips=dataflow
+00014e50: 5f75 7365 5f70 7562 6c69 635f 6970 732c  _use_public_ips,
+00014e60: 0a20 2020 2020 2020 2065 6e63 7279 7074  .        encrypt
+00014e70: 696f 6e5f 7370 6563 5f6b 6579 5f6e 616d  ion_spec_key_nam
+00014e80: 653d 656e 6372 7970 7469 6f6e 5f73 7065  e=encryption_spe
+00014e90: 635f 6b65 795f 6e61 6d65 2c0a 2020 2020  c_key_name,.    
+00014ea0: 290a 2020 656c 6966 2061 6c67 6f72 6974  ).  elif algorit
+00014eb0: 686d 203d 3d20 2777 6964 655f 616e 645f  hm == 'wide_and_
+00014ec0: 6465 6570 273a 0a20 2020 2072 6574 7572  deep':.    retur
+00014ed0: 6e20 6765 745f 7769 6465 5f61 6e64 5f64  n get_wide_and_d
+00014ee0: 6565 705f 6879 7065 7270 6172 616d 6574  eep_hyperparamet
+00014ef0: 6572 5f74 756e 696e 675f 6a6f 625f 7069  er_tuning_job_pi
+00014f00: 7065 6c69 6e65 5f61 6e64 5f70 6172 616d  peline_and_param
+00014f10: 6574 6572 7328 0a20 2020 2020 2020 2070  eters(.        p
+00014f20: 726f 6a65 6374 3d70 726f 6a65 6374 2c0a  roject=project,.
+00014f30: 2020 2020 2020 2020 6c6f 6361 7469 6f6e          location
+00014f40: 3d6c 6f63 6174 696f 6e2c 0a20 2020 2020  =location,.     
+00014f50: 2020 2072 6f6f 745f 6469 723d 726f 6f74     root_dir=root
+00014f60: 5f64 6972 2c0a 2020 2020 2020 2020 7461  _dir,.        ta
+00014f70: 7267 6574 5f63 6f6c 756d 6e3d 7461 7267  rget_column=targ
+00014f80: 6574 5f63 6f6c 756d 6e2c 0a20 2020 2020  et_column,.     
+00014f90: 2020 2070 7265 6469 6374 696f 6e5f 7479     prediction_ty
+00014fa0: 7065 3d70 7265 6469 6374 696f 6e5f 7479  pe=prediction_ty
+00014fb0: 7065 2c0a 2020 2020 2020 2020 7374 7564  pe,.        stud
+00014fc0: 795f 7370 6563 5f6d 6574 7269 635f 6964  y_spec_metric_id
+00014fd0: 3d73 7475 6479 5f73 7065 635f 6d65 7472  =study_spec_metr
+00014fe0: 6963 5f69 642c 0a20 2020 2020 2020 2073  ic_id,.        s
+00014ff0: 7475 6479 5f73 7065 635f 6d65 7472 6963  tudy_spec_metric
+00015000: 5f67 6f61 6c3d 7374 7564 795f 7370 6563  _goal=study_spec
+00015010: 5f6d 6574 7269 635f 676f 616c 2c0a 2020  _metric_goal,.  
+00015020: 2020 2020 2020 7374 7564 795f 7370 6563        study_spec
+00015030: 5f70 6172 616d 6574 6572 735f 6f76 6572  _parameters_over
+00015040: 7269 6465 3d73 7475 6479 5f73 7065 635f  ride=study_spec_
+00015050: 7061 7261 6d65 7465 7273 5f6f 7665 7272  parameters_overr
+00015060: 6964 652c 0a20 2020 2020 2020 206d 6178  ide,.        max
+00015070: 5f74 7269 616c 5f63 6f75 6e74 3d6d 6178  _trial_count=max
+00015080: 5f74 7269 616c 5f63 6f75 6e74 2c0a 2020  _trial_count,.  
+00015090: 2020 2020 2020 7061 7261 6c6c 656c 5f74        parallel_t
+000150a0: 7269 616c 5f63 6f75 6e74 3d70 6172 616c  rial_count=paral
+000150b0: 6c65 6c5f 7472 6961 6c5f 636f 756e 742c  lel_trial_count,
+000150c0: 0a20 2020 2020 2020 2074 7261 6e73 666f  .        transfo
+000150d0: 726d 5f63 6f6e 6669 673d 7472 616e 7366  rm_config=transf
+000150e0: 6f72 6d5f 636f 6e66 6967 2c0a 2020 2020  orm_config,.    
+000150f0: 2020 2020 6461 7461 7365 745f 6c65 7665      dataset_leve
+00015100: 6c5f 6375 7374 6f6d 5f74 7261 6e73 666f  l_custom_transfo
+00015110: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
+00015120: 6f6e 733d 6461 7461 7365 745f 6c65 7665  ons=dataset_leve
+00015130: 6c5f 6375 7374 6f6d 5f74 7261 6e73 666f  l_custom_transfo
+00015140: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
+00015150: 6f6e 732c 0a20 2020 2020 2020 2064 6174  ons,.        dat
+00015160: 6173 6574 5f6c 6576 656c 5f74 7261 6e73  aset_level_trans
+00015170: 666f 726d 6174 696f 6e73 3d64 6174 6173  formations=datas
+00015180: 6574 5f6c 6576 656c 5f74 7261 6e73 666f  et_level_transfo
+00015190: 726d 6174 696f 6e73 2c0a 2020 2020 2020  rmations,.      
+000151a0: 2020 7072 6564 6566 696e 6564 5f73 706c    predefined_spl
+000151b0: 6974 5f6b 6579 3d70 7265 6465 6669 6e65  it_key=predefine
+000151c0: 645f 7370 6c69 745f 6b65 792c 0a20 2020  d_split_key,.   
+000151d0: 2020 2020 2073 7472 6174 6966 6965 645f       stratified_
+000151e0: 7370 6c69 745f 6b65 793d 7374 7261 7469  split_key=strati
+000151f0: 6669 6564 5f73 706c 6974 5f6b 6579 2c0a  fied_split_key,.
+00015200: 2020 2020 2020 2020 7472 6169 6e69 6e67          training
+00015210: 5f66 7261 6374 696f 6e3d 7472 6169 6e69  _fraction=traini
+00015220: 6e67 5f66 7261 6374 696f 6e2c 0a20 2020  ng_fraction,.   
+00015230: 2020 2020 2076 616c 6964 6174 696f 6e5f       validation_
+00015240: 6672 6163 7469 6f6e 3d76 616c 6964 6174  fraction=validat
+00015250: 696f 6e5f 6672 6163 7469 6f6e 2c0a 2020  ion_fraction,.  
+00015260: 2020 2020 2020 7465 7374 5f66 7261 6374        test_fract
+00015270: 696f 6e3d 7465 7374 5f66 7261 6374 696f  ion=test_fractio
+00015280: 6e2c 0a20 2020 2020 2020 2074 665f 7472  n,.        tf_tr
+00015290: 616e 7366 6f72 6d5f 6578 6563 7574 696f  ansform_executio
+000152a0: 6e5f 656e 6769 6e65 3d74 665f 7472 616e  n_engine=tf_tran
+000152b0: 7366 6f72 6d5f 6578 6563 7574 696f 6e5f  sform_execution_
+000152c0: 656e 6769 6e65 2c0a 2020 2020 2020 2020  engine,.        
+000152d0: 7466 5f61 7574 6f5f 7472 616e 7366 6f72  tf_auto_transfor
+000152e0: 6d5f 6665 6174 7572 6573 3d74 665f 6175  m_features=tf_au
+000152f0: 746f 5f74 7261 6e73 666f 726d 5f66 6561  to_transform_fea
+00015300: 7475 7265 732c 0a20 2020 2020 2020 2074  tures,.        t
+00015310: 665f 6375 7374 6f6d 5f74 7261 6e73 666f  f_custom_transfo
+00015320: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
+00015330: 6f6e 733d 7466 5f63 7573 746f 6d5f 7472  ons=tf_custom_tr
+00015340: 616e 7366 6f72 6d61 7469 6f6e 5f64 6566  ansformation_def
+00015350: 696e 6974 696f 6e73 2c0a 2020 2020 2020  initions,.      
+00015360: 2020 7466 5f74 7261 6e73 666f 726d 6174    tf_transformat
+00015370: 696f 6e73 5f70 6174 683d 7466 5f74 7261  ions_path=tf_tra
+00015380: 6e73 666f 726d 6174 696f 6e73 5f70 6174  nsformations_pat
+00015390: 682c 0a20 2020 2020 2020 2065 6e61 626c  h,.        enabl
+000153a0: 655f 7072 6f66 696c 6572 3d65 6e61 626c  e_profiler=enabl
+000153b0: 655f 7072 6f66 696c 6572 2c0a 2020 2020  e_profiler,.    
+000153c0: 2020 2020 7365 6564 3d73 6565 642c 0a20      seed=seed,. 
+000153d0: 2020 2020 2020 2065 7661 6c5f 7374 6570         eval_step
+000153e0: 733d 6576 616c 5f73 7465 7073 2c0a 2020  s=eval_steps,.  
+000153f0: 2020 2020 2020 6576 616c 5f66 7265 7175        eval_frequ
+00015400: 656e 6379 5f73 6563 733d 6576 616c 5f66  ency_secs=eval_f
+00015410: 7265 7175 656e 6379 5f73 6563 732c 0a20  requency_secs,. 
+00015420: 2020 2020 2020 2064 6174 615f 736f 7572         data_sour
+00015430: 6365 5f63 7376 5f66 696c 656e 616d 6573  ce_csv_filenames
+00015440: 3d64 6174 615f 736f 7572 6365 5f63 7376  =data_source_csv
+00015450: 5f66 696c 656e 616d 6573 2c0a 2020 2020  _filenames,.    
+00015460: 2020 2020 6461 7461 5f73 6f75 7263 655f      data_source_
+00015470: 6269 6771 7565 7279 5f74 6162 6c65 5f70  bigquery_table_p
+00015480: 6174 683d 6461 7461 5f73 6f75 7263 655f  ath=data_source_
+00015490: 6269 6771 7565 7279 5f74 6162 6c65 5f70  bigquery_table_p
+000154a0: 6174 682c 0a20 2020 2020 2020 2062 6967  ath,.        big
+000154b0: 7175 6572 795f 7374 6167 696e 675f 6675  query_staging_fu
+000154c0: 6c6c 5f64 6174 6173 6574 5f69 643d 6269  ll_dataset_id=bi
+000154d0: 6771 7565 7279 5f73 7461 6769 6e67 5f66  gquery_staging_f
+000154e0: 756c 6c5f 6461 7461 7365 745f 6964 2c0a  ull_dataset_id,.
+000154f0: 2020 2020 2020 2020 7765 6967 6874 5f63          weight_c
+00015500: 6f6c 756d 6e3d 7765 6967 6874 5f63 6f6c  olumn=weight_col
+00015510: 756d 6e2c 0a20 2020 2020 2020 206d 6178  umn,.        max
+00015520: 5f66 6169 6c65 645f 7472 6961 6c5f 636f  _failed_trial_co
+00015530: 756e 743d 6d61 785f 6661 696c 6564 5f74  unt=max_failed_t
+00015540: 7269 616c 5f63 6f75 6e74 2c0a 2020 2020  rial_count,.    
+00015550: 2020 2020 7374 7564 795f 7370 6563 5f61      study_spec_a
+00015560: 6c67 6f72 6974 686d 3d73 7475 6479 5f73  lgorithm=study_s
+00015570: 7065 635f 616c 676f 7269 7468 6d2c 0a20  pec_algorithm,. 
+00015580: 2020 2020 2020 2073 7475 6479 5f73 7065         study_spe
+00015590: 635f 6d65 6173 7572 656d 656e 745f 7365  c_measurement_se
+000155a0: 6c65 6374 696f 6e5f 7479 7065 3d73 7475  lection_type=stu
+000155b0: 6479 5f73 7065 635f 6d65 6173 7572 656d  dy_spec_measurem
+000155c0: 656e 745f 7365 6c65 6374 696f 6e5f 7479  ent_selection_ty
+000155d0: 7065 2c0a 2020 2020 2020 2020 7472 616e  pe,.        tran
+000155e0: 7366 6f72 6d5f 6461 7461 666c 6f77 5f6d  sform_dataflow_m
+000155f0: 6163 6869 6e65 5f74 7970 653d 7472 616e  achine_type=tran
+00015600: 7366 6f72 6d5f 6461 7461 666c 6f77 5f6d  sform_dataflow_m
+00015610: 6163 6869 6e65 5f74 7970 652c 0a20 2020  achine_type,.   
+00015620: 2020 2020 2074 7261 6e73 666f 726d 5f64       transform_d
+00015630: 6174 6166 6c6f 775f 6d61 785f 6e75 6d5f  ataflow_max_num_
+00015640: 776f 726b 6572 733d 7472 616e 7366 6f72  workers=transfor
+00015650: 6d5f 6461 7461 666c 6f77 5f6d 6178 5f6e  m_dataflow_max_n
+00015660: 756d 5f77 6f72 6b65 7273 2c0a 2020 2020  um_workers,.    
+00015670: 2020 2020 7472 616e 7366 6f72 6d5f 6461      transform_da
+00015680: 7461 666c 6f77 5f64 6973 6b5f 7369 7a65  taflow_disk_size
+00015690: 5f67 623d 7472 616e 7366 6f72 6d5f 6461  _gb=transform_da
+000156a0: 7461 666c 6f77 5f64 6973 6b5f 7369 7a65  taflow_disk_size
+000156b0: 5f67 622c 0a20 2020 2020 2020 2077 6f72  _gb,.        wor
+000156c0: 6b65 725f 706f 6f6c 5f73 7065 6373 5f6f  ker_pool_specs_o
+000156d0: 7665 7272 6964 653d 776f 726b 6572 5f70  verride=worker_p
+000156e0: 6f6f 6c5f 7370 6563 735f 6f76 6572 7269  ool_specs_overri
+000156f0: 6465 2c0a 2020 2020 2020 2020 7275 6e5f  de,.        run_
+00015700: 6576 616c 7561 7469 6f6e 3d72 756e 5f65  evaluation=run_e
+00015710: 7661 6c75 6174 696f 6e2c 0a20 2020 2020  valuation,.     
+00015720: 2020 2065 7661 6c75 6174 696f 6e5f 6261     evaluation_ba
+00015730: 7463 685f 7072 6564 6963 745f 6d61 6368  tch_predict_mach
+00015740: 696e 655f 7479 7065 3d65 7661 6c75 6174  ine_type=evaluat
+00015750: 696f 6e5f 6261 7463 685f 7072 6564 6963  ion_batch_predic
+00015760: 745f 6d61 6368 696e 655f 7479 7065 2c0a  t_machine_type,.
+00015770: 2020 2020 2020 2020 6576 616c 7561 7469          evaluati
+00015780: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
+00015790: 5f73 7461 7274 696e 675f 7265 706c 6963  _starting_replic
+000157a0: 615f 636f 756e 743d 6576 616c 7561 7469  a_count=evaluati
+000157b0: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
+000157c0: 5f73 7461 7274 696e 675f 7265 706c 6963  _starting_replic
+000157d0: 615f 636f 756e 742c 0a20 2020 2020 2020  a_count,.       
+000157e0: 2065 7661 6c75 6174 696f 6e5f 6261 7463   evaluation_batc
+000157f0: 685f 7072 6564 6963 745f 6d61 785f 7265  h_predict_max_re
+00015800: 706c 6963 615f 636f 756e 743d 6576 616c  plica_count=eval
+00015810: 7561 7469 6f6e 5f62 6174 6368 5f70 7265  uation_batch_pre
+00015820: 6469 6374 5f6d 6178 5f72 6570 6c69 6361  dict_max_replica
+00015830: 5f63 6f75 6e74 2c0a 2020 2020 2020 2020  _count,.        
+00015840: 6576 616c 7561 7469 6f6e 5f64 6174 6166  evaluation_dataf
+00015850: 6c6f 775f 6d61 6368 696e 655f 7479 7065  low_machine_type
+00015860: 3d65 7661 6c75 6174 696f 6e5f 6461 7461  =evaluation_data
+00015870: 666c 6f77 5f6d 6163 6869 6e65 5f74 7970  flow_machine_typ
+00015880: 652c 0a20 2020 2020 2020 2065 7661 6c75  e,.        evalu
+00015890: 6174 696f 6e5f 6461 7461 666c 6f77 5f64  ation_dataflow_d
+000158a0: 6973 6b5f 7369 7a65 5f67 623d 6576 616c  isk_size_gb=eval
+000158b0: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
+000158c0: 6469 736b 5f73 697a 655f 6762 2c0a 2020  disk_size_gb,.  
+000158d0: 2020 2020 2020 6576 616c 7561 7469 6f6e        evaluation
+000158e0: 5f64 6174 6166 6c6f 775f 7374 6172 7469  _dataflow_starti
+000158f0: 6e67 5f6e 756d 5f77 6f72 6b65 7273 3d65  ng_num_workers=e
+00015900: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
+00015910: 6f77 5f73 7461 7274 696e 675f 6e75 6d5f  ow_starting_num_
+00015920: 776f 726b 6572 732c 0a20 2020 2020 2020  workers,.       
+00015930: 2065 7661 6c75 6174 696f 6e5f 6461 7461   evaluation_data
+00015940: 666c 6f77 5f6d 6178 5f6e 756d 5f77 6f72  flow_max_num_wor
+00015950: 6b65 7273 3d65 7661 6c75 6174 696f 6e5f  kers=evaluation_
+00015960: 6461 7461 666c 6f77 5f6d 6178 5f6e 756d  dataflow_max_num
+00015970: 5f77 6f72 6b65 7273 2c0a 2020 2020 2020  _workers,.      
+00015980: 2020 6461 7461 666c 6f77 5f73 6572 7669    dataflow_servi
+00015990: 6365 5f61 6363 6f75 6e74 3d64 6174 6166  ce_account=dataf
+000159a0: 6c6f 775f 7365 7276 6963 655f 6163 636f  low_service_acco
+000159b0: 756e 742c 0a20 2020 2020 2020 2064 6174  unt,.        dat
+000159c0: 6166 6c6f 775f 7375 626e 6574 776f 726b  aflow_subnetwork
+000159d0: 3d64 6174 6166 6c6f 775f 7375 626e 6574  =dataflow_subnet
+000159e0: 776f 726b 2c0a 2020 2020 2020 2020 6461  work,.        da
+000159f0: 7461 666c 6f77 5f75 7365 5f70 7562 6c69  taflow_use_publi
+00015a00: 635f 6970 733d 6461 7461 666c 6f77 5f75  c_ips=dataflow_u
+00015a10: 7365 5f70 7562 6c69 635f 6970 732c 0a20  se_public_ips,. 
+00015a20: 2020 2020 2020 2065 6e63 7279 7074 696f         encryptio
+00015a30: 6e5f 7370 6563 5f6b 6579 5f6e 616d 653d  n_spec_key_name=
+00015a40: 656e 6372 7970 7469 6f6e 5f73 7065 635f  encryption_spec_
+00015a50: 6b65 795f 6e61 6d65 2c0a 2020 2020 290a  key_name,.    ).
+00015a60: 2020 656c 7365 3a0a 2020 2020 7261 6973    else:.    rais
+00015a70: 6520 5661 6c75 6545 7272 6f72 280a 2020  e ValueError(.  
+00015a80: 2020 2020 2020 2749 6e76 616c 6964 2061        'Invalid a
+00015a90: 6c67 6f72 6974 686d 2070 726f 7669 6465  lgorithm provide
+00015aa0: 642e 2053 7570 706f 7274 6564 2076 616c  d. Supported val
+00015ab0: 7565 7320 6172 6520 2274 6162 6e65 7422  ues are "tabnet"
+00015ac0: 2061 6e64 270a 2020 2020 2020 2020 2720   and'.        ' 
+00015ad0: 2277 6964 655f 616e 645f 6465 6570 222e  "wide_and_deep".
+00015ae0: 270a 2020 2020 290a 0a0a 6465 6620 6765  '.    )...def ge
+00015af0: 745f 7461 626e 6574 5f68 7970 6572 7061  t_tabnet_hyperpa
+00015b00: 7261 6d65 7465 725f 7475 6e69 6e67 5f6a  rameter_tuning_j
+00015b10: 6f62 5f70 6970 656c 696e 655f 616e 645f  ob_pipeline_and_
+00015b20: 7061 7261 6d65 7465 7273 280a 2020 2020  parameters(.    
+00015b30: 7072 6f6a 6563 743a 2073 7472 2c0a 2020  project: str,.  
+00015b40: 2020 6c6f 6361 7469 6f6e 3a20 7374 722c    location: str,
+00015b50: 0a20 2020 2072 6f6f 745f 6469 723a 2073  .    root_dir: s
+00015b60: 7472 2c0a 2020 2020 7461 7267 6574 5f63  tr,.    target_c
+00015b70: 6f6c 756d 6e3a 2073 7472 2c0a 2020 2020  olumn: str,.    
+00015b80: 7072 6564 6963 7469 6f6e 5f74 7970 653a  prediction_type:
+00015b90: 2073 7472 2c0a 2020 2020 7374 7564 795f   str,.    study_
+00015ba0: 7370 6563 5f6d 6574 7269 635f 6964 3a20  spec_metric_id: 
+00015bb0: 7374 722c 0a20 2020 2073 7475 6479 5f73  str,.    study_s
+00015bc0: 7065 635f 6d65 7472 6963 5f67 6f61 6c3a  pec_metric_goal:
+00015bd0: 2073 7472 2c0a 2020 2020 7374 7564 795f   str,.    study_
+00015be0: 7370 6563 5f70 6172 616d 6574 6572 735f  spec_parameters_
+00015bf0: 6f76 6572 7269 6465 3a20 4c69 7374 5b44  override: List[D
+00015c00: 6963 745b 7374 722c 2041 6e79 5d5d 2c0a  ict[str, Any]],.
+00015c10: 2020 2020 6d61 785f 7472 6961 6c5f 636f      max_trial_co
+00015c20: 756e 743a 2069 6e74 2c0a 2020 2020 7061  unt: int,.    pa
+00015c30: 7261 6c6c 656c 5f74 7269 616c 5f63 6f75  rallel_trial_cou
+00015c40: 6e74 3a20 696e 742c 0a20 2020 2074 7261  nt: int,.    tra
+00015c50: 6e73 666f 726d 5f63 6f6e 6669 673a 204f  nsform_config: O
+00015c60: 7074 696f 6e61 6c5b 7374 725d 203d 204e  ptional[str] = N
+00015c70: 6f6e 652c 0a20 2020 2064 6174 6173 6574  one,.    dataset
+00015c80: 5f6c 6576 656c 5f63 7573 746f 6d5f 7472  _level_custom_tr
+00015c90: 616e 7366 6f72 6d61 7469 6f6e 5f64 6566  ansformation_def
+00015ca0: 696e 6974 696f 6e73 3a20 4f70 7469 6f6e  initions: Option
+00015cb0: 616c 5b0a 2020 2020 2020 2020 4c69 7374  al[.        List
+00015cc0: 5b44 6963 745b 7374 722c 2041 6e79 5d5d  [Dict[str, Any]]
+00015cd0: 0a20 2020 205d 203d 204e 6f6e 652c 0a20  .    ] = None,. 
+00015ce0: 2020 2064 6174 6173 6574 5f6c 6576 656c     dataset_level
+00015cf0: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
+00015d00: 3a20 4f70 7469 6f6e 616c 5b4c 6973 745b  : Optional[List[
+00015d10: 4469 6374 5b73 7472 2c20 416e 795d 5d5d  Dict[str, Any]]]
+00015d20: 203d 204e 6f6e 652c 0a20 2020 2072 756e   = None,.    run
+00015d30: 5f66 6561 7475 7265 5f73 656c 6563 7469  _feature_selecti
+00015d40: 6f6e 3a20 626f 6f6c 203d 2046 616c 7365  on: bool = False
+00015d50: 2c0a 2020 2020 6665 6174 7572 655f 7365  ,.    feature_se
+00015d60: 6c65 6374 696f 6e5f 616c 676f 7269 7468  lection_algorith
+00015d70: 6d3a 204f 7074 696f 6e61 6c5b 7374 725d  m: Optional[str]
+00015d80: 203d 204e 6f6e 652c 0a20 2020 206d 6174   = None,.    mat
+00015d90: 6572 6961 6c69 7a65 645f 6578 616d 706c  erialized_exampl
+00015da0: 6573 5f66 6f72 6d61 743a 204f 7074 696f  es_format: Optio
+00015db0: 6e61 6c5b 7374 725d 203d 204e 6f6e 652c  nal[str] = None,
+00015dc0: 0a20 2020 206d 6178 5f73 656c 6563 7465  .    max_selecte
+00015dd0: 645f 6665 6174 7572 6573 3a20 4f70 7469  d_features: Opti
+00015de0: 6f6e 616c 5b69 6e74 5d20 3d20 4e6f 6e65  onal[int] = None
+00015df0: 2c0a 2020 2020 7072 6564 6566 696e 6564  ,.    predefined
+00015e00: 5f73 706c 6974 5f6b 6579 3a20 4f70 7469  _split_key: Opti
+00015e10: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
+00015e20: 2c0a 2020 2020 7374 7261 7469 6669 6564  ,.    stratified
+00015e30: 5f73 706c 6974 5f6b 6579 3a20 4f70 7469  _split_key: Opti
+00015e40: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
+00015e50: 2c0a 2020 2020 7472 6169 6e69 6e67 5f66  ,.    training_f
+00015e60: 7261 6374 696f 6e3a 204f 7074 696f 6e61  raction: Optiona
+00015e70: 6c5b 666c 6f61 745d 203d 204e 6f6e 652c  l[float] = None,
+00015e80: 0a20 2020 2076 616c 6964 6174 696f 6e5f  .    validation_
+00015e90: 6672 6163 7469 6f6e 3a20 4f70 7469 6f6e  fraction: Option
+00015ea0: 616c 5b66 6c6f 6174 5d20 3d20 4e6f 6e65  al[float] = None
+00015eb0: 2c0a 2020 2020 7465 7374 5f66 7261 6374  ,.    test_fract
+00015ec0: 696f 6e3a 204f 7074 696f 6e61 6c5b 666c  ion: Optional[fl
+00015ed0: 6f61 745d 203d 204e 6f6e 652c 0a20 2020  oat] = None,.   
+00015ee0: 2074 665f 7472 616e 7366 6f72 6d5f 6578   tf_transform_ex
+00015ef0: 6563 7574 696f 6e5f 656e 6769 6e65 3a20  ecution_engine: 
+00015f00: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
+00015f10: 4e6f 6e65 2c0a 2020 2020 7466 5f61 7574  None,.    tf_aut
+00015f20: 6f5f 7472 616e 7366 6f72 6d5f 6665 6174  o_transform_feat
+00015f30: 7572 6573 3a20 4f70 7469 6f6e 616c 5b0a  ures: Optional[.
+00015f40: 2020 2020 2020 2020 556e 696f 6e5b 4c69          Union[Li
+00015f50: 7374 5b73 7472 5d2c 2044 6963 745b 7374  st[str], Dict[st
+00015f60: 722c 204c 6973 745b 7374 725d 5d5d 0a20  r, List[str]]]. 
+00015f70: 2020 205d 203d 204e 6f6e 652c 0a20 2020     ] = None,.   
+00015f80: 2074 665f 6375 7374 6f6d 5f74 7261 6e73   tf_custom_trans
+00015f90: 666f 726d 6174 696f 6e5f 6465 6669 6e69  formation_defini
+00015fa0: 7469 6f6e 733a 204f 7074 696f 6e61 6c5b  tions: Optional[
+00015fb0: 4c69 7374 5b44 6963 745b 7374 722c 2041  List[Dict[str, A
+00015fc0: 6e79 5d5d 5d20 3d20 4e6f 6e65 2c0a 2020  ny]]] = None,.  
+00015fd0: 2020 7466 5f74 7261 6e73 666f 726d 6174    tf_transformat
+00015fe0: 696f 6e73 5f70 6174 683a 204f 7074 696f  ions_path: Optio
+00015ff0: 6e61 6c5b 7374 725d 203d 204e 6f6e 652c  nal[str] = None,
+00016000: 0a20 2020 2065 6e61 626c 655f 7072 6f66  .    enable_prof
+00016010: 696c 6572 3a20 626f 6f6c 203d 2046 616c  iler: bool = Fal
+00016020: 7365 2c0a 2020 2020 6361 6368 655f 6461  se,.    cache_da
+00016030: 7461 3a20 7374 7220 3d20 2761 7574 6f27  ta: str = 'auto'
+00016040: 2c0a 2020 2020 7365 6564 3a20 696e 7420  ,.    seed: int 
+00016050: 3d20 312c 0a20 2020 2065 7661 6c5f 7374  = 1,.    eval_st
+00016060: 6570 733a 2069 6e74 203d 2030 2c0a 2020  eps: int = 0,.  
+00016070: 2020 6576 616c 5f66 7265 7175 656e 6379    eval_frequency
+00016080: 5f73 6563 733a 2069 6e74 203d 2036 3030  _secs: int = 600
+00016090: 2c0a 2020 2020 6461 7461 5f73 6f75 7263  ,.    data_sourc
+000160a0: 655f 6373 765f 6669 6c65 6e61 6d65 733a  e_csv_filenames:
+000160b0: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
+000160c0: 204e 6f6e 652c 0a20 2020 2064 6174 615f   None,.    data_
+000160d0: 736f 7572 6365 5f62 6967 7175 6572 795f  source_bigquery_
+000160e0: 7461 626c 655f 7061 7468 3a20 4f70 7469  table_path: Opti
+000160f0: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
+00016100: 2c0a 2020 2020 6269 6771 7565 7279 5f73  ,.    bigquery_s
+00016110: 7461 6769 6e67 5f66 756c 6c5f 6461 7461  taging_full_data
+00016120: 7365 745f 6964 3a20 4f70 7469 6f6e 616c  set_id: Optional
+00016130: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 2020  [str] = None,.  
+00016140: 2020 7765 6967 6874 5f63 6f6c 756d 6e3a    weight_column:
+00016150: 2073 7472 203d 2027 272c 0a20 2020 206d   str = '',.    m
+00016160: 6178 5f66 6169 6c65 645f 7472 6961 6c5f  ax_failed_trial_
+00016170: 636f 756e 743a 2069 6e74 203d 2030 2c0a  count: int = 0,.
+00016180: 2020 2020 7374 7564 795f 7370 6563 5f61      study_spec_a
+00016190: 6c67 6f72 6974 686d 3a20 7374 7220 3d20  lgorithm: str = 
+000161a0: 2741 4c47 4f52 4954 484d 5f55 4e53 5045  'ALGORITHM_UNSPE
+000161b0: 4349 4649 4544 272c 0a20 2020 2073 7475  CIFIED',.    stu
+000161c0: 6479 5f73 7065 635f 6d65 6173 7572 656d  dy_spec_measurem
+000161d0: 656e 745f 7365 6c65 6374 696f 6e5f 7479  ent_selection_ty
+000161e0: 7065 3a20 7374 7220 3d20 2742 4553 545f  pe: str = 'BEST_
+000161f0: 4d45 4153 5552 454d 454e 5427 2c0a 2020  MEASUREMENT',.  
+00016200: 2020 7472 616e 7366 6f72 6d5f 6461 7461    transform_data
+00016210: 666c 6f77 5f6d 6163 6869 6e65 5f74 7970  flow_machine_typ
+00016220: 653a 2073 7472 203d 2027 6e31 2d73 7461  e: str = 'n1-sta
+00016230: 6e64 6172 642d 3136 272c 0a20 2020 2074  ndard-16',.    t
+00016240: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
+00016250: 775f 6d61 785f 6e75 6d5f 776f 726b 6572  w_max_num_worker
+00016260: 733a 2069 6e74 203d 2032 352c 0a20 2020  s: int = 25,.   
+00016270: 2074 7261 6e73 666f 726d 5f64 6174 6166   transform_dataf
+00016280: 6c6f 775f 6469 736b 5f73 697a 655f 6762  low_disk_size_gb
+00016290: 3a20 696e 7420 3d20 3430 2c0a 2020 2020  : int = 40,.    
+000162a0: 776f 726b 6572 5f70 6f6f 6c5f 7370 6563  worker_pool_spec
+000162b0: 735f 6f76 6572 7269 6465 3a20 4f70 7469  s_override: Opti
+000162c0: 6f6e 616c 5b44 6963 745b 7374 722c 2041  onal[Dict[str, A
+000162d0: 6e79 5d5d 203d 204e 6f6e 652c 0a20 2020  ny]] = None,.   
+000162e0: 2072 756e 5f65 7661 6c75 6174 696f 6e3a   run_evaluation:
+000162f0: 2062 6f6f 6c20 3d20 5472 7565 2c0a 2020   bool = True,.  
+00016300: 2020 6576 616c 7561 7469 6f6e 5f62 6174    evaluation_bat
+00016310: 6368 5f70 7265 6469 6374 5f6d 6163 6869  ch_predict_machi
+00016320: 6e65 5f74 7970 653a 2073 7472 203d 205f  ne_type: str = _
+00016330: 4556 414c 5541 5449 4f4e 5f42 4154 4348  EVALUATION_BATCH
+00016340: 5f50 5245 4449 4354 5f4d 4143 4849 4e45  _PREDICT_MACHINE
+00016350: 5f54 5950 452c 0a20 2020 2065 7661 6c75  _TYPE,.    evalu
+00016360: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
+00016370: 6963 745f 7374 6172 7469 6e67 5f72 6570  ict_starting_rep
+00016380: 6c69 6361 5f63 6f75 6e74 3a20 696e 7420  lica_count: int 
+00016390: 3d20 5f45 5641 4c55 4154 494f 4e5f 4241  = _EVALUATION_BA
+000163a0: 5443 485f 5052 4544 4943 545f 5354 4152  TCH_PREDICT_STAR
+000163b0: 5449 4e47 5f52 4550 4c49 4341 5f43 4f55  TING_REPLICA_COU
+000163c0: 4e54 2c0a 2020 2020 6576 616c 7561 7469  NT,.    evaluati
+000163d0: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
+000163e0: 5f6d 6178 5f72 6570 6c69 6361 5f63 6f75  _max_replica_cou
+000163f0: 6e74 3a20 696e 7420 3d20 5f45 5641 4c55  nt: int = _EVALU
+00016400: 4154 494f 4e5f 4241 5443 485f 5052 4544  ATION_BATCH_PRED
+00016410: 4943 545f 4d41 585f 5245 504c 4943 415f  ICT_MAX_REPLICA_
+00016420: 434f 554e 542c 0a20 2020 2065 7661 6c75  COUNT,.    evalu
+00016430: 6174 696f 6e5f 6461 7461 666c 6f77 5f6d  ation_dataflow_m
+00016440: 6163 6869 6e65 5f74 7970 653a 2073 7472  achine_type: str
+00016450: 203d 205f 4556 414c 5541 5449 4f4e 5f44   = _EVALUATION_D
+00016460: 4154 4146 4c4f 575f 4d41 4348 494e 455f  ATAFLOW_MACHINE_
+00016470: 5459 5045 2c0a 2020 2020 6576 616c 7561  TYPE,.    evalua
+00016480: 7469 6f6e 5f64 6174 6166 6c6f 775f 7374  tion_dataflow_st
+00016490: 6172 7469 6e67 5f6e 756d 5f77 6f72 6b65  arting_num_worke
+000164a0: 7273 3a20 696e 7420 3d20 5f45 5641 4c55  rs: int = _EVALU
+000164b0: 4154 494f 4e5f 4441 5441 464c 4f57 5f53  ATION_DATAFLOW_S
+000164c0: 5441 5254 494e 475f 4e55 4d5f 574f 524b  TARTING_NUM_WORK
+000164d0: 4552 532c 0a20 2020 2065 7661 6c75 6174  ERS,.    evaluat
+000164e0: 696f 6e5f 6461 7461 666c 6f77 5f6d 6178  ion_dataflow_max
+000164f0: 5f6e 756d 5f77 6f72 6b65 7273 3a20 696e  _num_workers: in
+00016500: 7420 3d20 5f45 5641 4c55 4154 494f 4e5f  t = _EVALUATION_
+00016510: 4441 5441 464c 4f57 5f4d 4158 5f4e 554d  DATAFLOW_MAX_NUM
+00016520: 5f57 4f52 4b45 5253 2c0a 2020 2020 6576  _WORKERS,.    ev
+00016530: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
+00016540: 775f 6469 736b 5f73 697a 655f 6762 3a20  w_disk_size_gb: 
+00016550: 696e 7420 3d20 5f45 5641 4c55 4154 494f  int = _EVALUATIO
+00016560: 4e5f 4441 5441 464c 4f57 5f44 4953 4b5f  N_DATAFLOW_DISK_
+00016570: 5349 5a45 5f47 422c 0a20 2020 2064 6174  SIZE_GB,.    dat
+00016580: 6166 6c6f 775f 7365 7276 6963 655f 6163  aflow_service_ac
+00016590: 636f 756e 743a 2073 7472 203d 2027 272c  count: str = '',
+000165a0: 0a20 2020 2064 6174 6166 6c6f 775f 7375  .    dataflow_su
+000165b0: 626e 6574 776f 726b 3a20 7374 7220 3d20  bnetwork: str = 
+000165c0: 2727 2c0a 2020 2020 6461 7461 666c 6f77  '',.    dataflow
+000165d0: 5f75 7365 5f70 7562 6c69 635f 6970 733a  _use_public_ips:
+000165e0: 2062 6f6f 6c20 3d20 5472 7565 2c0a 2020   bool = True,.  
+000165f0: 2020 656e 6372 7970 7469 6f6e 5f73 7065    encryption_spe
+00016600: 635f 6b65 795f 6e61 6d65 3a20 7374 7220  c_key_name: str 
+00016610: 3d20 2727 2c0a 2920 2d3e 2054 7570 6c65  = '',.) -> Tuple
+00016620: 5b73 7472 2c20 4469 6374 5b73 7472 2c20  [str, Dict[str, 
+00016630: 416e 795d 5d3a 0a20 2023 2066 6d74 3a20  Any]]:.  # fmt: 
+00016640: 6f66 660a 2020 2222 2247 6574 2074 6865  off.  """Get the
+00016650: 2054 6162 4e65 7420 4879 7065 7270 6172   TabNet Hyperpar
+00016660: 616d 6574 6572 5475 6e69 6e67 4a6f 6220  ameterTuningJob 
+00016670: 7069 7065 6c69 6e65 2e0a 0a20 2041 7267  pipeline...  Arg
+00016680: 733a 0a20 2020 2070 726f 6a65 6374 3a20  s:.    project: 
+00016690: 5468 6520 4743 5020 7072 6f6a 6563 7420  The GCP project 
+000166a0: 7468 6174 2072 756e 7320 7468 6520 7069  that runs the pi
+000166b0: 7065 6c69 6e65 2063 6f6d 706f 6e65 6e74  peline component
+000166c0: 732e 0a20 2020 206c 6f63 6174 696f 6e3a  s..    location:
+000166d0: 2054 6865 2047 4350 2072 6567 696f 6e20   The GCP region 
+000166e0: 7468 6174 2072 756e 7320 7468 6520 7069  that runs the pi
+000166f0: 7065 6c69 6e65 2063 6f6d 706f 6e65 6e74  peline component
+00016700: 732e 0a20 2020 2072 6f6f 745f 6469 723a  s..    root_dir:
+00016710: 2054 6865 2072 6f6f 7420 4743 5320 6469   The root GCS di
+00016720: 7265 6374 6f72 7920 666f 7220 7468 6520  rectory for the 
+00016730: 7069 7065 6c69 6e65 2063 6f6d 706f 6e65  pipeline compone
+00016740: 6e74 732e 0a20 2020 2074 6172 6765 745f  nts..    target_
+00016750: 636f 6c75 6d6e 3a20 5468 6520 7461 7267  column: The targ
+00016760: 6574 2063 6f6c 756d 6e20 6e61 6d65 2e0a  et column name..
+00016770: 2020 2020 7072 6564 6963 7469 6f6e 5f74      prediction_t
+00016780: 7970 653a 2054 6865 2074 7970 6520 6f66  ype: The type of
+00016790: 2070 7265 6469 6374 696f 6e20 7468 6520   prediction the 
+000167a0: 6d6f 6465 6c20 6973 2074 6f20 7072 6f64  model is to prod
+000167b0: 7563 652e 2020 2263 6c61 7373 6966 6963  uce.  "classific
+000167c0: 6174 696f 6e22 206f 7220 2272 6567 7265  ation" or "regre
+000167d0: 7373 696f 6e22 2e0a 2020 2020 7374 7564  ssion"..    stud
+000167e0: 795f 7370 6563 5f6d 6574 7269 635f 6964  y_spec_metric_id
+000167f0: 3a20 4d65 7472 6963 2074 6f20 6f70 7469  : Metric to opti
+00016800: 6d69 7a65 2c20 706f 7373 6962 6c65 2076  mize, possible v
+00016810: 616c 7565 733a 205b 2027 6c6f 7373 272c  alues: [ 'loss',
+00016820: 2027 6176 6572 6167 655f 6c6f 7373 272c   'average_loss',
+00016830: 2027 726d 7365 272c 2027 6d61 6527 2c20   'rmse', 'mae', 
+00016840: 276d 716c 272c 2027 6163 6375 7261 6379  'mql', 'accuracy
+00016850: 272c 2027 6175 6327 2c20 2770 7265 6369  ', 'auc', 'preci
+00016860: 7369 6f6e 272c 2027 7265 6361 6c6c 275d  sion', 'recall']
+00016870: 2e0a 2020 2020 7374 7564 795f 7370 6563  ..    study_spec
+00016880: 5f6d 6574 7269 635f 676f 616c 3a20 4f70  _metric_goal: Op
+00016890: 7469 6d69 7a61 7469 6f6e 2067 6f61 6c20  timization goal 
+000168a0: 6f66 2074 6865 206d 6574 7269 632c 2070  of the metric, p
+000168b0: 6f73 7369 626c 6520 7661 6c75 6573 3a20  ossible values: 
+000168c0: 224d 4158 494d 495a 4522 2c20 224d 494e  "MAXIMIZE", "MIN
+000168d0: 494d 495a 4522 2e0a 2020 2020 7374 7564  IMIZE"..    stud
+000168e0: 795f 7370 6563 5f70 6172 616d 6574 6572  y_spec_parameter
+000168f0: 735f 6f76 6572 7269 6465 3a20 4c69 7374  s_override: List
+00016900: 206f 6620 6469 6374 696f 6e61 7269 6573   of dictionaries
+00016910: 2072 6570 7265 7365 6e74 696e 6720 7061   representing pa
+00016920: 7261 6d65 7465 7273 2074 6f20 6f70 7469  rameters to opti
+00016930: 6d69 7a65 2e20 5468 6520 6469 6374 696f  mize. The dictio
+00016940: 6e61 7279 206b 6579 2069 7320 7468 6520  nary key is the 
+00016950: 7061 7261 6d65 7465 725f 6964 2c20 7768  parameter_id, wh
+00016960: 6963 6820 6973 2070 6173 7365 6420 746f  ich is passed to
+00016970: 2074 7261 696e 696e 6720 6a6f 6220 6173   training job as
+00016980: 2061 2063 6f6d 6d61 6e64 206c 696e 6520   a command line 
+00016990: 6172 6775 6d65 6e74 2c20 616e 6420 7468  argument, and th
+000169a0: 6520 6469 6374 696f 6e61 7279 2076 616c  e dictionary val
+000169b0: 7565 2069 7320 7468 6520 7061 7261 6d65  ue is the parame
+000169c0: 7465 7220 7370 6563 6966 6963 6174 696f  ter specificatio
+000169d0: 6e20 6f66 2074 6865 206d 6574 7269 632e  n of the metric.
+000169e0: 0a20 2020 206d 6178 5f74 7269 616c 5f63  .    max_trial_c
+000169f0: 6f75 6e74 3a20 5468 6520 6465 7369 7265  ount: The desire
+00016a00: 6420 746f 7461 6c20 6e75 6d62 6572 206f  d total number o
+00016a10: 6620 7472 6961 6c73 2e0a 2020 2020 7061  f trials..    pa
+00016a20: 7261 6c6c 656c 5f74 7269 616c 5f63 6f75  rallel_trial_cou
+00016a30: 6e74 3a20 5468 6520 6465 7369 7265 6420  nt: The desired 
+00016a40: 6e75 6d62 6572 206f 6620 7472 6961 6c73  number of trials
+00016a50: 2074 6f20 7275 6e20 696e 2070 6172 616c   to run in paral
+00016a60: 6c65 6c2e 0a20 2020 2074 7261 6e73 666f  lel..    transfo
+00016a70: 726d 5f63 6f6e 6669 673a 2050 6174 6820  rm_config: Path 
+00016a80: 746f 2076 3120 5446 2074 7261 6e73 666f  to v1 TF transfo
+00016a90: 726d 6174 696f 6e20 636f 6e66 6967 7572  rmation configur
+00016aa0: 6174 696f 6e2e 0a20 2020 2064 6174 6173  ation..    datas
+00016ab0: 6574 5f6c 6576 656c 5f63 7573 746f 6d5f  et_level_custom_
+00016ac0: 7472 616e 7366 6f72 6d61 7469 6f6e 5f64  transformation_d
+00016ad0: 6566 696e 6974 696f 6e73 3a20 4461 7461  efinitions: Data
+00016ae0: 7365 742d 6c65 7665 6c20 6375 7374 6f6d  set-level custom
+00016af0: 2074 7261 6e73 666f 726d 6174 696f 6e20   transformation 
+00016b00: 6465 6669 6e69 7469 6f6e 7320 696e 2073  definitions in s
+00016b10: 7472 696e 6720 666f 726d 6174 2e0a 2020  tring format..  
+00016b20: 2020 6461 7461 7365 745f 6c65 7665 6c5f    dataset_level_
+00016b30: 7472 616e 7366 6f72 6d61 7469 6f6e 733a  transformations:
+00016b40: 2044 6174 6173 6574 2d6c 6576 656c 2074   Dataset-level t
+00016b50: 7261 6e73 666f 726d 6174 696f 6e20 636f  ransformation co
+00016b60: 6e66 6967 7572 6174 696f 6e20 696e 2073  nfiguration in s
+00016b70: 7472 696e 6720 666f 726d 6174 2e0a 2020  tring format..  
+00016b80: 2020 7275 6e5f 6665 6174 7572 655f 7365    run_feature_se
+00016b90: 6c65 6374 696f 6e3a 2057 6865 7468 6572  lection: Whether
+00016ba0: 2074 6f20 656e 6162 6c65 2066 6561 7475   to enable featu
+00016bb0: 7265 2073 656c 6563 7469 6f6e 2e0a 2020  re selection..  
+00016bc0: 2020 6665 6174 7572 655f 7365 6c65 6374    feature_select
+00016bd0: 696f 6e5f 616c 676f 7269 7468 6d3a 2046  ion_algorithm: F
+00016be0: 6561 7475 7265 2073 656c 6563 7469 6f6e  eature selection
+00016bf0: 2061 6c67 6f72 6974 686d 2e0a 2020 2020   algorithm..    
+00016c00: 6d61 7465 7269 616c 697a 6564 5f65 7861  materialized_exa
+00016c10: 6d70 6c65 735f 666f 726d 6174 3a20 5468  mples_format: Th
+00016c20: 6520 666f 726d 6174 2066 6f72 2074 6865  e format for the
+00016c30: 206d 6174 6572 6961 6c69 7a65 6420 6578   materialized ex
+00016c40: 616d 706c 6573 2e0a 2020 2020 6d61 785f  amples..    max_
+00016c50: 7365 6c65 6374 6564 5f66 6561 7475 7265  selected_feature
+00016c60: 733a 204d 6178 696d 756d 206e 756d 6265  s: Maximum numbe
+00016c70: 7220 6f66 2066 6561 7475 7265 7320 746f  r of features to
+00016c80: 2073 656c 6563 742e 0a20 2020 2070 7265   select..    pre
+00016c90: 6465 6669 6e65 645f 7370 6c69 745f 6b65  defined_split_ke
+00016ca0: 793a 2050 7265 6465 6669 6e65 6420 7370  y: Predefined sp
+00016cb0: 6c69 7420 6b65 792e 0a20 2020 2073 7472  lit key..    str
+00016cc0: 6174 6966 6965 645f 7370 6c69 745f 6b65  atified_split_ke
+00016cd0: 793a 2053 7472 6174 6966 6965 6420 7370  y: Stratified sp
+00016ce0: 6c69 7420 6b65 792e 0a20 2020 2074 7261  lit key..    tra
+00016cf0: 696e 696e 675f 6672 6163 7469 6f6e 3a20  ining_fraction: 
+00016d00: 5472 6169 6e69 6e67 2066 7261 6374 696f  Training fractio
+00016d10: 6e2e 0a20 2020 2076 616c 6964 6174 696f  n..    validatio
+00016d20: 6e5f 6672 6163 7469 6f6e 3a20 5661 6c69  n_fraction: Vali
+00016d30: 6461 7469 6f6e 2066 7261 6374 696f 6e2e  dation fraction.
+00016d40: 0a20 2020 2074 6573 745f 6672 6163 7469  .    test_fracti
+00016d50: 6f6e 3a20 5465 7374 2066 7261 6374 696f  on: Test fractio
+00016d60: 6e2e 0a20 2020 2074 665f 7472 616e 7366  n..    tf_transf
+00016d70: 6f72 6d5f 6578 6563 7574 696f 6e5f 656e  orm_execution_en
+00016d80: 6769 6e65 3a20 5468 6520 6578 6563 7574  gine: The execut
+00016d90: 696f 6e20 656e 6769 6e65 2075 7365 6420  ion engine used 
+00016da0: 746f 2065 7865 6375 7465 2054 462d 6261  to execute TF-ba
+00016db0: 7365 6420 7472 616e 7366 6f72 6d61 7469  sed transformati
+00016dc0: 6f6e 732e 0a20 2020 2074 665f 6175 746f  ons..    tf_auto
+00016dd0: 5f74 7261 6e73 666f 726d 5f66 6561 7475  _transform_featu
+00016de0: 7265 733a 204c 6973 7420 6f66 2061 7574  res: List of aut
+00016df0: 6f20 7472 616e 7366 6f72 6d20 6665 6174  o transform feat
+00016e00: 7572 6573 2069 6e20 7468 6520 636f 6d6d  ures in the comm
+00016e10: 612d 7365 7061 7261 7465 6420 7374 7269  a-separated stri
+00016e20: 6e67 2066 6f72 6d61 742e 0a20 2020 2074  ng format..    t
+00016e30: 665f 6375 7374 6f6d 5f74 7261 6e73 666f  f_custom_transfo
+00016e40: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
+00016e50: 6f6e 733a 2054 4620 6375 7374 6f6d 2074  ons: TF custom t
+00016e60: 7261 6e73 666f 726d 6174 696f 6e20 6465  ransformation de
+00016e70: 6669 6e69 7469 6f6e 7320 696e 2073 7472  finitions in str
+00016e80: 696e 6720 666f 726d 6174 2e0a 2020 2020  ing format..    
+00016e90: 7466 5f74 7261 6e73 666f 726d 6174 696f  tf_transformatio
+00016ea0: 6e73 5f70 6174 683a 2050 6174 6820 746f  ns_path: Path to
+00016eb0: 2054 4620 7472 616e 7366 6f72 6d61 7469   TF transformati
+00016ec0: 6f6e 2063 6f6e 6669 6775 7261 7469 6f6e  on configuration
+00016ed0: 2e0a 2020 2020 656e 6162 6c65 5f70 726f  ..    enable_pro
+00016ee0: 6669 6c65 723a 2045 6e61 626c 6573 2070  filer: Enables p
+00016ef0: 726f 6669 6c69 6e67 2061 6e64 2073 6176  rofiling and sav
+00016f00: 6573 2061 2074 7261 6365 2064 7572 696e  es a trace durin
+00016f10: 6720 6576 616c 7561 7469 6f6e 2e0a 2020  g evaluation..  
+00016f20: 2020 6361 6368 655f 6461 7461 3a20 5768    cache_data: Wh
+00016f30: 6574 6865 7220 746f 2063 6163 6865 2064  ether to cache d
+00016f40: 6174 6120 6f72 206e 6f74 2e20 4966 2073  ata or not. If s
+00016f50: 6574 2074 6f20 2761 7574 6f27 2c20 6361  et to 'auto', ca
+00016f60: 6368 696e 6720 6973 2064 6574 6572 6d69  ching is determi
+00016f70: 6e65 6420 6261 7365 6420 6f6e 2074 6865  ned based on the
+00016f80: 2064 6174 6173 6574 2073 697a 652e 0a20   dataset size.. 
+00016f90: 2020 2073 6565 643a 2053 6565 6420 746f     seed: Seed to
+00016fa0: 2062 6520 7573 6564 2066 6f72 2074 6869   be used for thi
+00016fb0: 7320 7275 6e2e 0a20 2020 2065 7661 6c5f  s run..    eval_
+00016fc0: 7374 6570 733a 204e 756d 6265 7220 6f66  steps: Number of
+00016fd0: 2073 7465 7073 2074 6f20 7275 6e20 6576   steps to run ev
+00016fe0: 616c 7561 7469 6f6e 2066 6f72 2e20 4966  aluation for. If
+00016ff0: 206e 6f74 2073 7065 6369 6669 6564 206f   not specified o
+00017000: 7220 6e65 6761 7469 7665 2c20 6974 206d  r negative, it m
+00017010: 6561 6e73 2072 756e 2065 7661 6c75 6174  eans run evaluat
+00017020: 696f 6e20 6f6e 2074 6865 2077 686f 6c65  ion on the whole
+00017030: 2076 616c 6964 6174 696f 6e20 6461 7461   validation data
+00017040: 7365 742e 2049 6620 7365 7420 746f 2030  set. If set to 0
+00017050: 2c20 6974 206d 6561 6e73 2072 756e 2065  , it means run e
+00017060: 7661 6c75 6174 696f 6e20 666f 7220 6120  valuation for a 
+00017070: 6669 7865 6420 6e75 6d62 6572 206f 6620  fixed number of 
+00017080: 7361 6d70 6c65 732e 0a20 2020 2065 7661  samples..    eva
+00017090: 6c5f 6672 6571 7565 6e63 795f 7365 6373  l_frequency_secs
+000170a0: 3a20 4672 6571 7565 6e63 7920 6174 2077  : Frequency at w
+000170b0: 6869 6368 2065 7661 6c75 6174 696f 6e20  hich evaluation 
+000170c0: 616e 6420 6368 6563 6b70 6f69 6e74 696e  and checkpointin
+000170d0: 6720 7769 6c6c 2074 616b 6520 706c 6163  g will take plac
+000170e0: 652e 0a20 2020 2064 6174 615f 736f 7572  e..    data_sour
+000170f0: 6365 5f63 7376 5f66 696c 656e 616d 6573  ce_csv_filenames
+00017100: 3a20 5468 6520 4353 5620 6461 7461 2073  : The CSV data s
+00017110: 6f75 7263 652e 0a20 2020 2064 6174 615f  ource..    data_
+00017120: 736f 7572 6365 5f62 6967 7175 6572 795f  source_bigquery_
+00017130: 7461 626c 655f 7061 7468 3a20 5468 6520  table_path: The 
+00017140: 4269 6751 7565 7279 2064 6174 6120 736f  BigQuery data so
+00017150: 7572 6365 2e0a 2020 2020 6269 6771 7565  urce..    bigque
+00017160: 7279 5f73 7461 6769 6e67 5f66 756c 6c5f  ry_staging_full_
+00017170: 6461 7461 7365 745f 6964 3a20 5468 6520  dataset_id: The 
+00017180: 4269 6751 7565 7279 2073 7461 6769 6e67  BigQuery staging
+00017190: 2066 756c 6c20 6461 7461 7365 7420 6964   full dataset id
+000171a0: 2066 6f72 2073 746f 7269 6e67 2069 6e74   for storing int
+000171b0: 6572 6d65 6469 6174 6520 7461 626c 6573  ermediate tables
+000171c0: 2e0a 2020 2020 7765 6967 6874 5f63 6f6c  ..    weight_col
+000171d0: 756d 6e3a 2054 6865 2077 6569 6768 7420  umn: The weight 
+000171e0: 636f 6c75 6d6e 206e 616d 652e 0a20 2020  column name..   
+000171f0: 206d 6178 5f66 6169 6c65 645f 7472 6961   max_failed_tria
+00017200: 6c5f 636f 756e 743a 2054 6865 206e 756d  l_count: The num
+00017210: 6265 7220 6f66 2066 6169 6c65 6420 7472  ber of failed tr
+00017220: 6961 6c73 2074 6861 7420 6e65 6564 2074  ials that need t
+00017230: 6f20 6265 2073 6565 6e20 6265 666f 7265  o be seen before
+00017240: 2066 6169 6c69 6e67 2074 6865 2048 7970   failing the Hyp
+00017250: 6572 7061 7261 6d65 7465 7254 756e 696e  erparameterTunin
+00017260: 674a 6f62 2e20 4966 2073 6574 2074 6f20  gJob. If set to 
+00017270: 302c 2056 6572 7465 7820 4149 2064 6563  0, Vertex AI dec
+00017280: 6964 6573 2068 6f77 206d 616e 7920 7472  ides how many tr
+00017290: 6961 6c73 206d 7573 7420 6661 696c 2062  ials must fail b
+000172a0: 6566 6f72 6520 7468 6520 7768 6f6c 6520  efore the whole 
+000172b0: 6a6f 6220 6661 696c 732e 0a20 2020 2073  job fails..    s
+000172c0: 7475 6479 5f73 7065 635f 616c 676f 7269  tudy_spec_algori
+000172d0: 7468 6d3a 2054 6865 2073 6561 7263 6820  thm: The search 
+000172e0: 616c 676f 7269 7468 6d20 7370 6563 6966  algorithm specif
+000172f0: 6965 6420 666f 7220 7468 6520 7374 7564  ied for the stud
+00017300: 792e 204f 6e65 206f 6620 2241 4c47 4f52  y. One of "ALGOR
+00017310: 4954 484d 5f55 4e53 5045 4349 4649 4544  ITHM_UNSPECIFIED
+00017320: 222c 2022 4752 4944 5f53 4541 5243 4822  ", "GRID_SEARCH"
+00017330: 2c20 6f72 2022 5241 4e44 4f4d 5f53 4541  , or "RANDOM_SEA
+00017340: 5243 4822 2e0a 2020 2020 7374 7564 795f  RCH"..    study_
+00017350: 7370 6563 5f6d 6561 7375 7265 6d65 6e74  spec_measurement
+00017360: 5f73 656c 6563 7469 6f6e 5f74 7970 653a  _selection_type:
+00017370: 2057 6869 6368 206d 6561 7375 7265 6d65   Which measureme
+00017380: 6e74 2074 6f20 7573 6520 6966 2f77 6865  nt to use if/whe
+00017390: 6e20 7468 6520 7365 7276 6963 6520 6175  n the service au
+000173a0: 746f 6d61 7469 6361 6c6c 7920 7365 6c65  tomatically sele
+000173b0: 6374 7320 7468 6520 6669 6e61 6c20 6d65  cts the final me
+000173c0: 6173 7572 656d 656e 7420 6672 6f6d 2070  asurement from p
+000173d0: 7265 7669 6f75 736c 7920 7265 706f 7274  reviously report
+000173e0: 6564 2069 6e74 6572 6d65 6469 6174 6520  ed intermediate 
+000173f0: 6d65 6173 7572 656d 656e 7473 2e20 4f6e  measurements. On
+00017400: 6520 6f66 2022 4245 5354 5f4d 4541 5355  e of "BEST_MEASU
+00017410: 5245 4d45 4e54 2220 6f72 2022 4c41 5354  REMENT" or "LAST
+00017420: 5f4d 4541 5355 5245 4d45 4e54 222e 0a20  _MEASUREMENT".. 
+00017430: 2020 2074 7261 6e73 666f 726d 5f64 6174     transform_dat
+00017440: 6166 6c6f 775f 6d61 6368 696e 655f 7479  aflow_machine_ty
+00017450: 7065 3a20 5468 6520 6461 7461 666c 6f77  pe: The dataflow
+00017460: 206d 6163 6869 6e65 2074 7970 6520 666f   machine type fo
+00017470: 7220 7472 616e 7366 6f72 6d20 636f 6d70  r transform comp
+00017480: 6f6e 656e 742e 0a20 2020 2074 7261 6e73  onent..    trans
+00017490: 666f 726d 5f64 6174 6166 6c6f 775f 6d61  form_dataflow_ma
+000174a0: 785f 6e75 6d5f 776f 726b 6572 733a 2054  x_num_workers: T
+000174b0: 6865 206d 6178 206e 756d 6265 7220 6f66  he max number of
+000174c0: 2044 6174 6166 6c6f 7720 776f 726b 6572   Dataflow worker
+000174d0: 7320 666f 7220 7472 616e 7366 6f72 6d20  s for transform 
+000174e0: 636f 6d70 6f6e 656e 742e 0a20 2020 2074  component..    t
+000174f0: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
+00017500: 775f 6469 736b 5f73 697a 655f 6762 3a20  w_disk_size_gb: 
+00017510: 4461 7461 666c 6f77 2077 6f72 6b65 7227  Dataflow worker'
+00017520: 7320 6469 736b 2073 697a 6520 696e 2047  s disk size in G
+00017530: 4220 666f 7220 7472 616e 7366 6f72 6d20  B for transform 
+00017540: 636f 6d70 6f6e 656e 742e 0a20 2020 2077  component..    w
+00017550: 6f72 6b65 725f 706f 6f6c 5f73 7065 6373  orker_pool_specs
+00017560: 5f6f 7665 7272 6964 653a 2054 6865 2064  _override: The d
+00017570: 6963 7469 6f6e 6172 7920 666f 7220 6f76  ictionary for ov
+00017580: 6572 7269 6469 6e67 2074 7261 696e 696e  erriding trainin
+00017590: 6720 616e 6420 6576 616c 7561 7469 6f6e  g and evaluation
+000175a0: 2077 6f72 6b65 7220 706f 6f6c 2073 7065   worker pool spe
+000175b0: 6373 2e20 5468 6520 6469 6374 696f 6e61  cs. The dictiona
+000175c0: 7279 2073 686f 756c 6420 6265 206f 6620  ry should be of 
+000175d0: 666f 726d 6174 2068 7474 7073 3a2f 2f67  format https://g
+000175e0: 6974 6875 622e 636f 6d2f 676f 6f67 6c65  ithub.com/google
+000175f0: 6170 6973 2f67 6f6f 676c 6561 7069 732f  apis/googleapis/
+00017600: 626c 6f62 2f34 6538 3336 6337 6332 3537  blob/4e836c7c257
+00017610: 6533 6532 3062 3164 6531 3464 3437 3039  e3e20b1de14d4709
+00017620: 3933 6132 6231 6634 3733 3661 382f 676f  93a2b1f4736a8/go
+00017630: 6f67 6c65 2f63 6c6f 7564 2f61 6970 6c61  ogle/cloud/aipla
+00017640: 7466 6f72 6d2f 7631 6265 7461 312f 6375  tform/v1beta1/cu
+00017650: 7374 6f6d 5f6a 6f62 2e70 726f 746f 234c  stom_job.proto#L
+00017660: 3137 322e 0a20 2020 2072 756e 5f65 7661  172..    run_eva
+00017670: 6c75 6174 696f 6e3a 2057 6865 7468 6572  luation: Whether
+00017680: 2074 6f20 7275 6e20 6576 616c 7561 7469   to run evaluati
+00017690: 6f6e 2073 7465 7073 2064 7572 696e 6720  on steps during 
+000176a0: 7472 6169 6e69 6e67 2e0a 2020 2020 6576  training..    ev
+000176b0: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+000176c0: 7265 6469 6374 5f6d 6163 6869 6e65 5f74  redict_machine_t
+000176d0: 7970 653a 2054 6865 2070 7265 6469 6374  ype: The predict
+000176e0: 696f 6e20 7365 7276 6572 206d 6163 6869  ion server machi
+000176f0: 6e65 2074 7970 6520 666f 7220 6261 7463  ne type for batc
+00017700: 6820 7072 6564 6963 7420 636f 6d70 6f6e  h predict compon
+00017710: 656e 7473 2064 7572 696e 6720 6576 616c  ents during eval
+00017720: 7561 7469 6f6e 2e0a 2020 2020 6576 616c  uation..    eval
+00017730: 7561 7469 6f6e 5f62 6174 6368 5f70 7265  uation_batch_pre
+00017740: 6469 6374 5f73 7461 7274 696e 675f 7265  dict_starting_re
+00017750: 706c 6963 615f 636f 756e 743a 2054 6865  plica_count: The
+00017760: 2069 6e69 7469 616c 206e 756d 6265 7220   initial number 
+00017770: 6f66 2070 7265 6469 6374 696f 6e20 7365  of prediction se
+00017780: 7276 6572 2066 6f72 2062 6174 6368 2070  rver for batch p
+00017790: 7265 6469 6374 2063 6f6d 706f 6e65 6e74  redict component
+000177a0: 7320 6475 7269 6e67 2065 7661 6c75 6174  s during evaluat
+000177b0: 696f 6e2e 0a20 2020 2065 7661 6c75 6174  ion..    evaluat
+000177c0: 696f 6e5f 6261 7463 685f 7072 6564 6963  ion_batch_predic
+000177d0: 745f 6d61 785f 7265 706c 6963 615f 636f  t_max_replica_co
+000177e0: 756e 743a 2054 6865 206d 6178 206e 756d  unt: The max num
+000177f0: 6265 7220 6f66 2070 7265 6469 6374 696f  ber of predictio
+00017800: 6e20 7365 7276 6572 2066 6f72 2062 6174  n server for bat
+00017810: 6368 2070 7265 6469 6374 2063 6f6d 706f  ch predict compo
+00017820: 6e65 6e74 7320 6475 7269 6e67 2065 7661  nents during eva
+00017830: 6c75 6174 696f 6e2e 0a20 2020 2065 7661  luation..    eva
+00017840: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
+00017850: 5f6d 6163 6869 6e65 5f74 7970 653a 2054  _machine_type: T
+00017860: 6865 2064 6174 6166 6c6f 7720 6d61 6368  he dataflow mach
+00017870: 696e 6520 7479 7065 2066 6f72 2065 7661  ine type for eva
+00017880: 6c75 6174 696f 6e20 636f 6d70 6f6e 656e  luation componen
+00017890: 7473 2e0a 2020 2020 6576 616c 7561 7469  ts..    evaluati
+000178a0: 6f6e 5f64 6174 6166 6c6f 775f 7374 6172  on_dataflow_star
+000178b0: 7469 6e67 5f6e 756d 5f77 6f72 6b65 7273  ting_num_workers
+000178c0: 3a20 5468 6520 696e 6974 6961 6c20 6e75  : The initial nu
+000178d0: 6d62 6572 206f 6620 4461 7461 666c 6f77  mber of Dataflow
+000178e0: 2077 6f72 6b65 7273 2066 6f72 2065 7661   workers for eva
+000178f0: 6c75 6174 696f 6e20 636f 6d70 6f6e 656e  luation componen
+00017900: 7473 2e0a 2020 2020 6576 616c 7561 7469  ts..    evaluati
+00017910: 6f6e 5f64 6174 6166 6c6f 775f 6d61 785f  on_dataflow_max_
+00017920: 6e75 6d5f 776f 726b 6572 733a 2054 6865  num_workers: The
+00017930: 206d 6178 206e 756d 6265 7220 6f66 2044   max number of D
+00017940: 6174 6166 6c6f 7720 776f 726b 6572 7320  ataflow workers 
+00017950: 666f 7220 6576 616c 7561 7469 6f6e 2063  for evaluation c
+00017960: 6f6d 706f 6e65 6e74 732e 0a20 2020 2065  omponents..    e
+00017970: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
+00017980: 6f77 5f64 6973 6b5f 7369 7a65 5f67 623a  ow_disk_size_gb:
+00017990: 2044 6174 6166 6c6f 7720 776f 726b 6572   Dataflow worker
+000179a0: 2773 2064 6973 6b20 7369 7a65 2069 6e20  's disk size in 
+000179b0: 4742 2066 6f72 2065 7661 6c75 6174 696f  GB for evaluatio
+000179c0: 6e20 636f 6d70 6f6e 656e 7473 2e0a 2020  n components..  
+000179d0: 2020 6461 7461 666c 6f77 5f73 6572 7669    dataflow_servi
+000179e0: 6365 5f61 6363 6f75 6e74 3a20 4375 7374  ce_account: Cust
+000179f0: 6f6d 2073 6572 7669 6365 2061 6363 6f75  om service accou
+00017a00: 6e74 2074 6f20 7275 6e20 6461 7461 666c  nt to run datafl
+00017a10: 6f77 206a 6f62 732e 0a20 2020 2064 6174  ow jobs..    dat
+00017a20: 6166 6c6f 775f 7375 626e 6574 776f 726b  aflow_subnetwork
+00017a30: 3a20 4461 7461 666c 6f77 2773 2066 756c  : Dataflow's ful
+00017a40: 6c79 2071 7561 6c69 6669 6564 2073 7562  ly qualified sub
+00017a50: 6e65 7477 6f72 6b20 6e61 6d65 2c20 7768  network name, wh
+00017a60: 656e 2065 6d70 7479 2074 6865 2064 6566  en empty the def
+00017a70: 6175 6c74 2073 7562 6e65 7477 6f72 6b20  ault subnetwork 
+00017a80: 7769 6c6c 2062 6520 7573 6564 2e20 4578  will be used. Ex
+00017a90: 616d 706c 653a 2068 7474 7073 3a2f 2f63  ample: https://c
+00017aa0: 6c6f 7564 2e67 6f6f 676c 652e 636f 6d2f  loud.google.com/
+00017ab0: 6461 7461 666c 6f77 2f64 6f63 732f 6775  dataflow/docs/gu
+00017ac0: 6964 6573 2f73 7065 6369 6679 696e 672d  ides/specifying-
+00017ad0: 6e65 7477 6f72 6b73 2365 7861 6d70 6c65  networks#example
+00017ae0: 5f6e 6574 776f 726b 5f61 6e64 5f73 7562  _network_and_sub
+00017af0: 6e65 7477 6f72 6b5f 7370 6563 6966 6963  network_specific
+00017b00: 6174 696f 6e73 0a20 2020 2064 6174 6166  ations.    dataf
+00017b10: 6c6f 775f 7573 655f 7075 626c 6963 5f69  low_use_public_i
+00017b20: 7073 3a20 5370 6563 6966 6965 7320 7768  ps: Specifies wh
+00017b30: 6574 6865 7220 4461 7461 666c 6f77 2077  ether Dataflow w
+00017b40: 6f72 6b65 7273 2075 7365 2070 7562 6c69  orkers use publi
+00017b50: 6320 4950 2061 6464 7265 7373 6573 2e0a  c IP addresses..
+00017b60: 2020 2020 656e 6372 7970 7469 6f6e 5f73      encryption_s
+00017b70: 7065 635f 6b65 795f 6e61 6d65 3a20 5468  pec_key_name: Th
+00017b80: 6520 4b4d 5320 6b65 7920 6e61 6d65 2e0a  e KMS key name..
+00017b90: 0a20 2052 6574 7572 6e73 3a0a 2020 2020  .  Returns:.    
+00017ba0: 5475 706c 6520 6f66 2070 6970 656c 696e  Tuple of pipelin
+00017bb0: 655f 6465 6669 6e69 7469 6f6e 5f70 6174  e_definition_pat
+00017bc0: 6820 616e 6420 7061 7261 6d65 7465 725f  h and parameter_
+00017bd0: 7661 6c75 6573 2e0a 2020 2222 220a 2020  values..  """.  
+00017be0: 2320 666d 743a 206f 6e0a 2020 6966 2069  # fmt: on.  if i
+00017bf0: 7369 6e73 7461 6e63 6528 7466 5f61 7574  sinstance(tf_aut
+00017c00: 6f5f 7472 616e 7366 6f72 6d5f 6665 6174  o_transform_feat
+00017c10: 7572 6573 2c20 6c69 7374 293a 0a20 2020  ures, list):.   
+00017c20: 2074 665f 6175 746f 5f74 7261 6e73 666f   tf_auto_transfo
+00017c30: 726d 5f66 6561 7475 7265 7320 3d20 7b27  rm_features = {'
+00017c40: 6175 746f 273a 2074 665f 6175 746f 5f74  auto': tf_auto_t
+00017c50: 7261 6e73 666f 726d 5f66 6561 7475 7265  ransform_feature
+00017c60: 737d 0a0a 2020 6966 2074 7261 6e73 666f  s}..  if transfo
+00017c70: 726d 5f63 6f6e 6669 6720 616e 6420 7466  rm_config and tf
+00017c80: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
+00017c90: 5f70 6174 683a 0a20 2020 2072 6169 7365  _path:.    raise
+00017ca0: 2056 616c 7565 4572 726f 7228 0a20 2020   ValueError(.   
+00017cb0: 2020 2020 2027 4f6e 6c79 206f 6e65 206f       'Only one o
+00017cc0: 6620 7472 616e 7366 6f72 6d5f 636f 6e66  f transform_conf
+00017cd0: 6967 2061 6e64 2074 665f 7472 616e 7366  ig and tf_transf
+00017ce0: 6f72 6d61 7469 6f6e 735f 7061 7468 2063  ormations_path c
+00017cf0: 616e 2027 0a20 2020 2020 2020 2027 6265  an '.        'be
+00017d00: 2073 7065 6369 6669 6564 2e27 0a20 2020   specified.'.   
+00017d10: 2029 0a0a 2020 656c 6966 2074 7261 6e73   )..  elif trans
+00017d20: 666f 726d 5f63 6f6e 6669 673a 0a20 2020  form_config:.   
+00017d30: 2077 6172 6e69 6e67 732e 7761 726e 280a   warnings.warn(.
+00017d40: 2020 2020 2020 2020 2774 7261 6e73 666f          'transfo
+00017d50: 726d 5f63 6f6e 6669 6720 7061 7261 6d65  rm_config parame
+00017d60: 7465 7220 6973 2064 6570 7265 6361 7465  ter is deprecate
+00017d70: 642e 2027 0a20 2020 2020 2020 2027 506c  d. '.        'Pl
+00017d80: 6561 7365 2075 7365 2074 6865 2066 6c61  ease use the fla
+00017d90: 7474 656e 6564 2074 7261 6e73 666f 726d  ttened transform
+00017da0: 2063 6f6e 6669 6720 6172 6775 6d65 6e74   config argument
+00017db0: 7320 696e 7374 6561 642e 270a 2020 2020  s instead.'.    
+00017dc0: 290a 2020 2020 7466 5f74 7261 6e73 666f  ).    tf_transfo
+00017dd0: 726d 6174 696f 6e73 5f70 6174 6820 3d20  rmations_path = 
+00017de0: 7472 616e 7366 6f72 6d5f 636f 6e66 6967  transform_config
+00017df0: 0a0a 2020 6966 206e 6f74 2077 6f72 6b65  ..  if not worke
+00017e00: 725f 706f 6f6c 5f73 7065 6373 5f6f 7665  r_pool_specs_ove
+00017e10: 7272 6964 653a 0a20 2020 2077 6f72 6b65  rride:.    worke
+00017e20: 725f 706f 6f6c 5f73 7065 6373 5f6f 7665  r_pool_specs_ove
+00017e30: 7272 6964 6520 3d20 5b5d 0a0a 2020 7061  rride = []..  pa
+00017e40: 7261 6d65 7465 725f 7661 6c75 6573 203d  rameter_values =
+00017e50: 207b 0a20 2020 2020 2027 7072 6f6a 6563   {.      'projec
+00017e60: 7427 3a20 7072 6f6a 6563 742c 0a20 2020  t': project,.   
+00017e70: 2020 2027 6c6f 6361 7469 6f6e 273a 206c     'location': l
+00017e80: 6f63 6174 696f 6e2c 0a20 2020 2020 2027  ocation,.      '
+00017e90: 726f 6f74 5f64 6972 273a 2072 6f6f 745f  root_dir': root_
+00017ea0: 6469 722c 0a20 2020 2020 2027 7461 7267  dir,.      'targ
+00017eb0: 6574 5f63 6f6c 756d 6e27 3a20 7461 7267  et_column': targ
+00017ec0: 6574 5f63 6f6c 756d 6e2c 0a20 2020 2020  et_column,.     
+00017ed0: 2027 7072 6564 6963 7469 6f6e 5f74 7970   'prediction_typ
+00017ee0: 6527 3a20 7072 6564 6963 7469 6f6e 5f74  e': prediction_t
+00017ef0: 7970 652c 0a20 2020 2020 2027 7374 7564  ype,.      'stud
+00017f00: 795f 7370 6563 5f6d 6574 7269 635f 6964  y_spec_metric_id
+00017f10: 273a 2073 7475 6479 5f73 7065 635f 6d65  ': study_spec_me
+00017f20: 7472 6963 5f69 642c 0a20 2020 2020 2027  tric_id,.      '
+00017f30: 7374 7564 795f 7370 6563 5f6d 6574 7269  study_spec_metri
+00017f40: 635f 676f 616c 273a 2073 7475 6479 5f73  c_goal': study_s
+00017f50: 7065 635f 6d65 7472 6963 5f67 6f61 6c2c  pec_metric_goal,
+00017f60: 0a20 2020 2020 2027 7374 7564 795f 7370  .      'study_sp
+00017f70: 6563 5f70 6172 616d 6574 6572 735f 6f76  ec_parameters_ov
+00017f80: 6572 7269 6465 273a 2073 7475 6479 5f73  erride': study_s
+00017f90: 7065 635f 7061 7261 6d65 7465 7273 5f6f  pec_parameters_o
+00017fa0: 7665 7272 6964 652c 0a20 2020 2020 2027  verride,.      '
+00017fb0: 6d61 785f 7472 6961 6c5f 636f 756e 7427  max_trial_count'
+00017fc0: 3a20 6d61 785f 7472 6961 6c5f 636f 756e  : max_trial_coun
+00017fd0: 742c 0a20 2020 2020 2027 7061 7261 6c6c  t,.      'parall
+00017fe0: 656c 5f74 7269 616c 5f63 6f75 6e74 273a  el_trial_count':
+00017ff0: 2070 6172 616c 6c65 6c5f 7472 6961 6c5f   parallel_trial_
+00018000: 636f 756e 742c 0a20 2020 2020 2027 656e  count,.      'en
+00018010: 6162 6c65 5f70 726f 6669 6c65 7227 3a20  able_profiler': 
+00018020: 656e 6162 6c65 5f70 726f 6669 6c65 722c  enable_profiler,
+00018030: 0a20 2020 2020 2027 6361 6368 655f 6461  .      'cache_da
+00018040: 7461 273a 2063 6163 6865 5f64 6174 612c  ta': cache_data,
+00018050: 0a20 2020 2020 2027 7365 6564 273a 2073  .      'seed': s
+00018060: 6565 642c 0a20 2020 2020 2027 6576 616c  eed,.      'eval
+00018070: 5f73 7465 7073 273a 2065 7661 6c5f 7374  _steps': eval_st
+00018080: 6570 732c 0a20 2020 2020 2027 6576 616c  eps,.      'eval
+00018090: 5f66 7265 7175 656e 6379 5f73 6563 7327  _frequency_secs'
+000180a0: 3a20 6576 616c 5f66 7265 7175 656e 6379  : eval_frequency
+000180b0: 5f73 6563 732c 0a20 2020 2020 2027 7765  _secs,.      'we
+000180c0: 6967 6874 5f63 6f6c 756d 6e27 3a20 7765  ight_column': we
+000180d0: 6967 6874 5f63 6f6c 756d 6e2c 0a20 2020  ight_column,.   
+000180e0: 2020 2027 6d61 785f 6661 696c 6564 5f74     'max_failed_t
+000180f0: 7269 616c 5f63 6f75 6e74 273a 206d 6178  rial_count': max
+00018100: 5f66 6169 6c65 645f 7472 6961 6c5f 636f  _failed_trial_co
+00018110: 756e 742c 0a20 2020 2020 2027 7374 7564  unt,.      'stud
+00018120: 795f 7370 6563 5f61 6c67 6f72 6974 686d  y_spec_algorithm
+00018130: 273a 2073 7475 6479 5f73 7065 635f 616c  ': study_spec_al
+00018140: 676f 7269 7468 6d2c 0a20 2020 2020 2027  gorithm,.      '
+00018150: 7374 7564 795f 7370 6563 5f6d 6561 7375  study_spec_measu
+00018160: 7265 6d65 6e74 5f73 656c 6563 7469 6f6e  rement_selection
+00018170: 5f74 7970 6527 3a20 280a 2020 2020 2020  _type': (.      
+00018180: 2020 2020 7374 7564 795f 7370 6563 5f6d      study_spec_m
+00018190: 6561 7375 7265 6d65 6e74 5f73 656c 6563  easurement_selec
+000181a0: 7469 6f6e 5f74 7970 650a 2020 2020 2020  tion_type.      
+000181b0: 292c 0a20 2020 2020 2027 7472 616e 7366  ),.      'transf
+000181c0: 6f72 6d5f 6461 7461 666c 6f77 5f6d 6163  orm_dataflow_mac
+000181d0: 6869 6e65 5f74 7970 6527 3a20 7472 616e  hine_type': tran
+000181e0: 7366 6f72 6d5f 6461 7461 666c 6f77 5f6d  sform_dataflow_m
+000181f0: 6163 6869 6e65 5f74 7970 652c 0a20 2020  achine_type,.   
+00018200: 2020 2027 7472 616e 7366 6f72 6d5f 6461     'transform_da
+00018210: 7461 666c 6f77 5f6d 6178 5f6e 756d 5f77  taflow_max_num_w
+00018220: 6f72 6b65 7273 273a 2074 7261 6e73 666f  orkers': transfo
+00018230: 726d 5f64 6174 6166 6c6f 775f 6d61 785f  rm_dataflow_max_
+00018240: 6e75 6d5f 776f 726b 6572 732c 0a20 2020  num_workers,.   
+00018250: 2020 2027 7472 616e 7366 6f72 6d5f 6461     'transform_da
+00018260: 7461 666c 6f77 5f64 6973 6b5f 7369 7a65  taflow_disk_size
+00018270: 5f67 6227 3a20 7472 616e 7366 6f72 6d5f  _gb': transform_
+00018280: 6461 7461 666c 6f77 5f64 6973 6b5f 7369  dataflow_disk_si
+00018290: 7a65 5f67 622c 0a20 2020 2020 2027 776f  ze_gb,.      'wo
+000182a0: 726b 6572 5f70 6f6f 6c5f 7370 6563 735f  rker_pool_specs_
+000182b0: 6f76 6572 7269 6465 273a 2077 6f72 6b65  override': worke
+000182c0: 725f 706f 6f6c 5f73 7065 6373 5f6f 7665  r_pool_specs_ove
+000182d0: 7272 6964 652c 0a20 2020 2020 2027 7275  rride,.      'ru
+000182e0: 6e5f 6576 616c 7561 7469 6f6e 273a 2072  n_evaluation': r
+000182f0: 756e 5f65 7661 6c75 6174 696f 6e2c 0a20  un_evaluation,. 
+00018300: 2020 2020 2027 6576 616c 7561 7469 6f6e       'evaluation
+00018310: 5f62 6174 6368 5f70 7265 6469 6374 5f6d  _batch_predict_m
+00018320: 6163 6869 6e65 5f74 7970 6527 3a20 280a  achine_type': (.
+00018330: 2020 2020 2020 2020 2020 6576 616c 7561            evalua
+00018340: 7469 6f6e 5f62 6174 6368 5f70 7265 6469  tion_batch_predi
+00018350: 6374 5f6d 6163 6869 6e65 5f74 7970 650a  ct_machine_type.
+00018360: 2020 2020 2020 292c 0a20 2020 2020 2027        ),.      '
+00018370: 6576 616c 7561 7469 6f6e 5f62 6174 6368  evaluation_batch
+00018380: 5f70 7265 6469 6374 5f73 7461 7274 696e  _predict_startin
+00018390: 675f 7265 706c 6963 615f 636f 756e 7427  g_replica_count'
+000183a0: 3a20 280a 2020 2020 2020 2020 2020 6576  : (.          ev
+000183b0: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+000183c0: 7265 6469 6374 5f73 7461 7274 696e 675f  redict_starting_
+000183d0: 7265 706c 6963 615f 636f 756e 740a 2020  replica_count.  
+000183e0: 2020 2020 292c 0a20 2020 2020 2027 6576      ),.      'ev
+000183f0: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+00018400: 7265 6469 6374 5f6d 6178 5f72 6570 6c69  redict_max_repli
+00018410: 6361 5f63 6f75 6e74 273a 2028 0a20 2020  ca_count': (.   
+00018420: 2020 2020 2020 2065 7661 6c75 6174 696f         evaluatio
+00018430: 6e5f 6261 7463 685f 7072 6564 6963 745f  n_batch_predict_
+00018440: 6d61 785f 7265 706c 6963 615f 636f 756e  max_replica_coun
+00018450: 740a 2020 2020 2020 292c 0a20 2020 2020  t.      ),.     
+00018460: 2027 6576 616c 7561 7469 6f6e 5f64 6174   'evaluation_dat
+00018470: 6166 6c6f 775f 6d61 6368 696e 655f 7479  aflow_machine_ty
+00018480: 7065 273a 2065 7661 6c75 6174 696f 6e5f  pe': evaluation_
+00018490: 6461 7461 666c 6f77 5f6d 6163 6869 6e65  dataflow_machine
+000184a0: 5f74 7970 652c 0a20 2020 2020 2027 6576  _type,.      'ev
+000184b0: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
+000184c0: 775f 7374 6172 7469 6e67 5f6e 756d 5f77  w_starting_num_w
+000184d0: 6f72 6b65 7273 273a 2028 0a20 2020 2020  orkers': (.     
+000184e0: 2020 2020 2065 7661 6c75 6174 696f 6e5f       evaluation_
+000184f0: 6461 7461 666c 6f77 5f73 7461 7274 696e  dataflow_startin
+00018500: 675f 6e75 6d5f 776f 726b 6572 730a 2020  g_num_workers.  
+00018510: 2020 2020 292c 0a20 2020 2020 2027 6576      ),.      'ev
+00018520: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
+00018530: 775f 6d61 785f 6e75 6d5f 776f 726b 6572  w_max_num_worker
+00018540: 7327 3a20 280a 2020 2020 2020 2020 2020  s': (.          
+00018550: 6576 616c 7561 7469 6f6e 5f64 6174 6166  evaluation_dataf
+00018560: 6c6f 775f 6d61 785f 6e75 6d5f 776f 726b  low_max_num_work
+00018570: 6572 730a 2020 2020 2020 292c 0a20 2020  ers.      ),.   
+00018580: 2020 2027 6576 616c 7561 7469 6f6e 5f64     'evaluation_d
+00018590: 6174 6166 6c6f 775f 6469 736b 5f73 697a  ataflow_disk_siz
+000185a0: 655f 6762 273a 2065 7661 6c75 6174 696f  e_gb': evaluatio
+000185b0: 6e5f 6461 7461 666c 6f77 5f64 6973 6b5f  n_dataflow_disk_
+000185c0: 7369 7a65 5f67 622c 0a20 2020 2020 2027  size_gb,.      '
+000185d0: 6461 7461 666c 6f77 5f73 6572 7669 6365  dataflow_service
+000185e0: 5f61 6363 6f75 6e74 273a 2064 6174 6166  _account': dataf
+000185f0: 6c6f 775f 7365 7276 6963 655f 6163 636f  low_service_acco
+00018600: 756e 742c 0a20 2020 2020 2027 6461 7461  unt,.      'data
+00018610: 666c 6f77 5f73 7562 6e65 7477 6f72 6b27  flow_subnetwork'
+00018620: 3a20 6461 7461 666c 6f77 5f73 7562 6e65  : dataflow_subne
+00018630: 7477 6f72 6b2c 0a20 2020 2020 2027 6461  twork,.      'da
+00018640: 7461 666c 6f77 5f75 7365 5f70 7562 6c69  taflow_use_publi
+00018650: 635f 6970 7327 3a20 6461 7461 666c 6f77  c_ips': dataflow
+00018660: 5f75 7365 5f70 7562 6c69 635f 6970 732c  _use_public_ips,
+00018670: 0a20 2020 2020 2027 656e 6372 7970 7469  .      'encrypti
+00018680: 6f6e 5f73 7065 635f 6b65 795f 6e61 6d65  on_spec_key_name
+00018690: 273a 2065 6e63 7279 7074 696f 6e5f 7370  ': encryption_sp
+000186a0: 6563 5f6b 6579 5f6e 616d 652c 0a20 207d  ec_key_name,.  }
+000186b0: 0a0a 2020 6674 655f 7061 7261 6d73 203d  ..  fte_params =
+000186c0: 207b 0a20 2020 2020 2027 6461 7461 7365   {.      'datase
+000186d0: 745f 6c65 7665 6c5f 6375 7374 6f6d 5f74  t_level_custom_t
+000186e0: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
+000186f0: 6669 6e69 7469 6f6e 7327 3a20 280a 2020  finitions': (.  
+00018700: 2020 2020 2020 2020 6461 7461 7365 745f          dataset_
+00018710: 6c65 7665 6c5f 6375 7374 6f6d 5f74 7261  level_custom_tra
+00018720: 6e73 666f 726d 6174 696f 6e5f 6465 6669  nsformation_defi
+00018730: 6e69 7469 6f6e 730a 2020 2020 2020 2020  nitions.        
+00018740: 2020 6966 2064 6174 6173 6574 5f6c 6576    if dataset_lev
+00018750: 656c 5f63 7573 746f 6d5f 7472 616e 7366  el_custom_transf
+00018760: 6f72 6d61 7469 6f6e 5f64 6566 696e 6974  ormation_definit
+00018770: 696f 6e73 0a20 2020 2020 2020 2020 2065  ions.          e
+00018780: 6c73 6520 5b5d 0a20 2020 2020 2029 2c0a  lse [].      ),.
+00018790: 2020 2020 2020 2764 6174 6173 6574 5f6c        'dataset_l
+000187a0: 6576 656c 5f74 7261 6e73 666f 726d 6174  evel_transformat
+000187b0: 696f 6e73 273a 2028 0a20 2020 2020 2020  ions': (.       
+000187c0: 2020 2064 6174 6173 6574 5f6c 6576 656c     dataset_level
+000187d0: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
+000187e0: 2069 6620 6461 7461 7365 745f 6c65 7665   if dataset_leve
+000187f0: 6c5f 7472 616e 7366 6f72 6d61 7469 6f6e  l_transformation
+00018800: 7320 656c 7365 205b 5d0a 2020 2020 2020  s else [].      
+00018810: 292c 0a20 2020 2020 2027 7275 6e5f 6665  ),.      'run_fe
+00018820: 6174 7572 655f 7365 6c65 6374 696f 6e27  ature_selection'
+00018830: 3a20 7275 6e5f 6665 6174 7572 655f 7365  : run_feature_se
+00018840: 6c65 6374 696f 6e2c 0a20 2020 2020 2027  lection,.      '
+00018850: 6665 6174 7572 655f 7365 6c65 6374 696f  feature_selectio
+00018860: 6e5f 616c 676f 7269 7468 6d27 3a20 6665  n_algorithm': fe
+00018870: 6174 7572 655f 7365 6c65 6374 696f 6e5f  ature_selection_
+00018880: 616c 676f 7269 7468 6d2c 0a20 2020 2020  algorithm,.     
+00018890: 2027 6d61 785f 7365 6c65 6374 6564 5f66   'max_selected_f
+000188a0: 6561 7475 7265 7327 3a20 6d61 785f 7365  eatures': max_se
+000188b0: 6c65 6374 6564 5f66 6561 7475 7265 732c  lected_features,
+000188c0: 0a20 2020 2020 2027 7072 6564 6566 696e  .      'predefin
+000188d0: 6564 5f73 706c 6974 5f6b 6579 273a 2070  ed_split_key': p
+000188e0: 7265 6465 6669 6e65 645f 7370 6c69 745f  redefined_split_
+000188f0: 6b65 792c 0a20 2020 2020 2027 7374 7261  key,.      'stra
+00018900: 7469 6669 6564 5f73 706c 6974 5f6b 6579  tified_split_key
+00018910: 273a 2073 7472 6174 6966 6965 645f 7370  ': stratified_sp
+00018920: 6c69 745f 6b65 792c 0a20 2020 2020 2027  lit_key,.      '
+00018930: 7472 6169 6e69 6e67 5f66 7261 6374 696f  training_fractio
+00018940: 6e27 3a20 7472 6169 6e69 6e67 5f66 7261  n': training_fra
+00018950: 6374 696f 6e2c 0a20 2020 2020 2027 7661  ction,.      'va
+00018960: 6c69 6461 7469 6f6e 5f66 7261 6374 696f  lidation_fractio
+00018970: 6e27 3a20 7661 6c69 6461 7469 6f6e 5f66  n': validation_f
+00018980: 7261 6374 696f 6e2c 0a20 2020 2020 2027  raction,.      '
+00018990: 7465 7374 5f66 7261 6374 696f 6e27 3a20  test_fraction': 
+000189a0: 7465 7374 5f66 7261 6374 696f 6e2c 0a20  test_fraction,. 
+000189b0: 2020 2020 2027 7466 5f61 7574 6f5f 7472       'tf_auto_tr
+000189c0: 616e 7366 6f72 6d5f 6665 6174 7572 6573  ansform_features
+000189d0: 273a 2028 0a20 2020 2020 2020 2020 2074  ': (.          t
+000189e0: 665f 6175 746f 5f74 7261 6e73 666f 726d  f_auto_transform
+000189f0: 5f66 6561 7475 7265 7320 6966 2074 665f  _features if tf_
+00018a00: 6175 746f 5f74 7261 6e73 666f 726d 5f66  auto_transform_f
+00018a10: 6561 7475 7265 7320 656c 7365 207b 7d0a  eatures else {}.
+00018a20: 2020 2020 2020 292c 0a20 2020 2020 2027        ),.      '
+00018a30: 7466 5f63 7573 746f 6d5f 7472 616e 7366  tf_custom_transf
+00018a40: 6f72 6d61 7469 6f6e 5f64 6566 696e 6974  ormation_definit
+00018a50: 696f 6e73 273a 2028 0a20 2020 2020 2020  ions': (.       
+00018a60: 2020 2074 665f 6375 7374 6f6d 5f74 7261     tf_custom_tra
+00018a70: 6e73 666f 726d 6174 696f 6e5f 6465 6669  nsformation_defi
+00018a80: 6e69 7469 6f6e 730a 2020 2020 2020 2020  nitions.        
+00018a90: 2020 6966 2074 665f 6375 7374 6f6d 5f74    if tf_custom_t
+00018aa0: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
+00018ab0: 6669 6e69 7469 6f6e 730a 2020 2020 2020  finitions.      
+00018ac0: 2020 2020 656c 7365 205b 5d0a 2020 2020      else [].    
+00018ad0: 2020 292c 0a20 2020 2020 2027 7466 5f74    ),.      'tf_t
+00018ae0: 7261 6e73 666f 726d 6174 696f 6e73 5f70  ransformations_p
+00018af0: 6174 6827 3a20 7466 5f74 7261 6e73 666f  ath': tf_transfo
+00018b00: 726d 6174 696f 6e73 5f70 6174 682c 0a20  rmations_path,. 
+00018b10: 2020 2020 2027 6d61 7465 7269 616c 697a       'materializ
+00018b20: 6564 5f65 7861 6d70 6c65 735f 666f 726d  ed_examples_form
+00018b30: 6174 273a 2028 0a20 2020 2020 2020 2020  at': (.         
+00018b40: 206d 6174 6572 6961 6c69 7a65 645f 6578   materialized_ex
+00018b50: 616d 706c 6573 5f66 6f72 6d61 740a 2020  amples_format.  
+00018b60: 2020 2020 2020 2020 6966 206d 6174 6572          if mater
+00018b70: 6961 6c69 7a65 645f 6578 616d 706c 6573  ialized_examples
+00018b80: 5f66 6f72 6d61 740a 2020 2020 2020 2020  _format.        
+00018b90: 2020 656c 7365 2027 7466 7265 636f 7264    else 'tfrecord
+00018ba0: 735f 677a 6970 270a 2020 2020 2020 292c  s_gzip'.      ),
+00018bb0: 0a20 2020 2020 2027 7466 5f74 7261 6e73  .      'tf_trans
+00018bc0: 666f 726d 5f65 7865 6375 7469 6f6e 5f65  form_execution_e
+00018bd0: 6e67 696e 6527 3a20 280a 2020 2020 2020  ngine': (.      
+00018be0: 2020 2020 7466 5f74 7261 6e73 666f 726d      tf_transform
+00018bf0: 5f65 7865 6375 7469 6f6e 5f65 6e67 696e  _execution_engin
+00018c00: 650a 2020 2020 2020 2020 2020 6966 2074  e.          if t
+00018c10: 665f 7472 616e 7366 6f72 6d5f 6578 6563  f_transform_exec
+00018c20: 7574 696f 6e5f 656e 6769 6e65 0a20 2020  ution_engine.   
+00018c30: 2020 2020 2020 2065 6c73 6520 2764 6174         else 'dat
+00018c40: 6166 6c6f 7727 0a20 2020 2020 2029 2c0a  aflow'.      ),.
+00018c50: 2020 7d0a 2020 5f75 7064 6174 655f 7061    }.  _update_pa
+00018c60: 7261 6d65 7465 7273 2870 6172 616d 6574  rameters(paramet
+00018c70: 6572 5f76 616c 7565 732c 2066 7465 5f70  er_values, fte_p
+00018c80: 6172 616d 7329 0a0a 2020 6461 7461 5f73  arams)..  data_s
+00018c90: 6f75 7263 655f 616e 645f 7370 6c69 745f  ource_and_split_
+00018ca0: 7061 7261 6d65 7465 7273 203d 207b 0a20  parameters = {. 
+00018cb0: 2020 2020 2027 6461 7461 5f73 6f75 7263       'data_sourc
+00018cc0: 655f 6373 765f 6669 6c65 6e61 6d65 7327  e_csv_filenames'
+00018cd0: 3a20 6461 7461 5f73 6f75 7263 655f 6373  : data_source_cs
+00018ce0: 765f 6669 6c65 6e61 6d65 732c 0a20 2020  v_filenames,.   
+00018cf0: 2020 2027 6461 7461 5f73 6f75 7263 655f     'data_source_
+00018d00: 6269 6771 7565 7279 5f74 6162 6c65 5f70  bigquery_table_p
+00018d10: 6174 6827 3a20 6461 7461 5f73 6f75 7263  ath': data_sourc
+00018d20: 655f 6269 6771 7565 7279 5f74 6162 6c65  e_bigquery_table
+00018d30: 5f70 6174 682c 0a20 2020 2020 2027 6269  _path,.      'bi
+00018d40: 6771 7565 7279 5f73 7461 6769 6e67 5f66  gquery_staging_f
+00018d50: 756c 6c5f 6461 7461 7365 745f 6964 273a  ull_dataset_id':
+00018d60: 2062 6967 7175 6572 795f 7374 6167 696e   bigquery_stagin
+00018d70: 675f 6675 6c6c 5f64 6174 6173 6574 5f69  g_full_dataset_i
+00018d80: 642c 0a20 207d 0a20 205f 7570 6461 7465  d,.  }.  _update
+00018d90: 5f70 6172 616d 6574 6572 7328 7061 7261  _parameters(para
+00018da0: 6d65 7465 725f 7661 6c75 6573 2c20 6461  meter_values, da
+00018db0: 7461 5f73 6f75 7263 655f 616e 645f 7370  ta_source_and_sp
+00018dc0: 6c69 745f 7061 7261 6d65 7465 7273 290a  lit_parameters).
+00018dd0: 0a20 2070 6970 656c 696e 655f 6465 6669  .  pipeline_defi
+00018de0: 6e69 7469 6f6e 5f70 6174 6820 3d20 6f73  nition_path = os
+00018df0: 2e70 6174 682e 6a6f 696e 280a 2020 2020  .path.join(.    
+00018e00: 2020 7061 7468 6c69 622e 5061 7468 285f    pathlib.Path(_
+00018e10: 5f66 696c 655f 5f29 2e70 6172 656e 742e  _file__).parent.
+00018e20: 7265 736f 6c76 6528 292c 0a20 2020 2020  resolve(),.     
+00018e30: 2027 7461 626e 6574 5f68 7970 6572 7061   'tabnet_hyperpa
+00018e40: 7261 6d65 7465 725f 7475 6e69 6e67 5f6a  rameter_tuning_j
+00018e50: 6f62 5f70 6970 656c 696e 652e 7961 6d6c  ob_pipeline.yaml
+00018e60: 272c 0a20 2029 0a0a 2020 7265 7475 726e  ',.  )..  return
+00018e70: 2070 6970 656c 696e 655f 6465 6669 6e69   pipeline_defini
+00018e80: 7469 6f6e 5f70 6174 682c 2070 6172 616d  tion_path, param
+00018e90: 6574 6572 5f76 616c 7565 730a 0a0a 6465  eter_values...de
+00018ea0: 6620 6765 745f 7769 6465 5f61 6e64 5f64  f get_wide_and_d
+00018eb0: 6565 705f 6879 7065 7270 6172 616d 6574  eep_hyperparamet
+00018ec0: 6572 5f74 756e 696e 675f 6a6f 625f 7069  er_tuning_job_pi
+00018ed0: 7065 6c69 6e65 5f61 6e64 5f70 6172 616d  peline_and_param
+00018ee0: 6574 6572 7328 0a20 2020 2070 726f 6a65  eters(.    proje
+00018ef0: 6374 3a20 7374 722c 0a20 2020 206c 6f63  ct: str,.    loc
+00018f00: 6174 696f 6e3a 2073 7472 2c0a 2020 2020  ation: str,.    
+00018f10: 726f 6f74 5f64 6972 3a20 7374 722c 0a20  root_dir: str,. 
+00018f20: 2020 2074 6172 6765 745f 636f 6c75 6d6e     target_column
+00018f30: 3a20 7374 722c 0a20 2020 2070 7265 6469  : str,.    predi
+00018f40: 6374 696f 6e5f 7479 7065 3a20 7374 722c  ction_type: str,
+00018f50: 0a20 2020 2073 7475 6479 5f73 7065 635f  .    study_spec_
+00018f60: 6d65 7472 6963 5f69 643a 2073 7472 2c0a  metric_id: str,.
+00018f70: 2020 2020 7374 7564 795f 7370 6563 5f6d      study_spec_m
+00018f80: 6574 7269 635f 676f 616c 3a20 7374 722c  etric_goal: str,
+00018f90: 0a20 2020 2073 7475 6479 5f73 7065 635f  .    study_spec_
+00018fa0: 7061 7261 6d65 7465 7273 5f6f 7665 7272  parameters_overr
+00018fb0: 6964 653a 204c 6973 745b 4469 6374 5b73  ide: List[Dict[s
+00018fc0: 7472 2c20 416e 795d 5d2c 0a20 2020 206d  tr, Any]],.    m
+00018fd0: 6178 5f74 7269 616c 5f63 6f75 6e74 3a20  ax_trial_count: 
+00018fe0: 696e 742c 0a20 2020 2070 6172 616c 6c65  int,.    paralle
+00018ff0: 6c5f 7472 6961 6c5f 636f 756e 743a 2069  l_trial_count: i
+00019000: 6e74 2c0a 2020 2020 7472 616e 7366 6f72  nt,.    transfor
+00019010: 6d5f 636f 6e66 6967 3a20 4f70 7469 6f6e  m_config: Option
+00019020: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
+00019030: 2020 2020 6461 7461 7365 745f 6c65 7665      dataset_leve
+00019040: 6c5f 6375 7374 6f6d 5f74 7261 6e73 666f  l_custom_transfo
+00019050: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
+00019060: 6f6e 733a 204f 7074 696f 6e61 6c5b 0a20  ons: Optional[. 
+00019070: 2020 2020 2020 204c 6973 745b 4469 6374         List[Dict
+00019080: 5b73 7472 2c20 416e 795d 5d0a 2020 2020  [str, Any]].    
+00019090: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6461  ] = None,.    da
+000190a0: 7461 7365 745f 6c65 7665 6c5f 7472 616e  taset_level_tran
+000190b0: 7366 6f72 6d61 7469 6f6e 733a 204f 7074  sformations: Opt
+000190c0: 696f 6e61 6c5b 4c69 7374 5b44 6963 745b  ional[List[Dict[
+000190d0: 7374 722c 2041 6e79 5d5d 5d20 3d20 4e6f  str, Any]]] = No
+000190e0: 6e65 2c0a 2020 2020 7275 6e5f 6665 6174  ne,.    run_feat
+000190f0: 7572 655f 7365 6c65 6374 696f 6e3a 2062  ure_selection: b
+00019100: 6f6f 6c20 3d20 4661 6c73 652c 0a20 2020  ool = False,.   
+00019110: 2066 6561 7475 7265 5f73 656c 6563 7469   feature_selecti
+00019120: 6f6e 5f61 6c67 6f72 6974 686d 3a20 4f70  on_algorithm: Op
+00019130: 7469 6f6e 616c 5b73 7472 5d20 3d20 4e6f  tional[str] = No
+00019140: 6e65 2c0a 2020 2020 6d61 7465 7269 616c  ne,.    material
+00019150: 697a 6564 5f65 7861 6d70 6c65 735f 666f  ized_examples_fo
+00019160: 726d 6174 3a20 4f70 7469 6f6e 616c 5b73  rmat: Optional[s
+00019170: 7472 5d20 3d20 4e6f 6e65 2c0a 2020 2020  tr] = None,.    
+00019180: 6d61 785f 7365 6c65 6374 6564 5f66 6561  max_selected_fea
+00019190: 7475 7265 733a 204f 7074 696f 6e61 6c5b  tures: Optional[
+000191a0: 696e 745d 203d 204e 6f6e 652c 0a20 2020  int] = None,.   
+000191b0: 2070 7265 6465 6669 6e65 645f 7370 6c69   predefined_spli
+000191c0: 745f 6b65 793a 204f 7074 696f 6e61 6c5b  t_key: Optional[
+000191d0: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
+000191e0: 2073 7472 6174 6966 6965 645f 7370 6c69   stratified_spli
+000191f0: 745f 6b65 793a 204f 7074 696f 6e61 6c5b  t_key: Optional[
+00019200: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
+00019210: 2074 7261 696e 696e 675f 6672 6163 7469   training_fracti
+00019220: 6f6e 3a20 4f70 7469 6f6e 616c 5b66 6c6f  on: Optional[flo
+00019230: 6174 5d20 3d20 4e6f 6e65 2c0a 2020 2020  at] = None,.    
+00019240: 7661 6c69 6461 7469 6f6e 5f66 7261 6374  validation_fract
+00019250: 696f 6e3a 204f 7074 696f 6e61 6c5b 666c  ion: Optional[fl
+00019260: 6f61 745d 203d 204e 6f6e 652c 0a20 2020  oat] = None,.   
+00019270: 2074 6573 745f 6672 6163 7469 6f6e 3a20   test_fraction: 
+00019280: 4f70 7469 6f6e 616c 5b66 6c6f 6174 5d20  Optional[float] 
+00019290: 3d20 4e6f 6e65 2c0a 2020 2020 7466 5f74  = None,.    tf_t
+000192a0: 7261 6e73 666f 726d 5f65 7865 6375 7469  ransform_executi
+000192b0: 6f6e 5f65 6e67 696e 653a 204f 7074 696f  on_engine: Optio
+000192c0: 6e61 6c5b 7374 725d 203d 204e 6f6e 652c  nal[str] = None,
+000192d0: 0a20 2020 2074 665f 6175 746f 5f74 7261  .    tf_auto_tra
+000192e0: 6e73 666f 726d 5f66 6561 7475 7265 733a  nsform_features:
+000192f0: 204f 7074 696f 6e61 6c5b 0a20 2020 2020   Optional[.     
+00019300: 2020 2055 6e69 6f6e 5b4c 6973 745b 7374     Union[List[st
+00019310: 725d 2c20 4469 6374 5b73 7472 2c20 4c69  r], Dict[str, Li
+00019320: 7374 5b73 7472 5d5d 5d0a 2020 2020 5d20  st[str]]].    ] 
+00019330: 3d20 4e6f 6e65 2c0a 2020 2020 7466 5f63  = None,.    tf_c
+00019340: 7573 746f 6d5f 7472 616e 7366 6f72 6d61  ustom_transforma
+00019350: 7469 6f6e 5f64 6566 696e 6974 696f 6e73  tion_definitions
+00019360: 3a20 4f70 7469 6f6e 616c 5b4c 6973 745b  : Optional[List[
+00019370: 4469 6374 5b73 7472 2c20 416e 795d 5d5d  Dict[str, Any]]]
+00019380: 203d 204e 6f6e 652c 0a20 2020 2074 665f   = None,.    tf_
+00019390: 7472 616e 7366 6f72 6d61 7469 6f6e 735f  transformations_
+000193a0: 7061 7468 3a20 4f70 7469 6f6e 616c 5b73  path: Optional[s
+000193b0: 7472 5d20 3d20 4e6f 6e65 2c0a 2020 2020  tr] = None,.    
+000193c0: 656e 6162 6c65 5f70 726f 6669 6c65 723a  enable_profiler:
+000193d0: 2062 6f6f 6c20 3d20 4661 6c73 652c 0a20   bool = False,. 
+000193e0: 2020 2063 6163 6865 5f64 6174 613a 2073     cache_data: s
+000193f0: 7472 203d 2027 6175 746f 272c 0a20 2020  tr = 'auto',.   
+00019400: 2073 6565 643a 2069 6e74 203d 2031 2c0a   seed: int = 1,.
+00019410: 2020 2020 6576 616c 5f73 7465 7073 3a20      eval_steps: 
+00019420: 696e 7420 3d20 302c 0a20 2020 2065 7661  int = 0,.    eva
+00019430: 6c5f 6672 6571 7565 6e63 795f 7365 6373  l_frequency_secs
+00019440: 3a20 696e 7420 3d20 3630 302c 0a20 2020  : int = 600,.   
+00019450: 2064 6174 615f 736f 7572 6365 5f63 7376   data_source_csv
+00019460: 5f66 696c 656e 616d 6573 3a20 4f70 7469  _filenames: Opti
+00019470: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
+00019480: 2c0a 2020 2020 6461 7461 5f73 6f75 7263  ,.    data_sourc
+00019490: 655f 6269 6771 7565 7279 5f74 6162 6c65  e_bigquery_table
+000194a0: 5f70 6174 683a 204f 7074 696f 6e61 6c5b  _path: Optional[
+000194b0: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
+000194c0: 2062 6967 7175 6572 795f 7374 6167 696e   bigquery_stagin
+000194d0: 675f 6675 6c6c 5f64 6174 6173 6574 5f69  g_full_dataset_i
+000194e0: 643a 204f 7074 696f 6e61 6c5b 7374 725d  d: Optional[str]
+000194f0: 203d 204e 6f6e 652c 0a20 2020 2077 6569   = None,.    wei
+00019500: 6768 745f 636f 6c75 6d6e 3a20 7374 7220  ght_column: str 
+00019510: 3d20 2727 2c0a 2020 2020 6d61 785f 6661  = '',.    max_fa
+00019520: 696c 6564 5f74 7269 616c 5f63 6f75 6e74  iled_trial_count
+00019530: 3a20 696e 7420 3d20 302c 0a20 2020 2073  : int = 0,.    s
+00019540: 7475 6479 5f73 7065 635f 616c 676f 7269  tudy_spec_algori
+00019550: 7468 6d3a 2073 7472 203d 2027 414c 474f  thm: str = 'ALGO
+00019560: 5249 5448 4d5f 554e 5350 4543 4946 4945  RITHM_UNSPECIFIE
+00019570: 4427 2c0a 2020 2020 7374 7564 795f 7370  D',.    study_sp
+00019580: 6563 5f6d 6561 7375 7265 6d65 6e74 5f73  ec_measurement_s
+00019590: 656c 6563 7469 6f6e 5f74 7970 653a 2073  election_type: s
+000195a0: 7472 203d 2027 4245 5354 5f4d 4541 5355  tr = 'BEST_MEASU
+000195b0: 5245 4d45 4e54 272c 0a20 2020 2074 7261  REMENT',.    tra
+000195c0: 6e73 666f 726d 5f64 6174 6166 6c6f 775f  nsform_dataflow_
+000195d0: 6d61 6368 696e 655f 7479 7065 3a20 7374  machine_type: st
+000195e0: 7220 3d20 276e 312d 7374 616e 6461 7264  r = 'n1-standard
+000195f0: 2d31 3627 2c0a 2020 2020 7472 616e 7366  -16',.    transf
+00019600: 6f72 6d5f 6461 7461 666c 6f77 5f6d 6178  orm_dataflow_max
+00019610: 5f6e 756d 5f77 6f72 6b65 7273 3a20 696e  _num_workers: in
+00019620: 7420 3d20 3235 2c0a 2020 2020 7472 616e  t = 25,.    tran
+00019630: 7366 6f72 6d5f 6461 7461 666c 6f77 5f64  sform_dataflow_d
+00019640: 6973 6b5f 7369 7a65 5f67 623a 2069 6e74  isk_size_gb: int
+00019650: 203d 2034 302c 0a20 2020 2077 6f72 6b65   = 40,.    worke
+00019660: 725f 706f 6f6c 5f73 7065 6373 5f6f 7665  r_pool_specs_ove
+00019670: 7272 6964 653a 204f 7074 696f 6e61 6c5b  rride: Optional[
+00019680: 4469 6374 5b73 7472 2c20 416e 795d 5d20  Dict[str, Any]] 
+00019690: 3d20 4e6f 6e65 2c0a 2020 2020 7275 6e5f  = None,.    run_
+000196a0: 6576 616c 7561 7469 6f6e 3a20 626f 6f6c  evaluation: bool
+000196b0: 203d 2054 7275 652c 0a20 2020 2065 7661   = True,.    eva
+000196c0: 6c75 6174 696f 6e5f 6261 7463 685f 7072  luation_batch_pr
+000196d0: 6564 6963 745f 6d61 6368 696e 655f 7479  edict_machine_ty
+000196e0: 7065 3a20 7374 7220 3d20 5f45 5641 4c55  pe: str = _EVALU
+000196f0: 4154 494f 4e5f 4241 5443 485f 5052 4544  ATION_BATCH_PRED
+00019700: 4943 545f 4d41 4348 494e 455f 5459 5045  ICT_MACHINE_TYPE
+00019710: 2c0a 2020 2020 6576 616c 7561 7469 6f6e  ,.    evaluation
+00019720: 5f62 6174 6368 5f70 7265 6469 6374 5f73  _batch_predict_s
+00019730: 7461 7274 696e 675f 7265 706c 6963 615f  tarting_replica_
+00019740: 636f 756e 743a 2069 6e74 203d 205f 4556  count: int = _EV
+00019750: 414c 5541 5449 4f4e 5f42 4154 4348 5f50  ALUATION_BATCH_P
+00019760: 5245 4449 4354 5f53 5441 5254 494e 475f  REDICT_STARTING_
+00019770: 5245 504c 4943 415f 434f 554e 542c 0a20  REPLICA_COUNT,. 
+00019780: 2020 2065 7661 6c75 6174 696f 6e5f 6261     evaluation_ba
+00019790: 7463 685f 7072 6564 6963 745f 6d61 785f  tch_predict_max_
+000197a0: 7265 706c 6963 615f 636f 756e 743a 2069  replica_count: i
+000197b0: 6e74 203d 205f 4556 414c 5541 5449 4f4e  nt = _EVALUATION
+000197c0: 5f42 4154 4348 5f50 5245 4449 4354 5f4d  _BATCH_PREDICT_M
+000197d0: 4158 5f52 4550 4c49 4341 5f43 4f55 4e54  AX_REPLICA_COUNT
+000197e0: 2c0a 2020 2020 6576 616c 7561 7469 6f6e  ,.    evaluation
+000197f0: 5f64 6174 6166 6c6f 775f 6d61 6368 696e  _dataflow_machin
+00019800: 655f 7479 7065 3a20 7374 7220 3d20 5f45  e_type: str = _E
+00019810: 5641 4c55 4154 494f 4e5f 4441 5441 464c  VALUATION_DATAFL
+00019820: 4f57 5f4d 4143 4849 4e45 5f54 5950 452c  OW_MACHINE_TYPE,
+00019830: 0a20 2020 2065 7661 6c75 6174 696f 6e5f  .    evaluation_
+00019840: 6461 7461 666c 6f77 5f73 7461 7274 696e  dataflow_startin
+00019850: 675f 6e75 6d5f 776f 726b 6572 733a 2069  g_num_workers: i
+00019860: 6e74 203d 205f 4556 414c 5541 5449 4f4e  nt = _EVALUATION
+00019870: 5f44 4154 4146 4c4f 575f 5354 4152 5449  _DATAFLOW_STARTI
+00019880: 4e47 5f4e 554d 5f57 4f52 4b45 5253 2c0a  NG_NUM_WORKERS,.
+00019890: 2020 2020 6576 616c 7561 7469 6f6e 5f64      evaluation_d
+000198a0: 6174 6166 6c6f 775f 6d61 785f 6e75 6d5f  ataflow_max_num_
+000198b0: 776f 726b 6572 733a 2069 6e74 203d 205f  workers: int = _
+000198c0: 4556 414c 5541 5449 4f4e 5f44 4154 4146  EVALUATION_DATAF
+000198d0: 4c4f 575f 4d41 585f 4e55 4d5f 574f 524b  LOW_MAX_NUM_WORK
+000198e0: 4552 532c 0a20 2020 2065 7661 6c75 6174  ERS,.    evaluat
+000198f0: 696f 6e5f 6461 7461 666c 6f77 5f64 6973  ion_dataflow_dis
+00019900: 6b5f 7369 7a65 5f67 623a 2069 6e74 203d  k_size_gb: int =
+00019910: 205f 4556 414c 5541 5449 4f4e 5f44 4154   _EVALUATION_DAT
+00019920: 4146 4c4f 575f 4449 534b 5f53 495a 455f  AFLOW_DISK_SIZE_
+00019930: 4742 2c0a 2020 2020 6461 7461 666c 6f77  GB,.    dataflow
+00019940: 5f73 6572 7669 6365 5f61 6363 6f75 6e74  _service_account
+00019950: 3a20 7374 7220 3d20 2727 2c0a 2020 2020  : str = '',.    
+00019960: 6461 7461 666c 6f77 5f73 7562 6e65 7477  dataflow_subnetw
+00019970: 6f72 6b3a 2073 7472 203d 2027 272c 0a20  ork: str = '',. 
+00019980: 2020 2064 6174 6166 6c6f 775f 7573 655f     dataflow_use_
+00019990: 7075 626c 6963 5f69 7073 3a20 626f 6f6c  public_ips: bool
+000199a0: 203d 2054 7275 652c 0a20 2020 2065 6e63   = True,.    enc
+000199b0: 7279 7074 696f 6e5f 7370 6563 5f6b 6579  ryption_spec_key
+000199c0: 5f6e 616d 653a 2073 7472 203d 2027 272c  _name: str = '',
+000199d0: 0a29 202d 3e20 5475 706c 655b 7374 722c  .) -> Tuple[str,
+000199e0: 2044 6963 745b 7374 722c 2041 6e79 5d5d   Dict[str, Any]]
+000199f0: 3a0a 2020 2320 666d 743a 206f 6666 0a20  :.  # fmt: off. 
+00019a00: 2022 2222 4765 7420 7468 6520 5769 6465   """Get the Wide
+00019a10: 2026 2044 6565 7020 616c 676f 7269 7468   & Deep algorith
+00019a20: 6d20 4879 7065 7270 6172 616d 6574 6572  m Hyperparameter
+00019a30: 5475 6e69 6e67 4a6f 6220 7069 7065 6c69  TuningJob pipeli
+00019a40: 6e65 2e0a 0a20 2041 7267 733a 0a20 2020  ne...  Args:.   
+00019a50: 2070 726f 6a65 6374 3a20 5468 6520 4743   project: The GC
+00019a60: 5020 7072 6f6a 6563 7420 7468 6174 2072  P project that r
+00019a70: 756e 7320 7468 6520 7069 7065 6c69 6e65  uns the pipeline
+00019a80: 2063 6f6d 706f 6e65 6e74 732e 0a20 2020   components..   
+00019a90: 206c 6f63 6174 696f 6e3a 2054 6865 2047   location: The G
+00019aa0: 4350 2072 6567 696f 6e20 7468 6174 2072  CP region that r
+00019ab0: 756e 7320 7468 6520 7069 7065 6c69 6e65  uns the pipeline
+00019ac0: 2063 6f6d 706f 6e65 6e74 732e 0a20 2020   components..   
+00019ad0: 2072 6f6f 745f 6469 723a 2054 6865 2072   root_dir: The r
+00019ae0: 6f6f 7420 4743 5320 6469 7265 6374 6f72  oot GCS director
+00019af0: 7920 666f 7220 7468 6520 7069 7065 6c69  y for the pipeli
+00019b00: 6e65 2063 6f6d 706f 6e65 6e74 732e 0a20  ne components.. 
+00019b10: 2020 2074 6172 6765 745f 636f 6c75 6d6e     target_column
+00019b20: 3a20 5468 6520 7461 7267 6574 2063 6f6c  : The target col
+00019b30: 756d 6e20 6e61 6d65 2e0a 2020 2020 7072  umn name..    pr
+00019b40: 6564 6963 7469 6f6e 5f74 7970 653a 2054  ediction_type: T
+00019b50: 6865 2074 7970 6520 6f66 2070 7265 6469  he type of predi
+00019b60: 6374 696f 6e20 7468 6520 6d6f 6465 6c20  ction the model 
+00019b70: 6973 2074 6f20 7072 6f64 7563 652e 2020  is to produce.  
+00019b80: 2263 6c61 7373 6966 6963 6174 696f 6e22  "classification"
+00019b90: 206f 7220 2272 6567 7265 7373 696f 6e22   or "regression"
+00019ba0: 2e0a 2020 2020 7374 7564 795f 7370 6563  ..    study_spec
+00019bb0: 5f6d 6574 7269 635f 6964 3a20 4d65 7472  _metric_id: Metr
+00019bc0: 6963 2074 6f20 6f70 7469 6d69 7a65 2c20  ic to optimize, 
+00019bd0: 706f 7373 6962 6c65 2076 616c 7565 733a  possible values:
+00019be0: 205b 2027 6c6f 7373 272c 2027 6176 6572   [ 'loss', 'aver
+00019bf0: 6167 655f 6c6f 7373 272c 2027 726d 7365  age_loss', 'rmse
+00019c00: 272c 2027 6d61 6527 2c20 276d 716c 272c  ', 'mae', 'mql',
+00019c10: 2027 6163 6375 7261 6379 272c 2027 6175   'accuracy', 'au
+00019c20: 6327 2c20 2770 7265 6369 7369 6f6e 272c  c', 'precision',
+00019c30: 2027 7265 6361 6c6c 275d 2e0a 2020 2020   'recall']..    
+00019c40: 7374 7564 795f 7370 6563 5f6d 6574 7269  study_spec_metri
+00019c50: 635f 676f 616c 3a20 4f70 7469 6d69 7a61  c_goal: Optimiza
+00019c60: 7469 6f6e 2067 6f61 6c20 6f66 2074 6865  tion goal of the
+00019c70: 206d 6574 7269 632c 2070 6f73 7369 626c   metric, possibl
+00019c80: 6520 7661 6c75 6573 3a20 224d 4158 494d  e values: "MAXIM
+00019c90: 495a 4522 2c20 224d 494e 494d 495a 4522  IZE", "MINIMIZE"
+00019ca0: 2e0a 2020 2020 7374 7564 795f 7370 6563  ..    study_spec
+00019cb0: 5f70 6172 616d 6574 6572 735f 6f76 6572  _parameters_over
+00019cc0: 7269 6465 3a20 4c69 7374 206f 6620 6469  ride: List of di
+00019cd0: 6374 696f 6e61 7269 6573 2072 6570 7265  ctionaries repre
+00019ce0: 7365 6e74 696e 6720 7061 7261 6d65 7465  senting paramete
+00019cf0: 7273 2074 6f20 6f70 7469 6d69 7a65 2e20  rs to optimize. 
+00019d00: 5468 6520 6469 6374 696f 6e61 7279 206b  The dictionary k
+00019d10: 6579 2069 7320 7468 6520 7061 7261 6d65  ey is the parame
+00019d20: 7465 725f 6964 2c20 7768 6963 6820 6973  ter_id, which is
+00019d30: 2070 6173 7365 6420 746f 2074 7261 696e   passed to train
+00019d40: 696e 6720 6a6f 6220 6173 2061 2063 6f6d  ing job as a com
+00019d50: 6d61 6e64 206c 696e 6520 6172 6775 6d65  mand line argume
+00019d60: 6e74 2c20 616e 6420 7468 6520 6469 6374  nt, and the dict
+00019d70: 696f 6e61 7279 2076 616c 7565 2069 7320  ionary value is 
+00019d80: 7468 6520 7061 7261 6d65 7465 7220 7370  the parameter sp
+00019d90: 6563 6966 6963 6174 696f 6e20 6f66 2074  ecification of t
+00019da0: 6865 206d 6574 7269 632e 0a20 2020 206d  he metric..    m
+00019db0: 6178 5f74 7269 616c 5f63 6f75 6e74 3a20  ax_trial_count: 
+00019dc0: 5468 6520 6465 7369 7265 6420 746f 7461  The desired tota
+00019dd0: 6c20 6e75 6d62 6572 206f 6620 7472 6961  l number of tria
+00019de0: 6c73 2e0a 2020 2020 7061 7261 6c6c 656c  ls..    parallel
+00019df0: 5f74 7269 616c 5f63 6f75 6e74 3a20 5468  _trial_count: Th
+00019e00: 6520 6465 7369 7265 6420 6e75 6d62 6572  e desired number
+00019e10: 206f 6620 7472 6961 6c73 2074 6f20 7275   of trials to ru
+00019e20: 6e20 696e 2070 6172 616c 6c65 6c2e 0a20  n in parallel.. 
+00019e30: 2020 2074 7261 6e73 666f 726d 5f63 6f6e     transform_con
+00019e40: 6669 673a 2050 6174 6820 746f 2076 3120  fig: Path to v1 
+00019e50: 5446 2074 7261 6e73 666f 726d 6174 696f  TF transformatio
+00019e60: 6e20 636f 6e66 6967 7572 6174 696f 6e2e  n configuration.
+00019e70: 0a20 2020 2064 6174 6173 6574 5f6c 6576  .    dataset_lev
+00019e80: 656c 5f63 7573 746f 6d5f 7472 616e 7366  el_custom_transf
+00019e90: 6f72 6d61 7469 6f6e 5f64 6566 696e 6974  ormation_definit
+00019ea0: 696f 6e73 3a20 4461 7461 7365 742d 6c65  ions: Dataset-le
+00019eb0: 7665 6c20 6375 7374 6f6d 2074 7261 6e73  vel custom trans
+00019ec0: 666f 726d 6174 696f 6e20 6465 6669 6e69  formation defini
+00019ed0: 7469 6f6e 7320 696e 2073 7472 696e 6720  tions in string 
+00019ee0: 666f 726d 6174 2e0a 2020 2020 6461 7461  format..    data
+00019ef0: 7365 745f 6c65 7665 6c5f 7472 616e 7366  set_level_transf
+00019f00: 6f72 6d61 7469 6f6e 733a 2044 6174 6173  ormations: Datas
+00019f10: 6574 2d6c 6576 656c 2074 7261 6e73 666f  et-level transfo
+00019f20: 726d 6174 696f 6e20 636f 6e66 6967 7572  rmation configur
+00019f30: 6174 696f 6e20 696e 2073 7472 696e 6720  ation in string 
+00019f40: 666f 726d 6174 2e0a 2020 2020 7275 6e5f  format..    run_
+00019f50: 6665 6174 7572 655f 7365 6c65 6374 696f  feature_selectio
+00019f60: 6e3a 2057 6865 7468 6572 2074 6f20 656e  n: Whether to en
+00019f70: 6162 6c65 2066 6561 7475 7265 2073 656c  able feature sel
+00019f80: 6563 7469 6f6e 2e0a 2020 2020 6665 6174  ection..    feat
+00019f90: 7572 655f 7365 6c65 6374 696f 6e5f 616c  ure_selection_al
+00019fa0: 676f 7269 7468 6d3a 2046 6561 7475 7265  gorithm: Feature
+00019fb0: 2073 656c 6563 7469 6f6e 2061 6c67 6f72   selection algor
+00019fc0: 6974 686d 2e0a 2020 2020 6d61 7465 7269  ithm..    materi
+00019fd0: 616c 697a 6564 5f65 7861 6d70 6c65 735f  alized_examples_
+00019fe0: 666f 726d 6174 3a20 5468 6520 666f 726d  format: The form
+00019ff0: 6174 2066 6f72 2074 6865 206d 6174 6572  at for the mater
+0001a000: 6961 6c69 7a65 6420 6578 616d 706c 6573  ialized examples
+0001a010: 2e0a 2020 2020 6d61 785f 7365 6c65 6374  ..    max_select
+0001a020: 6564 5f66 6561 7475 7265 733a 204d 6178  ed_features: Max
+0001a030: 696d 756d 206e 756d 6265 7220 6f66 2066  imum number of f
+0001a040: 6561 7475 7265 7320 746f 2073 656c 6563  eatures to selec
+0001a050: 742e 0a20 2020 2070 7265 6465 6669 6e65  t..    predefine
+0001a060: 645f 7370 6c69 745f 6b65 793a 2050 7265  d_split_key: Pre
+0001a070: 6465 6669 6e65 6420 7370 6c69 7420 6b65  defined split ke
+0001a080: 792e 0a20 2020 2073 7472 6174 6966 6965  y..    stratifie
+0001a090: 645f 7370 6c69 745f 6b65 793a 2053 7472  d_split_key: Str
+0001a0a0: 6174 6966 6965 6420 7370 6c69 7420 6b65  atified split ke
+0001a0b0: 792e 0a20 2020 2074 7261 696e 696e 675f  y..    training_
+0001a0c0: 6672 6163 7469 6f6e 3a20 5472 6169 6e69  fraction: Traini
+0001a0d0: 6e67 2066 7261 6374 696f 6e2e 0a20 2020  ng fraction..   
+0001a0e0: 2076 616c 6964 6174 696f 6e5f 6672 6163   validation_frac
+0001a0f0: 7469 6f6e 3a20 5661 6c69 6461 7469 6f6e  tion: Validation
+0001a100: 2066 7261 6374 696f 6e2e 0a20 2020 2074   fraction..    t
+0001a110: 6573 745f 6672 6163 7469 6f6e 3a20 5465  est_fraction: Te
+0001a120: 7374 2066 7261 6374 696f 6e2e 0a20 2020  st fraction..   
+0001a130: 2074 665f 7472 616e 7366 6f72 6d5f 6578   tf_transform_ex
+0001a140: 6563 7574 696f 6e5f 656e 6769 6e65 3a20  ecution_engine: 
+0001a150: 5468 6520 6578 6563 7574 696f 6e20 656e  The execution en
+0001a160: 6769 6e65 2075 7365 6420 746f 2065 7865  gine used to exe
+0001a170: 6375 7465 2054 462d 6261 7365 6420 7472  cute TF-based tr
+0001a180: 616e 7366 6f72 6d61 7469 6f6e 732e 0a20  ansformations.. 
+0001a190: 2020 2074 665f 6175 746f 5f74 7261 6e73     tf_auto_trans
+0001a1a0: 666f 726d 5f66 6561 7475 7265 733a 204c  form_features: L
+0001a1b0: 6973 7420 6f66 2061 7574 6f20 7472 616e  ist of auto tran
+0001a1c0: 7366 6f72 6d20 6665 6174 7572 6573 2069  sform features i
+0001a1d0: 6e20 7468 6520 636f 6d6d 612d 7365 7061  n the comma-sepa
+0001a1e0: 7261 7465 6420 7374 7269 6e67 2066 6f72  rated string for
+0001a1f0: 6d61 742e 0a20 2020 2074 665f 6375 7374  mat..    tf_cust
+0001a200: 6f6d 5f74 7261 6e73 666f 726d 6174 696f  om_transformatio
+0001a210: 6e5f 6465 6669 6e69 7469 6f6e 733a 2054  n_definitions: T
+0001a220: 4620 6375 7374 6f6d 2074 7261 6e73 666f  F custom transfo
+0001a230: 726d 6174 696f 6e20 6465 6669 6e69 7469  rmation definiti
+0001a240: 6f6e 7320 696e 2073 7472 696e 6720 666f  ons in string fo
+0001a250: 726d 6174 2e0a 2020 2020 7466 5f74 7261  rmat..    tf_tra
+0001a260: 6e73 666f 726d 6174 696f 6e73 5f70 6174  nsformations_pat
+0001a270: 683a 2050 6174 6820 746f 2054 4620 7472  h: Path to TF tr
+0001a280: 616e 7366 6f72 6d61 7469 6f6e 2063 6f6e  ansformation con
+0001a290: 6669 6775 7261 7469 6f6e 2e0a 2020 2020  figuration..    
+0001a2a0: 656e 6162 6c65 5f70 726f 6669 6c65 723a  enable_profiler:
+0001a2b0: 2045 6e61 626c 6573 2070 726f 6669 6c69   Enables profili
+0001a2c0: 6e67 2061 6e64 2073 6176 6573 2061 2074  ng and saves a t
+0001a2d0: 7261 6365 2064 7572 696e 6720 6576 616c  race during eval
+0001a2e0: 7561 7469 6f6e 2e0a 2020 2020 6361 6368  uation..    cach
+0001a2f0: 655f 6461 7461 3a20 5768 6574 6865 7220  e_data: Whether 
+0001a300: 746f 2063 6163 6865 2064 6174 6120 6f72  to cache data or
+0001a310: 206e 6f74 2e20 4966 2073 6574 2074 6f20   not. If set to 
+0001a320: 2761 7574 6f27 2c20 6361 6368 696e 6720  'auto', caching 
+0001a330: 6973 2064 6574 6572 6d69 6e65 6420 6261  is determined ba
+0001a340: 7365 6420 6f6e 2074 6865 2064 6174 6173  sed on the datas
+0001a350: 6574 2073 697a 652e 0a20 2020 2073 6565  et size..    see
+0001a360: 643a 2053 6565 6420 746f 2062 6520 7573  d: Seed to be us
+0001a370: 6564 2066 6f72 2074 6869 7320 7275 6e2e  ed for this run.
+0001a380: 0a20 2020 2065 7661 6c5f 7374 6570 733a  .    eval_steps:
+0001a390: 204e 756d 6265 7220 6f66 2073 7465 7073   Number of steps
+0001a3a0: 2074 6f20 7275 6e20 6576 616c 7561 7469   to run evaluati
+0001a3b0: 6f6e 2066 6f72 2e20 4966 206e 6f74 2073  on for. If not s
+0001a3c0: 7065 6369 6669 6564 206f 7220 6e65 6761  pecified or nega
+0001a3d0: 7469 7665 2c20 6974 206d 6561 6e73 2072  tive, it means r
+0001a3e0: 756e 2065 7661 6c75 6174 696f 6e20 6f6e  un evaluation on
+0001a3f0: 2074 6865 2077 686f 6c65 2076 616c 6964   the whole valid
+0001a400: 6174 696f 6e20 6461 7461 7365 742e 2049  ation dataset. I
+0001a410: 6620 7365 7420 746f 2030 2c20 6974 206d  f set to 0, it m
+0001a420: 6561 6e73 2072 756e 2065 7661 6c75 6174  eans run evaluat
+0001a430: 696f 6e20 666f 7220 6120 6669 7865 6420  ion for a fixed 
+0001a440: 6e75 6d62 6572 206f 6620 7361 6d70 6c65  number of sample
+0001a450: 732e 0a20 2020 2065 7661 6c5f 6672 6571  s..    eval_freq
+0001a460: 7565 6e63 795f 7365 6373 3a20 4672 6571  uency_secs: Freq
+0001a470: 7565 6e63 7920 6174 2077 6869 6368 2065  uency at which e
+0001a480: 7661 6c75 6174 696f 6e20 616e 6420 6368  valuation and ch
+0001a490: 6563 6b70 6f69 6e74 696e 6720 7769 6c6c  eckpointing will
+0001a4a0: 2074 616b 6520 706c 6163 652e 0a20 2020   take place..   
+0001a4b0: 2064 6174 615f 736f 7572 6365 5f63 7376   data_source_csv
+0001a4c0: 5f66 696c 656e 616d 6573 3a20 5468 6520  _filenames: The 
+0001a4d0: 4353 5620 6461 7461 2073 6f75 7263 652e  CSV data source.
+0001a4e0: 0a20 2020 2064 6174 615f 736f 7572 6365  .    data_source
+0001a4f0: 5f62 6967 7175 6572 795f 7461 626c 655f  _bigquery_table_
+0001a500: 7061 7468 3a20 5468 6520 4269 6751 7565  path: The BigQue
+0001a510: 7279 2064 6174 6120 736f 7572 6365 2e0a  ry data source..
+0001a520: 2020 2020 6269 6771 7565 7279 5f73 7461      bigquery_sta
+0001a530: 6769 6e67 5f66 756c 6c5f 6461 7461 7365  ging_full_datase
+0001a540: 745f 6964 3a20 5468 6520 4269 6751 7565  t_id: The BigQue
+0001a550: 7279 2073 7461 6769 6e67 2066 756c 6c20  ry staging full 
+0001a560: 6461 7461 7365 7420 6964 2066 6f72 2073  dataset id for s
+0001a570: 746f 7269 6e67 2069 6e74 6572 6d65 6469  toring intermedi
+0001a580: 6174 6520 7461 626c 6573 2e0a 2020 2020  ate tables..    
+0001a590: 7765 6967 6874 5f63 6f6c 756d 6e3a 2054  weight_column: T
+0001a5a0: 6865 2077 6569 6768 7420 636f 6c75 6d6e  he weight column
+0001a5b0: 206e 616d 652e 0a20 2020 206d 6178 5f66   name..    max_f
+0001a5c0: 6169 6c65 645f 7472 6961 6c5f 636f 756e  ailed_trial_coun
+0001a5d0: 743a 2054 6865 206e 756d 6265 7220 6f66  t: The number of
+0001a5e0: 2066 6169 6c65 6420 7472 6961 6c73 2074   failed trials t
+0001a5f0: 6861 7420 6e65 6564 2074 6f20 6265 2073  hat need to be s
+0001a600: 6565 6e20 6265 666f 7265 2066 6169 6c69  een before faili
+0001a610: 6e67 2074 6865 2048 7970 6572 7061 7261  ng the Hyperpara
+0001a620: 6d65 7465 7254 756e 696e 674a 6f62 2e20  meterTuningJob. 
+0001a630: 4966 2073 6574 2074 6f20 302c 2056 6572  If set to 0, Ver
+0001a640: 7465 7820 4149 2064 6563 6964 6573 2068  tex AI decides h
+0001a650: 6f77 206d 616e 7920 7472 6961 6c73 206d  ow many trials m
+0001a660: 7573 7420 6661 696c 2062 6566 6f72 6520  ust fail before 
+0001a670: 7468 6520 7768 6f6c 6520 6a6f 6220 6661  the whole job fa
+0001a680: 696c 732e 0a20 2020 2073 7475 6479 5f73  ils..    study_s
+0001a690: 7065 635f 616c 676f 7269 7468 6d3a 2054  pec_algorithm: T
+0001a6a0: 6865 2073 6561 7263 6820 616c 676f 7269  he search algori
+0001a6b0: 7468 6d20 7370 6563 6966 6965 6420 666f  thm specified fo
+0001a6c0: 7220 7468 6520 7374 7564 792e 204f 6e65  r the study. One
+0001a6d0: 206f 6620 2241 4c47 4f52 4954 484d 5f55   of "ALGORITHM_U
+0001a6e0: 4e53 5045 4349 4649 4544 222c 2022 4752  NSPECIFIED", "GR
+0001a6f0: 4944 5f53 4541 5243 4822 2c20 6f72 2022  ID_SEARCH", or "
+0001a700: 5241 4e44 4f4d 5f53 4541 5243 4822 2e0a  RANDOM_SEARCH"..
+0001a710: 2020 2020 7374 7564 795f 7370 6563 5f6d      study_spec_m
+0001a720: 6561 7375 7265 6d65 6e74 5f73 656c 6563  easurement_selec
+0001a730: 7469 6f6e 5f74 7970 653a 2057 6869 6368  tion_type: Which
+0001a740: 206d 6561 7375 7265 6d65 6e74 2074 6f20   measurement to 
+0001a750: 7573 6520 6966 2f77 6865 6e20 7468 6520  use if/when the 
+0001a760: 7365 7276 6963 6520 6175 746f 6d61 7469  service automati
+0001a770: 6361 6c6c 7920 7365 6c65 6374 7320 7468  cally selects th
+0001a780: 6520 6669 6e61 6c20 6d65 6173 7572 656d  e final measurem
+0001a790: 656e 7420 6672 6f6d 2070 7265 7669 6f75  ent from previou
+0001a7a0: 736c 7920 7265 706f 7274 6564 2069 6e74  sly reported int
+0001a7b0: 6572 6d65 6469 6174 6520 6d65 6173 7572  ermediate measur
+0001a7c0: 656d 656e 7473 2e20 4f6e 6520 6f66 2022  ements. One of "
+0001a7d0: 4245 5354 5f4d 4541 5355 5245 4d45 4e54  BEST_MEASUREMENT
+0001a7e0: 2220 6f72 2022 4c41 5354 5f4d 4541 5355  " or "LAST_MEASU
+0001a7f0: 5245 4d45 4e54 222e 0a20 2020 2074 7261  REMENT"..    tra
+0001a800: 6e73 666f 726d 5f64 6174 6166 6c6f 775f  nsform_dataflow_
+0001a810: 6d61 6368 696e 655f 7479 7065 3a20 5468  machine_type: Th
+0001a820: 6520 6461 7461 666c 6f77 206d 6163 6869  e dataflow machi
+0001a830: 6e65 2074 7970 6520 666f 7220 7472 616e  ne type for tran
+0001a840: 7366 6f72 6d20 636f 6d70 6f6e 656e 742e  sform component.
+0001a850: 0a20 2020 2074 7261 6e73 666f 726d 5f64  .    transform_d
+0001a860: 6174 6166 6c6f 775f 6d61 785f 6e75 6d5f  ataflow_max_num_
+0001a870: 776f 726b 6572 733a 2054 6865 206d 6178  workers: The max
+0001a880: 206e 756d 6265 7220 6f66 2044 6174 6166   number of Dataf
+0001a890: 6c6f 7720 776f 726b 6572 7320 666f 7220  low workers for 
+0001a8a0: 7472 616e 7366 6f72 6d20 636f 6d70 6f6e  transform compon
+0001a8b0: 656e 742e 0a20 2020 2074 7261 6e73 666f  ent..    transfo
+0001a8c0: 726d 5f64 6174 6166 6c6f 775f 6469 736b  rm_dataflow_disk
+0001a8d0: 5f73 697a 655f 6762 3a20 4461 7461 666c  _size_gb: Datafl
+0001a8e0: 6f77 2077 6f72 6b65 7227 7320 6469 736b  ow worker's disk
+0001a8f0: 2073 697a 6520 696e 2047 4220 666f 7220   size in GB for 
+0001a900: 7472 616e 7366 6f72 6d20 636f 6d70 6f6e  transform compon
+0001a910: 656e 742e 0a20 2020 2077 6f72 6b65 725f  ent..    worker_
+0001a920: 706f 6f6c 5f73 7065 6373 5f6f 7665 7272  pool_specs_overr
+0001a930: 6964 653a 2054 6865 2064 6963 7469 6f6e  ide: The diction
+0001a940: 6172 7920 666f 7220 6f76 6572 7269 6469  ary for overridi
+0001a950: 6e67 2074 7261 696e 696e 6720 616e 6420  ng training and 
+0001a960: 6576 616c 7561 7469 6f6e 2077 6f72 6b65  evaluation worke
+0001a970: 7220 706f 6f6c 2073 7065 6373 2e20 5468  r pool specs. Th
+0001a980: 6520 6469 6374 696f 6e61 7279 2073 686f  e dictionary sho
+0001a990: 756c 6420 6265 206f 6620 666f 726d 6174  uld be of format
+0001a9a0: 2068 7474 7073 3a2f 2f67 6974 6875 622e   https://github.
+0001a9b0: 636f 6d2f 676f 6f67 6c65 6170 6973 2f67  com/googleapis/g
+0001a9c0: 6f6f 676c 6561 7069 732f 626c 6f62 2f34  oogleapis/blob/4
+0001a9d0: 6538 3336 6337 6332 3537 6533 6532 3062  e836c7c257e3e20b
+0001a9e0: 3164 6531 3464 3437 3039 3933 6132 6231  1de14d470993a2b1
+0001a9f0: 6634 3733 3661 382f 676f 6f67 6c65 2f63  f4736a8/google/c
+0001aa00: 6c6f 7564 2f61 6970 6c61 7466 6f72 6d2f  loud/aiplatform/
+0001aa10: 7631 6265 7461 312f 6375 7374 6f6d 5f6a  v1beta1/custom_j
+0001aa20: 6f62 2e70 726f 746f 234c 3137 322e 0a20  ob.proto#L172.. 
+0001aa30: 2020 2072 756e 5f65 7661 6c75 6174 696f     run_evaluatio
+0001aa40: 6e3a 2057 6865 7468 6572 2074 6f20 7275  n: Whether to ru
+0001aa50: 6e20 6576 616c 7561 7469 6f6e 2073 7465  n evaluation ste
+0001aa60: 7073 2064 7572 696e 6720 7472 6169 6e69  ps during traini
+0001aa70: 6e67 2e0a 2020 2020 6576 616c 7561 7469  ng..    evaluati
+0001aa80: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
+0001aa90: 5f6d 6163 6869 6e65 5f74 7970 653a 2054  _machine_type: T
+0001aaa0: 6865 2070 7265 6469 6374 696f 6e20 7365  he prediction se
+0001aab0: 7276 6572 206d 6163 6869 6e65 2074 7970  rver machine typ
+0001aac0: 6520 666f 7220 6261 7463 6820 7072 6564  e for batch pred
+0001aad0: 6963 7420 636f 6d70 6f6e 656e 7473 2064  ict components d
+0001aae0: 7572 696e 6720 6576 616c 7561 7469 6f6e  uring evaluation
+0001aaf0: 2e0a 2020 2020 6576 616c 7561 7469 6f6e  ..    evaluation
+0001ab00: 5f62 6174 6368 5f70 7265 6469 6374 5f73  _batch_predict_s
+0001ab10: 7461 7274 696e 675f 7265 706c 6963 615f  tarting_replica_
+0001ab20: 636f 756e 743a 2054 6865 2069 6e69 7469  count: The initi
+0001ab30: 616c 206e 756d 6265 7220 6f66 2070 7265  al number of pre
+0001ab40: 6469 6374 696f 6e20 7365 7276 6572 2066  diction server f
+0001ab50: 6f72 2062 6174 6368 2070 7265 6469 6374  or batch predict
+0001ab60: 2063 6f6d 706f 6e65 6e74 7320 6475 7269   components duri
+0001ab70: 6e67 2065 7661 6c75 6174 696f 6e2e 0a20  ng evaluation.. 
+0001ab80: 2020 2065 7661 6c75 6174 696f 6e5f 6261     evaluation_ba
+0001ab90: 7463 685f 7072 6564 6963 745f 6d61 785f  tch_predict_max_
+0001aba0: 7265 706c 6963 615f 636f 756e 743a 2054  replica_count: T
+0001abb0: 6865 206d 6178 206e 756d 6265 7220 6f66  he max number of
+0001abc0: 2070 7265 6469 6374 696f 6e20 7365 7276   prediction serv
+0001abd0: 6572 2066 6f72 2062 6174 6368 2070 7265  er for batch pre
+0001abe0: 6469 6374 2063 6f6d 706f 6e65 6e74 7320  dict components 
+0001abf0: 6475 7269 6e67 2065 7661 6c75 6174 696f  during evaluatio
+0001ac00: 6e2e 0a20 2020 2065 7661 6c75 6174 696f  n..    evaluatio
+0001ac10: 6e5f 6461 7461 666c 6f77 5f6d 6163 6869  n_dataflow_machi
+0001ac20: 6e65 5f74 7970 653a 2054 6865 2064 6174  ne_type: The dat
+0001ac30: 6166 6c6f 7720 6d61 6368 696e 6520 7479  aflow machine ty
+0001ac40: 7065 2066 6f72 2065 7661 6c75 6174 696f  pe for evaluatio
+0001ac50: 6e20 636f 6d70 6f6e 656e 7473 2e0a 2020  n components..  
+0001ac60: 2020 6576 616c 7561 7469 6f6e 5f64 6174    evaluation_dat
+0001ac70: 6166 6c6f 775f 7374 6172 7469 6e67 5f6e  aflow_starting_n
+0001ac80: 756d 5f77 6f72 6b65 7273 3a20 5468 6520  um_workers: The 
+0001ac90: 696e 6974 6961 6c20 6e75 6d62 6572 206f  initial number o
+0001aca0: 6620 4461 7461 666c 6f77 2077 6f72 6b65  f Dataflow worke
+0001acb0: 7273 2066 6f72 2065 7661 6c75 6174 696f  rs for evaluatio
+0001acc0: 6e20 636f 6d70 6f6e 656e 7473 2e0a 2020  n components..  
+0001acd0: 2020 6576 616c 7561 7469 6f6e 5f64 6174    evaluation_dat
+0001ace0: 6166 6c6f 775f 6d61 785f 6e75 6d5f 776f  aflow_max_num_wo
+0001acf0: 726b 6572 733a 2054 6865 206d 6178 206e  rkers: The max n
+0001ad00: 756d 6265 7220 6f66 2044 6174 6166 6c6f  umber of Dataflo
+0001ad10: 7720 776f 726b 6572 7320 666f 7220 6576  w workers for ev
+0001ad20: 616c 7561 7469 6f6e 2063 6f6d 706f 6e65  aluation compone
+0001ad30: 6e74 732e 0a20 2020 2065 7661 6c75 6174  nts..    evaluat
+0001ad40: 696f 6e5f 6461 7461 666c 6f77 5f64 6973  ion_dataflow_dis
+0001ad50: 6b5f 7369 7a65 5f67 623a 2044 6174 6166  k_size_gb: Dataf
+0001ad60: 6c6f 7720 776f 726b 6572 2773 2064 6973  low worker's dis
+0001ad70: 6b20 7369 7a65 2069 6e20 4742 2066 6f72  k size in GB for
+0001ad80: 2065 7661 6c75 6174 696f 6e20 636f 6d70   evaluation comp
+0001ad90: 6f6e 656e 7473 2e0a 2020 2020 6461 7461  onents..    data
+0001ada0: 666c 6f77 5f73 6572 7669 6365 5f61 6363  flow_service_acc
+0001adb0: 6f75 6e74 3a20 4375 7374 6f6d 2073 6572  ount: Custom ser
+0001adc0: 7669 6365 2061 6363 6f75 6e74 2074 6f20  vice account to 
+0001add0: 7275 6e20 6461 7461 666c 6f77 206a 6f62  run dataflow job
+0001ade0: 732e 0a20 2020 2064 6174 6166 6c6f 775f  s..    dataflow_
+0001adf0: 7375 626e 6574 776f 726b 3a20 4461 7461  subnetwork: Data
+0001ae00: 666c 6f77 2773 2066 756c 6c79 2071 7561  flow's fully qua
+0001ae10: 6c69 6669 6564 2073 7562 6e65 7477 6f72  lified subnetwor
+0001ae20: 6b20 6e61 6d65 2c20 7768 656e 2065 6d70  k name, when emp
+0001ae30: 7479 2074 6865 2064 6566 6175 6c74 2073  ty the default s
+0001ae40: 7562 6e65 7477 6f72 6b20 7769 6c6c 2062  ubnetwork will b
+0001ae50: 6520 7573 6564 2e20 4578 616d 706c 653a  e used. Example:
+0001ae60: 2068 7474 7073 3a2f 2f63 6c6f 7564 2e67   https://cloud.g
+0001ae70: 6f6f 676c 652e 636f 6d2f 6461 7461 666c  oogle.com/datafl
+0001ae80: 6f77 2f64 6f63 732f 6775 6964 6573 2f73  ow/docs/guides/s
+0001ae90: 7065 6369 6679 696e 672d 6e65 7477 6f72  pecifying-networ
+0001aea0: 6b73 2365 7861 6d70 6c65 5f6e 6574 776f  ks#example_netwo
+0001aeb0: 726b 5f61 6e64 5f73 7562 6e65 7477 6f72  rk_and_subnetwor
+0001aec0: 6b5f 7370 6563 6966 6963 6174 696f 6e73  k_specifications
+0001aed0: 0a20 2020 2064 6174 6166 6c6f 775f 7573  .    dataflow_us
+0001aee0: 655f 7075 626c 6963 5f69 7073 3a20 5370  e_public_ips: Sp
+0001aef0: 6563 6966 6965 7320 7768 6574 6865 7220  ecifies whether 
+0001af00: 4461 7461 666c 6f77 2077 6f72 6b65 7273  Dataflow workers
+0001af10: 2075 7365 2070 7562 6c69 6320 4950 2061   use public IP a
+0001af20: 6464 7265 7373 6573 2e0a 2020 2020 656e  ddresses..    en
+0001af30: 6372 7970 7469 6f6e 5f73 7065 635f 6b65  cryption_spec_ke
+0001af40: 795f 6e61 6d65 3a20 5468 6520 4b4d 5320  y_name: The KMS 
+0001af50: 6b65 7920 6e61 6d65 2e0a 0a20 2052 6574  key name...  Ret
+0001af60: 7572 6e73 3a0a 2020 2020 5475 706c 6520  urns:.    Tuple 
+0001af70: 6f66 2070 6970 656c 696e 655f 6465 6669  of pipeline_defi
+0001af80: 6e69 7469 6f6e 5f70 6174 6820 616e 6420  nition_path and 
+0001af90: 7061 7261 6d65 7465 725f 7661 6c75 6573  parameter_values
+0001afa0: 2e0a 2020 2222 220a 2020 2320 666d 743a  ..  """.  # fmt:
+0001afb0: 206f 6e0a 2020 6966 2069 7369 6e73 7461   on.  if isinsta
+0001afc0: 6e63 6528 7466 5f61 7574 6f5f 7472 616e  nce(tf_auto_tran
+0001afd0: 7366 6f72 6d5f 6665 6174 7572 6573 2c20  sform_features, 
+0001afe0: 6c69 7374 293a 0a20 2020 2074 665f 6175  list):.    tf_au
+0001aff0: 746f 5f74 7261 6e73 666f 726d 5f66 6561  to_transform_fea
+0001b000: 7475 7265 7320 3d20 7b27 6175 746f 273a  tures = {'auto':
+0001b010: 2074 665f 6175 746f 5f74 7261 6e73 666f   tf_auto_transfo
+0001b020: 726d 5f66 6561 7475 7265 737d 0a0a 2020  rm_features}..  
+0001b030: 6966 2074 7261 6e73 666f 726d 5f63 6f6e  if transform_con
+0001b040: 6669 6720 616e 6420 7466 5f74 7261 6e73  fig and tf_trans
+0001b050: 666f 726d 6174 696f 6e73 5f70 6174 683a  formations_path:
+0001b060: 0a20 2020 2072 6169 7365 2056 616c 7565  .    raise Value
+0001b070: 4572 726f 7228 0a20 2020 2020 2020 2027  Error(.        '
+0001b080: 4f6e 6c79 206f 6e65 206f 6620 7472 616e  Only one of tran
+0001b090: 7366 6f72 6d5f 636f 6e66 6967 2061 6e64  sform_config and
+0001b0a0: 2074 665f 7472 616e 7366 6f72 6d61 7469   tf_transformati
+0001b0b0: 6f6e 735f 7061 7468 2063 616e 2027 0a20  ons_path can '. 
+0001b0c0: 2020 2020 2020 2027 6265 2073 7065 6369         'be speci
+0001b0d0: 6669 6564 2e27 0a20 2020 2029 0a0a 2020  fied.'.    )..  
+0001b0e0: 656c 6966 2074 7261 6e73 666f 726d 5f63  elif transform_c
+0001b0f0: 6f6e 6669 673a 0a20 2020 2077 6172 6e69  onfig:.    warni
+0001b100: 6e67 732e 7761 726e 280a 2020 2020 2020  ngs.warn(.      
+0001b110: 2020 2774 7261 6e73 666f 726d 5f63 6f6e    'transform_con
+0001b120: 6669 6720 7061 7261 6d65 7465 7220 6973  fig parameter is
+0001b130: 2064 6570 7265 6361 7465 642e 2027 0a20   deprecated. '. 
+0001b140: 2020 2020 2020 2027 506c 6561 7365 2075         'Please u
+0001b150: 7365 2074 6865 2066 6c61 7474 656e 6564  se the flattened
+0001b160: 2074 7261 6e73 666f 726d 2063 6f6e 6669   transform confi
+0001b170: 6720 6172 6775 6d65 6e74 7320 696e 7374  g arguments inst
+0001b180: 6561 642e 270a 2020 2020 290a 2020 2020  ead.'.    ).    
+0001b190: 7466 5f74 7261 6e73 666f 726d 6174 696f  tf_transformatio
+0001b1a0: 6e73 5f70 6174 6820 3d20 7472 616e 7366  ns_path = transf
+0001b1b0: 6f72 6d5f 636f 6e66 6967 0a0a 2020 6966  orm_config..  if
+0001b1c0: 206e 6f74 2077 6f72 6b65 725f 706f 6f6c   not worker_pool
+0001b1d0: 5f73 7065 6373 5f6f 7665 7272 6964 653a  _specs_override:
+0001b1e0: 0a20 2020 2077 6f72 6b65 725f 706f 6f6c  .    worker_pool
+0001b1f0: 5f73 7065 6373 5f6f 7665 7272 6964 6520  _specs_override 
+0001b200: 3d20 5b5d 0a0a 2020 7061 7261 6d65 7465  = []..  paramete
+0001b210: 725f 7661 6c75 6573 203d 207b 0a20 2020  r_values = {.   
+0001b220: 2020 2027 7072 6f6a 6563 7427 3a20 7072     'project': pr
+0001b230: 6f6a 6563 742c 0a20 2020 2020 2027 6c6f  oject,.      'lo
+0001b240: 6361 7469 6f6e 273a 206c 6f63 6174 696f  cation': locatio
+0001b250: 6e2c 0a20 2020 2020 2027 726f 6f74 5f64  n,.      'root_d
+0001b260: 6972 273a 2072 6f6f 745f 6469 722c 0a20  ir': root_dir,. 
+0001b270: 2020 2020 2027 7461 7267 6574 5f63 6f6c       'target_col
+0001b280: 756d 6e27 3a20 7461 7267 6574 5f63 6f6c  umn': target_col
+0001b290: 756d 6e2c 0a20 2020 2020 2027 7072 6564  umn,.      'pred
+0001b2a0: 6963 7469 6f6e 5f74 7970 6527 3a20 7072  iction_type': pr
+0001b2b0: 6564 6963 7469 6f6e 5f74 7970 652c 0a20  ediction_type,. 
+0001b2c0: 2020 2020 2027 7374 7564 795f 7370 6563       'study_spec
+0001b2d0: 5f6d 6574 7269 635f 6964 273a 2073 7475  _metric_id': stu
+0001b2e0: 6479 5f73 7065 635f 6d65 7472 6963 5f69  dy_spec_metric_i
+0001b2f0: 642c 0a20 2020 2020 2027 7374 7564 795f  d,.      'study_
+0001b300: 7370 6563 5f6d 6574 7269 635f 676f 616c  spec_metric_goal
+0001b310: 273a 2073 7475 6479 5f73 7065 635f 6d65  ': study_spec_me
+0001b320: 7472 6963 5f67 6f61 6c2c 0a20 2020 2020  tric_goal,.     
+0001b330: 2027 7374 7564 795f 7370 6563 5f70 6172   'study_spec_par
+0001b340: 616d 6574 6572 735f 6f76 6572 7269 6465  ameters_override
+0001b350: 273a 2073 7475 6479 5f73 7065 635f 7061  ': study_spec_pa
+0001b360: 7261 6d65 7465 7273 5f6f 7665 7272 6964  rameters_overrid
+0001b370: 652c 0a20 2020 2020 2027 6d61 785f 7472  e,.      'max_tr
+0001b380: 6961 6c5f 636f 756e 7427 3a20 6d61 785f  ial_count': max_
+0001b390: 7472 6961 6c5f 636f 756e 742c 0a20 2020  trial_count,.   
+0001b3a0: 2020 2027 7061 7261 6c6c 656c 5f74 7269     'parallel_tri
+0001b3b0: 616c 5f63 6f75 6e74 273a 2070 6172 616c  al_count': paral
+0001b3c0: 6c65 6c5f 7472 6961 6c5f 636f 756e 742c  lel_trial_count,
+0001b3d0: 0a20 2020 2020 2027 656e 6162 6c65 5f70  .      'enable_p
+0001b3e0: 726f 6669 6c65 7227 3a20 656e 6162 6c65  rofiler': enable
+0001b3f0: 5f70 726f 6669 6c65 722c 0a20 2020 2020  _profiler,.     
+0001b400: 2027 6361 6368 655f 6461 7461 273a 2063   'cache_data': c
+0001b410: 6163 6865 5f64 6174 612c 0a20 2020 2020  ache_data,.     
+0001b420: 2027 7365 6564 273a 2073 6565 642c 0a20   'seed': seed,. 
+0001b430: 2020 2020 2027 6576 616c 5f73 7465 7073       'eval_steps
+0001b440: 273a 2065 7661 6c5f 7374 6570 732c 0a20  ': eval_steps,. 
+0001b450: 2020 2020 2027 6576 616c 5f66 7265 7175       'eval_frequ
+0001b460: 656e 6379 5f73 6563 7327 3a20 6576 616c  ency_secs': eval
+0001b470: 5f66 7265 7175 656e 6379 5f73 6563 732c  _frequency_secs,
+0001b480: 0a20 2020 2020 2027 7765 6967 6874 5f63  .      'weight_c
+0001b490: 6f6c 756d 6e27 3a20 7765 6967 6874 5f63  olumn': weight_c
+0001b4a0: 6f6c 756d 6e2c 0a20 2020 2020 2027 6d61  olumn,.      'ma
+0001b4b0: 785f 6661 696c 6564 5f74 7269 616c 5f63  x_failed_trial_c
+0001b4c0: 6f75 6e74 273a 206d 6178 5f66 6169 6c65  ount': max_faile
+0001b4d0: 645f 7472 6961 6c5f 636f 756e 742c 0a20  d_trial_count,. 
+0001b4e0: 2020 2020 2027 7374 7564 795f 7370 6563       'study_spec
+0001b4f0: 5f61 6c67 6f72 6974 686d 273a 2073 7475  _algorithm': stu
+0001b500: 6479 5f73 7065 635f 616c 676f 7269 7468  dy_spec_algorith
+0001b510: 6d2c 0a20 2020 2020 2027 7374 7564 795f  m,.      'study_
+0001b520: 7370 6563 5f6d 6561 7375 7265 6d65 6e74  spec_measurement
+0001b530: 5f73 656c 6563 7469 6f6e 5f74 7970 6527  _selection_type'
+0001b540: 3a20 280a 2020 2020 2020 2020 2020 7374  : (.          st
+0001b550: 7564 795f 7370 6563 5f6d 6561 7375 7265  udy_spec_measure
+0001b560: 6d65 6e74 5f73 656c 6563 7469 6f6e 5f74  ment_selection_t
+0001b570: 7970 650a 2020 2020 2020 292c 0a20 2020  ype.      ),.   
+0001b580: 2020 2027 7472 616e 7366 6f72 6d5f 6461     'transform_da
+0001b590: 7461 666c 6f77 5f6d 6163 6869 6e65 5f74  taflow_machine_t
+0001b5a0: 7970 6527 3a20 7472 616e 7366 6f72 6d5f  ype': transform_
+0001b5b0: 6461 7461 666c 6f77 5f6d 6163 6869 6e65  dataflow_machine
+0001b5c0: 5f74 7970 652c 0a20 2020 2020 2027 7472  _type,.      'tr
+0001b5d0: 616e 7366 6f72 6d5f 6461 7461 666c 6f77  ansform_dataflow
+0001b5e0: 5f6d 6178 5f6e 756d 5f77 6f72 6b65 7273  _max_num_workers
+0001b5f0: 273a 2074 7261 6e73 666f 726d 5f64 6174  ': transform_dat
+0001b600: 6166 6c6f 775f 6d61 785f 6e75 6d5f 776f  aflow_max_num_wo
+0001b610: 726b 6572 732c 0a20 2020 2020 2027 7472  rkers,.      'tr
+0001b620: 616e 7366 6f72 6d5f 6461 7461 666c 6f77  ansform_dataflow
+0001b630: 5f64 6973 6b5f 7369 7a65 5f67 6227 3a20  _disk_size_gb': 
+0001b640: 7472 616e 7366 6f72 6d5f 6461 7461 666c  transform_datafl
+0001b650: 6f77 5f64 6973 6b5f 7369 7a65 5f67 622c  ow_disk_size_gb,
+0001b660: 0a20 2020 2020 2027 776f 726b 6572 5f70  .      'worker_p
+0001b670: 6f6f 6c5f 7370 6563 735f 6f76 6572 7269  ool_specs_overri
+0001b680: 6465 273a 2077 6f72 6b65 725f 706f 6f6c  de': worker_pool
+0001b690: 5f73 7065 6373 5f6f 7665 7272 6964 652c  _specs_override,
+0001b6a0: 0a20 2020 2020 2027 7275 6e5f 6576 616c  .      'run_eval
+0001b6b0: 7561 7469 6f6e 273a 2072 756e 5f65 7661  uation': run_eva
+0001b6c0: 6c75 6174 696f 6e2c 0a20 2020 2020 2027  luation,.      '
+0001b6d0: 6576 616c 7561 7469 6f6e 5f62 6174 6368  evaluation_batch
+0001b6e0: 5f70 7265 6469 6374 5f6d 6163 6869 6e65  _predict_machine
+0001b6f0: 5f74 7970 6527 3a20 280a 2020 2020 2020  _type': (.      
+0001b700: 2020 2020 6576 616c 7561 7469 6f6e 5f62      evaluation_b
+0001b710: 6174 6368 5f70 7265 6469 6374 5f6d 6163  atch_predict_mac
+0001b720: 6869 6e65 5f74 7970 650a 2020 2020 2020  hine_type.      
+0001b730: 292c 0a20 2020 2020 2027 6576 616c 7561  ),.      'evalua
+0001b740: 7469 6f6e 5f62 6174 6368 5f70 7265 6469  tion_batch_predi
+0001b750: 6374 5f73 7461 7274 696e 675f 7265 706c  ct_starting_repl
+0001b760: 6963 615f 636f 756e 7427 3a20 280a 2020  ica_count': (.  
+0001b770: 2020 2020 2020 2020 6576 616c 7561 7469          evaluati
+0001b780: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
+0001b790: 5f73 7461 7274 696e 675f 7265 706c 6963  _starting_replic
+0001b7a0: 615f 636f 756e 740a 2020 2020 2020 292c  a_count.      ),
+0001b7b0: 0a20 2020 2020 2027 6576 616c 7561 7469  .      'evaluati
+0001b7c0: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
+0001b7d0: 5f6d 6178 5f72 6570 6c69 6361 5f63 6f75  _max_replica_cou
+0001b7e0: 6e74 273a 2028 0a20 2020 2020 2020 2020  nt': (.         
+0001b7f0: 2065 7661 6c75 6174 696f 6e5f 6261 7463   evaluation_batc
+0001b800: 685f 7072 6564 6963 745f 6d61 785f 7265  h_predict_max_re
+0001b810: 706c 6963 615f 636f 756e 740a 2020 2020  plica_count.    
+0001b820: 2020 292c 0a20 2020 2020 2027 6576 616c    ),.      'eval
+0001b830: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
+0001b840: 6d61 6368 696e 655f 7479 7065 273a 2065  machine_type': e
+0001b850: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
+0001b860: 6f77 5f6d 6163 6869 6e65 5f74 7970 652c  ow_machine_type,
+0001b870: 0a20 2020 2020 2027 6576 616c 7561 7469  .      'evaluati
+0001b880: 6f6e 5f64 6174 6166 6c6f 775f 7374 6172  on_dataflow_star
+0001b890: 7469 6e67 5f6e 756d 5f77 6f72 6b65 7273  ting_num_workers
+0001b8a0: 273a 2028 0a20 2020 2020 2020 2020 2065  ': (.          e
+0001b8b0: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
+0001b8c0: 6f77 5f73 7461 7274 696e 675f 6e75 6d5f  ow_starting_num_
+0001b8d0: 776f 726b 6572 730a 2020 2020 2020 292c  workers.      ),
+0001b8e0: 0a20 2020 2020 2027 6576 616c 7561 7469  .      'evaluati
+0001b8f0: 6f6e 5f64 6174 6166 6c6f 775f 6d61 785f  on_dataflow_max_
+0001b900: 6e75 6d5f 776f 726b 6572 7327 3a20 280a  num_workers': (.
+0001b910: 2020 2020 2020 2020 2020 6576 616c 7561            evalua
+0001b920: 7469 6f6e 5f64 6174 6166 6c6f 775f 6d61  tion_dataflow_ma
+0001b930: 785f 6e75 6d5f 776f 726b 6572 730a 2020  x_num_workers.  
+0001b940: 2020 2020 292c 0a20 2020 2020 2027 6576      ),.      'ev
+0001b950: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
+0001b960: 775f 6469 736b 5f73 697a 655f 6762 273a  w_disk_size_gb':
+0001b970: 2065 7661 6c75 6174 696f 6e5f 6461 7461   evaluation_data
+0001b980: 666c 6f77 5f64 6973 6b5f 7369 7a65 5f67  flow_disk_size_g
+0001b990: 622c 0a20 2020 2020 2027 6461 7461 666c  b,.      'datafl
+0001b9a0: 6f77 5f73 6572 7669 6365 5f61 6363 6f75  ow_service_accou
+0001b9b0: 6e74 273a 2064 6174 6166 6c6f 775f 7365  nt': dataflow_se
+0001b9c0: 7276 6963 655f 6163 636f 756e 742c 0a20  rvice_account,. 
+0001b9d0: 2020 2020 2027 6461 7461 666c 6f77 5f73       'dataflow_s
+0001b9e0: 7562 6e65 7477 6f72 6b27 3a20 6461 7461  ubnetwork': data
+0001b9f0: 666c 6f77 5f73 7562 6e65 7477 6f72 6b2c  flow_subnetwork,
+0001ba00: 0a20 2020 2020 2027 6461 7461 666c 6f77  .      'dataflow
+0001ba10: 5f75 7365 5f70 7562 6c69 635f 6970 7327  _use_public_ips'
+0001ba20: 3a20 6461 7461 666c 6f77 5f75 7365 5f70  : dataflow_use_p
+0001ba30: 7562 6c69 635f 6970 732c 0a20 2020 2020  ublic_ips,.     
+0001ba40: 2027 656e 6372 7970 7469 6f6e 5f73 7065   'encryption_spe
+0001ba50: 635f 6b65 795f 6e61 6d65 273a 2065 6e63  c_key_name': enc
+0001ba60: 7279 7074 696f 6e5f 7370 6563 5f6b 6579  ryption_spec_key
+0001ba70: 5f6e 616d 652c 0a20 207d 0a0a 2020 6674  _name,.  }..  ft
+0001ba80: 655f 7061 7261 6d73 203d 207b 0a20 2020  e_params = {.   
+0001ba90: 2020 2027 6461 7461 7365 745f 6c65 7665     'dataset_leve
+0001baa0: 6c5f 6375 7374 6f6d 5f74 7261 6e73 666f  l_custom_transfo
+0001bab0: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
+0001bac0: 6f6e 7327 3a20 280a 2020 2020 2020 2020  ons': (.        
+0001bad0: 2020 6461 7461 7365 745f 6c65 7665 6c5f    dataset_level_
+0001bae0: 6375 7374 6f6d 5f74 7261 6e73 666f 726d  custom_transform
+0001baf0: 6174 696f 6e5f 6465 6669 6e69 7469 6f6e  ation_definition
+0001bb00: 730a 2020 2020 2020 2020 2020 6966 2064  s.          if d
+0001bb10: 6174 6173 6574 5f6c 6576 656c 5f63 7573  ataset_level_cus
+0001bb20: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
+0001bb30: 6f6e 5f64 6566 696e 6974 696f 6e73 0a20  on_definitions. 
+0001bb40: 2020 2020 2020 2020 2065 6c73 6520 5b5d           else []
+0001bb50: 0a20 2020 2020 2029 2c0a 2020 2020 2020  .      ),.      
+0001bb60: 2764 6174 6173 6574 5f6c 6576 656c 5f74  'dataset_level_t
+0001bb70: 7261 6e73 666f 726d 6174 696f 6e73 273a  ransformations':
+0001bb80: 2028 0a20 2020 2020 2020 2020 2064 6174   (.          dat
+0001bb90: 6173 6574 5f6c 6576 656c 5f74 7261 6e73  aset_level_trans
+0001bba0: 666f 726d 6174 696f 6e73 2069 6620 6461  formations if da
+0001bbb0: 7461 7365 745f 6c65 7665 6c5f 7472 616e  taset_level_tran
+0001bbc0: 7366 6f72 6d61 7469 6f6e 7320 656c 7365  sformations else
+0001bbd0: 205b 5d0a 2020 2020 2020 292c 0a20 2020   [].      ),.   
+0001bbe0: 2020 2027 7275 6e5f 6665 6174 7572 655f     'run_feature_
+0001bbf0: 7365 6c65 6374 696f 6e27 3a20 7275 6e5f  selection': run_
+0001bc00: 6665 6174 7572 655f 7365 6c65 6374 696f  feature_selectio
+0001bc10: 6e2c 0a20 2020 2020 2027 6665 6174 7572  n,.      'featur
+0001bc20: 655f 7365 6c65 6374 696f 6e5f 616c 676f  e_selection_algo
+0001bc30: 7269 7468 6d27 3a20 6665 6174 7572 655f  rithm': feature_
+0001bc40: 7365 6c65 6374 696f 6e5f 616c 676f 7269  selection_algori
+0001bc50: 7468 6d2c 0a20 2020 2020 2027 6d61 785f  thm,.      'max_
+0001bc60: 7365 6c65 6374 6564 5f66 6561 7475 7265  selected_feature
+0001bc70: 7327 3a20 6d61 785f 7365 6c65 6374 6564  s': max_selected
+0001bc80: 5f66 6561 7475 7265 732c 0a20 2020 2020  _features,.     
+0001bc90: 2027 7072 6564 6566 696e 6564 5f73 706c   'predefined_spl
+0001bca0: 6974 5f6b 6579 273a 2070 7265 6465 6669  it_key': predefi
+0001bcb0: 6e65 645f 7370 6c69 745f 6b65 792c 0a20  ned_split_key,. 
+0001bcc0: 2020 2020 2027 7374 7261 7469 6669 6564       'stratified
+0001bcd0: 5f73 706c 6974 5f6b 6579 273a 2073 7472  _split_key': str
+0001bce0: 6174 6966 6965 645f 7370 6c69 745f 6b65  atified_split_ke
+0001bcf0: 792c 0a20 2020 2020 2027 7472 6169 6e69  y,.      'traini
+0001bd00: 6e67 5f66 7261 6374 696f 6e27 3a20 7472  ng_fraction': tr
+0001bd10: 6169 6e69 6e67 5f66 7261 6374 696f 6e2c  aining_fraction,
+0001bd20: 0a20 2020 2020 2027 7661 6c69 6461 7469  .      'validati
+0001bd30: 6f6e 5f66 7261 6374 696f 6e27 3a20 7661  on_fraction': va
+0001bd40: 6c69 6461 7469 6f6e 5f66 7261 6374 696f  lidation_fractio
+0001bd50: 6e2c 0a20 2020 2020 2027 7465 7374 5f66  n,.      'test_f
+0001bd60: 7261 6374 696f 6e27 3a20 7465 7374 5f66  raction': test_f
+0001bd70: 7261 6374 696f 6e2c 0a20 2020 2020 2027  raction,.      '
+0001bd80: 7466 5f61 7574 6f5f 7472 616e 7366 6f72  tf_auto_transfor
+0001bd90: 6d5f 6665 6174 7572 6573 273a 2028 0a20  m_features': (. 
+0001bda0: 2020 2020 2020 2020 2074 665f 6175 746f           tf_auto
+0001bdb0: 5f74 7261 6e73 666f 726d 5f66 6561 7475  _transform_featu
+0001bdc0: 7265 7320 6966 2074 665f 6175 746f 5f74  res if tf_auto_t
+0001bdd0: 7261 6e73 666f 726d 5f66 6561 7475 7265  ransform_feature
+0001bde0: 7320 656c 7365 207b 7d0a 2020 2020 2020  s else {}.      
+0001bdf0: 292c 0a20 2020 2020 2027 7466 5f63 7573  ),.      'tf_cus
+0001be00: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
+0001be10: 6f6e 5f64 6566 696e 6974 696f 6e73 273a  on_definitions':
+0001be20: 2028 0a20 2020 2020 2020 2020 2074 665f   (.          tf_
+0001be30: 6375 7374 6f6d 5f74 7261 6e73 666f 726d  custom_transform
+0001be40: 6174 696f 6e5f 6465 6669 6e69 7469 6f6e  ation_definition
+0001be50: 730a 2020 2020 2020 2020 2020 6966 2074  s.          if t
+0001be60: 665f 6375 7374 6f6d 5f74 7261 6e73 666f  f_custom_transfo
+0001be70: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
+0001be80: 6f6e 730a 2020 2020 2020 2020 2020 656c  ons.          el
+0001be90: 7365 205b 5d0a 2020 2020 2020 292c 0a20  se [].      ),. 
+0001bea0: 2020 2020 2027 7466 5f74 7261 6e73 666f       'tf_transfo
+0001beb0: 726d 6174 696f 6e73 5f70 6174 6827 3a20  rmations_path': 
+0001bec0: 7466 5f74 7261 6e73 666f 726d 6174 696f  tf_transformatio
+0001bed0: 6e73 5f70 6174 682c 0a20 2020 2020 2027  ns_path,.      '
+0001bee0: 6d61 7465 7269 616c 697a 6564 5f65 7861  materialized_exa
+0001bef0: 6d70 6c65 735f 666f 726d 6174 273a 2028  mples_format': (
+0001bf00: 0a20 2020 2020 2020 2020 206d 6174 6572  .          mater
+0001bf10: 6961 6c69 7a65 645f 6578 616d 706c 6573  ialized_examples
+0001bf20: 5f66 6f72 6d61 740a 2020 2020 2020 2020  _format.        
+0001bf30: 2020 6966 206d 6174 6572 6961 6c69 7a65    if materialize
+0001bf40: 645f 6578 616d 706c 6573 5f66 6f72 6d61  d_examples_forma
+0001bf50: 740a 2020 2020 2020 2020 2020 656c 7365  t.          else
+0001bf60: 2027 7466 7265 636f 7264 735f 677a 6970   'tfrecords_gzip
+0001bf70: 270a 2020 2020 2020 292c 0a20 2020 2020  '.      ),.     
+0001bf80: 2027 7466 5f74 7261 6e73 666f 726d 5f65   'tf_transform_e
+0001bf90: 7865 6375 7469 6f6e 5f65 6e67 696e 6527  xecution_engine'
+0001bfa0: 3a20 280a 2020 2020 2020 2020 2020 7466  : (.          tf
+0001bfb0: 5f74 7261 6e73 666f 726d 5f65 7865 6375  _transform_execu
+0001bfc0: 7469 6f6e 5f65 6e67 696e 650a 2020 2020  tion_engine.    
+0001bfd0: 2020 2020 2020 6966 2074 665f 7472 616e        if tf_tran
+0001bfe0: 7366 6f72 6d5f 6578 6563 7574 696f 6e5f  sform_execution_
+0001bff0: 656e 6769 6e65 0a20 2020 2020 2020 2020  engine.         
+0001c000: 2065 6c73 6520 2764 6174 6166 6c6f 7727   else 'dataflow'
+0001c010: 0a20 2020 2020 2029 2c0a 2020 7d0a 2020  .      ),.  }.  
+0001c020: 5f75 7064 6174 655f 7061 7261 6d65 7465  _update_paramete
+0001c030: 7273 2870 6172 616d 6574 6572 5f76 616c  rs(parameter_val
+0001c040: 7565 732c 2066 7465 5f70 6172 616d 7329  ues, fte_params)
+0001c050: 0a0a 2020 6461 7461 5f73 6f75 7263 655f  ..  data_source_
+0001c060: 616e 645f 7370 6c69 745f 7061 7261 6d65  and_split_parame
+0001c070: 7465 7273 203d 207b 0a20 2020 2020 2027  ters = {.      '
+0001c080: 6461 7461 5f73 6f75 7263 655f 6373 765f  data_source_csv_
+0001c090: 6669 6c65 6e61 6d65 7327 3a20 6461 7461  filenames': data
+0001c0a0: 5f73 6f75 7263 655f 6373 765f 6669 6c65  _source_csv_file
+0001c0b0: 6e61 6d65 732c 0a20 2020 2020 2027 6461  names,.      'da
+0001c0c0: 7461 5f73 6f75 7263 655f 6269 6771 7565  ta_source_bigque
+0001c0d0: 7279 5f74 6162 6c65 5f70 6174 6827 3a20  ry_table_path': 
+0001c0e0: 6461 7461 5f73 6f75 7263 655f 6269 6771  data_source_bigq
+0001c0f0: 7565 7279 5f74 6162 6c65 5f70 6174 682c  uery_table_path,
+0001c100: 0a20 2020 2020 2027 6269 6771 7565 7279  .      'bigquery
+0001c110: 5f73 7461 6769 6e67 5f66 756c 6c5f 6461  _staging_full_da
+0001c120: 7461 7365 745f 6964 273a 2062 6967 7175  taset_id': bigqu
+0001c130: 6572 795f 7374 6167 696e 675f 6675 6c6c  ery_staging_full
+0001c140: 5f64 6174 6173 6574 5f69 642c 0a20 207d  _dataset_id,.  }
+0001c150: 0a20 205f 7570 6461 7465 5f70 6172 616d  .  _update_param
+0001c160: 6574 6572 7328 7061 7261 6d65 7465 725f  eters(parameter_
+0001c170: 7661 6c75 6573 2c20 6461 7461 5f73 6f75  values, data_sou
+0001c180: 7263 655f 616e 645f 7370 6c69 745f 7061  rce_and_split_pa
+0001c190: 7261 6d65 7465 7273 290a 0a20 2070 6970  rameters)..  pip
+0001c1a0: 656c 696e 655f 6465 6669 6e69 7469 6f6e  eline_definition
+0001c1b0: 5f70 6174 6820 3d20 6f73 2e70 6174 682e  _path = os.path.
+0001c1c0: 6a6f 696e 280a 2020 2020 2020 7061 7468  join(.      path
+0001c1d0: 6c69 622e 5061 7468 285f 5f66 696c 655f  lib.Path(__file_
+0001c1e0: 5f29 2e70 6172 656e 742e 7265 736f 6c76  _).parent.resolv
+0001c1f0: 6528 292c 0a20 2020 2020 2027 7769 6465  e(),.      'wide
+0001c200: 5f61 6e64 5f64 6565 705f 6879 7065 7270  _and_deep_hyperp
+0001c210: 6172 616d 6574 6572 5f74 756e 696e 675f  arameter_tuning_
+0001c220: 6a6f 625f 7069 7065 6c69 6e65 2e79 616d  job_pipeline.yam
+0001c230: 6c27 2c0a 2020 290a 0a20 2072 6574 7572  l',.  )..  retur
+0001c240: 6e20 7069 7065 6c69 6e65 5f64 6566 696e  n pipeline_defin
+0001c250: 6974 696f 6e5f 7061 7468 2c20 7061 7261  ition_path, para
+0001c260: 6d65 7465 725f 7661 6c75 6573 0a0a 0a64  meter_values...d
+0001c270: 6566 2067 6574 5f74 6162 6e65 745f 7472  ef get_tabnet_tr
+0001c280: 6169 6e65 725f 7069 7065 6c69 6e65 5f61  ainer_pipeline_a
+0001c290: 6e64 5f70 6172 616d 6574 6572 7328 0a20  nd_parameters(. 
+0001c2a0: 2020 2070 726f 6a65 6374 3a20 7374 722c     project: str,
+0001c2b0: 0a20 2020 206c 6f63 6174 696f 6e3a 2073  .    location: s
+0001c2c0: 7472 2c0a 2020 2020 726f 6f74 5f64 6972  tr,.    root_dir
+0001c2d0: 3a20 7374 722c 0a20 2020 2074 6172 6765  : str,.    targe
+0001c2e0: 745f 636f 6c75 6d6e 3a20 7374 722c 0a20  t_column: str,. 
+0001c2f0: 2020 2070 7265 6469 6374 696f 6e5f 7479     prediction_ty
+0001c300: 7065 3a20 7374 722c 0a20 2020 206c 6561  pe: str,.    lea
+0001c310: 726e 696e 675f 7261 7465 3a20 666c 6f61  rning_rate: floa
+0001c320: 742c 0a20 2020 2074 7261 6e73 666f 726d  t,.    transform
+0001c330: 5f63 6f6e 6669 673a 204f 7074 696f 6e61  _config: Optiona
+0001c340: 6c5b 7374 725d 203d 204e 6f6e 652c 0a20  l[str] = None,. 
+0001c350: 2020 2064 6174 6173 6574 5f6c 6576 656c     dataset_level
+0001c360: 5f63 7573 746f 6d5f 7472 616e 7366 6f72  _custom_transfor
+0001c370: 6d61 7469 6f6e 5f64 6566 696e 6974 696f  mation_definitio
+0001c380: 6e73 3a20 4f70 7469 6f6e 616c 5b0a 2020  ns: Optional[.  
+0001c390: 2020 2020 2020 4c69 7374 5b44 6963 745b        List[Dict[
+0001c3a0: 7374 722c 2041 6e79 5d5d 0a20 2020 205d  str, Any]].    ]
+0001c3b0: 203d 204e 6f6e 652c 0a20 2020 2064 6174   = None,.    dat
+0001c3c0: 6173 6574 5f6c 6576 656c 5f74 7261 6e73  aset_level_trans
+0001c3d0: 666f 726d 6174 696f 6e73 3a20 4f70 7469  formations: Opti
+0001c3e0: 6f6e 616c 5b4c 6973 745b 4469 6374 5b73  onal[List[Dict[s
+0001c3f0: 7472 2c20 416e 795d 5d5d 203d 204e 6f6e  tr, Any]]] = Non
+0001c400: 652c 0a20 2020 2072 756e 5f66 6561 7475  e,.    run_featu
+0001c410: 7265 5f73 656c 6563 7469 6f6e 3a20 626f  re_selection: bo
+0001c420: 6f6c 203d 2046 616c 7365 2c0a 2020 2020  ol = False,.    
+0001c430: 6665 6174 7572 655f 7365 6c65 6374 696f  feature_selectio
+0001c440: 6e5f 616c 676f 7269 7468 6d3a 204f 7074  n_algorithm: Opt
+0001c450: 696f 6e61 6c5b 7374 725d 203d 204e 6f6e  ional[str] = Non
+0001c460: 652c 0a20 2020 206d 6174 6572 6961 6c69  e,.    materiali
+0001c470: 7a65 645f 6578 616d 706c 6573 5f66 6f72  zed_examples_for
+0001c480: 6d61 743a 204f 7074 696f 6e61 6c5b 7374  mat: Optional[st
+0001c490: 725d 203d 204e 6f6e 652c 0a20 2020 206d  r] = None,.    m
+0001c4a0: 6178 5f73 656c 6563 7465 645f 6665 6174  ax_selected_feat
+0001c4b0: 7572 6573 3a20 4f70 7469 6f6e 616c 5b69  ures: Optional[i
+0001c4c0: 6e74 5d20 3d20 4e6f 6e65 2c0a 2020 2020  nt] = None,.    
+0001c4d0: 7072 6564 6566 696e 6564 5f73 706c 6974  predefined_split
+0001c4e0: 5f6b 6579 3a20 4f70 7469 6f6e 616c 5b73  _key: Optional[s
+0001c4f0: 7472 5d20 3d20 4e6f 6e65 2c0a 2020 2020  tr] = None,.    
+0001c500: 7374 7261 7469 6669 6564 5f73 706c 6974  stratified_split
+0001c510: 5f6b 6579 3a20 4f70 7469 6f6e 616c 5b73  _key: Optional[s
+0001c520: 7472 5d20 3d20 4e6f 6e65 2c0a 2020 2020  tr] = None,.    
+0001c530: 7472 6169 6e69 6e67 5f66 7261 6374 696f  training_fractio
+0001c540: 6e3a 204f 7074 696f 6e61 6c5b 666c 6f61  n: Optional[floa
+0001c550: 745d 203d 204e 6f6e 652c 0a20 2020 2076  t] = None,.    v
+0001c560: 616c 6964 6174 696f 6e5f 6672 6163 7469  alidation_fracti
+0001c570: 6f6e 3a20 4f70 7469 6f6e 616c 5b66 6c6f  on: Optional[flo
+0001c580: 6174 5d20 3d20 4e6f 6e65 2c0a 2020 2020  at] = None,.    
+0001c590: 7465 7374 5f66 7261 6374 696f 6e3a 204f  test_fraction: O
+0001c5a0: 7074 696f 6e61 6c5b 666c 6f61 745d 203d  ptional[float] =
+0001c5b0: 204e 6f6e 652c 0a20 2020 2074 665f 7472   None,.    tf_tr
+0001c5c0: 616e 7366 6f72 6d5f 6578 6563 7574 696f  ansform_executio
+0001c5d0: 6e5f 656e 6769 6e65 3a20 4f70 7469 6f6e  n_engine: Option
+0001c5e0: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
+0001c5f0: 2020 2020 7466 5f61 7574 6f5f 7472 616e      tf_auto_tran
+0001c600: 7366 6f72 6d5f 6665 6174 7572 6573 3a20  sform_features: 
+0001c610: 4f70 7469 6f6e 616c 5b0a 2020 2020 2020  Optional[.      
+0001c620: 2020 556e 696f 6e5b 4c69 7374 5b73 7472    Union[List[str
+0001c630: 5d2c 2044 6963 745b 7374 722c 204c 6973  ], Dict[str, Lis
+0001c640: 745b 7374 725d 5d5d 0a20 2020 205d 203d  t[str]]].    ] =
+0001c650: 204e 6f6e 652c 0a20 2020 2074 665f 6375   None,.    tf_cu
+0001c660: 7374 6f6d 5f74 7261 6e73 666f 726d 6174  stom_transformat
+0001c670: 696f 6e5f 6465 6669 6e69 7469 6f6e 733a  ion_definitions:
+0001c680: 204f 7074 696f 6e61 6c5b 4c69 7374 5b44   Optional[List[D
+0001c690: 6963 745b 7374 722c 2041 6e79 5d5d 5d20  ict[str, Any]]] 
+0001c6a0: 3d20 4e6f 6e65 2c0a 2020 2020 7466 5f74  = None,.    tf_t
+0001c6b0: 7261 6e73 666f 726d 6174 696f 6e73 5f70  ransformations_p
+0001c6c0: 6174 683a 204f 7074 696f 6e61 6c5b 7374  ath: Optional[st
+0001c6d0: 725d 203d 204e 6f6e 652c 0a20 2020 206d  r] = None,.    m
+0001c6e0: 6178 5f73 7465 7073 3a20 696e 7420 3d20  ax_steps: int = 
+0001c6f0: 2d31 2c0a 2020 2020 6d61 785f 7472 6169  -1,.    max_trai
+0001c700: 6e5f 7365 6373 3a20 696e 7420 3d20 2d31  n_secs: int = -1
+0001c710: 2c0a 2020 2020 6c61 7267 655f 6361 7465  ,.    large_cate
+0001c720: 676f 7279 5f64 696d 3a20 696e 7420 3d20  gory_dim: int = 
+0001c730: 312c 0a20 2020 206c 6172 6765 5f63 6174  1,.    large_cat
+0001c740: 6567 6f72 795f 7468 7265 7368 3a20 696e  egory_thresh: in
+0001c750: 7420 3d20 3330 302c 0a20 2020 2079 656f  t = 300,.    yeo
+0001c760: 5f6a 6f68 6e73 6f6e 5f74 7261 6e73 666f  _johnson_transfo
+0001c770: 726d 3a20 626f 6f6c 203d 2054 7275 652c  rm: bool = True,
+0001c780: 0a20 2020 2066 6561 7475 7265 5f64 696d  .    feature_dim
+0001c790: 3a20 696e 7420 3d20 3634 2c0a 2020 2020  : int = 64,.    
+0001c7a0: 6665 6174 7572 655f 6469 6d5f 7261 7469  feature_dim_rati
+0001c7b0: 6f3a 2066 6c6f 6174 203d 2030 2e35 2c0a  o: float = 0.5,.
+0001c7c0: 2020 2020 6e75 6d5f 6465 6369 7369 6f6e      num_decision
+0001c7d0: 5f73 7465 7073 3a20 696e 7420 3d20 362c  _steps: int = 6,
+0001c7e0: 0a20 2020 2072 656c 6178 6174 696f 6e5f  .    relaxation_
+0001c7f0: 6661 6374 6f72 3a20 666c 6f61 7420 3d20  factor: float = 
+0001c800: 312e 352c 0a20 2020 2064 6563 6179 5f65  1.5,.    decay_e
+0001c810: 7665 7279 3a20 666c 6f61 7420 3d20 3130  very: float = 10
+0001c820: 302c 0a20 2020 2064 6563 6179 5f72 6174  0,.    decay_rat
+0001c830: 653a 2066 6c6f 6174 203d 2030 2e39 352c  e: float = 0.95,
+0001c840: 0a20 2020 2067 7261 6469 656e 745f 7468  .    gradient_th
+0001c850: 7265 7368 3a20 666c 6f61 7420 3d20 3230  resh: float = 20
+0001c860: 3030 2c0a 2020 2020 7370 6172 7369 7479  00,.    sparsity
+0001c870: 5f6c 6f73 735f 7765 6967 6874 3a20 666c  _loss_weight: fl
+0001c880: 6f61 7420 3d20 302e 3030 3030 312c 0a20  oat = 0.00001,. 
+0001c890: 2020 2062 6174 6368 5f6d 6f6d 656e 7475     batch_momentu
+0001c8a0: 6d3a 2066 6c6f 6174 203d 2030 2e39 352c  m: float = 0.95,
+0001c8b0: 0a20 2020 2062 6174 6368 5f73 697a 655f  .    batch_size_
+0001c8c0: 7261 7469 6f3a 2066 6c6f 6174 203d 2030  ratio: float = 0
+0001c8d0: 2e32 352c 0a20 2020 206e 756d 5f74 7261  .25,.    num_tra
+0001c8e0: 6e73 666f 726d 6572 5f6c 6179 6572 733a  nsformer_layers:
+0001c8f0: 2069 6e74 203d 2034 2c0a 2020 2020 6e75   int = 4,.    nu
+0001c900: 6d5f 7472 616e 7366 6f72 6d65 725f 6c61  m_transformer_la
+0001c910: 7965 7273 5f72 6174 696f 3a20 666c 6f61  yers_ratio: floa
+0001c920: 7420 3d20 302e 3235 2c0a 2020 2020 636c  t = 0.25,.    cl
+0001c930: 6173 735f 7765 6967 6874 3a20 666c 6f61  ass_weight: floa
+0001c940: 7420 3d20 312e 302c 0a20 2020 206c 6f73  t = 1.0,.    los
+0001c950: 735f 6675 6e63 7469 6f6e 5f74 7970 653a  s_function_type:
+0001c960: 2073 7472 203d 2027 6465 6661 756c 7427   str = 'default'
+0001c970: 2c0a 2020 2020 616c 7068 615f 666f 6361  ,.    alpha_foca
+0001c980: 6c5f 6c6f 7373 3a20 666c 6f61 7420 3d20  l_loss: float = 
+0001c990: 302e 3235 2c0a 2020 2020 6761 6d6d 615f  0.25,.    gamma_
+0001c9a0: 666f 6361 6c5f 6c6f 7373 3a20 666c 6f61  focal_loss: floa
+0001c9b0: 7420 3d20 322e 302c 0a20 2020 2065 6e61  t = 2.0,.    ena
+0001c9c0: 626c 655f 7072 6f66 696c 6572 3a20 626f  ble_profiler: bo
+0001c9d0: 6f6c 203d 2046 616c 7365 2c0a 2020 2020  ol = False,.    
+0001c9e0: 6361 6368 655f 6461 7461 3a20 7374 7220  cache_data: str 
+0001c9f0: 3d20 2761 7574 6f27 2c0a 2020 2020 7365  = 'auto',.    se
+0001ca00: 6564 3a20 696e 7420 3d20 312c 0a20 2020  ed: int = 1,.   
+0001ca10: 2065 7661 6c5f 7374 6570 733a 2069 6e74   eval_steps: int
+0001ca20: 203d 2030 2c0a 2020 2020 6261 7463 685f   = 0,.    batch_
+0001ca30: 7369 7a65 3a20 696e 7420 3d20 3130 302c  size: int = 100,
+0001ca40: 0a20 2020 206d 6561 7375 7265 6d65 6e74  .    measurement
+0001ca50: 5f73 656c 6563 7469 6f6e 5f74 7970 653a  _selection_type:
+0001ca60: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
+0001ca70: 204e 6f6e 652c 0a20 2020 206f 7074 696d   None,.    optim
+0001ca80: 697a 6174 696f 6e5f 6d65 7472 6963 3a20  ization_metric: 
+0001ca90: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
+0001caa0: 4e6f 6e65 2c0a 2020 2020 6576 616c 5f66  None,.    eval_f
+0001cab0: 7265 7175 656e 6379 5f73 6563 733a 2069  requency_secs: i
+0001cac0: 6e74 203d 2036 3030 2c0a 2020 2020 6461  nt = 600,.    da
+0001cad0: 7461 5f73 6f75 7263 655f 6373 765f 6669  ta_source_csv_fi
+0001cae0: 6c65 6e61 6d65 733a 204f 7074 696f 6e61  lenames: Optiona
+0001caf0: 6c5b 7374 725d 203d 204e 6f6e 652c 0a20  l[str] = None,. 
+0001cb00: 2020 2064 6174 615f 736f 7572 6365 5f62     data_source_b
+0001cb10: 6967 7175 6572 795f 7461 626c 655f 7061  igquery_table_pa
+0001cb20: 7468 3a20 4f70 7469 6f6e 616c 5b73 7472  th: Optional[str
+0001cb30: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6269  ] = None,.    bi
+0001cb40: 6771 7565 7279 5f73 7461 6769 6e67 5f66  gquery_staging_f
+0001cb50: 756c 6c5f 6461 7461 7365 745f 6964 3a20  ull_dataset_id: 
+0001cb60: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
+0001cb70: 4e6f 6e65 2c0a 2020 2020 7765 6967 6874  None,.    weight
+0001cb80: 5f63 6f6c 756d 6e3a 2073 7472 203d 2027  _column: str = '
+0001cb90: 272c 0a20 2020 2074 7261 6e73 666f 726d  ',.    transform
+0001cba0: 5f64 6174 6166 6c6f 775f 6d61 6368 696e  _dataflow_machin
+0001cbb0: 655f 7479 7065 3a20 7374 7220 3d20 276e  e_type: str = 'n
+0001cbc0: 312d 7374 616e 6461 7264 2d31 3627 2c0a  1-standard-16',.
+0001cbd0: 2020 2020 7472 616e 7366 6f72 6d5f 6461      transform_da
+0001cbe0: 7461 666c 6f77 5f6d 6178 5f6e 756d 5f77  taflow_max_num_w
+0001cbf0: 6f72 6b65 7273 3a20 696e 7420 3d20 3235  orkers: int = 25
+0001cc00: 2c0a 2020 2020 7472 616e 7366 6f72 6d5f  ,.    transform_
+0001cc10: 6461 7461 666c 6f77 5f64 6973 6b5f 7369  dataflow_disk_si
+0001cc20: 7a65 5f67 623a 2069 6e74 203d 2034 302c  ze_gb: int = 40,
+0001cc30: 0a20 2020 2077 6f72 6b65 725f 706f 6f6c  .    worker_pool
+0001cc40: 5f73 7065 6373 5f6f 7665 7272 6964 653a  _specs_override:
+0001cc50: 204f 7074 696f 6e61 6c5b 4469 6374 5b73   Optional[Dict[s
+0001cc60: 7472 2c20 416e 795d 5d20 3d20 4e6f 6e65  tr, Any]] = None
+0001cc70: 2c0a 2020 2020 7275 6e5f 6576 616c 7561  ,.    run_evalua
+0001cc80: 7469 6f6e 3a20 626f 6f6c 203d 2054 7275  tion: bool = Tru
+0001cc90: 652c 0a20 2020 2065 7661 6c75 6174 696f  e,.    evaluatio
+0001cca0: 6e5f 6261 7463 685f 7072 6564 6963 745f  n_batch_predict_
+0001ccb0: 6d61 6368 696e 655f 7479 7065 3a20 7374  machine_type: st
+0001ccc0: 7220 3d20 5f45 5641 4c55 4154 494f 4e5f  r = _EVALUATION_
+0001ccd0: 4241 5443 485f 5052 4544 4943 545f 4d41  BATCH_PREDICT_MA
+0001cce0: 4348 494e 455f 5459 5045 2c0a 2020 2020  CHINE_TYPE,.    
+0001ccf0: 6576 616c 7561 7469 6f6e 5f62 6174 6368  evaluation_batch
+0001cd00: 5f70 7265 6469 6374 5f73 7461 7274 696e  _predict_startin
+0001cd10: 675f 7265 706c 6963 615f 636f 756e 743a  g_replica_count:
+0001cd20: 2069 6e74 203d 205f 4556 414c 5541 5449   int = _EVALUATI
+0001cd30: 4f4e 5f42 4154 4348 5f50 5245 4449 4354  ON_BATCH_PREDICT
+0001cd40: 5f53 5441 5254 494e 475f 5245 504c 4943  _STARTING_REPLIC
+0001cd50: 415f 434f 554e 542c 0a20 2020 2065 7661  A_COUNT,.    eva
+0001cd60: 6c75 6174 696f 6e5f 6261 7463 685f 7072  luation_batch_pr
+0001cd70: 6564 6963 745f 6d61 785f 7265 706c 6963  edict_max_replic
+0001cd80: 615f 636f 756e 743a 2069 6e74 203d 205f  a_count: int = _
+0001cd90: 4556 414c 5541 5449 4f4e 5f42 4154 4348  EVALUATION_BATCH
+0001cda0: 5f50 5245 4449 4354 5f4d 4158 5f52 4550  _PREDICT_MAX_REP
+0001cdb0: 4c49 4341 5f43 4f55 4e54 2c0a 2020 2020  LICA_COUNT,.    
+0001cdc0: 6576 616c 7561 7469 6f6e 5f64 6174 6166  evaluation_dataf
+0001cdd0: 6c6f 775f 6d61 6368 696e 655f 7479 7065  low_machine_type
+0001cde0: 3a20 7374 7220 3d20 5f45 5641 4c55 4154  : str = _EVALUAT
+0001cdf0: 494f 4e5f 4441 5441 464c 4f57 5f4d 4143  ION_DATAFLOW_MAC
+0001ce00: 4849 4e45 5f54 5950 452c 0a20 2020 2065  HINE_TYPE,.    e
+0001ce10: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
+0001ce20: 6f77 5f73 7461 7274 696e 675f 6e75 6d5f  ow_starting_num_
+0001ce30: 776f 726b 6572 733a 2069 6e74 203d 205f  workers: int = _
+0001ce40: 4556 414c 5541 5449 4f4e 5f44 4154 4146  EVALUATION_DATAF
+0001ce50: 4c4f 575f 5354 4152 5449 4e47 5f4e 554d  LOW_STARTING_NUM
+0001ce60: 5f57 4f52 4b45 5253 2c0a 2020 2020 6576  _WORKERS,.    ev
+0001ce70: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
 0001ce80: 775f 6d61 785f 6e75 6d5f 776f 726b 6572  w_max_num_worker
-0001ce90: 733a 2069 6e74 203d 2032 352c 0a20 2020  s: int = 25,.   
-0001cea0: 2074 7261 6e73 666f 726d 5f64 6174 6166   transform_dataf
-0001ceb0: 6c6f 775f 6469 736b 5f73 697a 655f 6762  low_disk_size_gb
-0001cec0: 3a20 696e 7420 3d20 3430 2c0a 2020 2020  : int = 40,.    
-0001ced0: 776f 726b 6572 5f70 6f6f 6c5f 7370 6563  worker_pool_spec
-0001cee0: 735f 6f76 6572 7269 6465 3a20 4f70 7469  s_override: Opti
-0001cef0: 6f6e 616c 5b44 6963 745b 7374 722c 2041  onal[Dict[str, A
-0001cf00: 6e79 5d5d 203d 204e 6f6e 652c 0a20 2020  ny]] = None,.   
-0001cf10: 2072 756e 5f65 7661 6c75 6174 696f 6e3a   run_evaluation:
-0001cf20: 2062 6f6f 6c20 3d20 5472 7565 2c0a 2020   bool = True,.  
-0001cf30: 2020 6576 616c 7561 7469 6f6e 5f62 6174    evaluation_bat
-0001cf40: 6368 5f70 7265 6469 6374 5f6d 6163 6869  ch_predict_machi
-0001cf50: 6e65 5f74 7970 653a 2073 7472 203d 205f  ne_type: str = _
-0001cf60: 4556 414c 5541 5449 4f4e 5f42 4154 4348  EVALUATION_BATCH
-0001cf70: 5f50 5245 4449 4354 5f4d 4143 4849 4e45  _PREDICT_MACHINE
-0001cf80: 5f54 5950 452c 0a20 2020 2065 7661 6c75  _TYPE,.    evalu
-0001cf90: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
-0001cfa0: 6963 745f 7374 6172 7469 6e67 5f72 6570  ict_starting_rep
-0001cfb0: 6c69 6361 5f63 6f75 6e74 3a20 696e 7420  lica_count: int 
-0001cfc0: 3d20 5f45 5641 4c55 4154 494f 4e5f 4241  = _EVALUATION_BA
-0001cfd0: 5443 485f 5052 4544 4943 545f 5354 4152  TCH_PREDICT_STAR
-0001cfe0: 5449 4e47 5f52 4550 4c49 4341 5f43 4f55  TING_REPLICA_COU
-0001cff0: 4e54 2c0a 2020 2020 6576 616c 7561 7469  NT,.    evaluati
-0001d000: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
-0001d010: 5f6d 6178 5f72 6570 6c69 6361 5f63 6f75  _max_replica_cou
-0001d020: 6e74 3a20 696e 7420 3d20 5f45 5641 4c55  nt: int = _EVALU
-0001d030: 4154 494f 4e5f 4241 5443 485f 5052 4544  ATION_BATCH_PRED
-0001d040: 4943 545f 4d41 585f 5245 504c 4943 415f  ICT_MAX_REPLICA_
-0001d050: 434f 554e 542c 0a20 2020 2065 7661 6c75  COUNT,.    evalu
-0001d060: 6174 696f 6e5f 6461 7461 666c 6f77 5f6d  ation_dataflow_m
-0001d070: 6163 6869 6e65 5f74 7970 653a 2073 7472  achine_type: str
-0001d080: 203d 205f 4556 414c 5541 5449 4f4e 5f44   = _EVALUATION_D
-0001d090: 4154 4146 4c4f 575f 4d41 4348 494e 455f  ATAFLOW_MACHINE_
-0001d0a0: 5459 5045 2c0a 2020 2020 6576 616c 7561  TYPE,.    evalua
-0001d0b0: 7469 6f6e 5f64 6174 6166 6c6f 775f 7374  tion_dataflow_st
-0001d0c0: 6172 7469 6e67 5f6e 756d 5f77 6f72 6b65  arting_num_worke
-0001d0d0: 7273 3a20 696e 7420 3d20 5f45 5641 4c55  rs: int = _EVALU
-0001d0e0: 4154 494f 4e5f 4441 5441 464c 4f57 5f53  ATION_DATAFLOW_S
-0001d0f0: 5441 5254 494e 475f 4e55 4d5f 574f 524b  TARTING_NUM_WORK
-0001d100: 4552 532c 0a20 2020 2065 7661 6c75 6174  ERS,.    evaluat
-0001d110: 696f 6e5f 6461 7461 666c 6f77 5f6d 6178  ion_dataflow_max
-0001d120: 5f6e 756d 5f77 6f72 6b65 7273 3a20 696e  _num_workers: in
-0001d130: 7420 3d20 5f45 5641 4c55 4154 494f 4e5f  t = _EVALUATION_
-0001d140: 4441 5441 464c 4f57 5f4d 4158 5f4e 554d  DATAFLOW_MAX_NUM
-0001d150: 5f57 4f52 4b45 5253 2c0a 2020 2020 6576  _WORKERS,.    ev
-0001d160: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
-0001d170: 775f 6469 736b 5f73 697a 655f 6762 3a20  w_disk_size_gb: 
-0001d180: 696e 7420 3d20 5f45 5641 4c55 4154 494f  int = _EVALUATIO
-0001d190: 4e5f 4441 5441 464c 4f57 5f44 4953 4b5f  N_DATAFLOW_DISK_
-0001d1a0: 5349 5a45 5f47 422c 0a20 2020 2064 6174  SIZE_GB,.    dat
-0001d1b0: 6166 6c6f 775f 7365 7276 6963 655f 6163  aflow_service_ac
-0001d1c0: 636f 756e 743a 2073 7472 203d 2027 272c  count: str = '',
-0001d1d0: 0a20 2020 2064 6174 6166 6c6f 775f 7375  .    dataflow_su
-0001d1e0: 626e 6574 776f 726b 3a20 7374 7220 3d20  bnetwork: str = 
-0001d1f0: 2727 2c0a 2020 2020 6461 7461 666c 6f77  '',.    dataflow
-0001d200: 5f75 7365 5f70 7562 6c69 635f 6970 733a  _use_public_ips:
-0001d210: 2062 6f6f 6c20 3d20 5472 7565 2c0a 2020   bool = True,.  
-0001d220: 2020 656e 6372 7970 7469 6f6e 5f73 7065    encryption_spe
-0001d230: 635f 6b65 795f 6e61 6d65 3a20 7374 7220  c_key_name: str 
-0001d240: 3d20 2727 2c0a 2920 2d3e 2054 7570 6c65  = '',.) -> Tuple
-0001d250: 5b73 7472 2c20 4469 6374 5b73 7472 2c20  [str, Dict[str, 
-0001d260: 416e 795d 5d3a 0a20 2022 2222 4765 7420  Any]]:.  """Get 
-0001d270: 7468 6520 5461 624e 6574 2074 7261 696e  the TabNet train
-0001d280: 696e 6720 7069 7065 6c69 6e65 2e0a 0a20  ing pipeline... 
-0001d290: 2041 7267 733a 0a20 2020 2070 726f 6a65   Args:.    proje
-0001d2a0: 6374 3a20 5468 6520 4743 5020 7072 6f6a  ct: The GCP proj
-0001d2b0: 6563 7420 7468 6174 2072 756e 7320 7468  ect that runs th
-0001d2c0: 6520 7069 7065 6c69 6e65 2063 6f6d 706f  e pipeline compo
-0001d2d0: 6e65 6e74 732e 0a20 2020 206c 6f63 6174  nents..    locat
-0001d2e0: 696f 6e3a 2054 6865 2047 4350 2072 6567  ion: The GCP reg
-0001d2f0: 696f 6e20 7468 6174 2072 756e 7320 7468  ion that runs th
-0001d300: 6520 7069 7065 6c69 6e65 2063 6f6d 706f  e pipeline compo
-0001d310: 6e65 6e74 732e 0a20 2020 2072 6f6f 745f  nents..    root_
-0001d320: 6469 723a 2054 6865 2072 6f6f 7420 4743  dir: The root GC
-0001d330: 5320 6469 7265 6374 6f72 7920 666f 7220  S directory for 
-0001d340: 7468 6520 7069 7065 6c69 6e65 2063 6f6d  the pipeline com
-0001d350: 706f 6e65 6e74 732e 0a20 2020 2074 6172  ponents..    tar
-0001d360: 6765 745f 636f 6c75 6d6e 3a20 5468 6520  get_column: The 
-0001d370: 7461 7267 6574 2063 6f6c 756d 6e20 6e61  target column na
-0001d380: 6d65 2e0a 2020 2020 7072 6564 6963 7469  me..    predicti
-0001d390: 6f6e 5f74 7970 653a 2054 6865 2074 7970  on_type: The typ
-0001d3a0: 6520 6f66 2070 7265 6469 6374 696f 6e20  e of prediction 
-0001d3b0: 7468 6520 6d6f 6465 6c20 6973 2074 6f20  the model is to 
-0001d3c0: 7072 6f64 7563 652e 0a20 2020 2020 2022  produce..      "
-0001d3d0: 636c 6173 7369 6669 6361 7469 6f6e 2220  classification" 
-0001d3e0: 6f72 2022 7265 6772 6573 7369 6f6e 222e  or "regression".
-0001d3f0: 0a20 2020 206c 6561 726e 696e 675f 7261  .    learning_ra
-0001d400: 7465 3a20 5468 6520 6c65 6172 6e69 6e67  te: The learning
-0001d410: 2072 6174 6520 7573 6564 2062 7920 7468   rate used by th
-0001d420: 6520 6c69 6e65 6172 206f 7074 696d 697a  e linear optimiz
-0001d430: 6572 2e0a 2020 2020 7472 616e 7366 6f72  er..    transfor
-0001d440: 6d5f 636f 6e66 6967 3a20 5061 7468 2074  m_config: Path t
-0001d450: 6f20 7631 2054 4620 7472 616e 7366 6f72  o v1 TF transfor
-0001d460: 6d61 7469 6f6e 2063 6f6e 6669 6775 7261  mation configura
-0001d470: 7469 6f6e 2e0a 2020 2020 6461 7461 7365  tion..    datase
-0001d480: 745f 6c65 7665 6c5f 6375 7374 6f6d 5f74  t_level_custom_t
-0001d490: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
-0001d4a0: 6669 6e69 7469 6f6e 733a 2044 6174 6173  finitions: Datas
-0001d4b0: 6574 2d6c 6576 656c 2063 7573 746f 6d0a  et-level custom.
-0001d4c0: 2020 2020 2020 7472 616e 7366 6f72 6d61        transforma
-0001d4d0: 7469 6f6e 2064 6566 696e 6974 696f 6e73  tion definitions
-0001d4e0: 2069 6e20 7374 7269 6e67 2066 6f72 6d61   in string forma
-0001d4f0: 742e 0a20 2020 2064 6174 6173 6574 5f6c  t..    dataset_l
-0001d500: 6576 656c 5f74 7261 6e73 666f 726d 6174  evel_transformat
-0001d510: 696f 6e73 3a20 4461 7461 7365 742d 6c65  ions: Dataset-le
-0001d520: 7665 6c20 7472 616e 7366 6f72 6d61 7469  vel transformati
-0001d530: 6f6e 2063 6f6e 6669 6775 7261 7469 6f6e  on configuration
-0001d540: 2069 6e0a 2020 2020 2020 7374 7269 6e67   in.      string
-0001d550: 2066 6f72 6d61 742e 0a20 2020 2072 756e   format..    run
-0001d560: 5f66 6561 7475 7265 5f73 656c 6563 7469  _feature_selecti
-0001d570: 6f6e 3a20 5768 6574 6865 7220 746f 2065  on: Whether to e
-0001d580: 6e61 626c 6520 6665 6174 7572 6520 7365  nable feature se
-0001d590: 6c65 6374 696f 6e2e 0a20 2020 2066 6561  lection..    fea
-0001d5a0: 7475 7265 5f73 656c 6563 7469 6f6e 5f61  ture_selection_a
-0001d5b0: 6c67 6f72 6974 686d 3a20 4665 6174 7572  lgorithm: Featur
-0001d5c0: 6520 7365 6c65 6374 696f 6e20 616c 676f  e selection algo
-0001d5d0: 7269 7468 6d2e 0a20 2020 206d 6174 6572  rithm..    mater
-0001d5e0: 6961 6c69 7a65 645f 6578 616d 706c 6573  ialized_examples
-0001d5f0: 5f66 6f72 6d61 743a 2054 6865 2066 6f72  _format: The for
-0001d600: 6d61 7420 666f 7220 7468 6520 6d61 7465  mat for the mate
-0001d610: 7269 616c 697a 6564 2065 7861 6d70 6c65  rialized example
-0001d620: 732e 0a20 2020 206d 6178 5f73 656c 6563  s..    max_selec
-0001d630: 7465 645f 6665 6174 7572 6573 3a20 4d61  ted_features: Ma
-0001d640: 7869 6d75 6d20 6e75 6d62 6572 206f 6620  ximum number of 
-0001d650: 6665 6174 7572 6573 2074 6f20 7365 6c65  features to sele
-0001d660: 6374 2e0a 2020 2020 7072 6564 6566 696e  ct..    predefin
-0001d670: 6564 5f73 706c 6974 5f6b 6579 3a20 5072  ed_split_key: Pr
-0001d680: 6564 6566 696e 6564 2073 706c 6974 206b  edefined split k
-0001d690: 6579 2e0a 2020 2020 7374 7261 7469 6669  ey..    stratifi
-0001d6a0: 6564 5f73 706c 6974 5f6b 6579 3a20 5374  ed_split_key: St
-0001d6b0: 7261 7469 6669 6564 2073 706c 6974 206b  ratified split k
-0001d6c0: 6579 2e0a 2020 2020 7472 6169 6e69 6e67  ey..    training
-0001d6d0: 5f66 7261 6374 696f 6e3a 2054 7261 696e  _fraction: Train
-0001d6e0: 696e 6720 6672 6163 7469 6f6e 2e0a 2020  ing fraction..  
-0001d6f0: 2020 7661 6c69 6461 7469 6f6e 5f66 7261    validation_fra
-0001d700: 6374 696f 6e3a 2056 616c 6964 6174 696f  ction: Validatio
-0001d710: 6e20 6672 6163 7469 6f6e 2e0a 2020 2020  n fraction..    
-0001d720: 7465 7374 5f66 7261 6374 696f 6e3a 2054  test_fraction: T
-0001d730: 6573 7420 6672 6163 7469 6f6e 2e0a 2020  est fraction..  
-0001d740: 2020 7466 5f74 7261 6e73 666f 726d 5f65    tf_transform_e
-0001d750: 7865 6375 7469 6f6e 5f65 6e67 696e 653a  xecution_engine:
-0001d760: 2054 6865 2065 7865 6375 7469 6f6e 2065   The execution e
-0001d770: 6e67 696e 6520 7573 6564 2074 6f20 6578  ngine used to ex
-0001d780: 6563 7574 6520 5446 2d62 6173 6564 0a20  ecute TF-based. 
-0001d790: 2020 2020 2074 7261 6e73 666f 726d 6174       transformat
-0001d7a0: 696f 6e73 2e0a 2020 2020 7466 5f61 7574  ions..    tf_aut
-0001d7b0: 6f5f 7472 616e 7366 6f72 6d5f 6665 6174  o_transform_feat
-0001d7c0: 7572 6573 3a20 4c69 7374 206f 6620 6175  ures: List of au
-0001d7d0: 746f 2074 7261 6e73 666f 726d 2066 6561  to transform fea
-0001d7e0: 7475 7265 7320 696e 2074 6865 0a20 2020  tures in the.   
-0001d7f0: 2020 2063 6f6d 6d61 2d73 6570 6172 6174     comma-separat
-0001d800: 6564 2073 7472 696e 6720 666f 726d 6174  ed string format
-0001d810: 2e0a 2020 2020 7466 5f63 7573 746f 6d5f  ..    tf_custom_
-0001d820: 7472 616e 7366 6f72 6d61 7469 6f6e 5f64  transformation_d
-0001d830: 6566 696e 6974 696f 6e73 3a20 5446 2063  efinitions: TF c
-0001d840: 7573 746f 6d20 7472 616e 7366 6f72 6d61  ustom transforma
-0001d850: 7469 6f6e 2064 6566 696e 6974 696f 6e73  tion definitions
-0001d860: 0a20 2020 2020 2069 6e20 7374 7269 6e67  .      in string
-0001d870: 2066 6f72 6d61 742e 0a20 2020 2074 665f   format..    tf_
-0001d880: 7472 616e 7366 6f72 6d61 7469 6f6e 735f  transformations_
-0001d890: 7061 7468 3a20 5061 7468 2074 6f20 5446  path: Path to TF
-0001d8a0: 2074 7261 6e73 666f 726d 6174 696f 6e20   transformation 
-0001d8b0: 636f 6e66 6967 7572 6174 696f 6e2e 0a20  configuration.. 
-0001d8c0: 2020 206d 6178 5f73 7465 7073 3a20 4e75     max_steps: Nu
-0001d8d0: 6d62 6572 206f 6620 7374 6570 7320 746f  mber of steps to
-0001d8e0: 2072 756e 2074 6865 2074 7261 696e 6572   run the trainer
-0001d8f0: 2066 6f72 2e0a 2020 2020 6d61 785f 7472   for..    max_tr
-0001d900: 6169 6e5f 7365 6373 3a20 416d 6f75 6e74  ain_secs: Amount
-0001d910: 206f 6620 7469 6d65 2069 6e20 7365 636f   of time in seco
-0001d920: 6e64 7320 746f 2072 756e 2074 6865 2074  nds to run the t
-0001d930: 7261 696e 6572 2066 6f72 2e0a 2020 2020  rainer for..    
-0001d940: 6c61 7267 655f 6361 7465 676f 7279 5f64  large_category_d
-0001d950: 696d 3a20 456d 6265 6464 696e 6720 6469  im: Embedding di
-0001d960: 6d65 6e73 696f 6e20 666f 7220 6361 7465  mension for cate
-0001d970: 676f 7269 6361 6c20 6665 6174 7572 6520  gorical feature 
-0001d980: 7769 7468 206c 6172 6765 0a20 2020 2020  with large.     
-0001d990: 206e 756d 6265 7220 6f66 2063 6174 6567   number of categ
-0001d9a0: 6f72 6965 732e 0a20 2020 206c 6172 6765  ories..    large
-0001d9b0: 5f63 6174 6567 6f72 795f 7468 7265 7368  _category_thresh
-0001d9c0: 3a20 5468 7265 7368 6f6c 6420 666f 7220  : Threshold for 
-0001d9d0: 6e75 6d62 6572 206f 6620 6361 7465 676f  number of catego
-0001d9e0: 7269 6573 2074 6f20 6170 706c 790a 2020  ries to apply.  
-0001d9f0: 2020 2020 6c61 7267 655f 6361 7465 676f      large_catego
-0001da00: 7279 5f64 696d 2065 6d62 6564 6469 6e67  ry_dim embedding
-0001da10: 2064 696d 656e 7369 6f6e 2074 6f2e 0a20   dimension to.. 
-0001da20: 2020 2079 656f 5f6a 6f68 6e73 6f6e 5f74     yeo_johnson_t
-0001da30: 7261 6e73 666f 726d 3a20 456e 6162 6c65  ransform: Enable
-0001da40: 7320 7472 6169 6e61 626c 6520 5965 6f2d  s trainable Yeo-
-0001da50: 4a6f 686e 736f 6e20 706f 7765 7220 7472  Johnson power tr
-0001da60: 616e 7366 6f72 6d2e 0a20 2020 2066 6561  ansform..    fea
-0001da70: 7475 7265 5f64 696d 3a20 4469 6d65 6e73  ture_dim: Dimens
-0001da80: 696f 6e61 6c69 7479 206f 6620 7468 6520  ionality of the 
-0001da90: 6869 6464 656e 2072 6570 7265 7365 6e74  hidden represent
-0001daa0: 6174 696f 6e20 696e 2066 6561 7475 7265  ation in feature
-0001dab0: 0a20 2020 2020 2074 7261 6e73 666f 726d  .      transform
-0001dac0: 6174 696f 6e20 626c 6f63 6b2e 0a20 2020  ation block..   
-0001dad0: 2066 6561 7475 7265 5f64 696d 5f72 6174   feature_dim_rat
-0001dae0: 696f 3a20 5468 6520 7261 7469 6f20 6f66  io: The ratio of
-0001daf0: 206f 7574 7075 7420 6469 6d65 6e73 696f   output dimensio
-0001db00: 6e20 2864 696d 656e 7369 6f6e 616c 6974  n (dimensionalit
-0001db10: 7920 6f66 2074 6865 0a20 2020 2020 206f  y of the.      o
-0001db20: 7574 7075 7473 206f 6620 6561 6368 2064  utputs of each d
-0001db30: 6563 6973 696f 6e20 7374 6570 2920 746f  ecision step) to
-0001db40: 2066 6561 7475 7265 2064 696d 656e 7369   feature dimensi
-0001db50: 6f6e 2e0a 2020 2020 6e75 6d5f 6465 6369  on..    num_deci
-0001db60: 7369 6f6e 5f73 7465 7073 3a20 4e75 6d62  sion_steps: Numb
-0001db70: 6572 206f 6620 7365 7175 656e 7469 616c  er of sequential
-0001db80: 2064 6563 6973 696f 6e20 7374 6570 732e   decision steps.
-0001db90: 0a20 2020 2072 656c 6178 6174 696f 6e5f  .    relaxation_
-0001dba0: 6661 6374 6f72 3a20 5265 6c61 7861 7469  factor: Relaxati
-0001dbb0: 6f6e 2066 6163 746f 7220 7468 6174 2070  on factor that p
-0001dbc0: 726f 6d6f 7465 7320 7468 6520 7265 7573  romotes the reus
-0001dbd0: 6520 6f66 2065 6163 6820 6665 6174 7572  e of each featur
-0001dbe0: 650a 2020 2020 2020 6174 2064 6966 6665  e.      at diffe
-0001dbf0: 7265 6e74 2064 6563 6973 696f 6e20 7374  rent decision st
-0001dc00: 6570 732e 2057 6865 6e20 6974 2069 7320  eps. When it is 
-0001dc10: 312c 2061 2066 6561 7475 7265 2069 7320  1, a feature is 
-0001dc20: 656e 666f 7263 6564 2074 6f20 6265 0a20  enforced to be. 
-0001dc30: 2020 2020 2075 7365 6420 6f6e 6c79 2061       used only a
-0001dc40: 7420 6f6e 6520 6465 6369 7369 6f6e 2073  t one decision s
-0001dc50: 7465 7020 616e 6420 6173 2069 7420 696e  tep and as it in
-0001dc60: 6372 6561 7365 732c 206d 6f72 6520 666c  creases, more fl
-0001dc70: 6578 6962 696c 6974 7920 6973 0a20 2020  exibility is.   
-0001dc80: 2020 2070 726f 7669 6465 6420 746f 2075     provided to u
-0001dc90: 7365 2061 2066 6561 7475 7265 2061 7420  se a feature at 
-0001dca0: 6d75 6c74 6970 6c65 2064 6563 6973 696f  multiple decisio
-0001dcb0: 6e20 7374 6570 732e 0a20 2020 2064 6563  n steps..    dec
-0001dcc0: 6179 5f65 7665 7279 3a20 4e75 6d62 6572  ay_every: Number
-0001dcd0: 206f 6620 6974 6572 6174 696f 6e73 2066   of iterations f
-0001dce0: 6f72 2070 6572 696f 6469 6361 6c6c 7920  or periodically 
-0001dcf0: 6170 706c 7969 6e67 206c 6561 726e 696e  applying learnin
-0001dd00: 6720 7261 7465 0a20 2020 2020 2064 6563  g rate.      dec
-0001dd10: 6179 696e 672e 0a20 2020 2064 6563 6179  aying..    decay
-0001dd20: 5f72 6174 653a 204c 6561 726e 696e 6720  _rate: Learning 
-0001dd30: 7261 7465 2064 6563 6179 696e 672e 0a20  rate decaying.. 
-0001dd40: 2020 2067 7261 6469 656e 745f 7468 7265     gradient_thre
-0001dd50: 7368 3a20 5468 7265 7368 6f6c 6420 666f  sh: Threshold fo
-0001dd60: 7220 7468 6520 6e6f 726d 206f 6620 6772  r the norm of gr
-0001dd70: 6164 6965 6e74 7320 666f 7220 636c 6970  adients for clip
-0001dd80: 7069 6e67 2e0a 2020 2020 7370 6172 7369  ping..    sparsi
-0001dd90: 7479 5f6c 6f73 735f 7765 6967 6874 3a20  ty_loss_weight: 
-0001dda0: 5765 6967 6874 206f 6620 7468 6520 6c6f  Weight of the lo
-0001ddb0: 7373 2066 6f72 2073 7061 7273 6974 7920  ss for sparsity 
-0001ddc0: 7265 6775 6c61 7269 7a61 7469 6f6e 0a20  regularization. 
-0001ddd0: 2020 2020 2028 696e 6372 6561 7369 6e67       (increasing
-0001dde0: 2069 7420 7769 6c6c 2079 6965 6c64 206d   it will yield m
-0001ddf0: 6f72 6520 7370 6172 7365 2066 6561 7475  ore sparse featu
-0001de00: 7265 2073 656c 6563 7469 6f6e 292e 0a20  re selection).. 
-0001de10: 2020 2062 6174 6368 5f6d 6f6d 656e 7475     batch_momentu
-0001de20: 6d3a 204d 6f6d 656e 7475 6d20 696e 2067  m: Momentum in g
-0001de30: 686f 7374 2062 6174 6368 206e 6f72 6d61  host batch norma
-0001de40: 6c69 7a61 7469 6f6e 2e0a 2020 2020 6261  lization..    ba
-0001de50: 7463 685f 7369 7a65 5f72 6174 696f 3a20  tch_size_ratio: 
-0001de60: 5468 6520 7261 7469 6f20 6f66 2076 6972  The ratio of vir
-0001de70: 7475 616c 2062 6174 6368 2073 697a 6520  tual batch size 
-0001de80: 2873 697a 6520 6f66 2074 6865 2067 686f  (size of the gho
-0001de90: 7374 2062 6174 6368 0a20 2020 2020 206e  st batch.      n
-0001dea0: 6f72 6d61 6c69 7a61 7469 6f6e 2920 746f  ormalization) to
-0001deb0: 2062 6174 6368 2073 697a 652e 0a20 2020   batch size..   
-0001dec0: 206e 756d 5f74 7261 6e73 666f 726d 6572   num_transformer
-0001ded0: 5f6c 6179 6572 733a 2054 6865 206e 756d  _layers: The num
-0001dee0: 6265 7220 6f66 2074 7261 6e73 666f 726d  ber of transform
-0001def0: 6572 206c 6179 6572 7320 666f 7220 6561  er layers for ea
-0001df00: 6368 2064 6563 6973 696f 6e0a 2020 2020  ch decision.    
-0001df10: 2020 7374 6570 2e20 7573 6564 206f 6e6c    step. used onl
-0001df20: 7920 6174 206f 6e65 2064 6563 6973 696f  y at one decisio
-0001df30: 6e20 7374 6570 2061 6e64 2061 7320 6974  n step and as it
-0001df40: 2069 6e63 7265 6173 6573 2c20 6d6f 7265   increases, more
-0001df50: 2066 6c65 7869 6269 6c69 7479 0a20 2020   flexibility.   
-0001df60: 2020 2069 7320 7072 6f76 6964 6564 2074     is provided t
-0001df70: 6f20 7573 6520 6120 6665 6174 7572 6520  o use a feature 
-0001df80: 6174 206d 756c 7469 706c 6520 6465 6369  at multiple deci
-0001df90: 7369 6f6e 2073 7465 7073 2e0a 2020 2020  sion steps..    
-0001dfa0: 6e75 6d5f 7472 616e 7366 6f72 6d65 725f  num_transformer_
-0001dfb0: 6c61 7965 7273 5f72 6174 696f 3a20 5468  layers_ratio: Th
-0001dfc0: 6520 7261 7469 6f20 6f66 2073 6861 7265  e ratio of share
-0001dfd0: 6420 7472 616e 7366 6f72 6d65 7220 6c61  d transformer la
-0001dfe0: 7965 7220 746f 0a20 2020 2020 2074 7261  yer to.      tra
-0001dff0: 6e73 666f 726d 6572 206c 6179 6572 732e  nsformer layers.
-0001e000: 0a20 2020 2063 6c61 7373 5f77 6569 6768  .    class_weigh
-0001e010: 743a 2054 6865 2063 6c61 7373 2077 6569  t: The class wei
-0001e020: 6768 7420 6973 2075 7365 6420 746f 2063  ght is used to c
-0001e030: 6f6d 7075 7465 7320 6120 7765 6967 6874  omputes a weight
-0001e040: 6564 2063 726f 7373 2065 6e74 726f 7079  ed cross entropy
-0001e050: 0a20 2020 2020 2077 6869 6368 2069 7320  .      which is 
-0001e060: 6865 6c70 6675 6c20 696e 2063 6c61 7373  helpful in class
-0001e070: 6966 7920 696d 6261 6c61 6e63 6564 2064  ify imbalanced d
-0001e080: 6174 6173 6574 2e20 4f6e 6c79 2075 7365  ataset. Only use
-0001e090: 6420 666f 720a 2020 2020 2020 636c 6173  d for.      clas
-0001e0a0: 7369 6669 6361 7469 6f6e 2e0a 2020 2020  sification..    
-0001e0b0: 6c6f 7373 5f66 756e 6374 696f 6e5f 7479  loss_function_ty
-0001e0c0: 7065 3a20 4c6f 7373 2066 756e 6374 696f  pe: Loss functio
-0001e0d0: 6e20 7479 7065 2e20 4c6f 7373 2066 756e  n type. Loss fun
-0001e0e0: 6374 696f 6e20 696e 2063 6c61 7373 6966  ction in classif
-0001e0f0: 6963 6174 696f 6e0a 2020 2020 2020 5b63  ication.      [c
-0001e100: 726f 7373 5f65 6e74 726f 7079 2c20 7765  ross_entropy, we
-0001e110: 6967 6874 6564 5f63 726f 7373 5f65 6e74  ighted_cross_ent
-0001e120: 726f 7079 2c20 666f 6361 6c5f 6c6f 7373  ropy, focal_loss
-0001e130: 5d2c 2064 6566 6175 6c74 2069 730a 2020  ], default is.  
-0001e140: 2020 2020 6372 6f73 735f 656e 7472 6f70      cross_entrop
-0001e150: 792e 204c 6f73 7320 6675 6e63 7469 6f6e  y. Loss function
-0001e160: 2069 6e20 7265 6772 6573 7369 6f6e 3a20   in regression: 
-0001e170: 5b72 6d73 652c 206d 6165 2c20 6d73 655d  [rmse, mae, mse]
-0001e180: 2c20 6465 6661 756c 7420 6973 0a20 2020  , default is.   
-0001e190: 2020 206d 7365 2e0a 2020 2020 616c 7068     mse..    alph
-0001e1a0: 615f 666f 6361 6c5f 6c6f 7373 3a20 416c  a_focal_loss: Al
-0001e1b0: 7068 6120 7661 6c75 6520 2862 616c 616e  pha value (balan
-0001e1c0: 6369 6e67 2066 6163 746f 7229 2069 6e20  cing factor) in 
-0001e1d0: 666f 6361 6c5f 6c6f 7373 2066 756e 6374  focal_loss funct
-0001e1e0: 696f 6e2e 0a20 2020 2020 204f 6e6c 7920  ion..      Only 
-0001e1f0: 7573 6564 2066 6f72 2063 6c61 7373 6966  used for classif
-0001e200: 6963 6174 696f 6e2e 0a20 2020 2067 616d  ication..    gam
-0001e210: 6d61 5f66 6f63 616c 5f6c 6f73 733a 2047  ma_focal_loss: G
-0001e220: 616d 6d61 2076 616c 7565 2028 6d6f 6475  amma value (modu
-0001e230: 6c61 7469 6e67 2066 6163 746f 7229 2066  lating factor) f
-0001e240: 6f72 2066 6f63 616c 206c 6f73 7320 666f  or focal loss fo
-0001e250: 7220 666f 6361 6c0a 2020 2020 2020 6c6f  r focal.      lo
-0001e260: 7373 2e20 4f6e 6c79 2075 7365 6420 666f  ss. Only used fo
-0001e270: 7220 636c 6173 7369 6669 6361 7469 6f6e  r classification
-0001e280: 2e0a 2020 2020 656e 6162 6c65 5f70 726f  ..    enable_pro
-0001e290: 6669 6c65 723a 2045 6e61 626c 6573 2070  filer: Enables p
-0001e2a0: 726f 6669 6c69 6e67 2061 6e64 2073 6176  rofiling and sav
-0001e2b0: 6573 2061 2074 7261 6365 2064 7572 696e  es a trace durin
-0001e2c0: 6720 6576 616c 7561 7469 6f6e 2e0a 2020  g evaluation..  
-0001e2d0: 2020 6361 6368 655f 6461 7461 3a20 5768    cache_data: Wh
-0001e2e0: 6574 6865 7220 746f 2063 6163 6865 2064  ether to cache d
-0001e2f0: 6174 6120 6f72 206e 6f74 2e20 4966 2073  ata or not. If s
-0001e300: 6574 2074 6f20 2761 7574 6f27 2c20 6361  et to 'auto', ca
-0001e310: 6368 696e 6720 6973 0a20 2020 2020 2064  ching is.      d
-0001e320: 6574 6572 6d69 6e65 6420 6261 7365 6420  etermined based 
-0001e330: 6f6e 2074 6865 2064 6174 6173 6574 2073  on the dataset s
-0001e340: 697a 652e 0a20 2020 2073 6565 643a 2053  ize..    seed: S
-0001e350: 6565 6420 746f 2062 6520 7573 6564 2066  eed to be used f
-0001e360: 6f72 2074 6869 7320 7275 6e2e 0a20 2020  or this run..   
-0001e370: 2065 7661 6c5f 7374 6570 733a 204e 756d   eval_steps: Num
-0001e380: 6265 7220 6f66 2073 7465 7073 2074 6f20  ber of steps to 
-0001e390: 7275 6e20 6576 616c 7561 7469 6f6e 2066  run evaluation f
-0001e3a0: 6f72 2e20 4966 206e 6f74 2073 7065 6369  or. If not speci
-0001e3b0: 6669 6564 206f 720a 2020 2020 2020 6e65  fied or.      ne
-0001e3c0: 6761 7469 7665 2c20 6974 206d 6561 6e73  gative, it means
-0001e3d0: 2072 756e 2065 7661 6c75 6174 696f 6e20   run evaluation 
-0001e3e0: 6f6e 2074 6865 2077 686f 6c65 2076 616c  on the whole val
-0001e3f0: 6964 6174 696f 6e20 6461 7461 7365 742e  idation dataset.
-0001e400: 2049 6620 7365 740a 2020 2020 2020 746f   If set.      to
-0001e410: 2030 2c20 6974 206d 6561 6e73 2072 756e   0, it means run
-0001e420: 2065 7661 6c75 6174 696f 6e20 666f 7220   evaluation for 
-0001e430: 6120 6669 7865 6420 6e75 6d62 6572 206f  a fixed number o
-0001e440: 6620 7361 6d70 6c65 732e 0a20 2020 2062  f samples..    b
-0001e450: 6174 6368 5f73 697a 653a 2042 6174 6368  atch_size: Batch
-0001e460: 2073 697a 6520 666f 7220 7472 6169 6e69   size for traini
-0001e470: 6e67 2e0a 2020 2020 6d65 6173 7572 656d  ng..    measurem
-0001e480: 656e 745f 7365 6c65 6374 696f 6e5f 7479  ent_selection_ty
-0001e490: 7065 3a20 5768 6963 6820 6d65 6173 7572  pe: Which measur
-0001e4a0: 656d 656e 7420 746f 2075 7365 2069 662f  ement to use if/
-0001e4b0: 7768 656e 2074 6865 2073 6572 7669 6365  when the service
-0001e4c0: 0a20 2020 2020 2061 7574 6f6d 6174 6963  .      automatic
-0001e4d0: 616c 6c79 2073 656c 6563 7473 2074 6865  ally selects the
-0001e4e0: 2066 696e 616c 206d 6561 7375 7265 6d65   final measureme
-0001e4f0: 6e74 2066 726f 6d20 7072 6576 696f 7573  nt from previous
-0001e500: 6c79 2072 6570 6f72 7465 640a 2020 2020  ly reported.    
-0001e510: 2020 696e 7465 726d 6564 6961 7465 206d    intermediate m
-0001e520: 6561 7375 7265 6d65 6e74 732e 204f 6e65  easurements. One
-0001e530: 206f 6620 2242 4553 545f 4d45 4153 5552   of "BEST_MEASUR
-0001e540: 454d 454e 5422 206f 720a 2020 2020 2020  EMENT" or.      
-0001e550: 224c 4153 545f 4d45 4153 5552 454d 454e  "LAST_MEASUREMEN
-0001e560: 5422 2e0a 2020 2020 6f70 7469 6d69 7a61  T"..    optimiza
-0001e570: 7469 6f6e 5f6d 6574 7269 633a 204f 7074  tion_metric: Opt
-0001e580: 696d 697a 6174 696f 6e20 6d65 7472 6963  imization metric
-0001e590: 2075 7365 6420 666f 720a 2020 2020 2020   used for.      
-0001e5a0: 606d 6561 7375 7265 6d65 6e74 5f73 656c  `measurement_sel
-0001e5b0: 6563 7469 6f6e 5f74 7970 6560 2e20 4465  ection_type`. De
-0001e5c0: 6661 756c 7420 6973 2022 726d 7365 2220  fault is "rmse" 
-0001e5d0: 666f 7220 7265 6772 6573 7369 6f6e 2061  for regression a
-0001e5e0: 6e64 2022 6175 6322 0a20 2020 2020 2066  nd "auc".      f
-0001e5f0: 6f72 2063 6c61 7373 6966 6963 6174 696f  or classificatio
-0001e600: 6e2e 0a20 2020 2065 7661 6c5f 6672 6571  n..    eval_freq
-0001e610: 7565 6e63 795f 7365 6373 3a20 4672 6571  uency_secs: Freq
-0001e620: 7565 6e63 7920 6174 2077 6869 6368 2065  uency at which e
-0001e630: 7661 6c75 6174 696f 6e20 616e 6420 6368  valuation and ch
-0001e640: 6563 6b70 6f69 6e74 696e 6720 7769 6c6c  eckpointing will
-0001e650: 0a20 2020 2020 2074 616b 6520 706c 6163  .      take plac
-0001e660: 652e 0a20 2020 2064 6174 615f 736f 7572  e..    data_sour
-0001e670: 6365 5f63 7376 5f66 696c 656e 616d 6573  ce_csv_filenames
-0001e680: 3a20 5468 6520 4353 5620 6461 7461 2073  : The CSV data s
-0001e690: 6f75 7263 652e 0a20 2020 2064 6174 615f  ource..    data_
-0001e6a0: 736f 7572 6365 5f62 6967 7175 6572 795f  source_bigquery_
-0001e6b0: 7461 626c 655f 7061 7468 3a20 5468 6520  table_path: The 
-0001e6c0: 4269 6751 7565 7279 2064 6174 6120 736f  BigQuery data so
-0001e6d0: 7572 6365 2e0a 2020 2020 6269 6771 7565  urce..    bigque
-0001e6e0: 7279 5f73 7461 6769 6e67 5f66 756c 6c5f  ry_staging_full_
-0001e6f0: 6461 7461 7365 745f 6964 3a20 5468 6520  dataset_id: The 
-0001e700: 4269 6751 7565 7279 2073 7461 6769 6e67  BigQuery staging
-0001e710: 2066 756c 6c20 6461 7461 7365 7420 6964   full dataset id
-0001e720: 2066 6f72 0a20 2020 2020 2073 746f 7269   for.      stori
-0001e730: 6e67 2069 6e74 6572 6d65 6469 6174 6520  ng intermediate 
-0001e740: 7461 626c 6573 2e0a 2020 2020 7765 6967  tables..    weig
-0001e750: 6874 5f63 6f6c 756d 6e3a 2054 6865 2077  ht_column: The w
-0001e760: 6569 6768 7420 636f 6c75 6d6e 206e 616d  eight column nam
-0001e770: 652e 0a20 2020 2074 7261 6e73 666f 726d  e..    transform
-0001e780: 5f64 6174 6166 6c6f 775f 6d61 6368 696e  _dataflow_machin
-0001e790: 655f 7479 7065 3a20 5468 6520 6461 7461  e_type: The data
-0001e7a0: 666c 6f77 206d 6163 6869 6e65 2074 7970  flow machine typ
-0001e7b0: 6520 666f 7220 7472 616e 7366 6f72 6d0a  e for transform.
-0001e7c0: 2020 2020 2020 636f 6d70 6f6e 656e 742e        component.
-0001e7d0: 0a20 2020 2074 7261 6e73 666f 726d 5f64  .    transform_d
-0001e7e0: 6174 6166 6c6f 775f 6d61 785f 6e75 6d5f  ataflow_max_num_
-0001e7f0: 776f 726b 6572 733a 2054 6865 206d 6178  workers: The max
-0001e800: 206e 756d 6265 7220 6f66 2044 6174 6166   number of Dataf
-0001e810: 6c6f 7720 776f 726b 6572 7320 666f 720a  low workers for.
-0001e820: 2020 2020 2020 7472 616e 7366 6f72 6d20        transform 
-0001e830: 636f 6d70 6f6e 656e 742e 0a20 2020 2074  component..    t
-0001e840: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
-0001e850: 775f 6469 736b 5f73 697a 655f 6762 3a20  w_disk_size_gb: 
-0001e860: 4461 7461 666c 6f77 2077 6f72 6b65 7227  Dataflow worker'
-0001e870: 7320 6469 736b 2073 697a 6520 696e 2047  s disk size in G
-0001e880: 4220 666f 720a 2020 2020 2020 7472 616e  B for.      tran
-0001e890: 7366 6f72 6d20 636f 6d70 6f6e 656e 742e  sform component.
-0001e8a0: 0a20 2020 2077 6f72 6b65 725f 706f 6f6c  .    worker_pool
-0001e8b0: 5f73 7065 6373 5f6f 7665 7272 6964 653a  _specs_override:
-0001e8c0: 2054 6865 2064 6963 7469 6f6e 6172 7920   The dictionary 
-0001e8d0: 666f 7220 6f76 6572 7269 6469 6e67 2074  for overriding t
-0001e8e0: 7261 696e 696e 6720 616e 640a 2020 2020  raining and.    
-0001e8f0: 2020 6576 616c 7561 7469 6f6e 2077 6f72    evaluation wor
-0001e900: 6b65 7220 706f 6f6c 2073 7065 6373 2e20  ker pool specs. 
-0001e910: 5468 6520 6469 6374 696f 6e61 7279 2073  The dictionary s
-0001e920: 686f 756c 6420 6265 206f 6620 666f 726d  hould be of form
-0001e930: 6174 0a20 2020 2020 2020 2020 2068 7474  at.          htt
-0001e940: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
-0001e950: 676f 6f67 6c65 6170 6973 2f67 6f6f 676c  googleapis/googl
-0001e960: 6561 7069 732f 626c 6f62 2f34 6538 3336  eapis/blob/4e836
-0001e970: 6337 6332 3537 6533 6532 3062 3164 6531  c7c257e3e20b1de1
-0001e980: 3464 3437 3039 3933 6132 6231 6634 3733  4d470993a2b1f473
-0001e990: 3661 382f 676f 6f67 6c65 2f63 6c6f 7564  6a8/google/cloud
-0001e9a0: 2f61 6970 6c61 7466 6f72 6d2f 7631 6265  /aiplatform/v1be
-0001e9b0: 7461 312f 6375 7374 6f6d 5f6a 6f62 2e70  ta1/custom_job.p
-0001e9c0: 726f 746f 234c 3137 322e 0a20 2020 2072  roto#L172..    r
-0001e9d0: 756e 5f65 7661 6c75 6174 696f 6e3a 2057  un_evaluation: W
-0001e9e0: 6865 7468 6572 2074 6f20 7275 6e20 6576  hether to run ev
-0001e9f0: 616c 7561 7469 6f6e 2073 7465 7073 2064  aluation steps d
-0001ea00: 7572 696e 6720 7472 6169 6e69 6e67 2e0a  uring training..
-0001ea10: 2020 2020 6576 616c 7561 7469 6f6e 5f62      evaluation_b
-0001ea20: 6174 6368 5f70 7265 6469 6374 5f6d 6163  atch_predict_mac
-0001ea30: 6869 6e65 5f74 7970 653a 2054 6865 2070  hine_type: The p
-0001ea40: 7265 6469 6374 696f 6e20 7365 7276 6572  rediction server
-0001ea50: 206d 6163 6869 6e65 2074 7970 650a 2020   machine type.  
-0001ea60: 2020 2020 666f 7220 6261 7463 6820 7072      for batch pr
-0001ea70: 6564 6963 7420 636f 6d70 6f6e 656e 7473  edict components
-0001ea80: 2064 7572 696e 6720 6576 616c 7561 7469   during evaluati
-0001ea90: 6f6e 2e0a 2020 2020 6576 616c 7561 7469  on..    evaluati
-0001eaa0: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
-0001eab0: 5f73 7461 7274 696e 675f 7265 706c 6963  _starting_replic
-0001eac0: 615f 636f 756e 743a 2054 6865 2069 6e69  a_count: The ini
-0001ead0: 7469 616c 206e 756d 6265 7220 6f66 0a20  tial number of. 
-0001eae0: 2020 2020 2070 7265 6469 6374 696f 6e20       prediction 
-0001eaf0: 7365 7276 6572 2066 6f72 2062 6174 6368  server for batch
-0001eb00: 2070 7265 6469 6374 2063 6f6d 706f 6e65   predict compone
-0001eb10: 6e74 7320 6475 7269 6e67 2065 7661 6c75  nts during evalu
-0001eb20: 6174 696f 6e2e 0a20 2020 2065 7661 6c75  ation..    evalu
-0001eb30: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
-0001eb40: 6963 745f 6d61 785f 7265 706c 6963 615f  ict_max_replica_
-0001eb50: 636f 756e 743a 2054 6865 206d 6178 206e  count: The max n
-0001eb60: 756d 6265 7220 6f66 2070 7265 6469 6374  umber of predict
-0001eb70: 696f 6e0a 2020 2020 2020 7365 7276 6572  ion.      server
-0001eb80: 2066 6f72 2062 6174 6368 2070 7265 6469   for batch predi
-0001eb90: 6374 2063 6f6d 706f 6e65 6e74 7320 6475  ct components du
-0001eba0: 7269 6e67 2065 7661 6c75 6174 696f 6e2e  ring evaluation.
-0001ebb0: 0a20 2020 2065 7661 6c75 6174 696f 6e5f  .    evaluation_
-0001ebc0: 6461 7461 666c 6f77 5f6d 6163 6869 6e65  dataflow_machine
-0001ebd0: 5f74 7970 653a 2054 6865 2064 6174 6166  _type: The dataf
-0001ebe0: 6c6f 7720 6d61 6368 696e 6520 7479 7065  low machine type
-0001ebf0: 2066 6f72 2065 7661 6c75 6174 696f 6e0a   for evaluation.
-0001ec00: 2020 2020 2020 636f 6d70 6f6e 656e 7473        components
-0001ec10: 2e0a 2020 2020 6576 616c 7561 7469 6f6e  ..    evaluation
-0001ec20: 5f64 6174 6166 6c6f 775f 7374 6172 7469  _dataflow_starti
-0001ec30: 6e67 5f6e 756d 5f77 6f72 6b65 7273 3a20  ng_num_workers: 
-0001ec40: 5468 6520 696e 6974 6961 6c20 6e75 6d62  The initial numb
-0001ec50: 6572 206f 6620 4461 7461 666c 6f77 0a20  er of Dataflow. 
-0001ec60: 2020 2020 2077 6f72 6b65 7273 2066 6f72       workers for
-0001ec70: 2065 7661 6c75 6174 696f 6e20 636f 6d70   evaluation comp
-0001ec80: 6f6e 656e 7473 2e0a 2020 2020 6576 616c  onents..    eval
-0001ec90: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
-0001eca0: 6d61 785f 6e75 6d5f 776f 726b 6572 733a  max_num_workers:
-0001ecb0: 2054 6865 206d 6178 206e 756d 6265 7220   The max number 
-0001ecc0: 6f66 2044 6174 6166 6c6f 7720 776f 726b  of Dataflow work
-0001ecd0: 6572 7320 666f 720a 2020 2020 2020 6576  ers for.      ev
-0001ece0: 616c 7561 7469 6f6e 2063 6f6d 706f 6e65  aluation compone
-0001ecf0: 6e74 732e 0a20 2020 2065 7661 6c75 6174  nts..    evaluat
-0001ed00: 696f 6e5f 6461 7461 666c 6f77 5f64 6973  ion_dataflow_dis
-0001ed10: 6b5f 7369 7a65 5f67 623a 2044 6174 6166  k_size_gb: Dataf
-0001ed20: 6c6f 7720 776f 726b 6572 2773 2064 6973  low worker's dis
-0001ed30: 6b20 7369 7a65 2069 6e20 4742 2066 6f72  k size in GB for
-0001ed40: 0a20 2020 2020 2065 7661 6c75 6174 696f  .      evaluatio
-0001ed50: 6e20 636f 6d70 6f6e 656e 7473 2e0a 2020  n components..  
-0001ed60: 2020 6461 7461 666c 6f77 5f73 6572 7669    dataflow_servi
-0001ed70: 6365 5f61 6363 6f75 6e74 3a20 4375 7374  ce_account: Cust
-0001ed80: 6f6d 2073 6572 7669 6365 2061 6363 6f75  om service accou
-0001ed90: 6e74 2074 6f20 7275 6e20 6461 7461 666c  nt to run datafl
-0001eda0: 6f77 206a 6f62 732e 0a20 2020 2064 6174  ow jobs..    dat
-0001edb0: 6166 6c6f 775f 7375 626e 6574 776f 726b  aflow_subnetwork
-0001edc0: 3a20 4461 7461 666c 6f77 2773 2066 756c  : Dataflow's ful
-0001edd0: 6c79 2071 7561 6c69 6669 6564 2073 7562  ly qualified sub
-0001ede0: 6e65 7477 6f72 6b20 6e61 6d65 2c20 7768  network name, wh
-0001edf0: 656e 2065 6d70 7479 0a20 2020 2020 2074  en empty.      t
-0001ee00: 6865 2064 6566 6175 6c74 2073 7562 6e65  he default subne
-0001ee10: 7477 6f72 6b20 7769 6c6c 2062 6520 7573  twork will be us
-0001ee20: 6564 2e20 4578 616d 706c 653a 0a20 2020  ed. Example:.   
-0001ee30: 2020 2020 2068 7474 7073 3a2f 2f63 6c6f       https://clo
-0001ee40: 7564 2e67 6f6f 676c 652e 636f 6d2f 6461  ud.google.com/da
-0001ee50: 7461 666c 6f77 2f64 6f63 732f 6775 6964  taflow/docs/guid
-0001ee60: 6573 2f73 7065 6369 6679 696e 672d 6e65  es/specifying-ne
-0001ee70: 7477 6f72 6b73 2365 7861 6d70 6c65 5f6e  tworks#example_n
-0001ee80: 6574 776f 726b 5f61 6e64 5f73 7562 6e65  etwork_and_subne
-0001ee90: 7477 6f72 6b5f 7370 6563 6966 6963 6174  twork_specificat
-0001eea0: 696f 6e73 0a20 2020 2064 6174 6166 6c6f  ions.    dataflo
-0001eeb0: 775f 7573 655f 7075 626c 6963 5f69 7073  w_use_public_ips
-0001eec0: 3a20 5370 6563 6966 6965 7320 7768 6574  : Specifies whet
-0001eed0: 6865 7220 4461 7461 666c 6f77 2077 6f72  her Dataflow wor
-0001eee0: 6b65 7273 2075 7365 2070 7562 6c69 6320  kers use public 
-0001eef0: 4950 0a20 2020 2020 2061 6464 7265 7373  IP.      address
-0001ef00: 6573 2e0a 2020 2020 656e 6372 7970 7469  es..    encrypti
-0001ef10: 6f6e 5f73 7065 635f 6b65 795f 6e61 6d65  on_spec_key_name
-0001ef20: 3a20 5468 6520 4b4d 5320 6b65 7920 6e61  : The KMS key na
-0001ef30: 6d65 2e0a 0a20 2052 6574 7572 6e73 3a0a  me...  Returns:.
-0001ef40: 2020 2020 5475 706c 6520 6f66 2070 6970      Tuple of pip
-0001ef50: 656c 696e 655f 6465 6669 6e69 7469 6f6e  eline_definition
-0001ef60: 5f70 6174 6820 616e 6420 7061 7261 6d65  _path and parame
-0001ef70: 7465 725f 7661 6c75 6573 2e0a 2020 2222  ter_values..  ""
-0001ef80: 220a 2020 6966 2069 7369 6e73 7461 6e63  ".  if isinstanc
-0001ef90: 6528 7466 5f61 7574 6f5f 7472 616e 7366  e(tf_auto_transf
-0001efa0: 6f72 6d5f 6665 6174 7572 6573 2c20 6c69  orm_features, li
-0001efb0: 7374 293a 0a20 2020 2074 665f 6175 746f  st):.    tf_auto
-0001efc0: 5f74 7261 6e73 666f 726d 5f66 6561 7475  _transform_featu
-0001efd0: 7265 7320 3d20 7b27 6175 746f 273a 2074  res = {'auto': t
-0001efe0: 665f 6175 746f 5f74 7261 6e73 666f 726d  f_auto_transform
-0001eff0: 5f66 6561 7475 7265 737d 0a0a 2020 6966  _features}..  if
-0001f000: 2074 7261 6e73 666f 726d 5f63 6f6e 6669   transform_confi
-0001f010: 6720 616e 6420 7466 5f74 7261 6e73 666f  g and tf_transfo
-0001f020: 726d 6174 696f 6e73 5f70 6174 683a 0a20  rmations_path:. 
-0001f030: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
-0001f040: 726f 7228 0a20 2020 2020 2020 2027 4f6e  ror(.        'On
-0001f050: 6c79 206f 6e65 206f 6620 7472 616e 7366  ly one of transf
-0001f060: 6f72 6d5f 636f 6e66 6967 2061 6e64 2074  orm_config and t
-0001f070: 665f 7472 616e 7366 6f72 6d61 7469 6f6e  f_transformation
-0001f080: 735f 7061 7468 2063 616e 2027 0a20 2020  s_path can '.   
-0001f090: 2020 2020 2027 6265 2073 7065 6369 6669       'be specifi
-0001f0a0: 6564 2e27 0a20 2020 2029 0a0a 2020 656c  ed.'.    )..  el
-0001f0b0: 6966 2074 7261 6e73 666f 726d 5f63 6f6e  if transform_con
-0001f0c0: 6669 673a 0a20 2020 2077 6172 6e69 6e67  fig:.    warning
-0001f0d0: 732e 7761 726e 280a 2020 2020 2020 2020  s.warn(.        
-0001f0e0: 2774 7261 6e73 666f 726d 5f63 6f6e 6669  'transform_confi
-0001f0f0: 6720 7061 7261 6d65 7465 7220 6973 2064  g parameter is d
-0001f100: 6570 7265 6361 7465 642e 2027 0a20 2020  eprecated. '.   
-0001f110: 2020 2020 2027 506c 6561 7365 2075 7365       'Please use
-0001f120: 2074 6865 2066 6c61 7474 656e 6564 2074   the flattened t
-0001f130: 7261 6e73 666f 726d 2063 6f6e 6669 6720  ransform config 
-0001f140: 6172 6775 6d65 6e74 7320 696e 7374 6561  arguments instea
-0001f150: 642e 270a 2020 2020 290a 2020 2020 7466  d.'.    ).    tf
-0001f160: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
-0001f170: 5f70 6174 6820 3d20 7472 616e 7366 6f72  _path = transfor
-0001f180: 6d5f 636f 6e66 6967 0a0a 2020 6966 206e  m_config..  if n
-0001f190: 6f74 2077 6f72 6b65 725f 706f 6f6c 5f73  ot worker_pool_s
-0001f1a0: 7065 6373 5f6f 7665 7272 6964 653a 0a20  pecs_override:. 
-0001f1b0: 2020 2077 6f72 6b65 725f 706f 6f6c 5f73     worker_pool_s
-0001f1c0: 7065 6373 5f6f 7665 7272 6964 6520 3d20  pecs_override = 
-0001f1d0: 5b5d 0a0a 2020 7061 7261 6d65 7465 725f  []..  parameter_
-0001f1e0: 7661 6c75 6573 203d 207b 7d0a 2020 7472  values = {}.  tr
-0001f1f0: 6169 6e69 6e67 5f61 6e64 5f65 7661 6c5f  aining_and_eval_
-0001f200: 7061 7261 6d65 7465 7273 203d 207b 0a20  parameters = {. 
-0001f210: 2020 2020 2027 7072 6f6a 6563 7427 3a20       'project': 
-0001f220: 7072 6f6a 6563 742c 0a20 2020 2020 2027  project,.      '
-0001f230: 6c6f 6361 7469 6f6e 273a 206c 6f63 6174  location': locat
-0001f240: 696f 6e2c 0a20 2020 2020 2027 726f 6f74  ion,.      'root
-0001f250: 5f64 6972 273a 2072 6f6f 745f 6469 722c  _dir': root_dir,
-0001f260: 0a20 2020 2020 2027 7461 7267 6574 5f63  .      'target_c
-0001f270: 6f6c 756d 6e27 3a20 7461 7267 6574 5f63  olumn': target_c
-0001f280: 6f6c 756d 6e2c 0a20 2020 2020 2027 7072  olumn,.      'pr
-0001f290: 6564 6963 7469 6f6e 5f74 7970 6527 3a20  ediction_type': 
-0001f2a0: 7072 6564 6963 7469 6f6e 5f74 7970 652c  prediction_type,
-0001f2b0: 0a20 2020 2020 2027 6c65 6172 6e69 6e67  .      'learning
-0001f2c0: 5f72 6174 6527 3a20 6c65 6172 6e69 6e67  _rate': learning
-0001f2d0: 5f72 6174 652c 0a20 2020 2020 2027 6d61  _rate,.      'ma
-0001f2e0: 785f 7374 6570 7327 3a20 6d61 785f 7374  x_steps': max_st
-0001f2f0: 6570 732c 0a20 2020 2020 2027 6d61 785f  eps,.      'max_
-0001f300: 7472 6169 6e5f 7365 6373 273a 206d 6178  train_secs': max
-0001f310: 5f74 7261 696e 5f73 6563 732c 0a20 2020  _train_secs,.   
-0001f320: 2020 2027 6c61 7267 655f 6361 7465 676f     'large_catego
-0001f330: 7279 5f64 696d 273a 206c 6172 6765 5f63  ry_dim': large_c
-0001f340: 6174 6567 6f72 795f 6469 6d2c 0a20 2020  ategory_dim,.   
-0001f350: 2020 2027 6c61 7267 655f 6361 7465 676f     'large_catego
-0001f360: 7279 5f74 6872 6573 6827 3a20 6c61 7267  ry_thresh': larg
-0001f370: 655f 6361 7465 676f 7279 5f74 6872 6573  e_category_thres
-0001f380: 682c 0a20 2020 2020 2027 7965 6f5f 6a6f  h,.      'yeo_jo
-0001f390: 686e 736f 6e5f 7472 616e 7366 6f72 6d27  hnson_transform'
-0001f3a0: 3a20 7965 6f5f 6a6f 686e 736f 6e5f 7472  : yeo_johnson_tr
-0001f3b0: 616e 7366 6f72 6d2c 0a20 2020 2020 2027  ansform,.      '
-0001f3c0: 6665 6174 7572 655f 6469 6d27 3a20 6665  feature_dim': fe
-0001f3d0: 6174 7572 655f 6469 6d2c 0a20 2020 2020  ature_dim,.     
-0001f3e0: 2027 6665 6174 7572 655f 6469 6d5f 7261   'feature_dim_ra
-0001f3f0: 7469 6f27 3a20 6665 6174 7572 655f 6469  tio': feature_di
-0001f400: 6d5f 7261 7469 6f2c 0a20 2020 2020 2027  m_ratio,.      '
-0001f410: 6e75 6d5f 6465 6369 7369 6f6e 5f73 7465  num_decision_ste
-0001f420: 7073 273a 206e 756d 5f64 6563 6973 696f  ps': num_decisio
-0001f430: 6e5f 7374 6570 732c 0a20 2020 2020 2027  n_steps,.      '
-0001f440: 7265 6c61 7861 7469 6f6e 5f66 6163 746f  relaxation_facto
-0001f450: 7227 3a20 7265 6c61 7861 7469 6f6e 5f66  r': relaxation_f
-0001f460: 6163 746f 722c 0a20 2020 2020 2027 6465  actor,.      'de
-0001f470: 6361 795f 6576 6572 7927 3a20 6465 6361  cay_every': deca
-0001f480: 795f 6576 6572 792c 0a20 2020 2020 2027  y_every,.      '
-0001f490: 6465 6361 795f 7261 7465 273a 2064 6563  decay_rate': dec
-0001f4a0: 6179 5f72 6174 652c 0a20 2020 2020 2027  ay_rate,.      '
-0001f4b0: 6772 6164 6965 6e74 5f74 6872 6573 6827  gradient_thresh'
-0001f4c0: 3a20 6772 6164 6965 6e74 5f74 6872 6573  : gradient_thres
-0001f4d0: 682c 0a20 2020 2020 2027 7370 6172 7369  h,.      'sparsi
-0001f4e0: 7479 5f6c 6f73 735f 7765 6967 6874 273a  ty_loss_weight':
-0001f4f0: 2073 7061 7273 6974 795f 6c6f 7373 5f77   sparsity_loss_w
-0001f500: 6569 6768 742c 0a20 2020 2020 2027 6261  eight,.      'ba
-0001f510: 7463 685f 6d6f 6d65 6e74 756d 273a 2062  tch_momentum': b
-0001f520: 6174 6368 5f6d 6f6d 656e 7475 6d2c 0a20  atch_momentum,. 
-0001f530: 2020 2020 2027 6261 7463 685f 7369 7a65       'batch_size
-0001f540: 5f72 6174 696f 273a 2062 6174 6368 5f73  _ratio': batch_s
-0001f550: 697a 655f 7261 7469 6f2c 0a20 2020 2020  ize_ratio,.     
-0001f560: 2027 6e75 6d5f 7472 616e 7366 6f72 6d65   'num_transforme
-0001f570: 725f 6c61 7965 7273 273a 206e 756d 5f74  r_layers': num_t
-0001f580: 7261 6e73 666f 726d 6572 5f6c 6179 6572  ransformer_layer
-0001f590: 732c 0a20 2020 2020 2027 6e75 6d5f 7472  s,.      'num_tr
-0001f5a0: 616e 7366 6f72 6d65 725f 6c61 7965 7273  ansformer_layers
-0001f5b0: 5f72 6174 696f 273a 206e 756d 5f74 7261  _ratio': num_tra
-0001f5c0: 6e73 666f 726d 6572 5f6c 6179 6572 735f  nsformer_layers_
-0001f5d0: 7261 7469 6f2c 0a20 2020 2020 2027 636c  ratio,.      'cl
-0001f5e0: 6173 735f 7765 6967 6874 273a 2063 6c61  ass_weight': cla
-0001f5f0: 7373 5f77 6569 6768 742c 0a20 2020 2020  ss_weight,.     
-0001f600: 2027 6c6f 7373 5f66 756e 6374 696f 6e5f   'loss_function_
-0001f610: 7479 7065 273a 206c 6f73 735f 6675 6e63  type': loss_func
-0001f620: 7469 6f6e 5f74 7970 652c 0a20 2020 2020  tion_type,.     
-0001f630: 2027 616c 7068 615f 666f 6361 6c5f 6c6f   'alpha_focal_lo
-0001f640: 7373 273a 2061 6c70 6861 5f66 6f63 616c  ss': alpha_focal
-0001f650: 5f6c 6f73 732c 0a20 2020 2020 2027 6761  _loss,.      'ga
-0001f660: 6d6d 615f 666f 6361 6c5f 6c6f 7373 273a  mma_focal_loss':
-0001f670: 2067 616d 6d61 5f66 6f63 616c 5f6c 6f73   gamma_focal_los
-0001f680: 732c 0a20 2020 2020 2027 656e 6162 6c65  s,.      'enable
-0001f690: 5f70 726f 6669 6c65 7227 3a20 656e 6162  _profiler': enab
-0001f6a0: 6c65 5f70 726f 6669 6c65 722c 0a20 2020  le_profiler,.   
-0001f6b0: 2020 2027 6361 6368 655f 6461 7461 273a     'cache_data':
-0001f6c0: 2063 6163 6865 5f64 6174 612c 0a20 2020   cache_data,.   
-0001f6d0: 2020 2027 7365 6564 273a 2073 6565 642c     'seed': seed,
-0001f6e0: 0a20 2020 2020 2027 6576 616c 5f73 7465  .      'eval_ste
-0001f6f0: 7073 273a 2065 7661 6c5f 7374 6570 732c  ps': eval_steps,
-0001f700: 0a20 2020 2020 2027 6261 7463 685f 7369  .      'batch_si
-0001f710: 7a65 273a 2062 6174 6368 5f73 697a 652c  ze': batch_size,
-0001f720: 0a20 2020 2020 2027 6d65 6173 7572 656d  .      'measurem
-0001f730: 656e 745f 7365 6c65 6374 696f 6e5f 7479  ent_selection_ty
-0001f740: 7065 273a 206d 6561 7375 7265 6d65 6e74  pe': measurement
-0001f750: 5f73 656c 6563 7469 6f6e 5f74 7970 652c  _selection_type,
-0001f760: 0a20 2020 2020 2027 6f70 7469 6d69 7a61  .      'optimiza
-0001f770: 7469 6f6e 5f6d 6574 7269 6327 3a20 6f70  tion_metric': op
-0001f780: 7469 6d69 7a61 7469 6f6e 5f6d 6574 7269  timization_metri
-0001f790: 632c 0a20 2020 2020 2027 6576 616c 5f66  c,.      'eval_f
-0001f7a0: 7265 7175 656e 6379 5f73 6563 7327 3a20  requency_secs': 
-0001f7b0: 6576 616c 5f66 7265 7175 656e 6379 5f73  eval_frequency_s
-0001f7c0: 6563 732c 0a20 2020 2020 2027 7765 6967  ecs,.      'weig
-0001f7d0: 6874 5f63 6f6c 756d 6e27 3a20 7765 6967  ht_column': weig
-0001f7e0: 6874 5f63 6f6c 756d 6e2c 0a20 2020 2020  ht_column,.     
-0001f7f0: 2027 7472 616e 7366 6f72 6d5f 6461 7461   'transform_data
-0001f800: 666c 6f77 5f6d 6163 6869 6e65 5f74 7970  flow_machine_typ
-0001f810: 6527 3a20 7472 616e 7366 6f72 6d5f 6461  e': transform_da
-0001f820: 7461 666c 6f77 5f6d 6163 6869 6e65 5f74  taflow_machine_t
-0001f830: 7970 652c 0a20 2020 2020 2027 7472 616e  ype,.      'tran
-0001f840: 7366 6f72 6d5f 6461 7461 666c 6f77 5f6d  sform_dataflow_m
-0001f850: 6178 5f6e 756d 5f77 6f72 6b65 7273 273a  ax_num_workers':
-0001f860: 2074 7261 6e73 666f 726d 5f64 6174 6166   transform_dataf
-0001f870: 6c6f 775f 6d61 785f 6e75 6d5f 776f 726b  low_max_num_work
-0001f880: 6572 732c 0a20 2020 2020 2027 7472 616e  ers,.      'tran
-0001f890: 7366 6f72 6d5f 6461 7461 666c 6f77 5f64  sform_dataflow_d
-0001f8a0: 6973 6b5f 7369 7a65 5f67 6227 3a20 7472  isk_size_gb': tr
-0001f8b0: 616e 7366 6f72 6d5f 6461 7461 666c 6f77  ansform_dataflow
-0001f8c0: 5f64 6973 6b5f 7369 7a65 5f67 622c 0a20  _disk_size_gb,. 
-0001f8d0: 2020 2020 2027 776f 726b 6572 5f70 6f6f       'worker_poo
-0001f8e0: 6c5f 7370 6563 735f 6f76 6572 7269 6465  l_specs_override
-0001f8f0: 273a 2077 6f72 6b65 725f 706f 6f6c 5f73  ': worker_pool_s
-0001f900: 7065 6373 5f6f 7665 7272 6964 652c 0a20  pecs_override,. 
-0001f910: 2020 2020 2027 7275 6e5f 6576 616c 7561       'run_evalua
-0001f920: 7469 6f6e 273a 2072 756e 5f65 7661 6c75  tion': run_evalu
-0001f930: 6174 696f 6e2c 0a20 2020 2020 2027 6576  ation,.      'ev
-0001f940: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
-0001f950: 7265 6469 6374 5f6d 6163 6869 6e65 5f74  redict_machine_t
-0001f960: 7970 6527 3a20 280a 2020 2020 2020 2020  ype': (.        
-0001f970: 2020 6576 616c 7561 7469 6f6e 5f62 6174    evaluation_bat
-0001f980: 6368 5f70 7265 6469 6374 5f6d 6163 6869  ch_predict_machi
-0001f990: 6e65 5f74 7970 650a 2020 2020 2020 292c  ne_type.      ),
-0001f9a0: 0a20 2020 2020 2027 6576 616c 7561 7469  .      'evaluati
-0001f9b0: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
-0001f9c0: 5f73 7461 7274 696e 675f 7265 706c 6963  _starting_replic
-0001f9d0: 615f 636f 756e 7427 3a20 280a 2020 2020  a_count': (.    
-0001f9e0: 2020 2020 2020 6576 616c 7561 7469 6f6e        evaluation
-0001f9f0: 5f62 6174 6368 5f70 7265 6469 6374 5f73  _batch_predict_s
-0001fa00: 7461 7274 696e 675f 7265 706c 6963 615f  tarting_replica_
-0001fa10: 636f 756e 740a 2020 2020 2020 292c 0a20  count.      ),. 
-0001fa20: 2020 2020 2027 6576 616c 7561 7469 6f6e       'evaluation
-0001fa30: 5f62 6174 6368 5f70 7265 6469 6374 5f6d  _batch_predict_m
-0001fa40: 6178 5f72 6570 6c69 6361 5f63 6f75 6e74  ax_replica_count
-0001fa50: 273a 2028 0a20 2020 2020 2020 2020 2065  ': (.          e
-0001fa60: 7661 6c75 6174 696f 6e5f 6261 7463 685f  valuation_batch_
-0001fa70: 7072 6564 6963 745f 6d61 785f 7265 706c  predict_max_repl
-0001fa80: 6963 615f 636f 756e 740a 2020 2020 2020  ica_count.      
-0001fa90: 292c 0a20 2020 2020 2027 6576 616c 7561  ),.      'evalua
-0001faa0: 7469 6f6e 5f64 6174 6166 6c6f 775f 6d61  tion_dataflow_ma
-0001fab0: 6368 696e 655f 7479 7065 273a 2065 7661  chine_type': eva
-0001fac0: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
-0001fad0: 5f6d 6163 6869 6e65 5f74 7970 652c 0a20  _machine_type,. 
-0001fae0: 2020 2020 2027 6576 616c 7561 7469 6f6e       'evaluation
-0001faf0: 5f64 6174 6166 6c6f 775f 7374 6172 7469  _dataflow_starti
-0001fb00: 6e67 5f6e 756d 5f77 6f72 6b65 7273 273a  ng_num_workers':
-0001fb10: 2028 0a20 2020 2020 2020 2020 2065 7661   (.          eva
-0001fb20: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
-0001fb30: 5f73 7461 7274 696e 675f 6e75 6d5f 776f  _starting_num_wo
-0001fb40: 726b 6572 730a 2020 2020 2020 292c 0a20  rkers.      ),. 
-0001fb50: 2020 2020 2027 6576 616c 7561 7469 6f6e       'evaluation
-0001fb60: 5f64 6174 6166 6c6f 775f 6d61 785f 6e75  _dataflow_max_nu
-0001fb70: 6d5f 776f 726b 6572 7327 3a20 280a 2020  m_workers': (.  
-0001fb80: 2020 2020 2020 2020 6576 616c 7561 7469          evaluati
-0001fb90: 6f6e 5f64 6174 6166 6c6f 775f 6d61 785f  on_dataflow_max_
-0001fba0: 6e75 6d5f 776f 726b 6572 730a 2020 2020  num_workers.    
-0001fbb0: 2020 292c 0a20 2020 2020 2027 6576 616c    ),.      'eval
-0001fbc0: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
-0001fbd0: 6469 736b 5f73 697a 655f 6762 273a 2065  disk_size_gb': e
-0001fbe0: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
-0001fbf0: 6f77 5f64 6973 6b5f 7369 7a65 5f67 622c  ow_disk_size_gb,
-0001fc00: 0a20 2020 2020 2027 6461 7461 666c 6f77  .      'dataflow
-0001fc10: 5f73 6572 7669 6365 5f61 6363 6f75 6e74  _service_account
-0001fc20: 273a 2064 6174 6166 6c6f 775f 7365 7276  ': dataflow_serv
-0001fc30: 6963 655f 6163 636f 756e 742c 0a20 2020  ice_account,.   
-0001fc40: 2020 2027 6461 7461 666c 6f77 5f73 7562     'dataflow_sub
-0001fc50: 6e65 7477 6f72 6b27 3a20 6461 7461 666c  network': datafl
-0001fc60: 6f77 5f73 7562 6e65 7477 6f72 6b2c 0a20  ow_subnetwork,. 
-0001fc70: 2020 2020 2027 6461 7461 666c 6f77 5f75       'dataflow_u
-0001fc80: 7365 5f70 7562 6c69 635f 6970 7327 3a20  se_public_ips': 
-0001fc90: 6461 7461 666c 6f77 5f75 7365 5f70 7562  dataflow_use_pub
-0001fca0: 6c69 635f 6970 732c 0a20 2020 2020 2027  lic_ips,.      '
-0001fcb0: 656e 6372 7970 7469 6f6e 5f73 7065 635f  encryption_spec_
-0001fcc0: 6b65 795f 6e61 6d65 273a 2065 6e63 7279  key_name': encry
-0001fcd0: 7074 696f 6e5f 7370 6563 5f6b 6579 5f6e  ption_spec_key_n
-0001fce0: 616d 652c 0a20 207d 0a20 205f 7570 6461  ame,.  }.  _upda
-0001fcf0: 7465 5f70 6172 616d 6574 6572 7328 7061  te_parameters(pa
-0001fd00: 7261 6d65 7465 725f 7661 6c75 6573 2c20  rameter_values, 
-0001fd10: 7472 6169 6e69 6e67 5f61 6e64 5f65 7661  training_and_eva
-0001fd20: 6c5f 7061 7261 6d65 7465 7273 290a 0a20  l_parameters).. 
-0001fd30: 2066 7465 5f70 6172 616d 7320 3d20 7b0a   fte_params = {.
-0001fd40: 2020 2020 2020 2764 6174 6173 6574 5f6c        'dataset_l
-0001fd50: 6576 656c 5f63 7573 746f 6d5f 7472 616e  evel_custom_tran
-0001fd60: 7366 6f72 6d61 7469 6f6e 5f64 6566 696e  sformation_defin
-0001fd70: 6974 696f 6e73 273a 2028 0a20 2020 2020  itions': (.     
-0001fd80: 2020 2020 2064 6174 6173 6574 5f6c 6576       dataset_lev
-0001fd90: 656c 5f63 7573 746f 6d5f 7472 616e 7366  el_custom_transf
-0001fda0: 6f72 6d61 7469 6f6e 5f64 6566 696e 6974  ormation_definit
-0001fdb0: 696f 6e73 0a20 2020 2020 2020 2020 2069  ions.          i
-0001fdc0: 6620 6461 7461 7365 745f 6c65 7665 6c5f  f dataset_level_
-0001fdd0: 6375 7374 6f6d 5f74 7261 6e73 666f 726d  custom_transform
-0001fde0: 6174 696f 6e5f 6465 6669 6e69 7469 6f6e  ation_definition
-0001fdf0: 730a 2020 2020 2020 2020 2020 656c 7365  s.          else
-0001fe00: 205b 5d0a 2020 2020 2020 292c 0a20 2020   [].      ),.   
-0001fe10: 2020 2027 6461 7461 7365 745f 6c65 7665     'dataset_leve
-0001fe20: 6c5f 7472 616e 7366 6f72 6d61 7469 6f6e  l_transformation
-0001fe30: 7327 3a20 280a 2020 2020 2020 2020 2020  s': (.          
-0001fe40: 6461 7461 7365 745f 6c65 7665 6c5f 7472  dataset_level_tr
-0001fe50: 616e 7366 6f72 6d61 7469 6f6e 7320 6966  ansformations if
-0001fe60: 2064 6174 6173 6574 5f6c 6576 656c 5f74   dataset_level_t
-0001fe70: 7261 6e73 666f 726d 6174 696f 6e73 2065  ransformations e
-0001fe80: 6c73 6520 5b5d 0a20 2020 2020 2029 2c0a  lse [].      ),.
-0001fe90: 2020 2020 2020 2772 756e 5f66 6561 7475        'run_featu
-0001fea0: 7265 5f73 656c 6563 7469 6f6e 273a 2072  re_selection': r
-0001feb0: 756e 5f66 6561 7475 7265 5f73 656c 6563  un_feature_selec
-0001fec0: 7469 6f6e 2c0a 2020 2020 2020 2766 6561  tion,.      'fea
-0001fed0: 7475 7265 5f73 656c 6563 7469 6f6e 5f61  ture_selection_a
-0001fee0: 6c67 6f72 6974 686d 273a 2066 6561 7475  lgorithm': featu
-0001fef0: 7265 5f73 656c 6563 7469 6f6e 5f61 6c67  re_selection_alg
-0001ff00: 6f72 6974 686d 2c0a 2020 2020 2020 276d  orithm,.      'm
-0001ff10: 6178 5f73 656c 6563 7465 645f 6665 6174  ax_selected_feat
-0001ff20: 7572 6573 273a 206d 6178 5f73 656c 6563  ures': max_selec
-0001ff30: 7465 645f 6665 6174 7572 6573 2c0a 2020  ted_features,.  
-0001ff40: 2020 2020 2770 7265 6465 6669 6e65 645f      'predefined_
-0001ff50: 7370 6c69 745f 6b65 7927 3a20 7072 6564  split_key': pred
-0001ff60: 6566 696e 6564 5f73 706c 6974 5f6b 6579  efined_split_key
-0001ff70: 2c0a 2020 2020 2020 2773 7472 6174 6966  ,.      'stratif
-0001ff80: 6965 645f 7370 6c69 745f 6b65 7927 3a20  ied_split_key': 
-0001ff90: 7374 7261 7469 6669 6564 5f73 706c 6974  stratified_split
-0001ffa0: 5f6b 6579 2c0a 2020 2020 2020 2774 7261  _key,.      'tra
-0001ffb0: 696e 696e 675f 6672 6163 7469 6f6e 273a  ining_fraction':
-0001ffc0: 2074 7261 696e 696e 675f 6672 6163 7469   training_fracti
-0001ffd0: 6f6e 2c0a 2020 2020 2020 2776 616c 6964  on,.      'valid
-0001ffe0: 6174 696f 6e5f 6672 6163 7469 6f6e 273a  ation_fraction':
-0001fff0: 2076 616c 6964 6174 696f 6e5f 6672 6163   validation_frac
-00020000: 7469 6f6e 2c0a 2020 2020 2020 2774 6573  tion,.      'tes
-00020010: 745f 6672 6163 7469 6f6e 273a 2074 6573  t_fraction': tes
-00020020: 745f 6672 6163 7469 6f6e 2c0a 2020 2020  t_fraction,.    
-00020030: 2020 2774 665f 6175 746f 5f74 7261 6e73    'tf_auto_trans
-00020040: 666f 726d 5f66 6561 7475 7265 7327 3a20  form_features': 
-00020050: 280a 2020 2020 2020 2020 2020 7466 5f61  (.          tf_a
-00020060: 7574 6f5f 7472 616e 7366 6f72 6d5f 6665  uto_transform_fe
-00020070: 6174 7572 6573 2069 6620 7466 5f61 7574  atures if tf_aut
-00020080: 6f5f 7472 616e 7366 6f72 6d5f 6665 6174  o_transform_feat
-00020090: 7572 6573 2065 6c73 6520 7b7d 0a20 2020  ures else {}.   
-000200a0: 2020 2029 2c0a 2020 2020 2020 2774 665f     ),.      'tf_
-000200b0: 6375 7374 6f6d 5f74 7261 6e73 666f 726d  custom_transform
-000200c0: 6174 696f 6e5f 6465 6669 6e69 7469 6f6e  ation_definition
-000200d0: 7327 3a20 280a 2020 2020 2020 2020 2020  s': (.          
-000200e0: 7466 5f63 7573 746f 6d5f 7472 616e 7366  tf_custom_transf
-000200f0: 6f72 6d61 7469 6f6e 5f64 6566 696e 6974  ormation_definit
-00020100: 696f 6e73 0a20 2020 2020 2020 2020 2069  ions.          i
-00020110: 6620 7466 5f63 7573 746f 6d5f 7472 616e  f tf_custom_tran
-00020120: 7366 6f72 6d61 7469 6f6e 5f64 6566 696e  sformation_defin
-00020130: 6974 696f 6e73 0a20 2020 2020 2020 2020  itions.         
-00020140: 2065 6c73 6520 5b5d 0a20 2020 2020 2029   else [].      )
-00020150: 2c0a 2020 2020 2020 2774 665f 7472 616e  ,.      'tf_tran
-00020160: 7366 6f72 6d61 7469 6f6e 735f 7061 7468  sformations_path
-00020170: 273a 2074 665f 7472 616e 7366 6f72 6d61  ': tf_transforma
-00020180: 7469 6f6e 735f 7061 7468 2c0a 2020 2020  tions_path,.    
-00020190: 2020 276d 6174 6572 6961 6c69 7a65 645f    'materialized_
-000201a0: 6578 616d 706c 6573 5f66 6f72 6d61 7427  examples_format'
-000201b0: 3a20 280a 2020 2020 2020 2020 2020 6d61  : (.          ma
-000201c0: 7465 7269 616c 697a 6564 5f65 7861 6d70  terialized_examp
-000201d0: 6c65 735f 666f 726d 6174 0a20 2020 2020  les_format.     
-000201e0: 2020 2020 2069 6620 6d61 7465 7269 616c       if material
-000201f0: 697a 6564 5f65 7861 6d70 6c65 735f 666f  ized_examples_fo
-00020200: 726d 6174 0a20 2020 2020 2020 2020 2065  rmat.          e
-00020210: 6c73 6520 2774 6672 6563 6f72 6473 5f67  lse 'tfrecords_g
-00020220: 7a69 7027 0a20 2020 2020 2029 2c0a 2020  zip'.      ),.  
-00020230: 2020 2020 2774 665f 7472 616e 7366 6f72      'tf_transfor
-00020240: 6d5f 6578 6563 7574 696f 6e5f 656e 6769  m_execution_engi
-00020250: 6e65 273a 2028 0a20 2020 2020 2020 2020  ne': (.         
-00020260: 2074 665f 7472 616e 7366 6f72 6d5f 6578   tf_transform_ex
-00020270: 6563 7574 696f 6e5f 656e 6769 6e65 0a20  ecution_engine. 
-00020280: 2020 2020 2020 2020 2069 6620 7466 5f74           if tf_t
-00020290: 7261 6e73 666f 726d 5f65 7865 6375 7469  ransform_executi
-000202a0: 6f6e 5f65 6e67 696e 650a 2020 2020 2020  on_engine.      
-000202b0: 2020 2020 656c 7365 2027 6461 7461 666c      else 'datafl
-000202c0: 6f77 270a 2020 2020 2020 292c 0a20 207d  ow'.      ),.  }
-000202d0: 0a20 205f 7570 6461 7465 5f70 6172 616d  .  _update_param
-000202e0: 6574 6572 7328 7061 7261 6d65 7465 725f  eters(parameter_
-000202f0: 7661 6c75 6573 2c20 6674 655f 7061 7261  values, fte_para
-00020300: 6d73 290a 0a20 2064 6174 615f 736f 7572  ms)..  data_sour
-00020310: 6365 5f61 6e64 5f73 706c 6974 5f70 6172  ce_and_split_par
-00020320: 616d 6574 6572 7320 3d20 7b0a 2020 2020  ameters = {.    
-00020330: 2020 2764 6174 615f 736f 7572 6365 5f63    'data_source_c
-00020340: 7376 5f66 696c 656e 616d 6573 273a 2064  sv_filenames': d
-00020350: 6174 615f 736f 7572 6365 5f63 7376 5f66  ata_source_csv_f
-00020360: 696c 656e 616d 6573 2c0a 2020 2020 2020  ilenames,.      
-00020370: 2764 6174 615f 736f 7572 6365 5f62 6967  'data_source_big
-00020380: 7175 6572 795f 7461 626c 655f 7061 7468  query_table_path
-00020390: 273a 2064 6174 615f 736f 7572 6365 5f62  ': data_source_b
-000203a0: 6967 7175 6572 795f 7461 626c 655f 7061  igquery_table_pa
-000203b0: 7468 2c0a 2020 2020 2020 2762 6967 7175  th,.      'bigqu
-000203c0: 6572 795f 7374 6167 696e 675f 6675 6c6c  ery_staging_full
-000203d0: 5f64 6174 6173 6574 5f69 6427 3a20 6269  _dataset_id': bi
-000203e0: 6771 7565 7279 5f73 7461 6769 6e67 5f66  gquery_staging_f
-000203f0: 756c 6c5f 6461 7461 7365 745f 6964 2c0a  ull_dataset_id,.
-00020400: 2020 7d0a 2020 5f75 7064 6174 655f 7061    }.  _update_pa
-00020410: 7261 6d65 7465 7273 2870 6172 616d 6574  rameters(paramet
-00020420: 6572 5f76 616c 7565 732c 2064 6174 615f  er_values, data_
-00020430: 736f 7572 6365 5f61 6e64 5f73 706c 6974  source_and_split
-00020440: 5f70 6172 616d 6574 6572 7329 0a0a 2020  _parameters)..  
-00020450: 7069 7065 6c69 6e65 5f64 6566 696e 6974  pipeline_definit
-00020460: 696f 6e5f 7061 7468 203d 206f 732e 7061  ion_path = os.pa
-00020470: 7468 2e6a 6f69 6e28 0a20 2020 2020 2070  th.join(.      p
-00020480: 6174 686c 6962 2e50 6174 6828 5f5f 6669  athlib.Path(__fi
-00020490: 6c65 5f5f 292e 7061 7265 6e74 2e72 6573  le__).parent.res
-000204a0: 6f6c 7665 2829 2c20 2774 6162 6e65 745f  olve(), 'tabnet_
-000204b0: 7472 6169 6e65 725f 7069 7065 6c69 6e65  trainer_pipeline
-000204c0: 2e79 616d 6c27 0a20 2029 0a0a 2020 7265  .yaml'.  )..  re
-000204d0: 7475 726e 2070 6970 656c 696e 655f 6465  turn pipeline_de
-000204e0: 6669 6e69 7469 6f6e 5f70 6174 682c 2070  finition_path, p
-000204f0: 6172 616d 6574 6572 5f76 616c 7565 730a  arameter_values.
-00020500: 0a0a 6465 6620 6765 745f 7461 626e 6574  ..def get_tabnet
-00020510: 5f73 7475 6479 5f73 7065 635f 7061 7261  _study_spec_para
-00020520: 6d65 7465 7273 5f6f 7665 7272 6964 6528  meters_override(
-00020530: 0a20 2020 2064 6174 6173 6574 5f73 697a  .    dataset_siz
-00020540: 655f 6275 636b 6574 3a20 7374 722c 2070  e_bucket: str, p
-00020550: 7265 6469 6374 696f 6e5f 7479 7065 3a20  rediction_type: 
-00020560: 7374 722c 2074 7261 696e 696e 675f 6275  str, training_bu
-00020570: 6467 6574 5f62 7563 6b65 743a 2073 7472  dget_bucket: str
-00020580: 0a29 202d 3e20 4c69 7374 5b44 6963 745b  .) -> List[Dict[
-00020590: 7374 722c 2041 6e79 5d5d 3a0a 2020 2222  str, Any]]:.  ""
-000205a0: 2247 6574 2073 7475 6479 5f73 7065 635f  "Get study_spec_
-000205b0: 7061 7261 6d65 7465 7273 5f6f 7665 7272  parameters_overr
-000205c0: 6964 6520 666f 7220 6120 5461 624e 6574  ide for a TabNet
-000205d0: 2068 7970 6572 7061 7261 6d65 7465 7220   hyperparameter 
-000205e0: 7475 6e69 6e67 206a 6f62 2e0a 0a20 2041  tuning job...  A
-000205f0: 7267 733a 0a20 2020 2064 6174 6173 6574  rgs:.    dataset
-00020600: 5f73 697a 655f 6275 636b 6574 3a20 5369  _size_bucket: Si
-00020610: 7a65 206f 6620 7468 6520 6461 7461 7365  ze of the datase
-00020620: 742e 204f 6e65 206f 6620 2273 6d61 6c6c  t. One of "small
-00020630: 2220 283c 2031 4d20 726f 7773 292c 0a20  " (< 1M rows),. 
-00020640: 2020 2020 2022 6d65 6469 756d 2220 2831       "medium" (1
-00020650: 4d20 2d20 3130 304d 2072 6f77 7329 2c20  M - 100M rows), 
-00020660: 6f72 2022 6c61 7267 6522 2028 3e20 3130  or "large" (> 10
-00020670: 304d 2072 6f77 7329 2e0a 2020 2020 7072  0M rows)..    pr
-00020680: 6564 6963 7469 6f6e 5f74 7970 653a 2054  ediction_type: T
-00020690: 6865 2074 7970 6520 6f66 2070 7265 6469  he type of predi
-000206a0: 6374 696f 6e20 7468 6520 6d6f 6465 6c20  ction the model 
-000206b0: 6973 2074 6f20 7072 6f64 7563 652e 0a20  is to produce.. 
-000206c0: 2020 2020 2022 636c 6173 7369 6669 6361       "classifica
-000206d0: 7469 6f6e 2220 6f72 2022 7265 6772 6573  tion" or "regres
-000206e0: 7369 6f6e 222e 0a20 2020 2074 7261 696e  sion"..    train
-000206f0: 696e 675f 6275 6467 6574 5f62 7563 6b65  ing_budget_bucke
-00020700: 743a 2042 7563 6b65 7420 6f66 2074 6865  t: Bucket of the
-00020710: 2065 7374 696d 6174 6564 2074 7261 696e   estimated train
-00020720: 696e 6720 6275 6467 6574 2e20 4f6e 6520  ing budget. One 
-00020730: 6f66 0a20 2020 2020 2022 736d 616c 6c22  of.      "small"
-00020740: 2028 3c20 2436 3030 292c 2022 6d65 6469   (< $600), "medi
-00020750: 756d 2220 2824 3630 3020 2d20 2432 3430  um" ($600 - $240
-00020760: 3029 2c20 6f72 2022 6c61 7267 6522 2028  0), or "large" (
-00020770: 3e20 2432 3430 3029 2e20 5468 6973 0a20  > $2400). This. 
-00020780: 2020 2020 2070 6172 616d 6574 6572 2069       parameter i
-00020790: 7320 6f6e 6c79 2075 7365 6420 6173 2061  s only used as a
-000207a0: 2068 696e 7420 666f 7220 7468 6520 6879   hint for the hy
-000207b0: 7065 7270 6172 616d 6574 6572 2073 6561  perparameter sea
-000207c0: 7263 6820 7370 6163 652c 0a20 2020 2020  rch space,.     
-000207d0: 2075 6e72 656c 6174 6564 2074 6f20 7468   unrelated to th
-000207e0: 6520 7265 616c 2063 6f73 742e 0a0a 2020  e real cost...  
-000207f0: 5265 7475 726e 733a 0a20 2020 204c 6973  Returns:.    Lis
-00020800: 7420 6f66 2073 7475 6479 5f73 7065 635f  t of study_spec_
-00020810: 7061 7261 6d65 7465 7273 5f6f 7665 7272  parameters_overr
-00020820: 6964 652e 0a20 2022 2222 0a0a 2020 6966  ide..  """..  if
-00020830: 2064 6174 6173 6574 5f73 697a 655f 6275   dataset_size_bu
-00020840: 636b 6574 206e 6f74 2069 6e20 5b27 736d  cket not in ['sm
-00020850: 616c 6c27 2c20 276d 6564 6975 6d27 2c20  all', 'medium', 
-00020860: 276c 6172 6765 275d 3a0a 2020 2020 7261  'large']:.    ra
-00020870: 6973 6520 5661 6c75 6545 7272 6f72 280a  ise ValueError(.
-00020880: 2020 2020 2020 2020 2749 6e76 616c 6964          'Invalid
-00020890: 2064 6174 6173 6574 5f73 697a 655f 6275   dataset_size_bu
-000208a0: 636b 6574 2070 726f 7669 6465 642e 2053  cket provided. S
-000208b0: 7570 706f 7274 6564 2076 616c 7565 7320  upported values 
-000208c0: 270a 2020 2020 2020 2020 2720 6172 6520  '.        ' are 
-000208d0: 2273 6d61 6c6c 222c 2022 6d65 6469 756d  "small", "medium
-000208e0: 2220 6f72 2022 6c61 7267 6522 2e27 0a20  " or "large".'. 
-000208f0: 2020 2029 0a20 2069 6620 7472 6169 6e69     ).  if traini
-00020900: 6e67 5f62 7564 6765 745f 6275 636b 6574  ng_budget_bucket
-00020910: 206e 6f74 2069 6e20 5b27 736d 616c 6c27   not in ['small'
-00020920: 2c20 276d 6564 6975 6d27 2c20 276c 6172  , 'medium', 'lar
-00020930: 6765 275d 3a0a 2020 2020 7261 6973 6520  ge']:.    raise 
-00020940: 5661 6c75 6545 7272 6f72 280a 2020 2020  ValueError(.    
-00020950: 2020 2020 2749 6e76 616c 6964 2074 7261      'Invalid tra
-00020960: 696e 696e 675f 6275 6467 6574 5f62 7563  ining_budget_buc
-00020970: 6b65 7420 7072 6f76 6964 6564 2e20 5375  ket provided. Su
-00020980: 7070 6f72 7465 6420 7661 6c75 6573 2027  pported values '
-00020990: 0a20 2020 2020 2020 2027 6172 6520 2273  .        'are "s
-000209a0: 6d61 6c6c 222c 2022 6d65 6469 756d 2220  mall", "medium" 
-000209b0: 6f72 2022 6c61 7267 6522 2e27 0a20 2020  or "large".'.   
-000209c0: 2029 0a0a 2020 7061 7261 6d5f 7061 7468   )..  param_path
-000209d0: 203d 206f 732e 7061 7468 2e6a 6f69 6e28   = os.path.join(
-000209e0: 0a20 2020 2020 2070 6174 686c 6962 2e50  .      pathlib.P
-000209f0: 6174 6828 5f5f 6669 6c65 5f5f 292e 7061  ath(__file__).pa
-00020a00: 7265 6e74 2e72 6573 6f6c 7665 2829 2c0a  rent.resolve(),.
-00020a10: 2020 2020 2020 6627 636f 6e66 6967 732f        f'configs/
-00020a20: 7461 626e 6574 5f70 6172 616d 735f 7b64  tabnet_params_{d
-00020a30: 6174 6173 6574 5f73 697a 655f 6275 636b  ataset_size_buck
-00020a40: 6574 7d5f 6461 7461 5f7b 7472 6169 6e69  et}_data_{traini
-00020a50: 6e67 5f62 7564 6765 745f 6275 636b 6574  ng_budget_bucket
-00020a60: 7d5f 7365 6172 6368 5f73 7061 6365 2e6a  }_search_space.j
-00020a70: 736f 6e27 2c0a 2020 290a 2020 7769 7468  son',.  ).  with
-00020a80: 206f 7065 6e28 7061 7261 6d5f 7061 7468   open(param_path
-00020a90: 2c20 2772 2729 2061 7320 663a 0a20 2020  , 'r') as f:.   
-00020aa0: 2070 6172 616d 5f63 6f6e 7465 6e74 203d   param_content =
-00020ab0: 2066 2e72 6561 6428 290a 2020 2020 7061   f.read().    pa
-00020ac0: 7261 6d73 203d 206a 736f 6e2e 6c6f 6164  rams = json.load
-00020ad0: 7328 7061 7261 6d5f 636f 6e74 656e 7429  s(param_content)
-00020ae0: 0a0a 2020 6966 2070 7265 6469 6374 696f  ..  if predictio
-00020af0: 6e5f 7479 7065 203d 3d20 2772 6567 7265  n_type == 'regre
-00020b00: 7373 696f 6e27 3a0a 2020 2020 7265 7475  ssion':.    retu
-00020b10: 726e 205f 666f 726d 6174 5f74 6162 6e65  rn _format_tabne
-00020b20: 745f 7265 6772 6573 7369 6f6e 5f73 7475  t_regression_stu
-00020b30: 6479 5f73 7065 635f 7061 7261 6d65 7465  dy_spec_paramete
-00020b40: 7273 5f6f 7665 7272 6964 6528 0a20 2020  rs_override(.   
-00020b50: 2020 2020 2070 6172 616d 732c 2074 7261       params, tra
-00020b60: 696e 696e 675f 6275 6467 6574 5f62 7563  ining_budget_buc
-00020b70: 6b65 740a 2020 2020 290a 2020 7265 7475  ket.    ).  retu
-00020b80: 726e 2070 6172 616d 730a 0a0a 6465 6620  rn params...def 
-00020b90: 5f66 6f72 6d61 745f 7461 626e 6574 5f72  _format_tabnet_r
-00020ba0: 6567 7265 7373 696f 6e5f 7374 7564 795f  egression_study_
-00020bb0: 7370 6563 5f70 6172 616d 6574 6572 735f  spec_parameters_
-00020bc0: 6f76 6572 7269 6465 280a 2020 2020 7061  override(.    pa
-00020bd0: 7261 6d73 3a20 4c69 7374 5b44 6963 745b  rams: List[Dict[
-00020be0: 7374 722c 2041 6e79 5d5d 2c20 7472 6169  str, Any]], trai
-00020bf0: 6e69 6e67 5f62 7564 6765 745f 6275 636b  ning_budget_buck
-00020c00: 6574 3a20 7374 720a 2920 2d3e 204c 6973  et: str.) -> Lis
-00020c10: 745b 4469 6374 5b73 7472 2c20 416e 795d  t[Dict[str, Any]
-00020c20: 5d3a 0a20 2022 2222 4765 7420 7265 6772  ]:.  """Get regr
-00020c30: 6573 7369 6f6e 2073 7475 6479 5f73 7065  ession study_spe
-00020c40: 635f 7061 7261 6d65 7465 7273 5f6f 7665  c_parameters_ove
-00020c50: 7272 6964 6520 666f 7220 6120 5461 624e  rride for a TabN
-00020c60: 6574 2068 7970 6572 7061 7261 6d65 7465  et hyperparamete
-00020c70: 7220 7475 6e69 6e67 206a 6f62 2e0a 0a20  r tuning job... 
-00020c80: 2041 7267 733a 0a20 2020 2070 6172 616d   Args:.    param
-00020c90: 733a 204c 6973 7420 6f66 2064 6963 7469  s: List of dicti
-00020ca0: 6f6e 6172 6965 7320 7265 7072 6573 656e  onaries represen
-00020cb0: 7469 6e67 2070 6172 616d 6574 6572 7320  ting parameters 
-00020cc0: 746f 206f 7074 696d 697a 652e 2054 6865  to optimize. The
-00020cd0: 0a20 2020 2020 2064 6963 7469 6f6e 6172  .      dictionar
-00020ce0: 7920 6b65 7920 6973 2074 6865 2070 6172  y key is the par
-00020cf0: 616d 6574 6572 5f69 642c 2077 6869 6368  ameter_id, which
-00020d00: 2069 7320 7061 7373 6564 2074 6f20 7472   is passed to tr
-00020d10: 6169 6e69 6e67 206a 6f62 2061 7320 610a  aining job as a.
-00020d20: 2020 2020 2020 636f 6d6d 616e 6420 6c69        command li
-00020d30: 6e65 2061 7267 756d 656e 742c 2061 6e64  ne argument, and
-00020d40: 2074 6865 2064 6963 7469 6f6e 6172 7920   the dictionary 
-00020d50: 7661 6c75 6520 6973 2074 6865 2070 6172  value is the par
-00020d60: 616d 6574 6572 0a20 2020 2020 2073 7065  ameter.      spe
-00020d70: 6369 6669 6361 7469 6f6e 206f 6620 7468  cification of th
-00020d80: 6520 6d65 7472 6963 2e0a 2020 2020 7472  e metric..    tr
-00020d90: 6169 6e69 6e67 5f62 7564 6765 745f 6275  aining_budget_bu
-00020da0: 636b 6574 3a20 4275 636b 6574 206f 6620  cket: Bucket of 
-00020db0: 7468 6520 6573 7469 6d61 7465 6420 7472  the estimated tr
-00020dc0: 6169 6e69 6e67 2062 7564 6765 742e 204f  aining budget. O
-00020dd0: 6e65 206f 660a 2020 2020 2020 2273 6d61  ne of.      "sma
-00020de0: 6c6c 2220 283c 2024 3630 3029 2c20 226d  ll" (< $600), "m
-00020df0: 6564 6975 6d22 2028 2436 3030 202d 2024  edium" ($600 - $
-00020e00: 3234 3030 292c 206f 7220 226c 6172 6765  2400), or "large
-00020e10: 2220 283e 2024 3234 3030 292e 2054 6869  " (> $2400). Thi
-00020e20: 730a 2020 2020 2020 7061 7261 6d65 7465  s.      paramete
-00020e30: 7220 6973 206f 6e6c 7920 7573 6564 2061  r is only used a
-00020e40: 7320 6120 6869 6e74 2066 6f72 2074 6865  s a hint for the
-00020e50: 2068 7970 6572 7061 7261 6d65 7465 7220   hyperparameter 
-00020e60: 7365 6172 6368 2073 7061 6365 2c0a 2020  search space,.  
-00020e70: 2020 2020 756e 7265 6c61 7465 6420 746f      unrelated to
-00020e80: 2074 6865 2072 6561 6c20 636f 7374 2e0a   the real cost..
-00020e90: 0a20 2052 6574 7572 6e73 3a0a 2020 2020  .  Returns:.    
-00020ea0: 4c69 7374 206f 6620 7374 7564 795f 7370  List of study_sp
-00020eb0: 6563 5f70 6172 616d 6574 6572 735f 6f76  ec_parameters_ov
-00020ec0: 6572 7269 6465 2066 6f72 2072 6567 7265  erride for regre
-00020ed0: 7373 696f 6e2e 0a20 2022 2222 0a0a 2020  ssion..  """..  
-00020ee0: 2320 546f 2067 6574 2072 6567 7265 7373  # To get regress
-00020ef0: 696f 6e20 7374 7564 795f 7370 6563 5f70  ion study_spec_p
-00020f00: 6172 616d 6574 6572 732c 2077 6520 6e65  arameters, we ne
-00020f10: 6564 2074 6f20 7365 740a 2020 2320 606c  ed to set.  # `l
-00020f20: 6f73 735f 6675 6e63 7469 6f6e 5f74 7970  oss_function_typ
-00020f30: 6560 2074 6f20 e280 986d 6165 e280 9920  e` to ...mae... 
-00020f40: 28e2 8098 6d61 65e2 8099 2061 6e64 20e2  (...mae... and .
-00020f50: 8098 6d73 65e2 8099 2066 6f72 2022 6c61  ..mse... for "la
-00020f60: 7267 6522 2073 6561 7263 6820 7370 6163  rge" search spac
-00020f70: 6529 2c0a 2020 2320 7265 6d6f 7665 2074  e),.  # remove t
-00020f80: 6865 2060 616c 7068 615f 666f 6361 6c5f  he `alpha_focal_
-00020f90: 6c6f 7373 602c 2060 6761 6d6d 615f 666f  loss`, `gamma_fo
-00020fa0: 6361 6c5f 6c6f 7373 600a 2020 2320 616e  cal_loss`.  # an
-00020fb0: 6420 6063 6c61 7373 5f77 6569 6768 7460  d `class_weight`
-00020fc0: 2070 6172 616d 6574 6572 7320 616e 6420   parameters and 
-00020fd0: 696e 6372 6561 7365 2074 6865 206d 6178  increase the max
-00020fe0: 2066 6f72 0a20 2023 2060 7370 6172 7369   for.  # `sparsi
-00020ff0: 7479 5f6c 6f73 735f 7765 6967 6874 6020  ty_loss_weight` 
-00021000: 746f 2031 3030 2e0a 2020 666f 726d 6174  to 100..  format
-00021010: 7465 645f 7061 7261 6d73 203d 205b 5d0a  ted_params = [].
-00021020: 2020 666f 7220 7061 7261 6d20 696e 2070    for param in p
-00021030: 6172 616d 733a 0a20 2020 2069 6620 7061  arams:.    if pa
-00021040: 7261 6d5b 2770 6172 616d 6574 6572 5f69  ram['parameter_i
-00021050: 6427 5d20 696e 205b 0a20 2020 2020 2020  d'] in [.       
-00021060: 2027 616c 7068 615f 666f 6361 6c5f 6c6f   'alpha_focal_lo
-00021070: 7373 272c 0a20 2020 2020 2020 2027 6761  ss',.        'ga
-00021080: 6d6d 615f 666f 6361 6c5f 6c6f 7373 272c  mma_focal_loss',
-00021090: 0a20 2020 2020 2020 2027 636c 6173 735f  .        'class_
-000210a0: 7765 6967 6874 272c 0a20 2020 205d 3a0a  weight',.    ]:.
-000210b0: 2020 2020 2020 636f 6e74 696e 7565 0a20        continue. 
-000210c0: 2020 2065 6c69 6620 7061 7261 6d5b 2770     elif param['p
-000210d0: 6172 616d 6574 6572 5f69 6427 5d20 3d3d  arameter_id'] ==
-000210e0: 2027 7370 6172 7369 7479 5f6c 6f73 735f   'sparsity_loss_
-000210f0: 7765 6967 6874 273a 0a20 2020 2020 2070  weight':.      p
-00021100: 6172 616d 5b27 646f 7562 6c65 5f76 616c  aram['double_val
-00021110: 7565 5f73 7065 6327 5d5b 276d 6178 5f76  ue_spec']['max_v
-00021120: 616c 7565 275d 203d 2031 3030 0a20 2020  alue'] = 100.   
-00021130: 2065 6c69 6620 7061 7261 6d5b 2770 6172   elif param['par
-00021140: 616d 6574 6572 5f69 6427 5d20 3d3d 2027  ameter_id'] == '
-00021150: 6c6f 7373 5f66 756e 6374 696f 6e5f 7479  loss_function_ty
-00021160: 7065 273a 0a20 2020 2020 2069 6620 7472  pe':.      if tr
-00021170: 6169 6e69 6e67 5f62 7564 6765 745f 6275  aining_budget_bu
-00021180: 636b 6574 203d 3d20 276c 6172 6765 273a  cket == 'large':
-00021190: 0a20 2020 2020 2020 2070 6172 616d 5b27  .        param['
-000211a0: 6361 7465 676f 7269 6361 6c5f 7661 6c75  categorical_valu
-000211b0: 655f 7370 6563 275d 5b27 7661 6c75 6573  e_spec']['values
-000211c0: 275d 203d 205b 276d 6165 272c 2027 6d73  '] = ['mae', 'ms
-000211d0: 6527 5d0a 2020 2020 2020 656c 7365 3a0a  e'].      else:.
-000211e0: 2020 2020 2020 2020 7061 7261 6d5b 2763          param['c
-000211f0: 6174 6567 6f72 6963 616c 5f76 616c 7565  ategorical_value
-00021200: 5f73 7065 6327 5d5b 2776 616c 7565 7327  _spec']['values'
-00021210: 5d20 3d20 5b27 6d61 6527 5d0a 0a20 2020  ] = ['mae']..   
-00021220: 2066 6f72 6d61 7474 6564 5f70 6172 616d   formatted_param
-00021230: 732e 6170 7065 6e64 2870 6172 616d 290a  s.append(param).
-00021240: 0a20 2072 6574 7572 6e20 666f 726d 6174  .  return format
-00021250: 7465 645f 7061 7261 6d73 0a0a 0a64 6566  ted_params...def
-00021260: 2067 6574 5f77 6964 655f 616e 645f 6465   get_wide_and_de
-00021270: 6570 5f73 7475 6479 5f73 7065 635f 7061  ep_study_spec_pa
-00021280: 7261 6d65 7465 7273 5f6f 7665 7272 6964  rameters_overrid
-00021290: 6528 2920 2d3e 204c 6973 745b 4469 6374  e() -> List[Dict
-000212a0: 5b73 7472 2c20 416e 795d 5d3a 0a20 2022  [str, Any]]:.  "
-000212b0: 2222 4765 7420 7374 7564 795f 7370 6563  ""Get study_spec
-000212c0: 5f70 6172 616d 6574 6572 735f 6f76 6572  _parameters_over
-000212d0: 7269 6465 2066 6f72 2061 2057 6964 6520  ride for a Wide 
-000212e0: 2620 4465 6570 2068 7970 6572 7061 7261  & Deep hyperpara
-000212f0: 6d65 7465 7220 7475 6e69 6e67 206a 6f62  meter tuning job
-00021300: 2e0a 0a20 2052 6574 7572 6e73 3a0a 2020  ...  Returns:.  
-00021310: 2020 4c69 7374 206f 6620 7374 7564 795f    List of study_
-00021320: 7370 6563 5f70 6172 616d 6574 6572 735f  spec_parameters_
-00021330: 6f76 6572 7269 6465 2e0a 2020 2222 220a  override..  """.
-00021340: 2020 7061 7261 6d5f 7061 7468 203d 206f    param_path = o
-00021350: 732e 7061 7468 2e6a 6f69 6e28 0a20 2020  s.path.join(.   
-00021360: 2020 2070 6174 686c 6962 2e50 6174 6828     pathlib.Path(
-00021370: 5f5f 6669 6c65 5f5f 292e 7061 7265 6e74  __file__).parent
-00021380: 2e72 6573 6f6c 7665 2829 2c0a 2020 2020  .resolve(),.    
-00021390: 2020 2763 6f6e 6669 6773 2f77 6964 655f    'configs/wide_
-000213a0: 616e 645f 6465 6570 5f70 6172 616d 732e  and_deep_params.
-000213b0: 6a73 6f6e 272c 0a20 2029 0a20 2077 6974  json',.  ).  wit
-000213c0: 6820 6f70 656e 2870 6172 616d 5f70 6174  h open(param_pat
-000213d0: 682c 2027 7227 2920 6173 2066 3a0a 2020  h, 'r') as f:.  
-000213e0: 2020 7061 7261 6d5f 636f 6e74 656e 7420    param_content 
-000213f0: 3d20 662e 7265 6164 2829 0a20 2020 2070  = f.read().    p
-00021400: 6172 616d 7320 3d20 6a73 6f6e 2e6c 6f61  arams = json.loa
-00021410: 6473 2870 6172 616d 5f63 6f6e 7465 6e74  ds(param_content
-00021420: 290a 0a20 2072 6574 7572 6e20 7061 7261  )..  return para
-00021430: 6d73 0a0a 0a64 6566 2067 6574 5f78 6762  ms...def get_xgb
-00021440: 6f6f 7374 5f73 7475 6479 5f73 7065 635f  oost_study_spec_
-00021450: 7061 7261 6d65 7465 7273 5f6f 7665 7272  parameters_overr
-00021460: 6964 6528 2920 2d3e 204c 6973 745b 4469  ide() -> List[Di
-00021470: 6374 5b73 7472 2c20 416e 795d 5d3a 0a20  ct[str, Any]]:. 
-00021480: 2022 2222 4765 7420 7374 7564 795f 7370   """Get study_sp
-00021490: 6563 5f70 6172 616d 6574 6572 735f 6f76  ec_parameters_ov
-000214a0: 6572 7269 6465 2066 6f72 2061 6e20 5847  erride for an XG
-000214b0: 426f 6f73 7420 6879 7065 7270 6172 616d  Boost hyperparam
-000214c0: 6574 6572 2074 756e 696e 6720 6a6f 622e  eter tuning job.
-000214d0: 0a0a 2020 5265 7475 726e 733a 0a20 2020  ..  Returns:.   
-000214e0: 204c 6973 7420 6f66 2073 7475 6479 5f73   List of study_s
-000214f0: 7065 635f 7061 7261 6d65 7465 7273 5f6f  pec_parameters_o
-00021500: 7665 7272 6964 652e 0a20 2022 2222 0a20  verride..  """. 
-00021510: 2070 6172 616d 5f70 6174 6820 3d20 6f73   param_path = os
-00021520: 2e70 6174 682e 6a6f 696e 280a 2020 2020  .path.join(.    
-00021530: 2020 7061 7468 6c69 622e 5061 7468 285f    pathlib.Path(_
-00021540: 5f66 696c 655f 5f29 2e70 6172 656e 742e  _file__).parent.
-00021550: 7265 736f 6c76 6528 292c 2027 636f 6e66  resolve(), 'conf
-00021560: 6967 732f 7867 626f 6f73 745f 7061 7261  igs/xgboost_para
-00021570: 6d73 2e6a 736f 6e27 0a20 2029 0a20 2077  ms.json'.  ).  w
-00021580: 6974 6820 6f70 656e 2870 6172 616d 5f70  ith open(param_p
-00021590: 6174 682c 2027 7227 2920 6173 2066 3a0a  ath, 'r') as f:.
-000215a0: 2020 2020 7061 7261 6d5f 636f 6e74 656e      param_conten
-000215b0: 7420 3d20 662e 7265 6164 2829 0a20 2020  t = f.read().   
-000215c0: 2070 6172 616d 7320 3d20 6a73 6f6e 2e6c   params = json.l
-000215d0: 6f61 6473 2870 6172 616d 5f63 6f6e 7465  oads(param_conte
-000215e0: 6e74 290a 0a20 2072 6574 7572 6e20 7061  nt)..  return pa
-000215f0: 7261 6d73 0a0a 0a64 6566 2067 6574 5f78  rams...def get_x
-00021600: 6762 6f6f 7374 5f74 7261 696e 6572 5f70  gboost_trainer_p
-00021610: 6970 656c 696e 655f 616e 645f 7061 7261  ipeline_and_para
-00021620: 6d65 7465 7273 280a 2020 2020 7072 6f6a  meters(.    proj
-00021630: 6563 743a 2073 7472 2c0a 2020 2020 6c6f  ect: str,.    lo
-00021640: 6361 7469 6f6e 3a20 7374 722c 0a20 2020  cation: str,.   
-00021650: 2072 6f6f 745f 6469 723a 2073 7472 2c0a   root_dir: str,.
-00021660: 2020 2020 7461 7267 6574 5f63 6f6c 756d      target_colum
-00021670: 6e3a 2073 7472 2c0a 2020 2020 6f62 6a65  n: str,.    obje
-00021680: 6374 6976 653a 2073 7472 2c0a 2020 2020  ctive: str,.    
-00021690: 6576 616c 5f6d 6574 7269 633a 204f 7074  eval_metric: Opt
-000216a0: 696f 6e61 6c5b 7374 725d 203d 204e 6f6e  ional[str] = Non
-000216b0: 652c 0a20 2020 206e 756d 5f62 6f6f 7374  e,.    num_boost
-000216c0: 5f72 6f75 6e64 3a20 4f70 7469 6f6e 616c  _round: Optional
-000216d0: 5b69 6e74 5d20 3d20 4e6f 6e65 2c0a 2020  [int] = None,.  
-000216e0: 2020 6561 726c 795f 7374 6f70 7069 6e67    early_stopping
-000216f0: 5f72 6f75 6e64 733a 204f 7074 696f 6e61  _rounds: Optiona
-00021700: 6c5b 696e 745d 203d 204e 6f6e 652c 0a20  l[int] = None,. 
-00021710: 2020 2062 6173 655f 7363 6f72 653a 204f     base_score: O
-00021720: 7074 696f 6e61 6c5b 666c 6f61 745d 203d  ptional[float] =
-00021730: 204e 6f6e 652c 0a20 2020 2064 6973 6162   None,.    disab
-00021740: 6c65 5f64 6566 6175 6c74 5f65 7661 6c5f  le_default_eval_
-00021750: 6d65 7472 6963 3a20 4f70 7469 6f6e 616c  metric: Optional
-00021760: 5b69 6e74 5d20 3d20 4e6f 6e65 2c0a 2020  [int] = None,.  
-00021770: 2020 7365 6564 3a20 4f70 7469 6f6e 616c    seed: Optional
-00021780: 5b69 6e74 5d20 3d20 4e6f 6e65 2c0a 2020  [int] = None,.  
-00021790: 2020 7365 6564 5f70 6572 5f69 7465 7261    seed_per_itera
-000217a0: 7469 6f6e 3a20 4f70 7469 6f6e 616c 5b62  tion: Optional[b
-000217b0: 6f6f 6c5d 203d 204e 6f6e 652c 0a20 2020  ool] = None,.   
-000217c0: 2062 6f6f 7374 6572 3a20 4f70 7469 6f6e   booster: Option
-000217d0: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
-000217e0: 2020 2020 6574 613a 204f 7074 696f 6e61      eta: Optiona
-000217f0: 6c5b 666c 6f61 745d 203d 204e 6f6e 652c  l[float] = None,
-00021800: 0a20 2020 2067 616d 6d61 3a20 4f70 7469  .    gamma: Opti
-00021810: 6f6e 616c 5b66 6c6f 6174 5d20 3d20 4e6f  onal[float] = No
-00021820: 6e65 2c0a 2020 2020 6d61 785f 6465 7074  ne,.    max_dept
-00021830: 683a 204f 7074 696f 6e61 6c5b 696e 745d  h: Optional[int]
-00021840: 203d 204e 6f6e 652c 0a20 2020 206d 696e   = None,.    min
-00021850: 5f63 6869 6c64 5f77 6569 6768 743a 204f  _child_weight: O
-00021860: 7074 696f 6e61 6c5b 666c 6f61 745d 203d  ptional[float] =
-00021870: 204e 6f6e 652c 0a20 2020 206d 6178 5f64   None,.    max_d
-00021880: 656c 7461 5f73 7465 703a 204f 7074 696f  elta_step: Optio
-00021890: 6e61 6c5b 666c 6f61 745d 203d 204e 6f6e  nal[float] = Non
-000218a0: 652c 0a20 2020 2073 7562 7361 6d70 6c65  e,.    subsample
-000218b0: 3a20 4f70 7469 6f6e 616c 5b66 6c6f 6174  : Optional[float
-000218c0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 636f  ] = None,.    co
-000218d0: 6c73 616d 706c 655f 6279 7472 6565 3a20  lsample_bytree: 
-000218e0: 4f70 7469 6f6e 616c 5b66 6c6f 6174 5d20  Optional[float] 
-000218f0: 3d20 4e6f 6e65 2c0a 2020 2020 636f 6c73  = None,.    cols
-00021900: 616d 706c 655f 6279 6c65 7665 6c3a 204f  ample_bylevel: O
-00021910: 7074 696f 6e61 6c5b 666c 6f61 745d 203d  ptional[float] =
-00021920: 204e 6f6e 652c 0a20 2020 2063 6f6c 7361   None,.    colsa
-00021930: 6d70 6c65 5f62 796e 6f64 653a 204f 7074  mple_bynode: Opt
-00021940: 696f 6e61 6c5b 666c 6f61 745d 203d 204e  ional[float] = N
-00021950: 6f6e 652c 0a20 2020 2072 6567 5f6c 616d  one,.    reg_lam
-00021960: 6264 613a 204f 7074 696f 6e61 6c5b 666c  bda: Optional[fl
-00021970: 6f61 745d 203d 204e 6f6e 652c 0a20 2020  oat] = None,.   
-00021980: 2072 6567 5f61 6c70 6861 3a20 4f70 7469   reg_alpha: Opti
-00021990: 6f6e 616c 5b66 6c6f 6174 5d20 3d20 4e6f  onal[float] = No
-000219a0: 6e65 2c0a 2020 2020 7472 6565 5f6d 6574  ne,.    tree_met
-000219b0: 686f 643a 204f 7074 696f 6e61 6c5b 7374  hod: Optional[st
-000219c0: 725d 203d 204e 6f6e 652c 0a20 2020 2073  r] = None,.    s
-000219d0: 6361 6c65 5f70 6f73 5f77 6569 6768 743a  cale_pos_weight:
-000219e0: 204f 7074 696f 6e61 6c5b 666c 6f61 745d   Optional[float]
-000219f0: 203d 204e 6f6e 652c 0a20 2020 2075 7064   = None,.    upd
-00021a00: 6174 6572 3a20 4f70 7469 6f6e 616c 5b73  ater: Optional[s
-00021a10: 7472 5d20 3d20 4e6f 6e65 2c0a 2020 2020  tr] = None,.    
-00021a20: 7265 6672 6573 685f 6c65 6166 3a20 4f70  refresh_leaf: Op
-00021a30: 7469 6f6e 616c 5b69 6e74 5d20 3d20 4e6f  tional[int] = No
-00021a40: 6e65 2c0a 2020 2020 7072 6f63 6573 735f  ne,.    process_
-00021a50: 7479 7065 3a20 4f70 7469 6f6e 616c 5b73  type: Optional[s
-00021a60: 7472 5d20 3d20 4e6f 6e65 2c0a 2020 2020  tr] = None,.    
-00021a70: 6772 6f77 5f70 6f6c 6963 793a 204f 7074  grow_policy: Opt
-00021a80: 696f 6e61 6c5b 7374 725d 203d 204e 6f6e  ional[str] = Non
-00021a90: 652c 0a20 2020 2073 616d 706c 696e 675f  e,.    sampling_
-00021aa0: 6d65 7468 6f64 3a20 4f70 7469 6f6e 616c  method: Optional
-00021ab0: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 2020  [str] = None,.  
-00021ac0: 2020 6d6f 6e6f 746f 6e65 5f63 6f6e 7374    monotone_const
-00021ad0: 7261 696e 7473 3a20 4f70 7469 6f6e 616c  raints: Optional
-00021ae0: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 2020  [str] = None,.  
-00021af0: 2020 696e 7465 7261 6374 696f 6e5f 636f    interaction_co
-00021b00: 6e73 7472 6169 6e74 733a 204f 7074 696f  nstraints: Optio
-00021b10: 6e61 6c5b 7374 725d 203d 204e 6f6e 652c  nal[str] = None,
-00021b20: 0a20 2020 2073 616d 706c 655f 7479 7065  .    sample_type
-00021b30: 3a20 4f70 7469 6f6e 616c 5b73 7472 5d20  : Optional[str] 
-00021b40: 3d20 4e6f 6e65 2c0a 2020 2020 6e6f 726d  = None,.    norm
-00021b50: 616c 697a 655f 7479 7065 3a20 4f70 7469  alize_type: Opti
-00021b60: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
-00021b70: 2c0a 2020 2020 7261 7465 5f64 726f 703a  ,.    rate_drop:
-00021b80: 204f 7074 696f 6e61 6c5b 666c 6f61 745d   Optional[float]
-00021b90: 203d 204e 6f6e 652c 0a20 2020 206f 6e65   = None,.    one
-00021ba0: 5f64 726f 703a 204f 7074 696f 6e61 6c5b  _drop: Optional[
-00021bb0: 696e 745d 203d 204e 6f6e 652c 0a20 2020  int] = None,.   
-00021bc0: 2073 6b69 705f 6472 6f70 3a20 4f70 7469   skip_drop: Opti
-00021bd0: 6f6e 616c 5b66 6c6f 6174 5d20 3d20 4e6f  onal[float] = No
-00021be0: 6e65 2c0a 2020 2020 6e75 6d5f 7061 7261  ne,.    num_para
-00021bf0: 6c6c 656c 5f74 7265 653a 204f 7074 696f  llel_tree: Optio
-00021c00: 6e61 6c5b 696e 745d 203d 204e 6f6e 652c  nal[int] = None,
-00021c10: 0a20 2020 2066 6561 7475 7265 5f73 656c  .    feature_sel
-00021c20: 6563 746f 723a 204f 7074 696f 6e61 6c5b  ector: Optional[
-00021c30: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
-00021c40: 2074 6f70 5f6b 3a20 4f70 7469 6f6e 616c   top_k: Optional
-00021c50: 5b69 6e74 5d20 3d20 4e6f 6e65 2c0a 2020  [int] = None,.  
-00021c60: 2020 6d61 785f 6361 745f 746f 5f6f 6e65    max_cat_to_one
-00021c70: 686f 743a 204f 7074 696f 6e61 6c5b 696e  hot: Optional[in
-00021c80: 745d 203d 204e 6f6e 652c 0a20 2020 206d  t] = None,.    m
-00021c90: 6178 5f6c 6561 7665 733a 204f 7074 696f  ax_leaves: Optio
-00021ca0: 6e61 6c5b 696e 745d 203d 204e 6f6e 652c  nal[int] = None,
-00021cb0: 0a20 2020 206d 6178 5f62 696e 3a20 4f70  .    max_bin: Op
-00021cc0: 7469 6f6e 616c 5b69 6e74 5d20 3d20 4e6f  tional[int] = No
-00021cd0: 6e65 2c0a 2020 2020 7477 6565 6469 655f  ne,.    tweedie_
-00021ce0: 7661 7269 616e 6365 5f70 6f77 6572 3a20  variance_power: 
-00021cf0: 4f70 7469 6f6e 616c 5b66 6c6f 6174 5d20  Optional[float] 
-00021d00: 3d20 4e6f 6e65 2c0a 2020 2020 6875 6265  = None,.    hube
-00021d10: 725f 736c 6f70 653a 204f 7074 696f 6e61  r_slope: Optiona
-00021d20: 6c5b 666c 6f61 745d 203d 204e 6f6e 652c  l[float] = None,
-00021d30: 0a20 2020 2064 6174 6173 6574 5f6c 6576  .    dataset_lev
-00021d40: 656c 5f63 7573 746f 6d5f 7472 616e 7366  el_custom_transf
-00021d50: 6f72 6d61 7469 6f6e 5f64 6566 696e 6974  ormation_definit
-00021d60: 696f 6e73 3a20 4f70 7469 6f6e 616c 5b0a  ions: Optional[.
-00021d70: 2020 2020 2020 2020 4c69 7374 5b44 6963          List[Dic
-00021d80: 745b 7374 722c 2041 6e79 5d5d 0a20 2020  t[str, Any]].   
-00021d90: 205d 203d 204e 6f6e 652c 0a20 2020 2064   ] = None,.    d
-00021da0: 6174 6173 6574 5f6c 6576 656c 5f74 7261  ataset_level_tra
-00021db0: 6e73 666f 726d 6174 696f 6e73 3a20 4f70  nsformations: Op
-00021dc0: 7469 6f6e 616c 5b4c 6973 745b 4469 6374  tional[List[Dict
-00021dd0: 5b73 7472 2c20 416e 795d 5d5d 203d 204e  [str, Any]]] = N
-00021de0: 6f6e 652c 0a20 2020 2072 756e 5f66 6561  one,.    run_fea
-00021df0: 7475 7265 5f73 656c 6563 7469 6f6e 3a20  ture_selection: 
-00021e00: 4f70 7469 6f6e 616c 5b62 6f6f 6c5d 203d  Optional[bool] =
-00021e10: 204e 6f6e 652c 0a20 2020 2066 6561 7475   None,.    featu
-00021e20: 7265 5f73 656c 6563 7469 6f6e 5f61 6c67  re_selection_alg
-00021e30: 6f72 6974 686d 3a20 4f70 7469 6f6e 616c  orithm: Optional
-00021e40: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 2020  [str] = None,.  
-00021e50: 2020 6d61 785f 7365 6c65 6374 6564 5f66    max_selected_f
-00021e60: 6561 7475 7265 733a 204f 7074 696f 6e61  eatures: Optiona
-00021e70: 6c5b 696e 745d 203d 204e 6f6e 652c 0a20  l[int] = None,. 
-00021e80: 2020 2070 7265 6465 6669 6e65 645f 7370     predefined_sp
-00021e90: 6c69 745f 6b65 793a 204f 7074 696f 6e61  lit_key: Optiona
-00021ea0: 6c5b 7374 725d 203d 204e 6f6e 652c 0a20  l[str] = None,. 
-00021eb0: 2020 2073 7472 6174 6966 6965 645f 7370     stratified_sp
-00021ec0: 6c69 745f 6b65 793a 204f 7074 696f 6e61  lit_key: Optiona
-00021ed0: 6c5b 7374 725d 203d 204e 6f6e 652c 0a20  l[str] = None,. 
-00021ee0: 2020 2074 7261 696e 696e 675f 6672 6163     training_frac
-00021ef0: 7469 6f6e 3a20 4f70 7469 6f6e 616c 5b66  tion: Optional[f
-00021f00: 6c6f 6174 5d20 3d20 4e6f 6e65 2c0a 2020  loat] = None,.  
-00021f10: 2020 7661 6c69 6461 7469 6f6e 5f66 7261    validation_fra
-00021f20: 6374 696f 6e3a 204f 7074 696f 6e61 6c5b  ction: Optional[
-00021f30: 666c 6f61 745d 203d 204e 6f6e 652c 0a20  float] = None,. 
-00021f40: 2020 2074 6573 745f 6672 6163 7469 6f6e     test_fraction
-00021f50: 3a20 4f70 7469 6f6e 616c 5b66 6c6f 6174  : Optional[float
-00021f60: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7466  ] = None,.    tf
-00021f70: 5f61 7574 6f5f 7472 616e 7366 6f72 6d5f  _auto_transform_
-00021f80: 6665 6174 7572 6573 3a20 4f70 7469 6f6e  features: Option
-00021f90: 616c 5b0a 2020 2020 2020 2020 556e 696f  al[.        Unio
-00021fa0: 6e5b 4c69 7374 5b73 7472 5d2c 2044 6963  n[List[str], Dic
-00021fb0: 745b 7374 722c 204c 6973 745b 7374 725d  t[str, List[str]
-00021fc0: 5d5d 0a20 2020 205d 203d 204e 6f6e 652c  ]].    ] = None,
-00021fd0: 0a20 2020 2074 665f 6375 7374 6f6d 5f74  .    tf_custom_t
-00021fe0: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
-00021ff0: 6669 6e69 7469 6f6e 733a 204f 7074 696f  finitions: Optio
-00022000: 6e61 6c5b 4c69 7374 5b44 6963 745b 7374  nal[List[Dict[st
-00022010: 722c 2041 6e79 5d5d 5d20 3d20 4e6f 6e65  r, Any]]] = None
-00022020: 2c0a 2020 2020 7466 5f74 7261 6e73 666f  ,.    tf_transfo
-00022030: 726d 6174 696f 6e73 5f70 6174 683a 204f  rmations_path: O
-00022040: 7074 696f 6e61 6c5b 7374 725d 203d 204e  ptional[str] = N
-00022050: 6f6e 652c 0a20 2020 2064 6174 615f 736f  one,.    data_so
-00022060: 7572 6365 5f63 7376 5f66 696c 656e 616d  urce_csv_filenam
-00022070: 6573 3a20 4f70 7469 6f6e 616c 5b73 7472  es: Optional[str
-00022080: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6461  ] = None,.    da
-00022090: 7461 5f73 6f75 7263 655f 6269 6771 7565  ta_source_bigque
-000220a0: 7279 5f74 6162 6c65 5f70 6174 683a 204f  ry_table_path: O
-000220b0: 7074 696f 6e61 6c5b 7374 725d 203d 204e  ptional[str] = N
-000220c0: 6f6e 652c 0a20 2020 2062 6967 7175 6572  one,.    bigquer
-000220d0: 795f 7374 6167 696e 675f 6675 6c6c 5f64  y_staging_full_d
-000220e0: 6174 6173 6574 5f69 643a 204f 7074 696f  ataset_id: Optio
-000220f0: 6e61 6c5b 7374 725d 203d 204e 6f6e 652c  nal[str] = None,
-00022100: 0a20 2020 2077 6569 6768 745f 636f 6c75  .    weight_colu
-00022110: 6d6e 3a20 4f70 7469 6f6e 616c 5b73 7472  mn: Optional[str
-00022120: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7472  ] = None,.    tr
-00022130: 6169 6e69 6e67 5f6d 6163 6869 6e65 5f74  aining_machine_t
-00022140: 7970 653a 204f 7074 696f 6e61 6c5b 7374  ype: Optional[st
-00022150: 725d 203d 204e 6f6e 652c 0a20 2020 2074  r] = None,.    t
-00022160: 7261 696e 696e 675f 746f 7461 6c5f 7265  raining_total_re
-00022170: 706c 6963 615f 636f 756e 743a 204f 7074  plica_count: Opt
-00022180: 696f 6e61 6c5b 696e 745d 203d 204e 6f6e  ional[int] = Non
-00022190: 652c 0a20 2020 2074 7261 696e 696e 675f  e,.    training_
-000221a0: 6163 6365 6c65 7261 746f 725f 7479 7065  accelerator_type
-000221b0: 3a20 4f70 7469 6f6e 616c 5b73 7472 5d20  : Optional[str] 
-000221c0: 3d20 4e6f 6e65 2c0a 2020 2020 7472 6169  = None,.    trai
-000221d0: 6e69 6e67 5f61 6363 656c 6572 6174 6f72  ning_accelerator
-000221e0: 5f63 6f75 6e74 3a20 4f70 7469 6f6e 616c  _count: Optional
-000221f0: 5b69 6e74 5d20 3d20 4e6f 6e65 2c0a 2020  [int] = None,.  
-00022200: 2020 7472 616e 7366 6f72 6d5f 6461 7461    transform_data
-00022210: 666c 6f77 5f6d 6163 6869 6e65 5f74 7970  flow_machine_typ
-00022220: 653a 204f 7074 696f 6e61 6c5b 7374 725d  e: Optional[str]
-00022230: 203d 204e 6f6e 652c 0a20 2020 2074 7261   = None,.    tra
-00022240: 6e73 666f 726d 5f64 6174 6166 6c6f 775f  nsform_dataflow_
-00022250: 6d61 785f 6e75 6d5f 776f 726b 6572 733a  max_num_workers:
-00022260: 204f 7074 696f 6e61 6c5b 696e 745d 203d   Optional[int] =
-00022270: 204e 6f6e 652c 0a20 2020 2074 7261 6e73   None,.    trans
-00022280: 666f 726d 5f64 6174 6166 6c6f 775f 6469  form_dataflow_di
-00022290: 736b 5f73 697a 655f 6762 3a20 4f70 7469  sk_size_gb: Opti
-000222a0: 6f6e 616c 5b69 6e74 5d20 3d20 4e6f 6e65  onal[int] = None
-000222b0: 2c0a 2020 2020 7275 6e5f 6576 616c 7561  ,.    run_evalua
-000222c0: 7469 6f6e 3a20 4f70 7469 6f6e 616c 5b62  tion: Optional[b
-000222d0: 6f6f 6c5d 203d 204e 6f6e 652c 0a20 2020  ool] = None,.   
-000222e0: 2065 7661 6c75 6174 696f 6e5f 6261 7463   evaluation_batc
-000222f0: 685f 7072 6564 6963 745f 6d61 6368 696e  h_predict_machin
-00022300: 655f 7479 7065 3a20 4f70 7469 6f6e 616c  e_type: Optional
-00022310: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 2020  [str] = None,.  
-00022320: 2020 6576 616c 7561 7469 6f6e 5f62 6174    evaluation_bat
-00022330: 6368 5f70 7265 6469 6374 5f73 7461 7274  ch_predict_start
-00022340: 696e 675f 7265 706c 6963 615f 636f 756e  ing_replica_coun
-00022350: 743a 204f 7074 696f 6e61 6c5b 696e 745d  t: Optional[int]
-00022360: 203d 204e 6f6e 652c 0a20 2020 2065 7661   = None,.    eva
-00022370: 6c75 6174 696f 6e5f 6261 7463 685f 7072  luation_batch_pr
-00022380: 6564 6963 745f 6d61 785f 7265 706c 6963  edict_max_replic
-00022390: 615f 636f 756e 743a 204f 7074 696f 6e61  a_count: Optiona
-000223a0: 6c5b 696e 745d 203d 204e 6f6e 652c 0a20  l[int] = None,. 
-000223b0: 2020 2065 7661 6c75 6174 696f 6e5f 6461     evaluation_da
-000223c0: 7461 666c 6f77 5f6d 6163 6869 6e65 5f74  taflow_machine_t
-000223d0: 7970 653a 204f 7074 696f 6e61 6c5b 7374  ype: Optional[st
-000223e0: 725d 203d 204e 6f6e 652c 0a20 2020 2065  r] = None,.    e
-000223f0: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
-00022400: 6f77 5f73 7461 7274 696e 675f 6e75 6d5f  ow_starting_num_
-00022410: 776f 726b 6572 733a 204f 7074 696f 6e61  workers: Optiona
-00022420: 6c5b 696e 745d 203d 204e 6f6e 652c 0a20  l[int] = None,. 
-00022430: 2020 2065 7661 6c75 6174 696f 6e5f 6461     evaluation_da
-00022440: 7461 666c 6f77 5f6d 6178 5f6e 756d 5f77  taflow_max_num_w
-00022450: 6f72 6b65 7273 3a20 4f70 7469 6f6e 616c  orkers: Optional
-00022460: 5b69 6e74 5d20 3d20 4e6f 6e65 2c0a 2020  [int] = None,.  
-00022470: 2020 6576 616c 7561 7469 6f6e 5f64 6174    evaluation_dat
-00022480: 6166 6c6f 775f 6469 736b 5f73 697a 655f  aflow_disk_size_
-00022490: 6762 3a20 4f70 7469 6f6e 616c 5b69 6e74  gb: Optional[int
-000224a0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6461  ] = None,.    da
-000224b0: 7461 666c 6f77 5f73 6572 7669 6365 5f61  taflow_service_a
-000224c0: 6363 6f75 6e74 3a20 4f70 7469 6f6e 616c  ccount: Optional
-000224d0: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 2020  [str] = None,.  
-000224e0: 2020 6461 7461 666c 6f77 5f73 7562 6e65    dataflow_subne
-000224f0: 7477 6f72 6b3a 204f 7074 696f 6e61 6c5b  twork: Optional[
-00022500: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
-00022510: 2064 6174 6166 6c6f 775f 7573 655f 7075   dataflow_use_pu
-00022520: 626c 6963 5f69 7073 3a20 4f70 7469 6f6e  blic_ips: Option
-00022530: 616c 5b62 6f6f 6c5d 203d 204e 6f6e 652c  al[bool] = None,
-00022540: 0a20 2020 2065 6e63 7279 7074 696f 6e5f  .    encryption_
-00022550: 7370 6563 5f6b 6579 5f6e 616d 653a 204f  spec_key_name: O
-00022560: 7074 696f 6e61 6c5b 7374 725d 203d 204e  ptional[str] = N
-00022570: 6f6e 652c 0a29 3a0a 2020 2222 2247 6574  one,.):.  """Get
-00022580: 2074 6865 2058 4742 6f6f 7374 2074 7261   the XGBoost tra
-00022590: 696e 696e 6720 7069 7065 6c69 6e65 2e0a  ining pipeline..
-000225a0: 0a20 2041 7267 733a 0a20 2020 2070 726f  .  Args:.    pro
-000225b0: 6a65 6374 3a20 5468 6520 4743 5020 7072  ject: The GCP pr
-000225c0: 6f6a 6563 7420 7468 6174 2072 756e 7320  oject that runs 
-000225d0: 7468 6520 7069 7065 6c69 6e65 2063 6f6d  the pipeline com
-000225e0: 706f 6e65 6e74 732e 0a20 2020 206c 6f63  ponents..    loc
-000225f0: 6174 696f 6e3a 2054 6865 2047 4350 2072  ation: The GCP r
-00022600: 6567 696f 6e20 7468 6174 2072 756e 7320  egion that runs 
-00022610: 7468 6520 7069 7065 6c69 6e65 2063 6f6d  the pipeline com
-00022620: 706f 6e65 6e74 732e 0a20 2020 2072 6f6f  ponents..    roo
-00022630: 745f 6469 723a 2054 6865 2072 6f6f 7420  t_dir: The root 
-00022640: 4743 5320 6469 7265 6374 6f72 7920 666f  GCS directory fo
-00022650: 7220 7468 6520 7069 7065 6c69 6e65 2063  r the pipeline c
-00022660: 6f6d 706f 6e65 6e74 732e 0a20 2020 2074  omponents..    t
-00022670: 6172 6765 745f 636f 6c75 6d6e 3a20 5468  arget_column: Th
-00022680: 6520 7461 7267 6574 2063 6f6c 756d 6e20  e target column 
-00022690: 6e61 6d65 2e0a 2020 2020 6f62 6a65 6374  name..    object
-000226a0: 6976 653a 2053 7065 6369 6669 6573 2074  ive: Specifies t
-000226b0: 6865 206c 6561 726e 696e 6720 7461 736b  he learning task
-000226c0: 2061 6e64 2074 6865 206c 6561 726e 696e   and the learnin
-000226d0: 6720 6f62 6a65 6374 6976 652e 204d 7573  g objective. Mus
-000226e0: 7420 6265 0a20 2020 2020 206f 6e65 206f  t be.      one o
-000226f0: 6620 5b72 6567 3a73 7175 6172 6564 6572  f [reg:squareder
-00022700: 726f 722c 2072 6567 3a73 7175 6172 6564  ror, reg:squared
-00022710: 6c6f 6765 7272 6f72 2c0a 2020 2020 2020  logerror,.      
-00022720: 7265 673a 6c6f 6769 7374 6963 2c20 7265  reg:logistic, re
-00022730: 673a 6761 6d6d 612c 2072 6567 3a74 7765  g:gamma, reg:twe
-00022740: 6564 6965 2c20 7265 673a 7073 6575 646f  edie, reg:pseudo
-00022750: 6875 6265 7265 7272 6f72 2c0a 2020 2020  hubererror,.    
-00022760: 2020 6269 6e61 7279 3a6c 6f67 6973 7469    binary:logisti
-00022770: 632c 206d 756c 7469 3a73 6f66 7470 726f  c, multi:softpro
-00022780: 625d 2e0a 2020 2020 6576 616c 5f6d 6574  b]..    eval_met
-00022790: 7269 633a 2045 7661 6c75 6174 696f 6e20  ric: Evaluation 
-000227a0: 6d65 7472 6963 7320 666f 7220 7661 6c69  metrics for vali
-000227b0: 6461 7469 6f6e 2064 6174 6120 7265 7072  dation data repr
-000227c0: 6573 656e 7465 6420 6173 2061 0a20 2020  esented as a.   
-000227d0: 2020 2063 6f6d 6d61 2d73 6570 6172 6174     comma-separat
-000227e0: 6564 2073 7472 696e 672e 0a20 2020 206e  ed string..    n
-000227f0: 756d 5f62 6f6f 7374 5f72 6f75 6e64 3a20  um_boost_round: 
-00022800: 4e75 6d62 6572 206f 6620 626f 6f73 7469  Number of boosti
-00022810: 6e67 2069 7465 7261 7469 6f6e 732e 0a20  ng iterations.. 
-00022820: 2020 2065 6172 6c79 5f73 746f 7070 696e     early_stoppin
-00022830: 675f 726f 756e 6473 3a20 4163 7469 7661  g_rounds: Activa
-00022840: 7465 7320 6561 726c 7920 7374 6f70 7069  tes early stoppi
-00022850: 6e67 2e20 5661 6c69 6461 7469 6f6e 2065  ng. Validation e
-00022860: 7272 6f72 206e 6565 6473 2074 6f0a 2020  rror needs to.  
-00022870: 2020 2020 6465 6372 6561 7365 2061 7420      decrease at 
-00022880: 6c65 6173 7420 6576 6572 7920 6561 726c  least every earl
-00022890: 795f 7374 6f70 7069 6e67 5f72 6f75 6e64  y_stopping_round
-000228a0: 7320 726f 756e 6428 7329 2074 6f20 636f  s round(s) to co
-000228b0: 6e74 696e 7565 0a20 2020 2020 2074 7261  ntinue.      tra
-000228c0: 696e 696e 672e 0a20 2020 2062 6173 655f  ining..    base_
-000228d0: 7363 6f72 653a 2054 6865 2069 6e69 7469  score: The initi
-000228e0: 616c 2070 7265 6469 6374 696f 6e20 7363  al prediction sc
-000228f0: 6f72 6520 6f66 2061 6c6c 2069 6e73 7461  ore of all insta
-00022900: 6e63 6573 2c20 676c 6f62 616c 2062 6961  nces, global bia
-00022910: 732e 0a20 2020 2064 6973 6162 6c65 5f64  s..    disable_d
-00022920: 6566 6175 6c74 5f65 7661 6c5f 6d65 7472  efault_eval_metr
-00022930: 6963 3a20 466c 6167 2074 6f20 6469 7361  ic: Flag to disa
-00022940: 626c 6520 6465 6661 756c 7420 6d65 7472  ble default metr
-00022950: 6963 2e20 5365 7420 746f 203e 3020 746f  ic. Set to >0 to
-00022960: 0a20 2020 2020 2064 6973 6162 6c65 2e20  .      disable. 
-00022970: 4465 6661 756c 7420 746f 2030 2e0a 2020  Default to 0..  
-00022980: 2020 7365 6564 3a20 5261 6e64 6f6d 2073    seed: Random s
-00022990: 6565 642e 0a20 2020 2073 6565 645f 7065  eed..    seed_pe
-000229a0: 725f 6974 6572 6174 696f 6e3a 2053 6565  r_iteration: See
-000229b0: 6420 5052 4e47 2064 6574 6572 6d6e 6973  d PRNG determnis
-000229c0: 7469 636c 7920 7669 6120 6974 6572 6174  ticly via iterat
-000229d0: 6f72 206e 756d 6265 722e 0a20 2020 2062  or number..    b
-000229e0: 6f6f 7374 6572 3a20 5768 6963 6820 626f  ooster: Which bo
-000229f0: 6f73 7465 7220 746f 2075 7365 2c20 6361  oster to use, ca
-00022a00: 6e20 6265 2067 6274 7265 652c 2067 626c  n be gbtree, gbl
-00022a10: 696e 6561 7220 6f72 2064 6172 742e 2067  inear or dart. g
-00022a20: 6274 7265 6520 616e 640a 2020 2020 2020  btree and.      
-00022a30: 6461 7274 2075 7365 2074 7265 6520 6261  dart use tree ba
-00022a40: 7365 6420 6d6f 6465 6c20 7768 696c 6520  sed model while 
-00022a50: 6762 6c69 6e65 6172 2075 7365 7320 6c69  gblinear uses li
-00022a60: 6e65 6172 2066 756e 6374 696f 6e2e 0a20  near function.. 
-00022a70: 2020 2065 7461 3a20 4c65 6172 6e69 6e67     eta: Learning
-00022a80: 2072 6174 652e 0a20 2020 2067 616d 6d61   rate..    gamma
-00022a90: 3a20 4d69 6e69 6d75 6d20 6c6f 7373 2072  : Minimum loss r
-00022aa0: 6564 7563 7469 6f6e 2072 6571 7569 7265  eduction require
-00022ab0: 6420 746f 206d 616b 6520 6120 6675 7274  d to make a furt
-00022ac0: 6865 7220 7061 7274 6974 696f 6e20 6f6e  her partition on
-00022ad0: 2061 206c 6561 660a 2020 2020 2020 6e6f   a leaf.      no
-00022ae0: 6465 206f 6620 7468 6520 7472 6565 2e0a  de of the tree..
-00022af0: 2020 2020 6d61 785f 6465 7074 683a 204d      max_depth: M
-00022b00: 6178 696d 756d 2064 6570 7468 206f 6620  aximum depth of 
-00022b10: 6120 7472 6565 2e0a 2020 2020 6d69 6e5f  a tree..    min_
-00022b20: 6368 696c 645f 7765 6967 6874 3a20 4d69  child_weight: Mi
-00022b30: 6e69 6d75 6d20 7375 6d20 6f66 2069 6e73  nimum sum of ins
-00022b40: 7461 6e63 6520 7765 6967 6874 2868 6573  tance weight(hes
-00022b50: 7369 616e 2920 6e65 6564 6564 2069 6e20  sian) needed in 
-00022b60: 6120 6368 696c 642e 0a20 2020 206d 6178  a child..    max
-00022b70: 5f64 656c 7461 5f73 7465 703a 204d 6178  _delta_step: Max
-00022b80: 696d 756d 2064 656c 7461 2073 7465 7020  imum delta step 
-00022b90: 7765 2061 6c6c 6f77 2065 6163 6820 7472  we allow each tr
-00022ba0: 6565 2773 2077 6569 6768 7420 6573 7469  ee's weight esti
-00022bb0: 6d61 7469 6f6e 2074 6f0a 2020 2020 2020  mation to.      
-00022bc0: 6265 2e0a 2020 2020 7375 6273 616d 706c  be..    subsampl
-00022bd0: 653a 2053 7562 7361 6d70 6c65 2072 6174  e: Subsample rat
-00022be0: 696f 206f 6620 7468 6520 7472 6169 6e69  io of the traini
-00022bf0: 6e67 2069 6e73 7461 6e63 652e 0a20 2020  ng instance..   
-00022c00: 2063 6f6c 7361 6d70 6c65 5f62 7974 7265   colsample_bytre
-00022c10: 653a 2053 7562 7361 6d70 6c65 2072 6174  e: Subsample rat
-00022c20: 696f 206f 6620 636f 6c75 6d6e 7320 7768  io of columns wh
-00022c30: 656e 2063 6f6e 7374 7275 6374 696e 6720  en constructing 
-00022c40: 6561 6368 2074 7265 652e 0a20 2020 2063  each tree..    c
-00022c50: 6f6c 7361 6d70 6c65 5f62 796c 6576 656c  olsample_bylevel
-00022c60: 3a20 5375 6273 616d 706c 6520 7261 7469  : Subsample rati
-00022c70: 6f20 6f66 2063 6f6c 756d 6e73 2066 6f72  o of columns for
-00022c80: 2065 6163 6820 7370 6c69 742c 2069 6e20   each split, in 
-00022c90: 6561 6368 206c 6576 656c 2e0a 2020 2020  each level..    
-00022ca0: 636f 6c73 616d 706c 655f 6279 6e6f 6465  colsample_bynode
-00022cb0: 3a20 5375 6273 616d 706c 6520 7261 7469  : Subsample rati
-00022cc0: 6f20 6f66 2063 6f6c 756d 6e73 2066 6f72  o of columns for
-00022cd0: 2065 6163 6820 6e6f 6465 2028 7370 6c69   each node (spli
-00022ce0: 7429 2e0a 2020 2020 7265 675f 6c61 6d62  t)..    reg_lamb
-00022cf0: 6461 3a20 4c32 2072 6567 756c 6172 697a  da: L2 regulariz
-00022d00: 6174 696f 6e20 7465 726d 206f 6e20 7765  ation term on we
-00022d10: 6967 6874 732e 0a20 2020 2072 6567 5f61  ights..    reg_a
-00022d20: 6c70 6861 3a20 4c31 2072 6567 756c 6172  lpha: L1 regular
-00022d30: 697a 6174 696f 6e20 7465 726d 206f 6e20  ization term on 
-00022d40: 7765 6967 6874 732e 0a20 2020 2074 7265  weights..    tre
-00022d50: 655f 6d65 7468 6f64 3a20 5468 6520 7472  e_method: The tr
-00022d60: 6565 2063 6f6e 7374 7275 6374 696f 6e20  ee construction 
-00022d70: 616c 676f 7269 7468 6d20 7573 6564 2069  algorithm used i
-00022d80: 6e20 5847 426f 6f73 742e 2043 686f 6963  n XGBoost. Choic
-00022d90: 6573 3a0a 2020 2020 2020 5b22 6175 746f  es:.      ["auto
-00022da0: 222c 2022 6578 6163 7422 2c20 2261 7070  ", "exact", "app
-00022db0: 726f 7822 2c20 2268 6973 7422 2c20 2267  rox", "hist", "g
-00022dc0: 7075 5f65 7861 6374 222c 2022 6770 755f  pu_exact", "gpu_
-00022dd0: 6869 7374 225d 2e0a 2020 2020 7363 616c  hist"]..    scal
-00022de0: 655f 706f 735f 7765 6967 6874 3a20 436f  e_pos_weight: Co
-00022df0: 6e74 726f 6c20 7468 6520 6261 6c61 6e63  ntrol the balanc
-00022e00: 6520 6f66 2070 6f73 6974 6976 6520 616e  e of positive an
-00022e10: 6420 6e65 6761 7469 7665 2077 6569 6768  d negative weigh
-00022e20: 7473 2e0a 2020 2020 7570 6461 7465 723a  ts..    updater:
-00022e30: 2041 2063 6f6d 6d61 2073 6570 6172 6174   A comma separat
-00022e40: 6564 2073 7472 696e 6720 6465 6669 6e69  ed string defini
-00022e50: 6e67 2074 6865 2073 6571 7565 6e63 6520  ng the sequence 
-00022e60: 6f66 2074 7265 6520 7570 6461 7465 7273  of tree updaters
-00022e70: 2074 6f0a 2020 2020 2020 7275 6e2e 0a20   to.      run.. 
-00022e80: 2020 2072 6566 7265 7368 5f6c 6561 663a     refresh_leaf:
-00022e90: 2052 6566 7265 7368 2075 7064 6174 6572   Refresh updater
-00022ea0: 2070 6c75 6769 6e2e 2055 7064 6174 6520   plugin. Update 
-00022eb0: 7472 6565 206c 6561 6620 616e 6420 6e6f  tree leaf and no
-00022ec0: 6465 7327 7320 7374 6174 7320 6966 0a20  des's stats if. 
-00022ed0: 2020 2020 2054 7275 652e 2057 6865 6e20       True. When 
-00022ee0: 6974 2069 7320 4661 6c73 652c 206f 6e6c  it is False, onl
-00022ef0: 7920 6e6f 6465 2073 7461 7473 2061 7265  y node stats are
-00022f00: 2075 7064 6174 6564 2e0a 2020 2020 7072   updated..    pr
-00022f10: 6f63 6573 735f 7479 7065 3a20 4120 7479  ocess_type: A ty
-00022f20: 7065 206f 6620 626f 6f73 7469 6e67 2070  pe of boosting p
-00022f30: 726f 6365 7373 2074 6f20 7275 6e2e 2043  rocess to run. C
-00022f40: 686f 6963 6573 3a5b 2264 6566 6175 6c74  hoices:["default
-00022f50: 222c 0a20 2020 2020 2022 7570 6461 7465  ",.      "update
-00022f60: 225d 0a20 2020 2067 726f 775f 706f 6c69  "].    grow_poli
-00022f70: 6379 3a20 436f 6e74 726f 6c73 2061 2077  cy: Controls a w
-00022f80: 6179 206e 6577 206e 6f64 6573 2061 7265  ay new nodes are
-00022f90: 2061 6464 6564 2074 6f20 7468 6520 7472   added to the tr
-00022fa0: 6565 2e20 4f6e 6c79 2073 7570 706f 7274  ee. Only support
-00022fb0: 6564 0a20 2020 2020 2069 6620 7472 6565  ed.      if tree
-00022fc0: 5f6d 6574 686f 6420 6973 2068 6973 742e  _method is hist.
-00022fd0: 2043 686f 6963 6573 3a5b 2264 6570 7468   Choices:["depth
-00022fe0: 7769 7365 222c 2022 6c6f 7373 6775 6964  wise", "lossguid
-00022ff0: 6522 5d0a 2020 2020 7361 6d70 6c69 6e67  e"].    sampling
-00023000: 5f6d 6574 686f 643a 2054 6865 206d 6574  _method: The met
-00023010: 686f 6420 746f 2075 7365 2074 6f20 7361  hod to use to sa
-00023020: 6d70 6c65 2074 6865 2074 7261 696e 696e  mple the trainin
-00023030: 6720 696e 7374 616e 6365 732e 0a20 2020  g instances..   
-00023040: 206d 6f6e 6f74 6f6e 655f 636f 6e73 7472   monotone_constr
-00023050: 6169 6e74 733a 2043 6f6e 7374 7261 696e  aints: Constrain
-00023060: 7420 6f66 2076 6172 6961 626c 6520 6d6f  t of variable mo
-00023070: 6e6f 746f 6e69 6369 7479 2e0a 2020 2020  notonicity..    
-00023080: 696e 7465 7261 6374 696f 6e5f 636f 6e73  interaction_cons
-00023090: 7472 6169 6e74 733a 2043 6f6e 7374 7261  traints: Constra
-000230a0: 696e 7473 2066 6f72 2069 6e74 6572 6163  ints for interac
-000230b0: 7469 6f6e 2072 6570 7265 7365 6e74 696e  tion representin
-000230c0: 6720 7065 726d 6974 7465 640a 2020 2020  g permitted.    
-000230d0: 2020 696e 7465 7261 6374 696f 6e73 2e0a    interactions..
-000230e0: 2020 2020 7361 6d70 6c65 5f74 7970 653a      sample_type:
-000230f0: 205b 6461 7274 2062 6f6f 7374 6572 206f   [dart booster o
-00023100: 6e6c 795d 2054 7970 6520 6f66 2073 616d  nly] Type of sam
-00023110: 706c 696e 6720 616c 676f 7269 7468 6d2e  pling algorithm.
-00023120: 0a20 2020 2020 2043 686f 6963 6573 3a5b  .      Choices:[
-00023130: 2275 6e69 666f 726d 222c 2022 7765 6967  "uniform", "weig
-00023140: 6874 6564 225d 0a20 2020 206e 6f72 6d61  hted"].    norma
-00023150: 6c69 7a65 5f74 7970 653a 205b 6461 7274  lize_type: [dart
-00023160: 2062 6f6f 7374 6572 206f 6e6c 795d 2054   booster only] T
-00023170: 7970 6520 6f66 206e 6f72 6d61 6c69 7a61  ype of normaliza
-00023180: 7469 6f6e 2061 6c67 6f72 6974 686d 2c0a  tion algorithm,.
-00023190: 2020 2020 2020 4368 6f69 6365 733a 5b22        Choices:["
-000231a0: 7472 6565 222c 2022 666f 7265 7374 225d  tree", "forest"]
-000231b0: 0a20 2020 2072 6174 655f 6472 6f70 3a20  .    rate_drop: 
-000231c0: 5b64 6172 7420 626f 6f73 7465 7220 6f6e  [dart booster on
-000231d0: 6c79 5d20 4472 6f70 6f75 7420 7261 7465  ly] Dropout rate
-000231e0: 2e27 0a20 2020 206f 6e65 5f64 726f 703a  .'.    one_drop:
-000231f0: 205b 6461 7274 2062 6f6f 7374 6572 206f   [dart booster o
-00023200: 6e6c 795d 2057 6865 6e20 7468 6973 2066  nly] When this f
-00023210: 6c61 6720 6973 2065 6e61 626c 6564 2c20  lag is enabled, 
-00023220: 6174 206c 6561 7374 206f 6e65 2074 7265  at least one tre
-00023230: 650a 2020 2020 2020 6973 2061 6c77 6179  e.      is alway
-00023240: 7320 6472 6f70 7065 6420 6475 7269 6e67  s dropped during
-00023250: 2074 6865 2064 726f 706f 7574 2028 616c   the dropout (al
-00023260: 6c6f 7773 2042 696e 6f6d 6961 6c2d 706c  lows Binomial-pl
-00023270: 7573 2d6f 6e65 206f 720a 2020 2020 2020  us-one or.      
-00023280: 6570 7369 6c6f 6e2d 6472 6f70 6f75 7420  epsilon-dropout 
-00023290: 6672 6f6d 2074 6865 206f 7269 6769 6e61  from the origina
-000232a0: 6c20 4441 5254 2070 6170 6572 292e 0a20  l DART paper).. 
-000232b0: 2020 2073 6b69 705f 6472 6f70 3a20 5b64     skip_drop: [d
-000232c0: 6172 7420 626f 6f73 7465 7220 6f6e 6c79  art booster only
-000232d0: 5d20 5072 6f62 6162 696c 6974 7920 6f66  ] Probability of
-000232e0: 2073 6b69 7070 696e 6720 7468 6520 6472   skipping the dr
-000232f0: 6f70 6f75 7420 7072 6f63 6564 7572 650a  opout procedure.
-00023300: 2020 2020 2020 6475 7269 6e67 2061 2062        during a b
-00023310: 6f6f 7374 696e 6720 6974 6572 6174 696f  oosting iteratio
-00023320: 6e2e 0a20 2020 206e 756d 5f70 6172 616c  n..    num_paral
-00023330: 6c65 6c5f 7472 6565 3a20 4e75 6d62 6572  lel_tree: Number
-00023340: 206f 6620 7061 7261 6c6c 656c 2074 7265   of parallel tre
-00023350: 6573 2063 6f6e 7374 7275 6374 6564 2064  es constructed d
-00023360: 7572 696e 6720 6561 6368 0a20 2020 2020  uring each.     
-00023370: 2069 7465 7261 7469 6f6e 2e20 5468 6973   iteration. This
-00023380: 206f 7074 696f 6e20 6973 2075 7365 6420   option is used 
-00023390: 746f 2073 7570 706f 7274 2062 6f6f 7374  to support boost
-000233a0: 6564 2072 616e 646f 6d20 666f 7265 7374  ed random forest
-000233b0: 2e0a 2020 2020 6665 6174 7572 655f 7365  ..    feature_se
-000233c0: 6c65 6374 6f72 3a20 5b6c 696e 6561 7220  lector: [linear 
-000233d0: 626f 6f73 7465 7220 6f6e 6c79 5d20 4665  booster only] Fe
-000233e0: 6174 7572 6520 7365 6c65 6374 696f 6e20  ature selection 
-000233f0: 616e 6420 6f72 6465 7269 6e67 0a20 2020  and ordering.   
-00023400: 2020 206d 6574 686f 642e 0a20 2020 2074     method..    t
-00023410: 6f70 5f6b 3a20 5468 6520 6e75 6d62 6572  op_k: The number
-00023420: 206f 6620 746f 7020 6665 6174 7572 6573   of top features
-00023430: 2074 6f20 7365 6c65 6374 2069 6e20 6772   to select in gr
-00023440: 6565 6479 2061 6e64 2074 6872 6966 7479  eedy and thrifty
-00023450: 2066 6561 7475 7265 0a20 2020 2020 2073   feature.      s
-00023460: 656c 6563 746f 722e 2054 6865 2076 616c  elector. The val
-00023470: 7565 206f 6620 3020 6d65 616e 7320 7573  ue of 0 means us
-00023480: 696e 6720 616c 6c20 7468 6520 6665 6174  ing all the feat
-00023490: 7572 6573 2e0a 2020 2020 6d61 785f 6361  ures..    max_ca
-000234a0: 745f 746f 5f6f 6e65 686f 743a 2041 2074  t_to_onehot: A t
-000234b0: 6872 6573 686f 6c64 2066 6f72 2064 6563  hreshold for dec
-000234c0: 6964 696e 6720 7768 6574 6865 7220 5847  iding whether XG
-000234d0: 426f 6f73 7420 7368 6f75 6c64 2075 7365  Boost should use
-000234e0: 0a20 2020 2020 206f 6e65 2d68 6f74 2065  .      one-hot e
-000234f0: 6e63 6f64 696e 6720 6261 7365 6420 7370  ncoding based sp
-00023500: 6c69 7420 666f 7220 6361 7465 676f 7269  lit for categori
-00023510: 6361 6c20 6461 7461 2e0a 2020 2020 6d61  cal data..    ma
-00023520: 785f 6c65 6176 6573 3a20 4d61 7869 6d75  x_leaves: Maximu
-00023530: 6d20 6e75 6d62 6572 206f 6620 6e6f 6465  m number of node
-00023540: 7320 746f 2062 6520 6164 6465 642e 0a20  s to be added.. 
-00023550: 2020 206d 6178 5f62 696e 3a20 4d61 7869     max_bin: Maxi
-00023560: 6d75 6d20 6e75 6d62 6572 206f 6620 6469  mum number of di
-00023570: 7363 7265 7465 2062 696e 7320 746f 2062  screte bins to b
-00023580: 7563 6b65 7420 636f 6e74 696e 756f 7573  ucket continuous
-00023590: 2066 6561 7475 7265 732e 0a20 2020 2074   features..    t
-000235a0: 7765 6564 6965 5f76 6172 6961 6e63 655f  weedie_variance_
-000235b0: 706f 7765 723a 2050 6172 616d 6574 6572  power: Parameter
-000235c0: 2074 6861 7420 636f 6e74 726f 6c73 2074   that controls t
-000235d0: 6865 2076 6172 6961 6e63 6520 6f66 2074  he variance of t
-000235e0: 6865 2054 7765 6564 6965 0a20 2020 2020  he Tweedie.     
-000235f0: 2064 6973 7472 6962 7574 696f 6e2e 0a20   distribution.. 
-00023600: 2020 2068 7562 6572 5f73 6c6f 7065 3a20     huber_slope: 
-00023610: 4120 7061 7261 6d65 7465 7220 7573 6564  A parameter used
-00023620: 2066 6f72 2050 7365 7564 6f2d 4875 6265   for Pseudo-Hube
-00023630: 7220 6c6f 7373 2074 6f20 6465 6669 6e65  r loss to define
-00023640: 2074 6865 2064 656c 7461 0a20 2020 2020   the delta.     
-00023650: 2074 6572 6d2e 0a20 2020 2064 6174 6173   term..    datas
-00023660: 6574 5f6c 6576 656c 5f63 7573 746f 6d5f  et_level_custom_
-00023670: 7472 616e 7366 6f72 6d61 7469 6f6e 5f64  transformation_d
-00023680: 6566 696e 6974 696f 6e73 3a20 4461 7461  efinitions: Data
-00023690: 7365 742d 6c65 7665 6c20 6375 7374 6f6d  set-level custom
-000236a0: 0a20 2020 2020 2074 7261 6e73 666f 726d  .      transform
-000236b0: 6174 696f 6e20 6465 6669 6e69 7469 6f6e  ation definition
-000236c0: 7320 696e 2073 7472 696e 6720 666f 726d  s in string form
-000236d0: 6174 2e0a 2020 2020 6461 7461 7365 745f  at..    dataset_
-000236e0: 6c65 7665 6c5f 7472 616e 7366 6f72 6d61  level_transforma
-000236f0: 7469 6f6e 733a 2044 6174 6173 6574 2d6c  tions: Dataset-l
-00023700: 6576 656c 2074 7261 6e73 666f 726d 6174  evel transformat
-00023710: 696f 6e20 636f 6e66 6967 7572 6174 696f  ion configuratio
-00023720: 6e20 696e 0a20 2020 2020 2073 7472 696e  n in.      strin
-00023730: 6720 666f 726d 6174 2e0a 2020 2020 7275  g format..    ru
-00023740: 6e5f 6665 6174 7572 655f 7365 6c65 6374  n_feature_select
-00023750: 696f 6e3a 2057 6865 7468 6572 2074 6f20  ion: Whether to 
-00023760: 656e 6162 6c65 2066 6561 7475 7265 2073  enable feature s
-00023770: 656c 6563 7469 6f6e 2e0a 2020 2020 6665  election..    fe
-00023780: 6174 7572 655f 7365 6c65 6374 696f 6e5f  ature_selection_
-00023790: 616c 676f 7269 7468 6d3a 2046 6561 7475  algorithm: Featu
-000237a0: 7265 2073 656c 6563 7469 6f6e 2061 6c67  re selection alg
-000237b0: 6f72 6974 686d 2e0a 2020 2020 6d61 785f  orithm..    max_
-000237c0: 7365 6c65 6374 6564 5f66 6561 7475 7265  selected_feature
-000237d0: 733a 204d 6178 696d 756d 206e 756d 6265  s: Maximum numbe
-000237e0: 7220 6f66 2066 6561 7475 7265 7320 746f  r of features to
-000237f0: 2073 656c 6563 742e 0a20 2020 2070 7265   select..    pre
-00023800: 6465 6669 6e65 645f 7370 6c69 745f 6b65  defined_split_ke
-00023810: 793a 2050 7265 6465 6669 6e65 6420 7370  y: Predefined sp
-00023820: 6c69 7420 6b65 792e 0a20 2020 2073 7472  lit key..    str
-00023830: 6174 6966 6965 645f 7370 6c69 745f 6b65  atified_split_ke
-00023840: 793a 2053 7472 6174 6966 6965 6420 7370  y: Stratified sp
-00023850: 6c69 7420 6b65 792e 0a20 2020 2074 7261  lit key..    tra
-00023860: 696e 696e 675f 6672 6163 7469 6f6e 3a20  ining_fraction: 
-00023870: 5472 6169 6e69 6e67 2066 7261 6374 696f  Training fractio
-00023880: 6e2e 0a20 2020 2076 616c 6964 6174 696f  n..    validatio
-00023890: 6e5f 6672 6163 7469 6f6e 3a20 5661 6c69  n_fraction: Vali
-000238a0: 6461 7469 6f6e 2066 7261 6374 696f 6e2e  dation fraction.
-000238b0: 0a20 2020 2074 6573 745f 6672 6163 7469  .    test_fracti
-000238c0: 6f6e 3a20 5465 7374 2066 7261 6374 696f  on: Test fractio
-000238d0: 6e2e 0a20 2020 2074 665f 6175 746f 5f74  n..    tf_auto_t
-000238e0: 7261 6e73 666f 726d 5f66 6561 7475 7265  ransform_feature
-000238f0: 733a 204c 6973 7420 6f66 2061 7574 6f20  s: List of auto 
-00023900: 7472 616e 7366 6f72 6d20 6665 6174 7572  transform featur
-00023910: 6573 2069 6e20 7468 650a 2020 2020 2020  es in the.      
-00023920: 636f 6d6d 612d 7365 7061 7261 7465 6420  comma-separated 
-00023930: 7374 7269 6e67 2066 6f72 6d61 742e 0a20  string format.. 
-00023940: 2020 2074 665f 6375 7374 6f6d 5f74 7261     tf_custom_tra
-00023950: 6e73 666f 726d 6174 696f 6e5f 6465 6669  nsformation_defi
-00023960: 6e69 7469 6f6e 733a 2054 4620 6375 7374  nitions: TF cust
-00023970: 6f6d 2074 7261 6e73 666f 726d 6174 696f  om transformatio
-00023980: 6e20 6465 6669 6e69 7469 6f6e 730a 2020  n definitions.  
-00023990: 2020 2020 696e 2073 7472 696e 6720 666f      in string fo
-000239a0: 726d 6174 2e0a 2020 2020 7466 5f74 7261  rmat..    tf_tra
-000239b0: 6e73 666f 726d 6174 696f 6e73 5f70 6174  nsformations_pat
-000239c0: 683a 2050 6174 6820 746f 2054 4620 7472  h: Path to TF tr
-000239d0: 616e 7366 6f72 6d61 7469 6f6e 2063 6f6e  ansformation con
-000239e0: 6669 6775 7261 7469 6f6e 2e0a 2020 2020  figuration..    
-000239f0: 6461 7461 5f73 6f75 7263 655f 6373 765f  data_source_csv_
-00023a00: 6669 6c65 6e61 6d65 733a 2054 6865 2043  filenames: The C
-00023a10: 5356 2064 6174 6120 736f 7572 6365 2e0a  SV data source..
-00023a20: 2020 2020 6461 7461 5f73 6f75 7263 655f      data_source_
-00023a30: 6269 6771 7565 7279 5f74 6162 6c65 5f70  bigquery_table_p
-00023a40: 6174 683a 2054 6865 2042 6967 5175 6572  ath: The BigQuer
-00023a50: 7920 6461 7461 2073 6f75 7263 652e 0a20  y data source.. 
-00023a60: 2020 2062 6967 7175 6572 795f 7374 6167     bigquery_stag
-00023a70: 696e 675f 6675 6c6c 5f64 6174 6173 6574  ing_full_dataset
-00023a80: 5f69 643a 2054 6865 2042 6967 5175 6572  _id: The BigQuer
-00023a90: 7920 7374 6167 696e 6720 6675 6c6c 2064  y staging full d
-00023aa0: 6174 6173 6574 2069 6420 666f 720a 2020  ataset id for.  
-00023ab0: 2020 2020 7374 6f72 696e 6720 696e 7465      storing inte
-00023ac0: 726d 6564 6961 7465 2074 6162 6c65 732e  rmediate tables.
-00023ad0: 0a20 2020 2077 6569 6768 745f 636f 6c75  .    weight_colu
-00023ae0: 6d6e 3a20 5468 6520 7765 6967 6874 2063  mn: The weight c
-00023af0: 6f6c 756d 6e20 6e61 6d65 2e0a 2020 2020  olumn name..    
-00023b00: 7472 6169 6e69 6e67 5f6d 6163 6869 6e65  training_machine
-00023b10: 5f74 7970 653a 204d 6163 6869 6e65 2074  _type: Machine t
-00023b20: 7970 652e 0a20 2020 2074 7261 696e 696e  ype..    trainin
-00023b30: 675f 746f 7461 6c5f 7265 706c 6963 615f  g_total_replica_
-00023b40: 636f 756e 743a 204e 756d 6265 7220 6f66  count: Number of
-00023b50: 2077 6f72 6b65 7273 2e0a 2020 2020 7472   workers..    tr
-00023b60: 6169 6e69 6e67 5f61 6363 656c 6572 6174  aining_accelerat
-00023b70: 6f72 5f74 7970 653a 2041 6363 656c 6572  or_type: Acceler
-00023b80: 6174 6f72 2074 7970 652e 0a20 2020 2074  ator type..    t
-00023b90: 7261 696e 696e 675f 6163 6365 6c65 7261  raining_accelera
-00023ba0: 746f 725f 636f 756e 743a 2041 6363 656c  tor_count: Accel
-00023bb0: 6572 6174 6f72 2063 6f75 6e74 2e0a 2020  erator count..  
-00023bc0: 2020 7472 616e 7366 6f72 6d5f 6461 7461    transform_data
-00023bd0: 666c 6f77 5f6d 6163 6869 6e65 5f74 7970  flow_machine_typ
-00023be0: 653a 2054 6865 2064 6174 6166 6c6f 7720  e: The dataflow 
-00023bf0: 6d61 6368 696e 6520 7479 7065 2066 6f72  machine type for
-00023c00: 2074 7261 6e73 666f 726d 0a20 2020 2020   transform.     
-00023c10: 2063 6f6d 706f 6e65 6e74 2e0a 2020 2020   component..    
-00023c20: 7472 616e 7366 6f72 6d5f 6461 7461 666c  transform_datafl
-00023c30: 6f77 5f6d 6178 5f6e 756d 5f77 6f72 6b65  ow_max_num_worke
-00023c40: 7273 3a20 5468 6520 6d61 7820 6e75 6d62  rs: The max numb
-00023c50: 6572 206f 6620 4461 7461 666c 6f77 2077  er of Dataflow w
-00023c60: 6f72 6b65 7273 2066 6f72 0a20 2020 2020  orkers for.     
-00023c70: 2074 7261 6e73 666f 726d 2063 6f6d 706f   transform compo
-00023c80: 6e65 6e74 2e0a 2020 2020 7472 616e 7366  nent..    transf
-00023c90: 6f72 6d5f 6461 7461 666c 6f77 5f64 6973  orm_dataflow_dis
-00023ca0: 6b5f 7369 7a65 5f67 623a 2044 6174 6166  k_size_gb: Dataf
-00023cb0: 6c6f 7720 776f 726b 6572 2773 2064 6973  low worker's dis
-00023cc0: 6b20 7369 7a65 2069 6e20 4742 2066 6f72  k size in GB for
-00023cd0: 0a20 2020 2020 2074 7261 6e73 666f 726d  .      transform
-00023ce0: 2063 6f6d 706f 6e65 6e74 2e0a 2020 2020   component..    
-00023cf0: 7275 6e5f 6576 616c 7561 7469 6f6e 3a20  run_evaluation: 
-00023d00: 5768 6574 6865 7220 746f 2072 756e 2065  Whether to run e
-00023d10: 7661 6c75 6174 696f 6e20 7374 6570 7320  valuation steps 
-00023d20: 6475 7269 6e67 2074 7261 696e 696e 672e  during training.
-00023d30: 0a20 2020 2065 7661 6c75 6174 696f 6e5f  .    evaluation_
-00023d40: 6261 7463 685f 7072 6564 6963 745f 6d61  batch_predict_ma
-00023d50: 6368 696e 655f 7479 7065 3a20 5468 6520  chine_type: The 
-00023d60: 7072 6564 6963 7469 6f6e 2073 6572 7665  prediction serve
-00023d70: 7220 6d61 6368 696e 6520 7479 7065 0a20  r machine type. 
-00023d80: 2020 2020 2066 6f72 2062 6174 6368 2070       for batch p
-00023d90: 7265 6469 6374 2063 6f6d 706f 6e65 6e74  redict component
-00023da0: 7320 6475 7269 6e67 2065 7661 6c75 6174  s during evaluat
-00023db0: 696f 6e2e 0a20 2020 2065 7661 6c75 6174  ion..    evaluat
-00023dc0: 696f 6e5f 6261 7463 685f 7072 6564 6963  ion_batch_predic
-00023dd0: 745f 7374 6172 7469 6e67 5f72 6570 6c69  t_starting_repli
-00023de0: 6361 5f63 6f75 6e74 3a20 5468 6520 696e  ca_count: The in
-00023df0: 6974 6961 6c20 6e75 6d62 6572 206f 660a  itial number of.
-00023e00: 2020 2020 2020 7072 6564 6963 7469 6f6e        prediction
-00023e10: 2073 6572 7665 7220 666f 7220 6261 7463   server for batc
-00023e20: 6820 7072 6564 6963 7420 636f 6d70 6f6e  h predict compon
-00023e30: 656e 7473 2064 7572 696e 6720 6576 616c  ents during eval
-00023e40: 7561 7469 6f6e 2e0a 2020 2020 6576 616c  uation..    eval
-00023e50: 7561 7469 6f6e 5f62 6174 6368 5f70 7265  uation_batch_pre
-00023e60: 6469 6374 5f6d 6178 5f72 6570 6c69 6361  dict_max_replica
-00023e70: 5f63 6f75 6e74 3a20 5468 6520 6d61 7820  _count: The max 
-00023e80: 6e75 6d62 6572 206f 6620 7072 6564 6963  number of predic
-00023e90: 7469 6f6e 0a20 2020 2020 2073 6572 7665  tion.      serve
-00023ea0: 7220 666f 7220 6261 7463 6820 7072 6564  r for batch pred
-00023eb0: 6963 7420 636f 6d70 6f6e 656e 7473 2064  ict components d
-00023ec0: 7572 696e 6720 6576 616c 7561 7469 6f6e  uring evaluation
-00023ed0: 2e0a 2020 2020 6576 616c 7561 7469 6f6e  ..    evaluation
-00023ee0: 5f64 6174 6166 6c6f 775f 6d61 6368 696e  _dataflow_machin
-00023ef0: 655f 7479 7065 3a20 5468 6520 6461 7461  e_type: The data
-00023f00: 666c 6f77 206d 6163 6869 6e65 2074 7970  flow machine typ
-00023f10: 6520 666f 7220 6576 616c 7561 7469 6f6e  e for evaluation
-00023f20: 0a20 2020 2020 2063 6f6d 706f 6e65 6e74  .      component
-00023f30: 732e 0a20 2020 2065 7661 6c75 6174 696f  s..    evaluatio
-00023f40: 6e5f 6461 7461 666c 6f77 5f73 7461 7274  n_dataflow_start
-00023f50: 696e 675f 6e75 6d5f 776f 726b 6572 733a  ing_num_workers:
-00023f60: 2054 6865 2069 6e69 7469 616c 206e 756d   The initial num
-00023f70: 6265 7220 6f66 2044 6174 6166 6c6f 770a  ber of Dataflow.
-00023f80: 2020 2020 2020 776f 726b 6572 7320 666f        workers fo
-00023f90: 7220 6576 616c 7561 7469 6f6e 2063 6f6d  r evaluation com
-00023fa0: 706f 6e65 6e74 732e 0a20 2020 2065 7661  ponents..    eva
-00023fb0: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
-00023fc0: 5f6d 6178 5f6e 756d 5f77 6f72 6b65 7273  _max_num_workers
-00023fd0: 3a20 5468 6520 6d61 7820 6e75 6d62 6572  : The max number
-00023fe0: 206f 6620 4461 7461 666c 6f77 2077 6f72   of Dataflow wor
-00023ff0: 6b65 7273 2066 6f72 0a20 2020 2020 2065  kers for.      e
-00024000: 7661 6c75 6174 696f 6e20 636f 6d70 6f6e  valuation compon
-00024010: 656e 7473 2e0a 2020 2020 6576 616c 7561  ents..    evalua
-00024020: 7469 6f6e 5f64 6174 6166 6c6f 775f 6469  tion_dataflow_di
-00024030: 736b 5f73 697a 655f 6762 3a20 4461 7461  sk_size_gb: Data
-00024040: 666c 6f77 2077 6f72 6b65 7227 7320 6469  flow worker's di
-00024050: 736b 2073 697a 6520 696e 2047 4220 666f  sk size in GB fo
-00024060: 720a 2020 2020 2020 6576 616c 7561 7469  r.      evaluati
-00024070: 6f6e 2063 6f6d 706f 6e65 6e74 732e 0a20  on components.. 
-00024080: 2020 2064 6174 6166 6c6f 775f 7365 7276     dataflow_serv
-00024090: 6963 655f 6163 636f 756e 743a 2043 7573  ice_account: Cus
-000240a0: 746f 6d20 7365 7276 6963 6520 6163 636f  tom service acco
-000240b0: 756e 7420 746f 2072 756e 2064 6174 6166  unt to run dataf
-000240c0: 6c6f 7720 6a6f 6273 2e0a 2020 2020 6461  low jobs..    da
-000240d0: 7461 666c 6f77 5f73 7562 6e65 7477 6f72  taflow_subnetwor
-000240e0: 6b3a 2044 6174 6166 6c6f 7727 7320 6675  k: Dataflow's fu
-000240f0: 6c6c 7920 7175 616c 6966 6965 6420 7375  lly qualified su
-00024100: 626e 6574 776f 726b 206e 616d 652c 2077  bnetwork name, w
-00024110: 6865 6e20 656d 7074 790a 2020 2020 2020  hen empty.      
-00024120: 7468 6520 6465 6661 756c 7420 7375 626e  the default subn
-00024130: 6574 776f 726b 2077 696c 6c20 6265 2075  etwork will be u
-00024140: 7365 642e 2045 7861 6d70 6c65 3a0a 2020  sed. Example:.  
-00024150: 2020 2020 2020 6874 7470 733a 2f2f 636c        https://cl
-00024160: 6f75 642e 676f 6f67 6c65 2e63 6f6d 2f64  oud.google.com/d
-00024170: 6174 6166 6c6f 772f 646f 6373 2f67 7569  ataflow/docs/gui
-00024180: 6465 732f 7370 6563 6966 7969 6e67 2d6e  des/specifying-n
-00024190: 6574 776f 726b 7323 6578 616d 706c 655f  etworks#example_
-000241a0: 6e65 7477 6f72 6b5f 616e 645f 7375 626e  network_and_subn
-000241b0: 6574 776f 726b 5f73 7065 6369 6669 6361  etwork_specifica
-000241c0: 7469 6f6e 730a 2020 2020 6461 7461 666c  tions.    datafl
-000241d0: 6f77 5f75 7365 5f70 7562 6c69 635f 6970  ow_use_public_ip
-000241e0: 733a 2053 7065 6369 6669 6573 2077 6865  s: Specifies whe
-000241f0: 7468 6572 2044 6174 6166 6c6f 7720 776f  ther Dataflow wo
-00024200: 726b 6572 7320 7573 6520 7075 626c 6963  rkers use public
-00024210: 2049 500a 2020 2020 2020 6164 6472 6573   IP.      addres
-00024220: 7365 732e 0a20 2020 2065 6e63 7279 7074  ses..    encrypt
-00024230: 696f 6e5f 7370 6563 5f6b 6579 5f6e 616d  ion_spec_key_nam
-00024240: 653a 2054 6865 204b 4d53 206b 6579 206e  e: The KMS key n
-00024250: 616d 652e 0a0a 2020 5265 7475 726e 733a  ame...  Returns:
-00024260: 0a20 2020 2054 7570 6c65 206f 6620 7069  .    Tuple of pi
-00024270: 7065 6c69 6e65 5f64 6566 696e 6974 696f  peline_definitio
-00024280: 6e5f 7061 7468 2061 6e64 2070 6172 616d  n_path and param
-00024290: 6574 6572 5f76 616c 7565 732e 0a20 2022  eter_values..  "
-000242a0: 2222 0a20 2070 6172 616d 6574 6572 5f76  "".  parameter_v
-000242b0: 616c 7565 7320 3d20 7b7d 0a20 2069 6620  alues = {}.  if 
-000242c0: 6973 696e 7374 616e 6365 2874 665f 6175  isinstance(tf_au
-000242d0: 746f 5f74 7261 6e73 666f 726d 5f66 6561  to_transform_fea
-000242e0: 7475 7265 732c 206c 6973 7429 3a0a 2020  tures, list):.  
-000242f0: 2020 7466 5f61 7574 6f5f 7472 616e 7366    tf_auto_transf
-00024300: 6f72 6d5f 6665 6174 7572 6573 203d 207b  orm_features = {
-00024310: 2761 7574 6f27 3a20 7466 5f61 7574 6f5f  'auto': tf_auto_
-00024320: 7472 616e 7366 6f72 6d5f 6665 6174 7572  transform_featur
-00024330: 6573 7d0a 0a20 2074 7261 696e 696e 675f  es}..  training_
-00024340: 616e 645f 6576 616c 5f70 6172 616d 6574  and_eval_paramet
-00024350: 6572 7320 3d20 7b0a 2020 2020 2020 2770  ers = {.      'p
-00024360: 726f 6a65 6374 273a 2070 726f 6a65 6374  roject': project
-00024370: 2c0a 2020 2020 2020 276c 6f63 6174 696f  ,.      'locatio
-00024380: 6e27 3a20 6c6f 6361 7469 6f6e 2c0a 2020  n': location,.  
-00024390: 2020 2020 2772 6f6f 745f 6469 7227 3a20      'root_dir': 
-000243a0: 726f 6f74 5f64 6972 2c0a 2020 2020 2020  root_dir,.      
-000243b0: 2774 6172 6765 745f 636f 6c75 6d6e 273a  'target_column':
-000243c0: 2074 6172 6765 745f 636f 6c75 6d6e 2c0a   target_column,.
-000243d0: 2020 2020 2020 276f 626a 6563 7469 7665        'objective
-000243e0: 273a 206f 626a 6563 7469 7665 2c0a 2020  ': objective,.  
-000243f0: 2020 2020 2765 7661 6c5f 6d65 7472 6963      'eval_metric
-00024400: 273a 2065 7661 6c5f 6d65 7472 6963 2c0a  ': eval_metric,.
-00024410: 2020 2020 2020 276e 756d 5f62 6f6f 7374        'num_boost
-00024420: 5f72 6f75 6e64 273a 206e 756d 5f62 6f6f  _round': num_boo
-00024430: 7374 5f72 6f75 6e64 2c0a 2020 2020 2020  st_round,.      
-00024440: 2765 6172 6c79 5f73 746f 7070 696e 675f  'early_stopping_
-00024450: 726f 756e 6473 273a 2065 6172 6c79 5f73  rounds': early_s
-00024460: 746f 7070 696e 675f 726f 756e 6473 2c0a  topping_rounds,.
-00024470: 2020 2020 2020 2762 6173 655f 7363 6f72        'base_scor
-00024480: 6527 3a20 6261 7365 5f73 636f 7265 2c0a  e': base_score,.
-00024490: 2020 2020 2020 2764 6973 6162 6c65 5f64        'disable_d
-000244a0: 6566 6175 6c74 5f65 7661 6c5f 6d65 7472  efault_eval_metr
-000244b0: 6963 273a 2064 6973 6162 6c65 5f64 6566  ic': disable_def
-000244c0: 6175 6c74 5f65 7661 6c5f 6d65 7472 6963  ault_eval_metric
-000244d0: 2c0a 2020 2020 2020 2773 6565 6427 3a20  ,.      'seed': 
-000244e0: 7365 6564 2c0a 2020 2020 2020 2773 6565  seed,.      'see
-000244f0: 645f 7065 725f 6974 6572 6174 696f 6e27  d_per_iteration'
-00024500: 3a20 7365 6564 5f70 6572 5f69 7465 7261  : seed_per_itera
-00024510: 7469 6f6e 2c0a 2020 2020 2020 2762 6f6f  tion,.      'boo
-00024520: 7374 6572 273a 2062 6f6f 7374 6572 2c0a  ster': booster,.
-00024530: 2020 2020 2020 2765 7461 273a 2065 7461        'eta': eta
-00024540: 2c0a 2020 2020 2020 2767 616d 6d61 273a  ,.      'gamma':
-00024550: 2067 616d 6d61 2c0a 2020 2020 2020 276d   gamma,.      'm
-00024560: 6178 5f64 6570 7468 273a 206d 6178 5f64  ax_depth': max_d
-00024570: 6570 7468 2c0a 2020 2020 2020 276d 696e  epth,.      'min
-00024580: 5f63 6869 6c64 5f77 6569 6768 7427 3a20  _child_weight': 
-00024590: 6d69 6e5f 6368 696c 645f 7765 6967 6874  min_child_weight
-000245a0: 2c0a 2020 2020 2020 276d 6178 5f64 656c  ,.      'max_del
-000245b0: 7461 5f73 7465 7027 3a20 6d61 785f 6465  ta_step': max_de
-000245c0: 6c74 615f 7374 6570 2c0a 2020 2020 2020  lta_step,.      
-000245d0: 2773 7562 7361 6d70 6c65 273a 2073 7562  'subsample': sub
-000245e0: 7361 6d70 6c65 2c0a 2020 2020 2020 2763  sample,.      'c
-000245f0: 6f6c 7361 6d70 6c65 5f62 7974 7265 6527  olsample_bytree'
-00024600: 3a20 636f 6c73 616d 706c 655f 6279 7472  : colsample_bytr
-00024610: 6565 2c0a 2020 2020 2020 2763 6f6c 7361  ee,.      'colsa
-00024620: 6d70 6c65 5f62 796c 6576 656c 273a 2063  mple_bylevel': c
-00024630: 6f6c 7361 6d70 6c65 5f62 796c 6576 656c  olsample_bylevel
-00024640: 2c0a 2020 2020 2020 2763 6f6c 7361 6d70  ,.      'colsamp
-00024650: 6c65 5f62 796e 6f64 6527 3a20 636f 6c73  le_bynode': cols
-00024660: 616d 706c 655f 6279 6e6f 6465 2c0a 2020  ample_bynode,.  
-00024670: 2020 2020 2772 6567 5f6c 616d 6264 6127      'reg_lambda'
-00024680: 3a20 7265 675f 6c61 6d62 6461 2c0a 2020  : reg_lambda,.  
-00024690: 2020 2020 2772 6567 5f61 6c70 6861 273a      'reg_alpha':
-000246a0: 2072 6567 5f61 6c70 6861 2c0a 2020 2020   reg_alpha,.    
-000246b0: 2020 2774 7265 655f 6d65 7468 6f64 273a    'tree_method':
-000246c0: 2074 7265 655f 6d65 7468 6f64 2c0a 2020   tree_method,.  
-000246d0: 2020 2020 2773 6361 6c65 5f70 6f73 5f77      'scale_pos_w
-000246e0: 6569 6768 7427 3a20 7363 616c 655f 706f  eight': scale_po
-000246f0: 735f 7765 6967 6874 2c0a 2020 2020 2020  s_weight,.      
-00024700: 2775 7064 6174 6572 273a 2075 7064 6174  'updater': updat
-00024710: 6572 2c0a 2020 2020 2020 2772 6566 7265  er,.      'refre
-00024720: 7368 5f6c 6561 6627 3a20 7265 6672 6573  sh_leaf': refres
-00024730: 685f 6c65 6166 2c0a 2020 2020 2020 2770  h_leaf,.      'p
-00024740: 726f 6365 7373 5f74 7970 6527 3a20 7072  rocess_type': pr
-00024750: 6f63 6573 735f 7479 7065 2c0a 2020 2020  ocess_type,.    
-00024760: 2020 2767 726f 775f 706f 6c69 6379 273a    'grow_policy':
-00024770: 2067 726f 775f 706f 6c69 6379 2c0a 2020   grow_policy,.  
-00024780: 2020 2020 2773 616d 706c 696e 675f 6d65      'sampling_me
-00024790: 7468 6f64 273a 2073 616d 706c 696e 675f  thod': sampling_
-000247a0: 6d65 7468 6f64 2c0a 2020 2020 2020 276d  method,.      'm
-000247b0: 6f6e 6f74 6f6e 655f 636f 6e73 7472 6169  onotone_constrai
-000247c0: 6e74 7327 3a20 6d6f 6e6f 746f 6e65 5f63  nts': monotone_c
-000247d0: 6f6e 7374 7261 696e 7473 2c0a 2020 2020  onstraints,.    
-000247e0: 2020 2769 6e74 6572 6163 7469 6f6e 5f63    'interaction_c
-000247f0: 6f6e 7374 7261 696e 7473 273a 2069 6e74  onstraints': int
-00024800: 6572 6163 7469 6f6e 5f63 6f6e 7374 7261  eraction_constra
-00024810: 696e 7473 2c0a 2020 2020 2020 2773 616d  ints,.      'sam
-00024820: 706c 655f 7479 7065 273a 2073 616d 706c  ple_type': sampl
-00024830: 655f 7479 7065 2c0a 2020 2020 2020 276e  e_type,.      'n
-00024840: 6f72 6d61 6c69 7a65 5f74 7970 6527 3a20  ormalize_type': 
-00024850: 6e6f 726d 616c 697a 655f 7479 7065 2c0a  normalize_type,.
-00024860: 2020 2020 2020 2772 6174 655f 6472 6f70        'rate_drop
-00024870: 273a 2072 6174 655f 6472 6f70 2c0a 2020  ': rate_drop,.  
-00024880: 2020 2020 276f 6e65 5f64 726f 7027 3a20      'one_drop': 
-00024890: 6f6e 655f 6472 6f70 2c0a 2020 2020 2020  one_drop,.      
-000248a0: 2773 6b69 705f 6472 6f70 273a 2073 6b69  'skip_drop': ski
-000248b0: 705f 6472 6f70 2c0a 2020 2020 2020 276e  p_drop,.      'n
-000248c0: 756d 5f70 6172 616c 6c65 6c5f 7472 6565  um_parallel_tree
-000248d0: 273a 206e 756d 5f70 6172 616c 6c65 6c5f  ': num_parallel_
-000248e0: 7472 6565 2c0a 2020 2020 2020 2766 6561  tree,.      'fea
-000248f0: 7475 7265 5f73 656c 6563 746f 7227 3a20  ture_selector': 
-00024900: 6665 6174 7572 655f 7365 6c65 6374 6f72  feature_selector
-00024910: 2c0a 2020 2020 2020 2774 6f70 5f6b 273a  ,.      'top_k':
-00024920: 2074 6f70 5f6b 2c0a 2020 2020 2020 276d   top_k,.      'm
-00024930: 6178 5f63 6174 5f74 6f5f 6f6e 6568 6f74  ax_cat_to_onehot
-00024940: 273a 206d 6178 5f63 6174 5f74 6f5f 6f6e  ': max_cat_to_on
-00024950: 6568 6f74 2c0a 2020 2020 2020 276d 6178  ehot,.      'max
-00024960: 5f6c 6561 7665 7327 3a20 6d61 785f 6c65  _leaves': max_le
-00024970: 6176 6573 2c0a 2020 2020 2020 276d 6178  aves,.      'max
-00024980: 5f62 696e 273a 206d 6178 5f62 696e 2c0a  _bin': max_bin,.
-00024990: 2020 2020 2020 2774 7765 6564 6965 5f76        'tweedie_v
-000249a0: 6172 6961 6e63 655f 706f 7765 7227 3a20  ariance_power': 
-000249b0: 7477 6565 6469 655f 7661 7269 616e 6365  tweedie_variance
-000249c0: 5f70 6f77 6572 2c0a 2020 2020 2020 2768  _power,.      'h
-000249d0: 7562 6572 5f73 6c6f 7065 273a 2068 7562  uber_slope': hub
-000249e0: 6572 5f73 6c6f 7065 2c0a 2020 2020 2020  er_slope,.      
-000249f0: 2777 6569 6768 745f 636f 6c75 6d6e 273a  'weight_column':
-00024a00: 2077 6569 6768 745f 636f 6c75 6d6e 2c0a   weight_column,.
-00024a10: 2020 2020 2020 2774 7261 696e 696e 675f        'training_
-00024a20: 6d61 6368 696e 655f 7479 7065 273a 2074  machine_type': t
-00024a30: 7261 696e 696e 675f 6d61 6368 696e 655f  raining_machine_
-00024a40: 7479 7065 2c0a 2020 2020 2020 2774 7261  type,.      'tra
-00024a50: 696e 696e 675f 746f 7461 6c5f 7265 706c  ining_total_repl
-00024a60: 6963 615f 636f 756e 7427 3a20 7472 6169  ica_count': trai
-00024a70: 6e69 6e67 5f74 6f74 616c 5f72 6570 6c69  ning_total_repli
-00024a80: 6361 5f63 6f75 6e74 2c0a 2020 2020 2020  ca_count,.      
-00024a90: 2774 7261 696e 696e 675f 6163 6365 6c65  'training_accele
-00024aa0: 7261 746f 725f 7479 7065 273a 2074 7261  rator_type': tra
-00024ab0: 696e 696e 675f 6163 6365 6c65 7261 746f  ining_accelerato
-00024ac0: 725f 7479 7065 2c0a 2020 2020 2020 2774  r_type,.      't
-00024ad0: 7261 696e 696e 675f 6163 6365 6c65 7261  raining_accelera
-00024ae0: 746f 725f 636f 756e 7427 3a20 7472 6169  tor_count': trai
-00024af0: 6e69 6e67 5f61 6363 656c 6572 6174 6f72  ning_accelerator
-00024b00: 5f63 6f75 6e74 2c0a 2020 2020 2020 2774  _count,.      't
-00024b10: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
-00024b20: 775f 6d61 6368 696e 655f 7479 7065 273a  w_machine_type':
-00024b30: 2074 7261 6e73 666f 726d 5f64 6174 6166   transform_dataf
-00024b40: 6c6f 775f 6d61 6368 696e 655f 7479 7065  low_machine_type
-00024b50: 2c0a 2020 2020 2020 2774 7261 6e73 666f  ,.      'transfo
-00024b60: 726d 5f64 6174 6166 6c6f 775f 6d61 785f  rm_dataflow_max_
-00024b70: 6e75 6d5f 776f 726b 6572 7327 3a20 7472  num_workers': tr
-00024b80: 616e 7366 6f72 6d5f 6461 7461 666c 6f77  ansform_dataflow
-00024b90: 5f6d 6178 5f6e 756d 5f77 6f72 6b65 7273  _max_num_workers
-00024ba0: 2c0a 2020 2020 2020 2774 7261 6e73 666f  ,.      'transfo
-00024bb0: 726d 5f64 6174 6166 6c6f 775f 6469 736b  rm_dataflow_disk
-00024bc0: 5f73 697a 655f 6762 273a 2074 7261 6e73  _size_gb': trans
-00024bd0: 666f 726d 5f64 6174 6166 6c6f 775f 6469  form_dataflow_di
-00024be0: 736b 5f73 697a 655f 6762 2c0a 2020 2020  sk_size_gb,.    
-00024bf0: 2020 2772 756e 5f65 7661 6c75 6174 696f    'run_evaluatio
-00024c00: 6e27 3a20 7275 6e5f 6576 616c 7561 7469  n': run_evaluati
-00024c10: 6f6e 2c0a 2020 2020 2020 2765 7661 6c75  on,.      'evalu
-00024c20: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
-00024c30: 6963 745f 6d61 6368 696e 655f 7479 7065  ict_machine_type
-00024c40: 273a 2028 0a20 2020 2020 2020 2020 2065  ': (.          e
-00024c50: 7661 6c75 6174 696f 6e5f 6261 7463 685f  valuation_batch_
-00024c60: 7072 6564 6963 745f 6d61 6368 696e 655f  predict_machine_
-00024c70: 7479 7065 0a20 2020 2020 2029 2c0a 2020  type.      ),.  
-00024c80: 2020 2020 2765 7661 6c75 6174 696f 6e5f      'evaluation_
-00024c90: 6261 7463 685f 7072 6564 6963 745f 7374  batch_predict_st
-00024ca0: 6172 7469 6e67 5f72 6570 6c69 6361 5f63  arting_replica_c
-00024cb0: 6f75 6e74 273a 2028 0a20 2020 2020 2020  ount': (.       
-00024cc0: 2020 2065 7661 6c75 6174 696f 6e5f 6261     evaluation_ba
-00024cd0: 7463 685f 7072 6564 6963 745f 7374 6172  tch_predict_star
-00024ce0: 7469 6e67 5f72 6570 6c69 6361 5f63 6f75  ting_replica_cou
-00024cf0: 6e74 0a20 2020 2020 2029 2c0a 2020 2020  nt.      ),.    
-00024d00: 2020 2765 7661 6c75 6174 696f 6e5f 6261    'evaluation_ba
-00024d10: 7463 685f 7072 6564 6963 745f 6d61 785f  tch_predict_max_
-00024d20: 7265 706c 6963 615f 636f 756e 7427 3a20  replica_count': 
-00024d30: 280a 2020 2020 2020 2020 2020 6576 616c  (.          eval
-00024d40: 7561 7469 6f6e 5f62 6174 6368 5f70 7265  uation_batch_pre
-00024d50: 6469 6374 5f6d 6178 5f72 6570 6c69 6361  dict_max_replica
-00024d60: 5f63 6f75 6e74 0a20 2020 2020 2029 2c0a  _count.      ),.
-00024d70: 2020 2020 2020 2765 7661 6c75 6174 696f        'evaluatio
-00024d80: 6e5f 6461 7461 666c 6f77 5f6d 6163 6869  n_dataflow_machi
-00024d90: 6e65 5f74 7970 6527 3a20 6576 616c 7561  ne_type': evalua
-00024da0: 7469 6f6e 5f64 6174 6166 6c6f 775f 6d61  tion_dataflow_ma
-00024db0: 6368 696e 655f 7479 7065 2c0a 2020 2020  chine_type,.    
-00024dc0: 2020 2765 7661 6c75 6174 696f 6e5f 6461    'evaluation_da
-00024dd0: 7461 666c 6f77 5f73 7461 7274 696e 675f  taflow_starting_
-00024de0: 6e75 6d5f 776f 726b 6572 7327 3a20 280a  num_workers': (.
-00024df0: 2020 2020 2020 2020 2020 6576 616c 7561            evalua
-00024e00: 7469 6f6e 5f64 6174 6166 6c6f 775f 7374  tion_dataflow_st
-00024e10: 6172 7469 6e67 5f6e 756d 5f77 6f72 6b65  arting_num_worke
-00024e20: 7273 0a20 2020 2020 2029 2c0a 2020 2020  rs.      ),.    
-00024e30: 2020 2765 7661 6c75 6174 696f 6e5f 6461    'evaluation_da
-00024e40: 7461 666c 6f77 5f6d 6178 5f6e 756d 5f77  taflow_max_num_w
-00024e50: 6f72 6b65 7273 273a 2028 0a20 2020 2020  orkers': (.     
-00024e60: 2020 2020 2065 7661 6c75 6174 696f 6e5f       evaluation_
-00024e70: 6461 7461 666c 6f77 5f6d 6178 5f6e 756d  dataflow_max_num
-00024e80: 5f77 6f72 6b65 7273 0a20 2020 2020 2029  _workers.      )
-00024e90: 2c0a 2020 2020 2020 2765 7661 6c75 6174  ,.      'evaluat
-00024ea0: 696f 6e5f 6461 7461 666c 6f77 5f64 6973  ion_dataflow_dis
-00024eb0: 6b5f 7369 7a65 5f67 6227 3a20 6576 616c  k_size_gb': eval
-00024ec0: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
-00024ed0: 6469 736b 5f73 697a 655f 6762 2c0a 2020  disk_size_gb,.  
-00024ee0: 2020 2020 2764 6174 6166 6c6f 775f 7365      'dataflow_se
-00024ef0: 7276 6963 655f 6163 636f 756e 7427 3a20  rvice_account': 
-00024f00: 6461 7461 666c 6f77 5f73 6572 7669 6365  dataflow_service
-00024f10: 5f61 6363 6f75 6e74 2c0a 2020 2020 2020  _account,.      
-00024f20: 2764 6174 6166 6c6f 775f 7375 626e 6574  'dataflow_subnet
-00024f30: 776f 726b 273a 2064 6174 6166 6c6f 775f  work': dataflow_
-00024f40: 7375 626e 6574 776f 726b 2c0a 2020 2020  subnetwork,.    
-00024f50: 2020 2764 6174 6166 6c6f 775f 7573 655f    'dataflow_use_
-00024f60: 7075 626c 6963 5f69 7073 273a 2064 6174  public_ips': dat
-00024f70: 6166 6c6f 775f 7573 655f 7075 626c 6963  aflow_use_public
-00024f80: 5f69 7073 2c0a 2020 2020 2020 2765 6e63  _ips,.      'enc
-00024f90: 7279 7074 696f 6e5f 7370 6563 5f6b 6579  ryption_spec_key
-00024fa0: 5f6e 616d 6527 3a20 656e 6372 7970 7469  _name': encrypti
-00024fb0: 6f6e 5f73 7065 635f 6b65 795f 6e61 6d65  on_spec_key_name
-00024fc0: 2c0a 2020 7d0a 2020 5f75 7064 6174 655f  ,.  }.  _update_
-00024fd0: 7061 7261 6d65 7465 7273 2870 6172 616d  parameters(param
-00024fe0: 6574 6572 5f76 616c 7565 732c 2074 7261  eter_values, tra
-00024ff0: 696e 696e 675f 616e 645f 6576 616c 5f70  ining_and_eval_p
-00025000: 6172 616d 6574 6572 7329 0a0a 2020 6674  arameters)..  ft
-00025010: 655f 7061 7261 6d73 203d 207b 0a20 2020  e_params = {.   
-00025020: 2020 2027 6461 7461 7365 745f 6c65 7665     'dataset_leve
-00025030: 6c5f 6375 7374 6f6d 5f74 7261 6e73 666f  l_custom_transfo
-00025040: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
-00025050: 6f6e 7327 3a20 280a 2020 2020 2020 2020  ons': (.        
-00025060: 2020 6461 7461 7365 745f 6c65 7665 6c5f    dataset_level_
-00025070: 6375 7374 6f6d 5f74 7261 6e73 666f 726d  custom_transform
-00025080: 6174 696f 6e5f 6465 6669 6e69 7469 6f6e  ation_definition
-00025090: 730a 2020 2020 2020 2020 2020 6966 2064  s.          if d
-000250a0: 6174 6173 6574 5f6c 6576 656c 5f63 7573  ataset_level_cus
-000250b0: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
-000250c0: 6f6e 5f64 6566 696e 6974 696f 6e73 0a20  on_definitions. 
-000250d0: 2020 2020 2020 2020 2065 6c73 6520 5b5d           else []
-000250e0: 0a20 2020 2020 2029 2c0a 2020 2020 2020  .      ),.      
-000250f0: 2764 6174 6173 6574 5f6c 6576 656c 5f74  'dataset_level_t
-00025100: 7261 6e73 666f 726d 6174 696f 6e73 273a  ransformations':
-00025110: 2028 0a20 2020 2020 2020 2020 2064 6174   (.          dat
-00025120: 6173 6574 5f6c 6576 656c 5f74 7261 6e73  aset_level_trans
-00025130: 666f 726d 6174 696f 6e73 2069 6620 6461  formations if da
-00025140: 7461 7365 745f 6c65 7665 6c5f 7472 616e  taset_level_tran
-00025150: 7366 6f72 6d61 7469 6f6e 7320 656c 7365  sformations else
-00025160: 205b 5d0a 2020 2020 2020 292c 0a20 2020   [].      ),.   
-00025170: 2020 2027 7275 6e5f 6665 6174 7572 655f     'run_feature_
-00025180: 7365 6c65 6374 696f 6e27 3a20 7275 6e5f  selection': run_
-00025190: 6665 6174 7572 655f 7365 6c65 6374 696f  feature_selectio
-000251a0: 6e2c 0a20 2020 2020 2027 6665 6174 7572  n,.      'featur
-000251b0: 655f 7365 6c65 6374 696f 6e5f 616c 676f  e_selection_algo
-000251c0: 7269 7468 6d27 3a20 6665 6174 7572 655f  rithm': feature_
-000251d0: 7365 6c65 6374 696f 6e5f 616c 676f 7269  selection_algori
-000251e0: 7468 6d2c 0a20 2020 2020 2027 6d61 785f  thm,.      'max_
-000251f0: 7365 6c65 6374 6564 5f66 6561 7475 7265  selected_feature
-00025200: 7327 3a20 6d61 785f 7365 6c65 6374 6564  s': max_selected
-00025210: 5f66 6561 7475 7265 732c 0a20 2020 2020  _features,.     
-00025220: 2027 7072 6564 6566 696e 6564 5f73 706c   'predefined_spl
-00025230: 6974 5f6b 6579 273a 2070 7265 6465 6669  it_key': predefi
-00025240: 6e65 645f 7370 6c69 745f 6b65 792c 0a20  ned_split_key,. 
-00025250: 2020 2020 2027 7374 7261 7469 6669 6564       'stratified
-00025260: 5f73 706c 6974 5f6b 6579 273a 2073 7472  _split_key': str
-00025270: 6174 6966 6965 645f 7370 6c69 745f 6b65  atified_split_ke
-00025280: 792c 0a20 2020 2020 2027 7472 6169 6e69  y,.      'traini
-00025290: 6e67 5f66 7261 6374 696f 6e27 3a20 7472  ng_fraction': tr
-000252a0: 6169 6e69 6e67 5f66 7261 6374 696f 6e2c  aining_fraction,
-000252b0: 0a20 2020 2020 2027 7661 6c69 6461 7469  .      'validati
-000252c0: 6f6e 5f66 7261 6374 696f 6e27 3a20 7661  on_fraction': va
-000252d0: 6c69 6461 7469 6f6e 5f66 7261 6374 696f  lidation_fractio
-000252e0: 6e2c 0a20 2020 2020 2027 7465 7374 5f66  n,.      'test_f
-000252f0: 7261 6374 696f 6e27 3a20 7465 7374 5f66  raction': test_f
-00025300: 7261 6374 696f 6e2c 0a20 2020 2020 2027  raction,.      '
-00025310: 7466 5f61 7574 6f5f 7472 616e 7366 6f72  tf_auto_transfor
-00025320: 6d5f 6665 6174 7572 6573 273a 2028 0a20  m_features': (. 
-00025330: 2020 2020 2020 2020 2074 665f 6175 746f           tf_auto
-00025340: 5f74 7261 6e73 666f 726d 5f66 6561 7475  _transform_featu
-00025350: 7265 7320 6966 2074 665f 6175 746f 5f74  res if tf_auto_t
-00025360: 7261 6e73 666f 726d 5f66 6561 7475 7265  ransform_feature
-00025370: 7320 656c 7365 207b 7d0a 2020 2020 2020  s else {}.      
-00025380: 292c 0a20 2020 2020 2027 7466 5f63 7573  ),.      'tf_cus
-00025390: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
-000253a0: 6f6e 5f64 6566 696e 6974 696f 6e73 273a  on_definitions':
-000253b0: 2028 0a20 2020 2020 2020 2020 2074 665f   (.          tf_
-000253c0: 6375 7374 6f6d 5f74 7261 6e73 666f 726d  custom_transform
-000253d0: 6174 696f 6e5f 6465 6669 6e69 7469 6f6e  ation_definition
-000253e0: 730a 2020 2020 2020 2020 2020 6966 2074  s.          if t
-000253f0: 665f 6375 7374 6f6d 5f74 7261 6e73 666f  f_custom_transfo
-00025400: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
-00025410: 6f6e 730a 2020 2020 2020 2020 2020 656c  ons.          el
-00025420: 7365 205b 5d0a 2020 2020 2020 292c 0a20  se [].      ),. 
-00025430: 2020 2020 2027 7466 5f74 7261 6e73 666f       'tf_transfo
-00025440: 726d 6174 696f 6e73 5f70 6174 6827 3a20  rmations_path': 
-00025450: 7466 5f74 7261 6e73 666f 726d 6174 696f  tf_transformatio
-00025460: 6e73 5f70 6174 682c 0a20 207d 0a20 205f  ns_path,.  }.  _
-00025470: 7570 6461 7465 5f70 6172 616d 6574 6572  update_parameter
-00025480: 7328 7061 7261 6d65 7465 725f 7661 6c75  s(parameter_valu
-00025490: 6573 2c20 6674 655f 7061 7261 6d73 290a  es, fte_params).
-000254a0: 0a20 2064 6174 615f 736f 7572 6365 5f61  .  data_source_a
-000254b0: 6e64 5f73 706c 6974 5f70 6172 616d 6574  nd_split_paramet
-000254c0: 6572 7320 3d20 7b0a 2020 2020 2020 2764  ers = {.      'd
-000254d0: 6174 615f 736f 7572 6365 5f63 7376 5f66  ata_source_csv_f
-000254e0: 696c 656e 616d 6573 273a 2064 6174 615f  ilenames': data_
-000254f0: 736f 7572 6365 5f63 7376 5f66 696c 656e  source_csv_filen
-00025500: 616d 6573 2c0a 2020 2020 2020 2764 6174  ames,.      'dat
-00025510: 615f 736f 7572 6365 5f62 6967 7175 6572  a_source_bigquer
-00025520: 795f 7461 626c 655f 7061 7468 273a 2064  y_table_path': d
-00025530: 6174 615f 736f 7572 6365 5f62 6967 7175  ata_source_bigqu
-00025540: 6572 795f 7461 626c 655f 7061 7468 2c0a  ery_table_path,.
-00025550: 2020 2020 2020 2762 6967 7175 6572 795f        'bigquery_
-00025560: 7374 6167 696e 675f 6675 6c6c 5f64 6174  staging_full_dat
-00025570: 6173 6574 5f69 6427 3a20 6269 6771 7565  aset_id': bigque
-00025580: 7279 5f73 7461 6769 6e67 5f66 756c 6c5f  ry_staging_full_
-00025590: 6461 7461 7365 745f 6964 2c0a 2020 7d0a  dataset_id,.  }.
-000255a0: 2020 5f75 7064 6174 655f 7061 7261 6d65    _update_parame
-000255b0: 7465 7273 2870 6172 616d 6574 6572 5f76  ters(parameter_v
-000255c0: 616c 7565 732c 2064 6174 615f 736f 7572  alues, data_sour
-000255d0: 6365 5f61 6e64 5f73 706c 6974 5f70 6172  ce_and_split_par
-000255e0: 616d 6574 6572 7329 0a0a 2020 7069 7065  ameters)..  pipe
-000255f0: 6c69 6e65 5f64 6566 696e 6974 696f 6e5f  line_definition_
-00025600: 7061 7468 203d 206f 732e 7061 7468 2e6a  path = os.path.j
-00025610: 6f69 6e28 0a20 2020 2020 2070 6174 686c  oin(.      pathl
-00025620: 6962 2e50 6174 6828 5f5f 6669 6c65 5f5f  ib.Path(__file__
-00025630: 292e 7061 7265 6e74 2e72 6573 6f6c 7665  ).parent.resolve
-00025640: 2829 2c20 2778 6762 6f6f 7374 5f74 7261  (), 'xgboost_tra
-00025650: 696e 6572 5f70 6970 656c 696e 652e 7961  iner_pipeline.ya
-00025660: 6d6c 270a 2020 290a 0a20 2072 6574 7572  ml'.  )..  retur
-00025670: 6e20 7069 7065 6c69 6e65 5f64 6566 696e  n pipeline_defin
-00025680: 6974 696f 6e5f 7061 7468 2c20 7061 7261  ition_path, para
-00025690: 6d65 7465 725f 7661 6c75 6573 0a0a 0a64  meter_values...d
-000256a0: 6566 2067 6574 5f78 6762 6f6f 7374 5f68  ef get_xgboost_h
-000256b0: 7970 6572 7061 7261 6d65 7465 725f 7475  yperparameter_tu
-000256c0: 6e69 6e67 5f6a 6f62 5f70 6970 656c 696e  ning_job_pipelin
-000256d0: 655f 616e 645f 7061 7261 6d65 7465 7273  e_and_parameters
-000256e0: 280a 2020 2020 7072 6f6a 6563 743a 2073  (.    project: s
-000256f0: 7472 2c0a 2020 2020 6c6f 6361 7469 6f6e  tr,.    location
-00025700: 3a20 7374 722c 0a20 2020 2072 6f6f 745f  : str,.    root_
-00025710: 6469 723a 2073 7472 2c0a 2020 2020 7461  dir: str,.    ta
-00025720: 7267 6574 5f63 6f6c 756d 6e3a 2073 7472  rget_column: str
-00025730: 2c0a 2020 2020 6f62 6a65 6374 6976 653a  ,.    objective:
-00025740: 2073 7472 2c0a 2020 2020 7374 7564 795f   str,.    study_
-00025750: 7370 6563 5f6d 6574 7269 635f 6964 3a20  spec_metric_id: 
-00025760: 7374 722c 0a20 2020 2073 7475 6479 5f73  str,.    study_s
-00025770: 7065 635f 6d65 7472 6963 5f67 6f61 6c3a  pec_metric_goal:
-00025780: 2073 7472 2c0a 2020 2020 6d61 785f 7472   str,.    max_tr
-00025790: 6961 6c5f 636f 756e 743a 2069 6e74 2c0a  ial_count: int,.
-000257a0: 2020 2020 7061 7261 6c6c 656c 5f74 7269      parallel_tri
-000257b0: 616c 5f63 6f75 6e74 3a20 696e 742c 0a20  al_count: int,. 
-000257c0: 2020 2073 7475 6479 5f73 7065 635f 7061     study_spec_pa
-000257d0: 7261 6d65 7465 7273 5f6f 7665 7272 6964  rameters_overrid
-000257e0: 653a 204f 7074 696f 6e61 6c5b 4c69 7374  e: Optional[List
-000257f0: 5b44 6963 745b 7374 722c 2041 6e79 5d5d  [Dict[str, Any]]
-00025800: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6576  ] = None,.    ev
-00025810: 616c 5f6d 6574 7269 633a 204f 7074 696f  al_metric: Optio
-00025820: 6e61 6c5b 7374 725d 203d 204e 6f6e 652c  nal[str] = None,
-00025830: 0a20 2020 2064 6973 6162 6c65 5f64 6566  .    disable_def
-00025840: 6175 6c74 5f65 7661 6c5f 6d65 7472 6963  ault_eval_metric
-00025850: 3a20 4f70 7469 6f6e 616c 5b69 6e74 5d20  : Optional[int] 
-00025860: 3d20 4e6f 6e65 2c0a 2020 2020 7365 6564  = None,.    seed
-00025870: 3a20 4f70 7469 6f6e 616c 5b69 6e74 5d20  : Optional[int] 
-00025880: 3d20 4e6f 6e65 2c0a 2020 2020 7365 6564  = None,.    seed
-00025890: 5f70 6572 5f69 7465 7261 7469 6f6e 3a20  _per_iteration: 
-000258a0: 4f70 7469 6f6e 616c 5b62 6f6f 6c5d 203d  Optional[bool] =
-000258b0: 204e 6f6e 652c 0a20 2020 2064 6174 6173   None,.    datas
-000258c0: 6574 5f6c 6576 656c 5f63 7573 746f 6d5f  et_level_custom_
-000258d0: 7472 616e 7366 6f72 6d61 7469 6f6e 5f64  transformation_d
-000258e0: 6566 696e 6974 696f 6e73 3a20 4f70 7469  efinitions: Opti
-000258f0: 6f6e 616c 5b0a 2020 2020 2020 2020 4c69  onal[.        Li
-00025900: 7374 5b44 6963 745b 7374 722c 2041 6e79  st[Dict[str, Any
-00025910: 5d5d 0a20 2020 205d 203d 204e 6f6e 652c  ]].    ] = None,
-00025920: 0a20 2020 2064 6174 6173 6574 5f6c 6576  .    dataset_lev
-00025930: 656c 5f74 7261 6e73 666f 726d 6174 696f  el_transformatio
-00025940: 6e73 3a20 4f70 7469 6f6e 616c 5b4c 6973  ns: Optional[Lis
-00025950: 745b 4469 6374 5b73 7472 2c20 416e 795d  t[Dict[str, Any]
-00025960: 5d5d 203d 204e 6f6e 652c 0a20 2020 2072  ]] = None,.    r
-00025970: 756e 5f66 6561 7475 7265 5f73 656c 6563  un_feature_selec
-00025980: 7469 6f6e 3a20 4f70 7469 6f6e 616c 5b62  tion: Optional[b
-00025990: 6f6f 6c5d 203d 204e 6f6e 652c 0a20 2020  ool] = None,.   
-000259a0: 2066 6561 7475 7265 5f73 656c 6563 7469   feature_selecti
-000259b0: 6f6e 5f61 6c67 6f72 6974 686d 3a20 4f70  on_algorithm: Op
-000259c0: 7469 6f6e 616c 5b73 7472 5d20 3d20 4e6f  tional[str] = No
-000259d0: 6e65 2c0a 2020 2020 6d61 785f 7365 6c65  ne,.    max_sele
-000259e0: 6374 6564 5f66 6561 7475 7265 733a 204f  cted_features: O
-000259f0: 7074 696f 6e61 6c5b 696e 745d 203d 204e  ptional[int] = N
-00025a00: 6f6e 652c 0a20 2020 2070 7265 6465 6669  one,.    predefi
-00025a10: 6e65 645f 7370 6c69 745f 6b65 793a 204f  ned_split_key: O
-00025a20: 7074 696f 6e61 6c5b 7374 725d 203d 204e  ptional[str] = N
-00025a30: 6f6e 652c 0a20 2020 2073 7472 6174 6966  one,.    stratif
-00025a40: 6965 645f 7370 6c69 745f 6b65 793a 204f  ied_split_key: O
-00025a50: 7074 696f 6e61 6c5b 7374 725d 203d 204e  ptional[str] = N
-00025a60: 6f6e 652c 0a20 2020 2074 7261 696e 696e  one,.    trainin
-00025a70: 675f 6672 6163 7469 6f6e 3a20 4f70 7469  g_fraction: Opti
-00025a80: 6f6e 616c 5b66 6c6f 6174 5d20 3d20 4e6f  onal[float] = No
-00025a90: 6e65 2c0a 2020 2020 7661 6c69 6461 7469  ne,.    validati
-00025aa0: 6f6e 5f66 7261 6374 696f 6e3a 204f 7074  on_fraction: Opt
-00025ab0: 696f 6e61 6c5b 666c 6f61 745d 203d 204e  ional[float] = N
-00025ac0: 6f6e 652c 0a20 2020 2074 6573 745f 6672  one,.    test_fr
-00025ad0: 6163 7469 6f6e 3a20 4f70 7469 6f6e 616c  action: Optional
-00025ae0: 5b66 6c6f 6174 5d20 3d20 4e6f 6e65 2c0a  [float] = None,.
-00025af0: 2020 2020 7466 5f61 7574 6f5f 7472 616e      tf_auto_tran
-00025b00: 7366 6f72 6d5f 6665 6174 7572 6573 3a20  sform_features: 
-00025b10: 4f70 7469 6f6e 616c 5b0a 2020 2020 2020  Optional[.      
-00025b20: 2020 556e 696f 6e5b 4c69 7374 5b73 7472    Union[List[str
-00025b30: 5d2c 2044 6963 745b 7374 722c 204c 6973  ], Dict[str, Lis
-00025b40: 745b 7374 725d 5d5d 0a20 2020 205d 203d  t[str]]].    ] =
-00025b50: 204e 6f6e 652c 0a20 2020 2074 665f 6375   None,.    tf_cu
-00025b60: 7374 6f6d 5f74 7261 6e73 666f 726d 6174  stom_transformat
-00025b70: 696f 6e5f 6465 6669 6e69 7469 6f6e 733a  ion_definitions:
-00025b80: 204f 7074 696f 6e61 6c5b 4c69 7374 5b44   Optional[List[D
-00025b90: 6963 745b 7374 722c 2041 6e79 5d5d 5d20  ict[str, Any]]] 
-00025ba0: 3d20 4e6f 6e65 2c0a 2020 2020 7466 5f74  = None,.    tf_t
-00025bb0: 7261 6e73 666f 726d 6174 696f 6e73 5f70  ransformations_p
-00025bc0: 6174 683a 204f 7074 696f 6e61 6c5b 7374  ath: Optional[st
-00025bd0: 725d 203d 204e 6f6e 652c 0a20 2020 2064  r] = None,.    d
-00025be0: 6174 615f 736f 7572 6365 5f63 7376 5f66  ata_source_csv_f
-00025bf0: 696c 656e 616d 6573 3a20 4f70 7469 6f6e  ilenames: Option
-00025c00: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
-00025c10: 2020 2020 6461 7461 5f73 6f75 7263 655f      data_source_
-00025c20: 6269 6771 7565 7279 5f74 6162 6c65 5f70  bigquery_table_p
-00025c30: 6174 683a 204f 7074 696f 6e61 6c5b 7374  ath: Optional[st
-00025c40: 725d 203d 204e 6f6e 652c 0a20 2020 2062  r] = None,.    b
-00025c50: 6967 7175 6572 795f 7374 6167 696e 675f  igquery_staging_
-00025c60: 6675 6c6c 5f64 6174 6173 6574 5f69 643a  full_dataset_id:
-00025c70: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
-00025c80: 204e 6f6e 652c 0a20 2020 2077 6569 6768   None,.    weigh
-00025c90: 745f 636f 6c75 6d6e 3a20 4f70 7469 6f6e  t_column: Option
-00025ca0: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
-00025cb0: 2020 2020 6d61 785f 6661 696c 6564 5f74      max_failed_t
-00025cc0: 7269 616c 5f63 6f75 6e74 3a20 4f70 7469  rial_count: Opti
-00025cd0: 6f6e 616c 5b69 6e74 5d20 3d20 4e6f 6e65  onal[int] = None
-00025ce0: 2c0a 2020 2020 7472 6169 6e69 6e67 5f6d  ,.    training_m
-00025cf0: 6163 6869 6e65 5f74 7970 653a 204f 7074  achine_type: Opt
-00025d00: 696f 6e61 6c5b 7374 725d 203d 204e 6f6e  ional[str] = Non
-00025d10: 652c 0a20 2020 2074 7261 696e 696e 675f  e,.    training_
-00025d20: 746f 7461 6c5f 7265 706c 6963 615f 636f  total_replica_co
-00025d30: 756e 743a 204f 7074 696f 6e61 6c5b 696e  unt: Optional[in
-00025d40: 745d 203d 204e 6f6e 652c 0a20 2020 2074  t] = None,.    t
-00025d50: 7261 696e 696e 675f 6163 6365 6c65 7261  raining_accelera
-00025d60: 746f 725f 7479 7065 3a20 4f70 7469 6f6e  tor_type: Option
-00025d70: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
-00025d80: 2020 2020 7472 6169 6e69 6e67 5f61 6363      training_acc
-00025d90: 656c 6572 6174 6f72 5f63 6f75 6e74 3a20  elerator_count: 
-00025da0: 4f70 7469 6f6e 616c 5b69 6e74 5d20 3d20  Optional[int] = 
-00025db0: 4e6f 6e65 2c0a 2020 2020 7374 7564 795f  None,.    study_
-00025dc0: 7370 6563 5f61 6c67 6f72 6974 686d 3a20  spec_algorithm: 
-00025dd0: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
-00025de0: 4e6f 6e65 2c0a 2020 2020 7374 7564 795f  None,.    study_
-00025df0: 7370 6563 5f6d 6561 7375 7265 6d65 6e74  spec_measurement
-00025e00: 5f73 656c 6563 7469 6f6e 5f74 7970 653a  _selection_type:
-00025e10: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
-00025e20: 204e 6f6e 652c 0a20 2020 2074 7261 6e73   None,.    trans
-00025e30: 666f 726d 5f64 6174 6166 6c6f 775f 6d61  form_dataflow_ma
-00025e40: 6368 696e 655f 7479 7065 3a20 4f70 7469  chine_type: Opti
-00025e50: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
-00025e60: 2c0a 2020 2020 7472 616e 7366 6f72 6d5f  ,.    transform_
-00025e70: 6461 7461 666c 6f77 5f6d 6178 5f6e 756d  dataflow_max_num
-00025e80: 5f77 6f72 6b65 7273 3a20 4f70 7469 6f6e  _workers: Option
-00025e90: 616c 5b69 6e74 5d20 3d20 4e6f 6e65 2c0a  al[int] = None,.
-00025ea0: 2020 2020 7472 616e 7366 6f72 6d5f 6461      transform_da
-00025eb0: 7461 666c 6f77 5f64 6973 6b5f 7369 7a65  taflow_disk_size
-00025ec0: 5f67 623a 204f 7074 696f 6e61 6c5b 696e  _gb: Optional[in
-00025ed0: 745d 203d 204e 6f6e 652c 0a20 2020 2072  t] = None,.    r
-00025ee0: 756e 5f65 7661 6c75 6174 696f 6e3a 204f  un_evaluation: O
-00025ef0: 7074 696f 6e61 6c5b 626f 6f6c 5d20 3d20  ptional[bool] = 
-00025f00: 4e6f 6e65 2c0a 2020 2020 6576 616c 7561  None,.    evalua
-00025f10: 7469 6f6e 5f62 6174 6368 5f70 7265 6469  tion_batch_predi
-00025f20: 6374 5f6d 6163 6869 6e65 5f74 7970 653a  ct_machine_type:
-00025f30: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
-00025f40: 204e 6f6e 652c 0a20 2020 2065 7661 6c75   None,.    evalu
-00025f50: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
-00025f60: 6963 745f 7374 6172 7469 6e67 5f72 6570  ict_starting_rep
-00025f70: 6c69 6361 5f63 6f75 6e74 3a20 4f70 7469  lica_count: Opti
-00025f80: 6f6e 616c 5b69 6e74 5d20 3d20 4e6f 6e65  onal[int] = None
-00025f90: 2c0a 2020 2020 6576 616c 7561 7469 6f6e  ,.    evaluation
-00025fa0: 5f62 6174 6368 5f70 7265 6469 6374 5f6d  _batch_predict_m
-00025fb0: 6178 5f72 6570 6c69 6361 5f63 6f75 6e74  ax_replica_count
-00025fc0: 3a20 4f70 7469 6f6e 616c 5b69 6e74 5d20  : Optional[int] 
-00025fd0: 3d20 4e6f 6e65 2c0a 2020 2020 6576 616c  = None,.    eval
-00025fe0: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
-00025ff0: 6d61 6368 696e 655f 7479 7065 3a20 4f70  machine_type: Op
-00026000: 7469 6f6e 616c 5b73 7472 5d20 3d20 4e6f  tional[str] = No
-00026010: 6e65 2c0a 2020 2020 6576 616c 7561 7469  ne,.    evaluati
-00026020: 6f6e 5f64 6174 6166 6c6f 775f 7374 6172  on_dataflow_star
-00026030: 7469 6e67 5f6e 756d 5f77 6f72 6b65 7273  ting_num_workers
-00026040: 3a20 4f70 7469 6f6e 616c 5b69 6e74 5d20  : Optional[int] 
-00026050: 3d20 4e6f 6e65 2c0a 2020 2020 6576 616c  = None,.    eval
-00026060: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
-00026070: 6d61 785f 6e75 6d5f 776f 726b 6572 733a  max_num_workers:
-00026080: 204f 7074 696f 6e61 6c5b 696e 745d 203d   Optional[int] =
-00026090: 204e 6f6e 652c 0a20 2020 2065 7661 6c75   None,.    evalu
-000260a0: 6174 696f 6e5f 6461 7461 666c 6f77 5f64  ation_dataflow_d
-000260b0: 6973 6b5f 7369 7a65 5f67 623a 204f 7074  isk_size_gb: Opt
-000260c0: 696f 6e61 6c5b 696e 745d 203d 204e 6f6e  ional[int] = Non
-000260d0: 652c 0a20 2020 2064 6174 6166 6c6f 775f  e,.    dataflow_
-000260e0: 7365 7276 6963 655f 6163 636f 756e 743a  service_account:
-000260f0: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
-00026100: 204e 6f6e 652c 0a20 2020 2064 6174 6166   None,.    dataf
-00026110: 6c6f 775f 7375 626e 6574 776f 726b 3a20  low_subnetwork: 
-00026120: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
-00026130: 4e6f 6e65 2c0a 2020 2020 6461 7461 666c  None,.    datafl
-00026140: 6f77 5f75 7365 5f70 7562 6c69 635f 6970  ow_use_public_ip
-00026150: 733a 204f 7074 696f 6e61 6c5b 626f 6f6c  s: Optional[bool
-00026160: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 656e  ] = None,.    en
-00026170: 6372 7970 7469 6f6e 5f73 7065 635f 6b65  cryption_spec_ke
-00026180: 795f 6e61 6d65 3a20 4f70 7469 6f6e 616c  y_name: Optional
-00026190: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 293a  [str] = None,.):
-000261a0: 0a20 2022 2222 4765 7420 7468 6520 5847  .  """Get the XG
-000261b0: 426f 6f73 7420 4879 7065 7270 6172 616d  Boost Hyperparam
-000261c0: 6574 6572 5475 6e69 6e67 4a6f 6220 7069  eterTuningJob pi
-000261d0: 7065 6c69 6e65 2e0a 0a20 2041 7267 733a  peline...  Args:
-000261e0: 0a20 2020 2070 726f 6a65 6374 3a20 5468  .    project: Th
-000261f0: 6520 4743 5020 7072 6f6a 6563 7420 7468  e GCP project th
-00026200: 6174 2072 756e 7320 7468 6520 7069 7065  at runs the pipe
-00026210: 6c69 6e65 2063 6f6d 706f 6e65 6e74 732e  line components.
-00026220: 0a20 2020 206c 6f63 6174 696f 6e3a 2054  .    location: T
-00026230: 6865 2047 4350 2072 6567 696f 6e20 7468  he GCP region th
-00026240: 6174 2072 756e 7320 7468 6520 7069 7065  at runs the pipe
-00026250: 6c69 6e65 2063 6f6d 706f 6e65 6e74 732e  line components.
-00026260: 0a20 2020 2072 6f6f 745f 6469 723a 2054  .    root_dir: T
-00026270: 6865 2072 6f6f 7420 4743 5320 6469 7265  he root GCS dire
-00026280: 6374 6f72 7920 666f 7220 7468 6520 7069  ctory for the pi
-00026290: 7065 6c69 6e65 2063 6f6d 706f 6e65 6e74  peline component
-000262a0: 732e 0a20 2020 2074 6172 6765 745f 636f  s..    target_co
-000262b0: 6c75 6d6e 3a20 5468 6520 7461 7267 6574  lumn: The target
-000262c0: 2063 6f6c 756d 6e20 6e61 6d65 2e0a 2020   column name..  
-000262d0: 2020 6f62 6a65 6374 6976 653a 2053 7065    objective: Spe
-000262e0: 6369 6669 6573 2074 6865 206c 6561 726e  cifies the learn
-000262f0: 696e 6720 7461 736b 2061 6e64 2074 6865  ing task and the
-00026300: 206c 6561 726e 696e 6720 6f62 6a65 6374   learning object
-00026310: 6976 652e 204d 7573 7420 6265 0a20 2020  ive. Must be.   
-00026320: 2020 206f 6e65 206f 6620 5b72 6567 3a73     one of [reg:s
-00026330: 7175 6172 6564 6572 726f 722c 2072 6567  quarederror, reg
-00026340: 3a73 7175 6172 6564 6c6f 6765 7272 6f72  :squaredlogerror
-00026350: 2c0a 2020 2020 2020 7265 673a 6c6f 6769  ,.      reg:logi
-00026360: 7374 6963 2c20 7265 673a 6761 6d6d 612c  stic, reg:gamma,
-00026370: 2072 6567 3a74 7765 6564 6965 2c20 7265   reg:tweedie, re
-00026380: 673a 7073 6575 646f 6875 6265 7265 7272  g:pseudohubererr
-00026390: 6f72 2c0a 2020 2020 2020 6269 6e61 7279  or,.      binary
-000263a0: 3a6c 6f67 6973 7469 632c 206d 756c 7469  :logistic, multi
-000263b0: 3a73 6f66 7470 726f 625d 2e0a 2020 2020  :softprob]..    
-000263c0: 7374 7564 795f 7370 6563 5f6d 6574 7269  study_spec_metri
-000263d0: 635f 6964 3a20 4d65 7472 6963 2074 6f20  c_id: Metric to 
-000263e0: 6f70 7469 6d69 7a65 2e20 466f 7220 6f70  optimize. For op
-000263f0: 7469 6f6e 732c 2070 6c65 6173 6520 6c6f  tions, please lo
-00026400: 6f6b 2075 6e64 6572 0a20 2020 2020 2027  ok under.      '
-00026410: 6576 616c 5f6d 6574 7269 6327 2061 740a  eval_metric' at.
-00026420: 2020 2020 2020 6874 7470 733a 2f2f 7867        https://xg
-00026430: 626f 6f73 742e 7265 6164 7468 6564 6f63  boost.readthedoc
-00026440: 732e 696f 2f65 6e2f 7374 6162 6c65 2f70  s.io/en/stable/p
-00026450: 6172 616d 6574 6572 2e68 746d 6c23 6c65  arameter.html#le
-00026460: 6172 6e69 6e67 2d74 6173 6b2d 7061 7261  arning-task-para
-00026470: 6d65 7465 7273 2e0a 2020 2020 7374 7564  meters..    stud
-00026480: 795f 7370 6563 5f6d 6574 7269 635f 676f  y_spec_metric_go
-00026490: 616c 3a20 4f70 7469 6d69 7a61 7469 6f6e  al: Optimization
-000264a0: 2067 6f61 6c20 6f66 2074 6865 206d 6574   goal of the met
-000264b0: 7269 632c 2070 6f73 7369 626c 6520 7661  ric, possible va
-000264c0: 6c75 6573 3a0a 2020 2020 2020 224d 4158  lues:.      "MAX
-000264d0: 494d 495a 4522 2c20 224d 494e 494d 495a  IMIZE", "MINIMIZ
-000264e0: 4522 2e0a 2020 2020 6d61 785f 7472 6961  E"..    max_tria
-000264f0: 6c5f 636f 756e 743a 2054 6865 2064 6573  l_count: The des
-00026500: 6972 6564 2074 6f74 616c 206e 756d 6265  ired total numbe
-00026510: 7220 6f66 2074 7269 616c 732e 0a20 2020  r of trials..   
-00026520: 2070 6172 616c 6c65 6c5f 7472 6961 6c5f   parallel_trial_
-00026530: 636f 756e 743a 2054 6865 2064 6573 6972  count: The desir
-00026540: 6564 206e 756d 6265 7220 6f66 2074 7269  ed number of tri
-00026550: 616c 7320 746f 2072 756e 2069 6e20 7061  als to run in pa
-00026560: 7261 6c6c 656c 2e0a 2020 2020 7374 7564  rallel..    stud
-00026570: 795f 7370 6563 5f70 6172 616d 6574 6572  y_spec_parameter
-00026580: 735f 6f76 6572 7269 6465 3a20 4c69 7374  s_override: List
-00026590: 206f 6620 6469 6374 696f 6e61 7269 6573   of dictionaries
-000265a0: 2072 6570 7265 7365 6e74 696e 6720 7061   representing pa
-000265b0: 7261 6d65 7465 7273 0a20 2020 2020 2074  rameters.      t
-000265c0: 6f20 6f70 7469 6d69 7a65 2e20 5468 6520  o optimize. The 
-000265d0: 6469 6374 696f 6e61 7279 206b 6579 2069  dictionary key i
-000265e0: 7320 7468 6520 7061 7261 6d65 7465 725f  s the parameter_
-000265f0: 6964 2c20 7768 6963 6820 6973 2070 6173  id, which is pas
-00026600: 7365 6420 746f 0a20 2020 2020 2074 7261  sed to.      tra
-00026610: 696e 696e 6720 6a6f 6220 6173 2061 2063  ining job as a c
-00026620: 6f6d 6d61 6e64 206c 696e 6520 6172 6775  ommand line argu
-00026630: 6d65 6e74 2c20 616e 6420 7468 6520 6469  ment, and the di
-00026640: 6374 696f 6e61 7279 2076 616c 7565 2069  ctionary value i
-00026650: 7320 7468 650a 2020 2020 2020 7061 7261  s the.      para
-00026660: 6d65 7465 7220 7370 6563 6966 6963 6174  meter specificat
-00026670: 696f 6e20 6f66 2074 6865 206d 6574 7269  ion of the metri
-00026680: 632e 0a20 2020 2065 7661 6c5f 6d65 7472  c..    eval_metr
-00026690: 6963 3a20 4576 616c 7561 7469 6f6e 206d  ic: Evaluation m
-000266a0: 6574 7269 6373 2066 6f72 2076 616c 6964  etrics for valid
-000266b0: 6174 696f 6e20 6461 7461 2072 6570 7265  ation data repre
-000266c0: 7365 6e74 6564 2061 7320 610a 2020 2020  sented as a.    
-000266d0: 2020 636f 6d6d 612d 7365 7061 7261 7465    comma-separate
-000266e0: 6420 7374 7269 6e67 2e0a 2020 2020 6469  d string..    di
-000266f0: 7361 626c 655f 6465 6661 756c 745f 6576  sable_default_ev
-00026700: 616c 5f6d 6574 7269 633a 2046 6c61 6720  al_metric: Flag 
-00026710: 746f 2064 6973 6162 6c65 2064 6566 6175  to disable defau
-00026720: 6c74 206d 6574 7269 632e 2053 6574 2074  lt metric. Set t
-00026730: 6f20 3e30 2074 6f0a 2020 2020 2020 6469  o >0 to.      di
-00026740: 7361 626c 652e 2044 6566 6175 6c74 2074  sable. Default t
-00026750: 6f20 302e 0a20 2020 2073 6565 643a 2052  o 0..    seed: R
-00026760: 616e 646f 6d20 7365 6564 2e0a 2020 2020  andom seed..    
-00026770: 7365 6564 5f70 6572 5f69 7465 7261 7469  seed_per_iterati
-00026780: 6f6e 3a20 5365 6564 2050 524e 4720 6465  on: Seed PRNG de
-00026790: 7465 726d 6e69 7374 6963 6c79 2076 6961  termnisticly via
-000267a0: 2069 7465 7261 746f 7220 6e75 6d62 6572   iterator number
-000267b0: 2e0a 2020 2020 6461 7461 7365 745f 6c65  ..    dataset_le
-000267c0: 7665 6c5f 6375 7374 6f6d 5f74 7261 6e73  vel_custom_trans
-000267d0: 666f 726d 6174 696f 6e5f 6465 6669 6e69  formation_defini
-000267e0: 7469 6f6e 733a 2044 6174 6173 6574 2d6c  tions: Dataset-l
-000267f0: 6576 656c 2063 7573 746f 6d0a 2020 2020  evel custom.    
-00026800: 2020 7472 616e 7366 6f72 6d61 7469 6f6e    transformation
-00026810: 2064 6566 696e 6974 696f 6e73 2069 6e20   definitions in 
-00026820: 7374 7269 6e67 2066 6f72 6d61 742e 0a20  string format.. 
-00026830: 2020 2064 6174 6173 6574 5f6c 6576 656c     dataset_level
-00026840: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
-00026850: 3a20 4461 7461 7365 742d 6c65 7665 6c20  : Dataset-level 
-00026860: 7472 616e 7366 6f72 6d61 7469 6f6e 2063  transformation c
-00026870: 6f6e 6669 6775 7261 7469 6f6e 2069 6e0a  onfiguration in.
-00026880: 2020 2020 2020 7374 7269 6e67 2066 6f72        string for
-00026890: 6d61 742e 0a20 2020 2072 756e 5f66 6561  mat..    run_fea
-000268a0: 7475 7265 5f73 656c 6563 7469 6f6e 3a20  ture_selection: 
-000268b0: 5768 6574 6865 7220 746f 2065 6e61 626c  Whether to enabl
-000268c0: 6520 6665 6174 7572 6520 7365 6c65 6374  e feature select
-000268d0: 696f 6e2e 0a20 2020 2066 6561 7475 7265  ion..    feature
-000268e0: 5f73 656c 6563 7469 6f6e 5f61 6c67 6f72  _selection_algor
-000268f0: 6974 686d 3a20 4665 6174 7572 6520 7365  ithm: Feature se
-00026900: 6c65 6374 696f 6e20 616c 676f 7269 7468  lection algorith
-00026910: 6d2e 0a20 2020 206d 6178 5f73 656c 6563  m..    max_selec
-00026920: 7465 645f 6665 6174 7572 6573 3a20 4d61  ted_features: Ma
-00026930: 7869 6d75 6d20 6e75 6d62 6572 206f 6620  ximum number of 
-00026940: 6665 6174 7572 6573 2074 6f20 7365 6c65  features to sele
-00026950: 6374 2e0a 2020 2020 7072 6564 6566 696e  ct..    predefin
-00026960: 6564 5f73 706c 6974 5f6b 6579 3a20 5072  ed_split_key: Pr
-00026970: 6564 6566 696e 6564 2073 706c 6974 206b  edefined split k
-00026980: 6579 2e0a 2020 2020 7374 7261 7469 6669  ey..    stratifi
-00026990: 6564 5f73 706c 6974 5f6b 6579 3a20 5374  ed_split_key: St
-000269a0: 7261 7469 6669 6564 2073 706c 6974 206b  ratified split k
-000269b0: 6579 2e0a 2020 2020 7472 6169 6e69 6e67  ey..    training
-000269c0: 5f66 7261 6374 696f 6e3a 2054 7261 696e  _fraction: Train
-000269d0: 696e 6720 6672 6163 7469 6f6e 2e0a 2020  ing fraction..  
-000269e0: 2020 7661 6c69 6461 7469 6f6e 5f66 7261    validation_fra
-000269f0: 6374 696f 6e3a 2056 616c 6964 6174 696f  ction: Validatio
-00026a00: 6e20 6672 6163 7469 6f6e 2e0a 2020 2020  n fraction..    
-00026a10: 7465 7374 5f66 7261 6374 696f 6e3a 2054  test_fraction: T
-00026a20: 6573 7420 6672 6163 7469 6f6e 2e0a 2020  est fraction..  
-00026a30: 2020 7466 5f61 7574 6f5f 7472 616e 7366    tf_auto_transf
-00026a40: 6f72 6d5f 6665 6174 7572 6573 3a20 4c69  orm_features: Li
-00026a50: 7374 206f 6620 6175 746f 2074 7261 6e73  st of auto trans
-00026a60: 666f 726d 2066 6561 7475 7265 7320 696e  form features in
-00026a70: 2074 6865 0a20 2020 2020 2063 6f6d 6d61   the.      comma
-00026a80: 2d73 6570 6172 6174 6564 2073 7472 696e  -separated strin
-00026a90: 6720 666f 726d 6174 2e0a 2020 2020 7466  g format..    tf
-00026aa0: 5f63 7573 746f 6d5f 7472 616e 7366 6f72  _custom_transfor
-00026ab0: 6d61 7469 6f6e 5f64 6566 696e 6974 696f  mation_definitio
-00026ac0: 6e73 3a20 5446 2063 7573 746f 6d20 7472  ns: TF custom tr
-00026ad0: 616e 7366 6f72 6d61 7469 6f6e 2064 6566  ansformation def
-00026ae0: 696e 6974 696f 6e73 0a20 2020 2020 2069  initions.      i
-00026af0: 6e20 7374 7269 6e67 2066 6f72 6d61 742e  n string format.
-00026b00: 0a20 2020 2074 665f 7472 616e 7366 6f72  .    tf_transfor
-00026b10: 6d61 7469 6f6e 735f 7061 7468 3a20 5061  mations_path: Pa
-00026b20: 7468 2074 6f20 5446 2074 7261 6e73 666f  th to TF transfo
-00026b30: 726d 6174 696f 6e20 636f 6e66 6967 7572  rmation configur
-00026b40: 6174 696f 6e2e 0a20 2020 2064 6174 615f  ation..    data_
-00026b50: 736f 7572 6365 5f63 7376 5f66 696c 656e  source_csv_filen
-00026b60: 616d 6573 3a20 5468 6520 4353 5620 6461  ames: The CSV da
-00026b70: 7461 2073 6f75 7263 652e 0a20 2020 2064  ta source..    d
-00026b80: 6174 615f 736f 7572 6365 5f62 6967 7175  ata_source_bigqu
-00026b90: 6572 795f 7461 626c 655f 7061 7468 3a20  ery_table_path: 
-00026ba0: 5468 6520 4269 6751 7565 7279 2064 6174  The BigQuery dat
-00026bb0: 6120 736f 7572 6365 2e0a 2020 2020 6269  a source..    bi
-00026bc0: 6771 7565 7279 5f73 7461 6769 6e67 5f66  gquery_staging_f
-00026bd0: 756c 6c5f 6461 7461 7365 745f 6964 3a20  ull_dataset_id: 
-00026be0: 5468 6520 4269 6751 7565 7279 2073 7461  The BigQuery sta
-00026bf0: 6769 6e67 2066 756c 6c20 6461 7461 7365  ging full datase
-00026c00: 7420 6964 2066 6f72 0a20 2020 2020 2073  t id for.      s
-00026c10: 746f 7269 6e67 2069 6e74 6572 6d65 6469  toring intermedi
-00026c20: 6174 6520 7461 626c 6573 2e0a 2020 2020  ate tables..    
-00026c30: 7765 6967 6874 5f63 6f6c 756d 6e3a 2054  weight_column: T
-00026c40: 6865 2077 6569 6768 7420 636f 6c75 6d6e  he weight column
-00026c50: 206e 616d 652e 0a20 2020 206d 6178 5f66   name..    max_f
-00026c60: 6169 6c65 645f 7472 6961 6c5f 636f 756e  ailed_trial_coun
-00026c70: 743a 2054 6865 206e 756d 6265 7220 6f66  t: The number of
-00026c80: 2066 6169 6c65 6420 7472 6961 6c73 2074   failed trials t
-00026c90: 6861 7420 6e65 6564 2074 6f20 6265 2073  hat need to be s
-00026ca0: 6565 6e0a 2020 2020 2020 6265 666f 7265  een.      before
-00026cb0: 2066 6169 6c69 6e67 2074 6865 2048 7970   failing the Hyp
-00026cc0: 6572 7061 7261 6d65 7465 7254 756e 696e  erparameterTunin
-00026cd0: 674a 6f62 2e20 4966 2073 6574 2074 6f20  gJob. If set to 
-00026ce0: 302c 2056 6572 7465 7820 4149 2064 6563  0, Vertex AI dec
-00026cf0: 6964 6573 0a20 2020 2020 2068 6f77 206d  ides.      how m
-00026d00: 616e 7920 7472 6961 6c73 206d 7573 7420  any trials must 
-00026d10: 6661 696c 2062 6566 6f72 6520 7468 6520  fail before the 
-00026d20: 7768 6f6c 6520 6a6f 6220 6661 696c 732e  whole job fails.
-00026d30: 0a20 2020 2074 7261 696e 696e 675f 6d61  .    training_ma
-00026d40: 6368 696e 655f 7479 7065 3a20 4d61 6368  chine_type: Mach
-00026d50: 696e 6520 7479 7065 2e0a 2020 2020 7472  ine type..    tr
-00026d60: 6169 6e69 6e67 5f74 6f74 616c 5f72 6570  aining_total_rep
-00026d70: 6c69 6361 5f63 6f75 6e74 3a20 4e75 6d62  lica_count: Numb
-00026d80: 6572 206f 6620 776f 726b 6572 732e 0a20  er of workers.. 
-00026d90: 2020 2074 7261 696e 696e 675f 6163 6365     training_acce
-00026da0: 6c65 7261 746f 725f 7479 7065 3a20 4163  lerator_type: Ac
-00026db0: 6365 6c65 7261 746f 7220 7479 7065 2e0a  celerator type..
-00026dc0: 2020 2020 7472 6169 6e69 6e67 5f61 6363      training_acc
-00026dd0: 656c 6572 6174 6f72 5f63 6f75 6e74 3a20  elerator_count: 
-00026de0: 4163 6365 6c65 7261 746f 7220 636f 756e  Accelerator coun
-00026df0: 742e 0a20 2020 2073 7475 6479 5f73 7065  t..    study_spe
-00026e00: 635f 616c 676f 7269 7468 6d3a 2054 6865  c_algorithm: The
-00026e10: 2073 6561 7263 6820 616c 676f 7269 7468   search algorith
-00026e20: 6d20 7370 6563 6966 6965 6420 666f 7220  m specified for 
-00026e30: 7468 6520 7374 7564 792e 204f 6e65 206f  the study. One o
-00026e40: 660a 2020 2020 2020 2741 4c47 4f52 4954  f.      'ALGORIT
-00026e50: 484d 5f55 4e53 5045 4349 4649 4544 272c  HM_UNSPECIFIED',
-00026e60: 2027 4752 4944 5f53 4541 5243 4827 2c20   'GRID_SEARCH', 
-00026e70: 6f72 2027 5241 4e44 4f4d 5f53 4541 5243  or 'RANDOM_SEARC
-00026e80: 4827 2e0a 2020 2020 7374 7564 795f 7370  H'..    study_sp
-00026e90: 6563 5f6d 6561 7375 7265 6d65 6e74 5f73  ec_measurement_s
-00026ea0: 656c 6563 7469 6f6e 5f74 7970 653a 2020  election_type:  
-00026eb0: 5768 6963 6820 6d65 6173 7572 656d 656e  Which measuremen
-00026ec0: 7420 746f 2075 7365 2069 662f 7768 656e  t to use if/when
-00026ed0: 2074 6865 0a20 2020 2020 2073 6572 7669   the.      servi
-00026ee0: 6365 2061 7574 6f6d 6174 6963 616c 6c79  ce automatically
-00026ef0: 2073 656c 6563 7473 2074 6865 2066 696e   selects the fin
-00026f00: 616c 206d 6561 7375 7265 6d65 6e74 2066  al measurement f
-00026f10: 726f 6d20 7072 6576 696f 7573 6c79 0a20  rom previously. 
-00026f20: 2020 2020 2072 6570 6f72 7465 6420 696e       reported in
-00026f30: 7465 726d 6564 6961 7465 206d 6561 7375  termediate measu
-00026f40: 7265 6d65 6e74 732e 204f 6e65 206f 6620  rements. One of 
-00026f50: 2242 4553 545f 4d45 4153 5552 454d 454e  "BEST_MEASUREMEN
-00026f60: 5422 206f 720a 2020 2020 2020 224c 4153  T" or.      "LAS
-00026f70: 545f 4d45 4153 5552 454d 454e 5422 2e0a  T_MEASUREMENT"..
-00026f80: 2020 2020 7472 616e 7366 6f72 6d5f 6461      transform_da
-00026f90: 7461 666c 6f77 5f6d 6163 6869 6e65 5f74  taflow_machine_t
-00026fa0: 7970 653a 2054 6865 2064 6174 6166 6c6f  ype: The dataflo
-00026fb0: 7720 6d61 6368 696e 6520 7479 7065 2066  w machine type f
-00026fc0: 6f72 2074 7261 6e73 666f 726d 0a20 2020  or transform.   
-00026fd0: 2020 2063 6f6d 706f 6e65 6e74 2e0a 2020     component..  
-00026fe0: 2020 7472 616e 7366 6f72 6d5f 6461 7461    transform_data
-00026ff0: 666c 6f77 5f6d 6178 5f6e 756d 5f77 6f72  flow_max_num_wor
-00027000: 6b65 7273 3a20 5468 6520 6d61 7820 6e75  kers: The max nu
-00027010: 6d62 6572 206f 6620 4461 7461 666c 6f77  mber of Dataflow
-00027020: 2077 6f72 6b65 7273 2066 6f72 0a20 2020   workers for.   
-00027030: 2020 2074 7261 6e73 666f 726d 2063 6f6d     transform com
-00027040: 706f 6e65 6e74 2e0a 2020 2020 7472 616e  ponent..    tran
-00027050: 7366 6f72 6d5f 6461 7461 666c 6f77 5f64  sform_dataflow_d
-00027060: 6973 6b5f 7369 7a65 5f67 623a 2044 6174  isk_size_gb: Dat
-00027070: 6166 6c6f 7720 776f 726b 6572 2773 2064  aflow worker's d
-00027080: 6973 6b20 7369 7a65 2069 6e20 4742 2066  isk size in GB f
-00027090: 6f72 0a20 2020 2020 2074 7261 6e73 666f  or.      transfo
-000270a0: 726d 2063 6f6d 706f 6e65 6e74 2e0a 2020  rm component..  
-000270b0: 2020 7275 6e5f 6576 616c 7561 7469 6f6e    run_evaluation
-000270c0: 3a20 5768 6574 6865 7220 746f 2072 756e  : Whether to run
-000270d0: 2065 7661 6c75 6174 696f 6e20 7374 6570   evaluation step
-000270e0: 7320 6475 7269 6e67 2074 7261 696e 696e  s during trainin
-000270f0: 672e 0a20 2020 2065 7661 6c75 6174 696f  g..    evaluatio
-00027100: 6e5f 6261 7463 685f 7072 6564 6963 745f  n_batch_predict_
-00027110: 6d61 6368 696e 655f 7479 7065 3a20 5468  machine_type: Th
-00027120: 6520 7072 6564 6963 7469 6f6e 2073 6572  e prediction ser
-00027130: 7665 7220 6d61 6368 696e 6520 7479 7065  ver machine type
-00027140: 0a20 2020 2020 2066 6f72 2062 6174 6368  .      for batch
-00027150: 2070 7265 6469 6374 2063 6f6d 706f 6e65   predict compone
-00027160: 6e74 7320 6475 7269 6e67 2065 7661 6c75  nts during evalu
-00027170: 6174 696f 6e2e 0a20 2020 2065 7661 6c75  ation..    evalu
-00027180: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
-00027190: 6963 745f 7374 6172 7469 6e67 5f72 6570  ict_starting_rep
-000271a0: 6c69 6361 5f63 6f75 6e74 3a20 5468 6520  lica_count: The 
-000271b0: 696e 6974 6961 6c20 6e75 6d62 6572 206f  initial number o
-000271c0: 660a 2020 2020 2020 7072 6564 6963 7469  f.      predicti
-000271d0: 6f6e 2073 6572 7665 7220 666f 7220 6261  on server for ba
-000271e0: 7463 6820 7072 6564 6963 7420 636f 6d70  tch predict comp
-000271f0: 6f6e 656e 7473 2064 7572 696e 6720 6576  onents during ev
-00027200: 616c 7561 7469 6f6e 2e0a 2020 2020 6576  aluation..    ev
-00027210: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
-00027220: 7265 6469 6374 5f6d 6178 5f72 6570 6c69  redict_max_repli
-00027230: 6361 5f63 6f75 6e74 3a20 5468 6520 6d61  ca_count: The ma
-00027240: 7820 6e75 6d62 6572 206f 6620 7072 6564  x number of pred
-00027250: 6963 7469 6f6e 0a20 2020 2020 2073 6572  iction.      ser
-00027260: 7665 7220 666f 7220 6261 7463 6820 7072  ver for batch pr
-00027270: 6564 6963 7420 636f 6d70 6f6e 656e 7473  edict components
-00027280: 2064 7572 696e 6720 6576 616c 7561 7469   during evaluati
-00027290: 6f6e 2e0a 2020 2020 6576 616c 7561 7469  on..    evaluati
-000272a0: 6f6e 5f64 6174 6166 6c6f 775f 6d61 6368  on_dataflow_mach
-000272b0: 696e 655f 7479 7065 3a20 5468 6520 6461  ine_type: The da
-000272c0: 7461 666c 6f77 206d 6163 6869 6e65 2074  taflow machine t
-000272d0: 7970 6520 666f 7220 6576 616c 7561 7469  ype for evaluati
-000272e0: 6f6e 0a20 2020 2020 2063 6f6d 706f 6e65  on.      compone
-000272f0: 6e74 732e 0a20 2020 2065 7661 6c75 6174  nts..    evaluat
-00027300: 696f 6e5f 6461 7461 666c 6f77 5f73 7461  ion_dataflow_sta
-00027310: 7274 696e 675f 6e75 6d5f 776f 726b 6572  rting_num_worker
-00027320: 733a 2054 6865 2069 6e69 7469 616c 206e  s: The initial n
-00027330: 756d 6265 7220 6f66 2044 6174 6166 6c6f  umber of Dataflo
-00027340: 770a 2020 2020 2020 776f 726b 6572 7320  w.      workers 
-00027350: 666f 7220 6576 616c 7561 7469 6f6e 2063  for evaluation c
-00027360: 6f6d 706f 6e65 6e74 732e 0a20 2020 2065  omponents..    e
-00027370: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
-00027380: 6f77 5f6d 6178 5f6e 756d 5f77 6f72 6b65  ow_max_num_worke
-00027390: 7273 3a20 5468 6520 6d61 7820 6e75 6d62  rs: The max numb
-000273a0: 6572 206f 6620 4461 7461 666c 6f77 2077  er of Dataflow w
-000273b0: 6f72 6b65 7273 2066 6f72 0a20 2020 2020  orkers for.     
-000273c0: 2065 7661 6c75 6174 696f 6e20 636f 6d70   evaluation comp
-000273d0: 6f6e 656e 7473 2e0a 2020 2020 6576 616c  onents..    eval
-000273e0: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
-000273f0: 6469 736b 5f73 697a 655f 6762 3a20 4461  disk_size_gb: Da
-00027400: 7461 666c 6f77 2077 6f72 6b65 7227 7320  taflow worker's 
-00027410: 6469 736b 2073 697a 6520 696e 2047 4220  disk size in GB 
-00027420: 666f 720a 2020 2020 2020 6576 616c 7561  for.      evalua
-00027430: 7469 6f6e 2063 6f6d 706f 6e65 6e74 732e  tion components.
-00027440: 0a20 2020 2064 6174 6166 6c6f 775f 7365  .    dataflow_se
-00027450: 7276 6963 655f 6163 636f 756e 743a 2043  rvice_account: C
-00027460: 7573 746f 6d20 7365 7276 6963 6520 6163  ustom service ac
-00027470: 636f 756e 7420 746f 2072 756e 2064 6174  count to run dat
-00027480: 6166 6c6f 7720 6a6f 6273 2e0a 2020 2020  aflow jobs..    
-00027490: 6461 7461 666c 6f77 5f73 7562 6e65 7477  dataflow_subnetw
-000274a0: 6f72 6b3a 2044 6174 6166 6c6f 7727 7320  ork: Dataflow's 
-000274b0: 6675 6c6c 7920 7175 616c 6966 6965 6420  fully qualified 
-000274c0: 7375 626e 6574 776f 726b 206e 616d 652c  subnetwork name,
-000274d0: 2077 6865 6e20 656d 7074 790a 2020 2020   when empty.    
-000274e0: 2020 7468 6520 6465 6661 756c 7420 7375    the default su
-000274f0: 626e 6574 776f 726b 2077 696c 6c20 6265  bnetwork will be
-00027500: 2075 7365 642e 2045 7861 6d70 6c65 3a0a   used. Example:.
-00027510: 2020 2020 2020 2020 6874 7470 733a 2f2f          https://
-00027520: 636c 6f75 642e 676f 6f67 6c65 2e63 6f6d  cloud.google.com
-00027530: 2f64 6174 6166 6c6f 772f 646f 6373 2f67  /dataflow/docs/g
-00027540: 7569 6465 732f 7370 6563 6966 7969 6e67  uides/specifying
-00027550: 2d6e 6574 776f 726b 7323 6578 616d 706c  -networks#exampl
-00027560: 655f 6e65 7477 6f72 6b5f 616e 645f 7375  e_network_and_su
-00027570: 626e 6574 776f 726b 5f73 7065 6369 6669  bnetwork_specifi
-00027580: 6361 7469 6f6e 730a 2020 2020 6461 7461  cations.    data
-00027590: 666c 6f77 5f75 7365 5f70 7562 6c69 635f  flow_use_public_
-000275a0: 6970 733a 2053 7065 6369 6669 6573 2077  ips: Specifies w
-000275b0: 6865 7468 6572 2044 6174 6166 6c6f 7720  hether Dataflow 
-000275c0: 776f 726b 6572 7320 7573 6520 7075 626c  workers use publ
-000275d0: 6963 2049 500a 2020 2020 2020 6164 6472  ic IP.      addr
-000275e0: 6573 7365 732e 0a20 2020 2065 6e63 7279  esses..    encry
-000275f0: 7074 696f 6e5f 7370 6563 5f6b 6579 5f6e  ption_spec_key_n
-00027600: 616d 653a 2054 6865 204b 4d53 206b 6579  ame: The KMS key
-00027610: 206e 616d 652e 0a0a 2020 5265 7475 726e   name...  Return
-00027620: 733a 0a20 2020 2054 7570 6c65 206f 6620  s:.    Tuple of 
-00027630: 7069 7065 6c69 6e65 5f64 6566 696e 6974  pipeline_definit
-00027640: 696f 6e5f 7061 7468 2061 6e64 2070 6172  ion_path and par
-00027650: 616d 6574 6572 5f76 616c 7565 732e 0a20  ameter_values.. 
-00027660: 2022 2222 0a20 2070 6172 616d 6574 6572   """.  parameter
-00027670: 5f76 616c 7565 7320 3d20 7b7d 0a20 2069  _values = {}.  i
-00027680: 6620 6973 696e 7374 616e 6365 2874 665f  f isinstance(tf_
-00027690: 6175 746f 5f74 7261 6e73 666f 726d 5f66  auto_transform_f
-000276a0: 6561 7475 7265 732c 206c 6973 7429 3a0a  eatures, list):.
-000276b0: 2020 2020 7466 5f61 7574 6f5f 7472 616e      tf_auto_tran
-000276c0: 7366 6f72 6d5f 6665 6174 7572 6573 203d  sform_features =
-000276d0: 207b 2761 7574 6f27 3a20 7466 5f61 7574   {'auto': tf_aut
-000276e0: 6f5f 7472 616e 7366 6f72 6d5f 6665 6174  o_transform_feat
-000276f0: 7572 6573 7d0a 0a20 2074 7261 696e 696e  ures}..  trainin
-00027700: 675f 616e 645f 6576 616c 5f70 6172 616d  g_and_eval_param
-00027710: 6574 6572 7320 3d20 7b0a 2020 2020 2020  eters = {.      
-00027720: 2770 726f 6a65 6374 273a 2070 726f 6a65  'project': proje
-00027730: 6374 2c0a 2020 2020 2020 276c 6f63 6174  ct,.      'locat
-00027740: 696f 6e27 3a20 6c6f 6361 7469 6f6e 2c0a  ion': location,.
-00027750: 2020 2020 2020 2772 6f6f 745f 6469 7227        'root_dir'
-00027760: 3a20 726f 6f74 5f64 6972 2c0a 2020 2020  : root_dir,.    
-00027770: 2020 2774 6172 6765 745f 636f 6c75 6d6e    'target_column
-00027780: 273a 2074 6172 6765 745f 636f 6c75 6d6e  ': target_column
-00027790: 2c0a 2020 2020 2020 276f 626a 6563 7469  ,.      'objecti
-000277a0: 7665 273a 206f 626a 6563 7469 7665 2c0a  ve': objective,.
-000277b0: 2020 2020 2020 2765 7661 6c5f 6d65 7472        'eval_metr
-000277c0: 6963 273a 2065 7661 6c5f 6d65 7472 6963  ic': eval_metric
-000277d0: 2c0a 2020 2020 2020 2773 7475 6479 5f73  ,.      'study_s
-000277e0: 7065 635f 6d65 7472 6963 5f69 6427 3a20  pec_metric_id': 
-000277f0: 7374 7564 795f 7370 6563 5f6d 6574 7269  study_spec_metri
-00027800: 635f 6964 2c0a 2020 2020 2020 2773 7475  c_id,.      'stu
-00027810: 6479 5f73 7065 635f 6d65 7472 6963 5f67  dy_spec_metric_g
-00027820: 6f61 6c27 3a20 7374 7564 795f 7370 6563  oal': study_spec
-00027830: 5f6d 6574 7269 635f 676f 616c 2c0a 2020  _metric_goal,.  
-00027840: 2020 2020 276d 6178 5f74 7269 616c 5f63      'max_trial_c
-00027850: 6f75 6e74 273a 206d 6178 5f74 7269 616c  ount': max_trial
-00027860: 5f63 6f75 6e74 2c0a 2020 2020 2020 2770  _count,.      'p
-00027870: 6172 616c 6c65 6c5f 7472 6961 6c5f 636f  arallel_trial_co
-00027880: 756e 7427 3a20 7061 7261 6c6c 656c 5f74  unt': parallel_t
-00027890: 7269 616c 5f63 6f75 6e74 2c0a 2020 2020  rial_count,.    
-000278a0: 2020 2773 7475 6479 5f73 7065 635f 7061    'study_spec_pa
-000278b0: 7261 6d65 7465 7273 5f6f 7665 7272 6964  rameters_overrid
-000278c0: 6527 3a20 280a 2020 2020 2020 2020 2020  e': (.          
-000278d0: 7374 7564 795f 7370 6563 5f70 6172 616d  study_spec_param
-000278e0: 6574 6572 735f 6f76 6572 7269 6465 0a20  eters_override. 
-000278f0: 2020 2020 2020 2020 2069 6620 7374 7564           if stud
-00027900: 795f 7370 6563 5f70 6172 616d 6574 6572  y_spec_parameter
-00027910: 735f 6f76 6572 7269 6465 0a20 2020 2020  s_override.     
-00027920: 2020 2020 2065 6c73 6520 5b5d 0a20 2020       else [].   
-00027930: 2020 2029 2c0a 2020 2020 2020 2764 6973     ),.      'dis
-00027940: 6162 6c65 5f64 6566 6175 6c74 5f65 7661  able_default_eva
-00027950: 6c5f 6d65 7472 6963 273a 2064 6973 6162  l_metric': disab
-00027960: 6c65 5f64 6566 6175 6c74 5f65 7661 6c5f  le_default_eval_
-00027970: 6d65 7472 6963 2c0a 2020 2020 2020 2773  metric,.      's
-00027980: 6565 6427 3a20 7365 6564 2c0a 2020 2020  eed': seed,.    
-00027990: 2020 2773 6565 645f 7065 725f 6974 6572    'seed_per_iter
-000279a0: 6174 696f 6e27 3a20 7365 6564 5f70 6572  ation': seed_per
-000279b0: 5f69 7465 7261 7469 6f6e 2c0a 2020 2020  _iteration,.    
-000279c0: 2020 2777 6569 6768 745f 636f 6c75 6d6e    'weight_column
-000279d0: 273a 2077 6569 6768 745f 636f 6c75 6d6e  ': weight_column
-000279e0: 2c0a 2020 2020 2020 276d 6178 5f66 6169  ,.      'max_fai
-000279f0: 6c65 645f 7472 6961 6c5f 636f 756e 7427  led_trial_count'
-00027a00: 3a20 6d61 785f 6661 696c 6564 5f74 7269  : max_failed_tri
-00027a10: 616c 5f63 6f75 6e74 2c0a 2020 2020 2020  al_count,.      
-00027a20: 2774 7261 696e 696e 675f 6d61 6368 696e  'training_machin
-00027a30: 655f 7479 7065 273a 2074 7261 696e 696e  e_type': trainin
-00027a40: 675f 6d61 6368 696e 655f 7479 7065 2c0a  g_machine_type,.
-00027a50: 2020 2020 2020 2774 7261 696e 696e 675f        'training_
-00027a60: 746f 7461 6c5f 7265 706c 6963 615f 636f  total_replica_co
-00027a70: 756e 7427 3a20 7472 6169 6e69 6e67 5f74  unt': training_t
-00027a80: 6f74 616c 5f72 6570 6c69 6361 5f63 6f75  otal_replica_cou
-00027a90: 6e74 2c0a 2020 2020 2020 2774 7261 696e  nt,.      'train
-00027aa0: 696e 675f 6163 6365 6c65 7261 746f 725f  ing_accelerator_
-00027ab0: 7479 7065 273a 2074 7261 696e 696e 675f  type': training_
-00027ac0: 6163 6365 6c65 7261 746f 725f 7479 7065  accelerator_type
-00027ad0: 2c0a 2020 2020 2020 2774 7261 696e 696e  ,.      'trainin
-00027ae0: 675f 6163 6365 6c65 7261 746f 725f 636f  g_accelerator_co
-00027af0: 756e 7427 3a20 7472 6169 6e69 6e67 5f61  unt': training_a
-00027b00: 6363 656c 6572 6174 6f72 5f63 6f75 6e74  ccelerator_count
-00027b10: 2c0a 2020 2020 2020 2773 7475 6479 5f73  ,.      'study_s
-00027b20: 7065 635f 616c 676f 7269 7468 6d27 3a20  pec_algorithm': 
-00027b30: 7374 7564 795f 7370 6563 5f61 6c67 6f72  study_spec_algor
-00027b40: 6974 686d 2c0a 2020 2020 2020 2773 7475  ithm,.      'stu
-00027b50: 6479 5f73 7065 635f 6d65 6173 7572 656d  dy_spec_measurem
-00027b60: 656e 745f 7365 6c65 6374 696f 6e5f 7479  ent_selection_ty
-00027b70: 7065 273a 2028 0a20 2020 2020 2020 2020  pe': (.         
-00027b80: 2073 7475 6479 5f73 7065 635f 6d65 6173   study_spec_meas
-00027b90: 7572 656d 656e 745f 7365 6c65 6374 696f  urement_selectio
-00027ba0: 6e5f 7479 7065 0a20 2020 2020 2029 2c0a  n_type.      ),.
-00027bb0: 2020 2020 2020 2774 7261 6e73 666f 726d        'transform
-00027bc0: 5f64 6174 6166 6c6f 775f 6d61 6368 696e  _dataflow_machin
-00027bd0: 655f 7479 7065 273a 2074 7261 6e73 666f  e_type': transfo
-00027be0: 726d 5f64 6174 6166 6c6f 775f 6d61 6368  rm_dataflow_mach
-00027bf0: 696e 655f 7479 7065 2c0a 2020 2020 2020  ine_type,.      
-00027c00: 2774 7261 6e73 666f 726d 5f64 6174 6166  'transform_dataf
-00027c10: 6c6f 775f 6d61 785f 6e75 6d5f 776f 726b  low_max_num_work
-00027c20: 6572 7327 3a20 7472 616e 7366 6f72 6d5f  ers': transform_
-00027c30: 6461 7461 666c 6f77 5f6d 6178 5f6e 756d  dataflow_max_num
-00027c40: 5f77 6f72 6b65 7273 2c0a 2020 2020 2020  _workers,.      
-00027c50: 2774 7261 6e73 666f 726d 5f64 6174 6166  'transform_dataf
-00027c60: 6c6f 775f 6469 736b 5f73 697a 655f 6762  low_disk_size_gb
-00027c70: 273a 2074 7261 6e73 666f 726d 5f64 6174  ': transform_dat
-00027c80: 6166 6c6f 775f 6469 736b 5f73 697a 655f  aflow_disk_size_
-00027c90: 6762 2c0a 2020 2020 2020 2772 756e 5f65  gb,.      'run_e
-00027ca0: 7661 6c75 6174 696f 6e27 3a20 7275 6e5f  valuation': run_
-00027cb0: 6576 616c 7561 7469 6f6e 2c0a 2020 2020  evaluation,.    
-00027cc0: 2020 2765 7661 6c75 6174 696f 6e5f 6261    'evaluation_ba
-00027cd0: 7463 685f 7072 6564 6963 745f 6d61 6368  tch_predict_mach
-00027ce0: 696e 655f 7479 7065 273a 2028 0a20 2020  ine_type': (.   
-00027cf0: 2020 2020 2020 2065 7661 6c75 6174 696f         evaluatio
-00027d00: 6e5f 6261 7463 685f 7072 6564 6963 745f  n_batch_predict_
-00027d10: 6d61 6368 696e 655f 7479 7065 0a20 2020  machine_type.   
-00027d20: 2020 2029 2c0a 2020 2020 2020 2765 7661     ),.      'eva
-00027d30: 6c75 6174 696f 6e5f 6261 7463 685f 7072  luation_batch_pr
-00027d40: 6564 6963 745f 7374 6172 7469 6e67 5f72  edict_starting_r
-00027d50: 6570 6c69 6361 5f63 6f75 6e74 273a 2028  eplica_count': (
-00027d60: 0a20 2020 2020 2020 2020 2065 7661 6c75  .          evalu
-00027d70: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
-00027d80: 6963 745f 7374 6172 7469 6e67 5f72 6570  ict_starting_rep
-00027d90: 6c69 6361 5f63 6f75 6e74 0a20 2020 2020  lica_count.     
-00027da0: 2029 2c0a 2020 2020 2020 2765 7661 6c75   ),.      'evalu
-00027db0: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
-00027dc0: 6963 745f 6d61 785f 7265 706c 6963 615f  ict_max_replica_
-00027dd0: 636f 756e 7427 3a20 280a 2020 2020 2020  count': (.      
-00027de0: 2020 2020 6576 616c 7561 7469 6f6e 5f62      evaluation_b
-00027df0: 6174 6368 5f70 7265 6469 6374 5f6d 6178  atch_predict_max
-00027e00: 5f72 6570 6c69 6361 5f63 6f75 6e74 0a20  _replica_count. 
-00027e10: 2020 2020 2029 2c0a 2020 2020 2020 2765       ),.      'e
-00027e20: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
-00027e30: 6f77 5f6d 6163 6869 6e65 5f74 7970 6527  ow_machine_type'
-00027e40: 3a20 6576 616c 7561 7469 6f6e 5f64 6174  : evaluation_dat
-00027e50: 6166 6c6f 775f 6d61 6368 696e 655f 7479  aflow_machine_ty
-00027e60: 7065 2c0a 2020 2020 2020 2765 7661 6c75  pe,.      'evalu
-00027e70: 6174 696f 6e5f 6461 7461 666c 6f77 5f73  ation_dataflow_s
-00027e80: 7461 7274 696e 675f 6e75 6d5f 776f 726b  tarting_num_work
-00027e90: 6572 7327 3a20 280a 2020 2020 2020 2020  ers': (.        
-00027ea0: 2020 6576 616c 7561 7469 6f6e 5f64 6174    evaluation_dat
-00027eb0: 6166 6c6f 775f 7374 6172 7469 6e67 5f6e  aflow_starting_n
-00027ec0: 756d 5f77 6f72 6b65 7273 0a20 2020 2020  um_workers.     
-00027ed0: 2029 2c0a 2020 2020 2020 2765 7661 6c75   ),.      'evalu
-00027ee0: 6174 696f 6e5f 6461 7461 666c 6f77 5f6d  ation_dataflow_m
-00027ef0: 6178 5f6e 756d 5f77 6f72 6b65 7273 273a  ax_num_workers':
-00027f00: 2028 0a20 2020 2020 2020 2020 2065 7661   (.          eva
-00027f10: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
-00027f20: 5f6d 6178 5f6e 756d 5f77 6f72 6b65 7273  _max_num_workers
-00027f30: 0a20 2020 2020 2029 2c0a 2020 2020 2020  .      ),.      
-00027f40: 2765 7661 6c75 6174 696f 6e5f 6461 7461  'evaluation_data
-00027f50: 666c 6f77 5f64 6973 6b5f 7369 7a65 5f67  flow_disk_size_g
-00027f60: 6227 3a20 6576 616c 7561 7469 6f6e 5f64  b': evaluation_d
-00027f70: 6174 6166 6c6f 775f 6469 736b 5f73 697a  ataflow_disk_siz
-00027f80: 655f 6762 2c0a 2020 2020 2020 2764 6174  e_gb,.      'dat
-00027f90: 6166 6c6f 775f 7365 7276 6963 655f 6163  aflow_service_ac
-00027fa0: 636f 756e 7427 3a20 6461 7461 666c 6f77  count': dataflow
-00027fb0: 5f73 6572 7669 6365 5f61 6363 6f75 6e74  _service_account
-00027fc0: 2c0a 2020 2020 2020 2764 6174 6166 6c6f  ,.      'dataflo
-00027fd0: 775f 7375 626e 6574 776f 726b 273a 2064  w_subnetwork': d
-00027fe0: 6174 6166 6c6f 775f 7375 626e 6574 776f  ataflow_subnetwo
-00027ff0: 726b 2c0a 2020 2020 2020 2764 6174 6166  rk,.      'dataf
-00028000: 6c6f 775f 7573 655f 7075 626c 6963 5f69  low_use_public_i
-00028010: 7073 273a 2064 6174 6166 6c6f 775f 7573  ps': dataflow_us
-00028020: 655f 7075 626c 6963 5f69 7073 2c0a 2020  e_public_ips,.  
-00028030: 2020 2020 2765 6e63 7279 7074 696f 6e5f      'encryption_
-00028040: 7370 6563 5f6b 6579 5f6e 616d 6527 3a20  spec_key_name': 
-00028050: 656e 6372 7970 7469 6f6e 5f73 7065 635f  encryption_spec_
-00028060: 6b65 795f 6e61 6d65 2c0a 2020 7d0a 2020  key_name,.  }.  
-00028070: 5f75 7064 6174 655f 7061 7261 6d65 7465  _update_paramete
-00028080: 7273 2870 6172 616d 6574 6572 5f76 616c  rs(parameter_val
-00028090: 7565 732c 2074 7261 696e 696e 675f 616e  ues, training_an
-000280a0: 645f 6576 616c 5f70 6172 616d 6574 6572  d_eval_parameter
-000280b0: 7329 0a0a 2020 6674 655f 7061 7261 6d73  s)..  fte_params
-000280c0: 203d 207b 0a20 2020 2020 2027 6461 7461   = {.      'data
-000280d0: 7365 745f 6c65 7665 6c5f 6375 7374 6f6d  set_level_custom
-000280e0: 5f74 7261 6e73 666f 726d 6174 696f 6e5f  _transformation_
-000280f0: 6465 6669 6e69 7469 6f6e 7327 3a20 280a  definitions': (.
-00028100: 2020 2020 2020 2020 2020 6461 7461 7365            datase
-00028110: 745f 6c65 7665 6c5f 6375 7374 6f6d 5f74  t_level_custom_t
-00028120: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
-00028130: 6669 6e69 7469 6f6e 730a 2020 2020 2020  finitions.      
-00028140: 2020 2020 6966 2064 6174 6173 6574 5f6c      if dataset_l
-00028150: 6576 656c 5f63 7573 746f 6d5f 7472 616e  evel_custom_tran
-00028160: 7366 6f72 6d61 7469 6f6e 5f64 6566 696e  sformation_defin
-00028170: 6974 696f 6e73 0a20 2020 2020 2020 2020  itions.         
-00028180: 2065 6c73 6520 5b5d 0a20 2020 2020 2029   else [].      )
-00028190: 2c0a 2020 2020 2020 2764 6174 6173 6574  ,.      'dataset
-000281a0: 5f6c 6576 656c 5f74 7261 6e73 666f 726d  _level_transform
-000281b0: 6174 696f 6e73 273a 2028 0a20 2020 2020  ations': (.     
-000281c0: 2020 2020 2064 6174 6173 6574 5f6c 6576       dataset_lev
-000281d0: 656c 5f74 7261 6e73 666f 726d 6174 696f  el_transformatio
-000281e0: 6e73 2069 6620 6461 7461 7365 745f 6c65  ns if dataset_le
-000281f0: 7665 6c5f 7472 616e 7366 6f72 6d61 7469  vel_transformati
-00028200: 6f6e 7320 656c 7365 205b 5d0a 2020 2020  ons else [].    
-00028210: 2020 292c 0a20 2020 2020 2027 7275 6e5f    ),.      'run_
-00028220: 6665 6174 7572 655f 7365 6c65 6374 696f  feature_selectio
-00028230: 6e27 3a20 7275 6e5f 6665 6174 7572 655f  n': run_feature_
-00028240: 7365 6c65 6374 696f 6e2c 0a20 2020 2020  selection,.     
-00028250: 2027 6665 6174 7572 655f 7365 6c65 6374   'feature_select
-00028260: 696f 6e5f 616c 676f 7269 7468 6d27 3a20  ion_algorithm': 
-00028270: 6665 6174 7572 655f 7365 6c65 6374 696f  feature_selectio
-00028280: 6e5f 616c 676f 7269 7468 6d2c 0a20 2020  n_algorithm,.   
-00028290: 2020 2027 6d61 785f 7365 6c65 6374 6564     'max_selected
-000282a0: 5f66 6561 7475 7265 7327 3a20 6d61 785f  _features': max_
-000282b0: 7365 6c65 6374 6564 5f66 6561 7475 7265  selected_feature
-000282c0: 732c 0a20 2020 2020 2027 7072 6564 6566  s,.      'predef
-000282d0: 696e 6564 5f73 706c 6974 5f6b 6579 273a  ined_split_key':
-000282e0: 2070 7265 6465 6669 6e65 645f 7370 6c69   predefined_spli
-000282f0: 745f 6b65 792c 0a20 2020 2020 2027 7374  t_key,.      'st
-00028300: 7261 7469 6669 6564 5f73 706c 6974 5f6b  ratified_split_k
-00028310: 6579 273a 2073 7472 6174 6966 6965 645f  ey': stratified_
-00028320: 7370 6c69 745f 6b65 792c 0a20 2020 2020  split_key,.     
-00028330: 2027 7472 6169 6e69 6e67 5f66 7261 6374   'training_fract
-00028340: 696f 6e27 3a20 7472 6169 6e69 6e67 5f66  ion': training_f
-00028350: 7261 6374 696f 6e2c 0a20 2020 2020 2027  raction,.      '
-00028360: 7661 6c69 6461 7469 6f6e 5f66 7261 6374  validation_fract
-00028370: 696f 6e27 3a20 7661 6c69 6461 7469 6f6e  ion': validation
-00028380: 5f66 7261 6374 696f 6e2c 0a20 2020 2020  _fraction,.     
-00028390: 2027 7465 7374 5f66 7261 6374 696f 6e27   'test_fraction'
-000283a0: 3a20 7465 7374 5f66 7261 6374 696f 6e2c  : test_fraction,
-000283b0: 0a20 2020 2020 2027 7466 5f61 7574 6f5f  .      'tf_auto_
-000283c0: 7472 616e 7366 6f72 6d5f 6665 6174 7572  transform_featur
-000283d0: 6573 273a 2028 0a20 2020 2020 2020 2020  es': (.         
-000283e0: 2074 665f 6175 746f 5f74 7261 6e73 666f   tf_auto_transfo
-000283f0: 726d 5f66 6561 7475 7265 7320 6966 2074  rm_features if t
-00028400: 665f 6175 746f 5f74 7261 6e73 666f 726d  f_auto_transform
-00028410: 5f66 6561 7475 7265 7320 656c 7365 207b  _features else {
-00028420: 7d0a 2020 2020 2020 292c 0a20 2020 2020  }.      ),.     
-00028430: 2027 7466 5f63 7573 746f 6d5f 7472 616e   'tf_custom_tran
-00028440: 7366 6f72 6d61 7469 6f6e 5f64 6566 696e  sformation_defin
-00028450: 6974 696f 6e73 273a 2028 0a20 2020 2020  itions': (.     
-00028460: 2020 2020 2074 665f 6375 7374 6f6d 5f74       tf_custom_t
-00028470: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
-00028480: 6669 6e69 7469 6f6e 730a 2020 2020 2020  finitions.      
-00028490: 2020 2020 6966 2074 665f 6375 7374 6f6d      if tf_custom
-000284a0: 5f74 7261 6e73 666f 726d 6174 696f 6e5f  _transformation_
-000284b0: 6465 6669 6e69 7469 6f6e 730a 2020 2020  definitions.    
-000284c0: 2020 2020 2020 656c 7365 205b 5d0a 2020        else [].  
-000284d0: 2020 2020 292c 0a20 2020 2020 2027 7466      ),.      'tf
-000284e0: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
-000284f0: 5f70 6174 6827 3a20 7466 5f74 7261 6e73  _path': tf_trans
-00028500: 666f 726d 6174 696f 6e73 5f70 6174 682c  formations_path,
-00028510: 0a20 207d 0a20 205f 7570 6461 7465 5f70  .  }.  _update_p
-00028520: 6172 616d 6574 6572 7328 7061 7261 6d65  arameters(parame
-00028530: 7465 725f 7661 6c75 6573 2c20 6674 655f  ter_values, fte_
-00028540: 7061 7261 6d73 290a 0a20 2064 6174 615f  params)..  data_
-00028550: 736f 7572 6365 5f61 6e64 5f73 706c 6974  source_and_split
-00028560: 5f70 6172 616d 6574 6572 7320 3d20 7b0a  _parameters = {.
-00028570: 2020 2020 2020 2764 6174 615f 736f 7572        'data_sour
-00028580: 6365 5f63 7376 5f66 696c 656e 616d 6573  ce_csv_filenames
-00028590: 273a 2064 6174 615f 736f 7572 6365 5f63  ': data_source_c
-000285a0: 7376 5f66 696c 656e 616d 6573 2c0a 2020  sv_filenames,.  
-000285b0: 2020 2020 2764 6174 615f 736f 7572 6365      'data_source
-000285c0: 5f62 6967 7175 6572 795f 7461 626c 655f  _bigquery_table_
-000285d0: 7061 7468 273a 2064 6174 615f 736f 7572  path': data_sour
-000285e0: 6365 5f62 6967 7175 6572 795f 7461 626c  ce_bigquery_tabl
-000285f0: 655f 7061 7468 2c0a 2020 2020 2020 2762  e_path,.      'b
-00028600: 6967 7175 6572 795f 7374 6167 696e 675f  igquery_staging_
-00028610: 6675 6c6c 5f64 6174 6173 6574 5f69 6427  full_dataset_id'
-00028620: 3a20 6269 6771 7565 7279 5f73 7461 6769  : bigquery_stagi
-00028630: 6e67 5f66 756c 6c5f 6461 7461 7365 745f  ng_full_dataset_
-00028640: 6964 2c0a 2020 7d0a 2020 5f75 7064 6174  id,.  }.  _updat
-00028650: 655f 7061 7261 6d65 7465 7273 2870 6172  e_parameters(par
-00028660: 616d 6574 6572 5f76 616c 7565 732c 2064  ameter_values, d
-00028670: 6174 615f 736f 7572 6365 5f61 6e64 5f73  ata_source_and_s
-00028680: 706c 6974 5f70 6172 616d 6574 6572 7329  plit_parameters)
-00028690: 0a0a 2020 7069 7065 6c69 6e65 5f64 6566  ..  pipeline_def
-000286a0: 696e 6974 696f 6e5f 7061 7468 203d 206f  inition_path = o
-000286b0: 732e 7061 7468 2e6a 6f69 6e28 0a20 2020  s.path.join(.   
-000286c0: 2020 2070 6174 686c 6962 2e50 6174 6828     pathlib.Path(
-000286d0: 5f5f 6669 6c65 5f5f 292e 7061 7265 6e74  __file__).parent
-000286e0: 2e72 6573 6f6c 7665 2829 2c0a 2020 2020  .resolve(),.    
-000286f0: 2020 2778 6762 6f6f 7374 5f68 7970 6572    'xgboost_hyper
-00028700: 7061 7261 6d65 7465 725f 7475 6e69 6e67  parameter_tuning
-00028710: 5f6a 6f62 5f70 6970 656c 696e 652e 7961  _job_pipeline.ya
-00028720: 6d6c 272c 0a20 2029 0a0a 2020 7265 7475  ml',.  )..  retu
-00028730: 726e 2070 6970 656c 696e 655f 6465 6669  rn pipeline_defi
-00028740: 6e69 7469 6f6e 5f70 6174 682c 2070 6172  nition_path, par
-00028750: 616d 6574 6572 5f76 616c 7565 730a 0a0a  ameter_values...
-00028760: 6465 6620 6765 745f 6665 6174 7572 655f  def get_feature_
-00028770: 7365 6c65 6374 696f 6e5f 7069 7065 6c69  selection_pipeli
-00028780: 6e65 5f61 6e64 5f70 6172 616d 6574 6572  ne_and_parameter
-00028790: 7328 0a20 2020 2072 6f6f 745f 6469 723a  s(.    root_dir:
-000287a0: 2073 7472 2c0a 2020 2020 7072 6f6a 6563   str,.    projec
-000287b0: 743a 2073 7472 2c0a 2020 2020 6c6f 6361  t: str,.    loca
-000287c0: 7469 6f6e 3a20 7374 722c 0a20 2020 2074  tion: str,.    t
-000287d0: 6172 6765 745f 636f 6c75 6d6e 3a20 7374  arget_column: st
-000287e0: 722c 0a20 2020 2070 7265 6469 6374 696f  r,.    predictio
-000287f0: 6e5f 7479 7065 3a20 7374 722c 0a20 2020  n_type: str,.   
-00028800: 206f 7074 696d 697a 6174 696f 6e5f 6f62   optimization_ob
-00028810: 6a65 6374 6976 653a 2073 7472 2c0a 2020  jective: str,.  
-00028820: 2020 6461 7461 7365 745f 6c65 7665 6c5f    dataset_level_
-00028830: 6375 7374 6f6d 5f74 7261 6e73 666f 726d  custom_transform
-00028840: 6174 696f 6e5f 6465 6669 6e69 7469 6f6e  ation_definition
-00028850: 733a 204f 7074 696f 6e61 6c5b 0a20 2020  s: Optional[.   
-00028860: 2020 2020 204c 6973 745b 4469 6374 5b73       List[Dict[s
-00028870: 7472 2c20 416e 795d 5d0a 2020 2020 5d20  tr, Any]].    ] 
-00028880: 3d20 4e6f 6e65 2c0a 2020 2020 6461 7461  = None,.    data
-00028890: 7365 745f 6c65 7665 6c5f 7472 616e 7366  set_level_transf
-000288a0: 6f72 6d61 7469 6f6e 733a 204f 7074 696f  ormations: Optio
-000288b0: 6e61 6c5b 4c69 7374 5b44 6963 745b 7374  nal[List[Dict[st
-000288c0: 722c 2041 6e79 5d5d 5d20 3d20 4e6f 6e65  r, Any]]] = None
-000288d0: 2c0a 2020 2020 7275 6e5f 6665 6174 7572  ,.    run_featur
-000288e0: 655f 7365 6c65 6374 696f 6e3a 204f 7074  e_selection: Opt
-000288f0: 696f 6e61 6c5b 626f 6f6c 5d20 3d20 4e6f  ional[bool] = No
-00028900: 6e65 2c0a 2020 2020 6665 6174 7572 655f  ne,.    feature_
-00028910: 7365 6c65 6374 696f 6e5f 616c 676f 7269  selection_algori
-00028920: 7468 6d3a 204f 7074 696f 6e61 6c5b 7374  thm: Optional[st
-00028930: 725d 203d 204e 6f6e 652c 0a20 2020 2066  r] = None,.    f
-00028940: 6561 7475 7265 5f73 656c 6563 7469 6f6e  eature_selection
-00028950: 5f65 7865 6375 7469 6f6e 5f65 6e67 696e  _execution_engin
-00028960: 653a 204f 7074 696f 6e61 6c5b 0a20 2020  e: Optional[.   
-00028970: 2020 2020 2073 7472 0a20 2020 205d 203d       str.    ] =
-00028980: 205f 4645 4154 5552 455f 5345 4c45 4354   _FEATURE_SELECT
-00028990: 494f 4e5f 4558 4543 5554 494f 4e5f 454e  ION_EXECUTION_EN
-000289a0: 4749 4e45 5f42 4947 5155 4552 592c 0a20  GINE_BIGQUERY,. 
-000289b0: 2020 206d 6178 5f73 656c 6563 7465 645f     max_selected_
-000289c0: 6665 6174 7572 6573 3a20 4f70 7469 6f6e  features: Option
-000289d0: 616c 5b69 6e74 5d20 3d20 4e6f 6e65 2c0a  al[int] = None,.
-000289e0: 2020 2020 7072 6564 6566 696e 6564 5f73      predefined_s
-000289f0: 706c 6974 5f6b 6579 3a20 4f70 7469 6f6e  plit_key: Option
-00028a00: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
-00028a10: 2020 2020 7374 7261 7469 6669 6564 5f73      stratified_s
-00028a20: 706c 6974 5f6b 6579 3a20 4f70 7469 6f6e  plit_key: Option
-00028a30: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
-00028a40: 2020 2020 7472 6169 6e69 6e67 5f66 7261      training_fra
-00028a50: 6374 696f 6e3a 204f 7074 696f 6e61 6c5b  ction: Optional[
-00028a60: 666c 6f61 745d 203d 204e 6f6e 652c 0a20  float] = None,. 
-00028a70: 2020 2076 616c 6964 6174 696f 6e5f 6672     validation_fr
-00028a80: 6163 7469 6f6e 3a20 4f70 7469 6f6e 616c  action: Optional
-00028a90: 5b66 6c6f 6174 5d20 3d20 4e6f 6e65 2c0a  [float] = None,.
-00028aa0: 2020 2020 7465 7374 5f66 7261 6374 696f      test_fractio
-00028ab0: 6e3a 204f 7074 696f 6e61 6c5b 666c 6f61  n: Optional[floa
-00028ac0: 745d 203d 204e 6f6e 652c 0a20 2020 2074  t] = None,.    t
-00028ad0: 665f 6175 746f 5f74 7261 6e73 666f 726d  f_auto_transform
-00028ae0: 5f66 6561 7475 7265 733a 204f 7074 696f  _features: Optio
-00028af0: 6e61 6c5b 0a20 2020 2020 2020 2055 6e69  nal[.        Uni
-00028b00: 6f6e 5b4c 6973 745b 7374 725d 2c20 4469  on[List[str], Di
-00028b10: 6374 5b73 7472 2c20 4c69 7374 5b73 7472  ct[str, List[str
-00028b20: 5d5d 5d0a 2020 2020 5d20 3d20 4e6f 6e65  ]]].    ] = None
-00028b30: 2c0a 2020 2020 7765 6967 6874 5f63 6f6c  ,.    weight_col
-00028b40: 756d 6e3a 204f 7074 696f 6e61 6c5b 7374  umn: Optional[st
-00028b50: 725d 203d 204e 6f6e 652c 0a20 2020 2064  r] = None,.    d
-00028b60: 6174 615f 736f 7572 6365 5f63 7376 5f66  ata_source_csv_f
-00028b70: 696c 656e 616d 6573 3a20 4f70 7469 6f6e  ilenames: Option
-00028b80: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
-00028b90: 2020 2020 6461 7461 5f73 6f75 7263 655f      data_source_
-00028ba0: 6269 6771 7565 7279 5f74 6162 6c65 5f70  bigquery_table_p
-00028bb0: 6174 683a 204f 7074 696f 6e61 6c5b 7374  ath: Optional[st
-00028bc0: 725d 203d 204e 6f6e 652c 0a20 2020 2062  r] = None,.    b
-00028bd0: 6967 7175 6572 795f 7374 6167 696e 675f  igquery_staging_
-00028be0: 6675 6c6c 5f64 6174 6173 6574 5f69 643a  full_dataset_id:
-00028bf0: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
-00028c00: 204e 6f6e 652c 0a20 2020 2064 6174 6166   None,.    dataf
-00028c10: 6c6f 775f 6d61 6368 696e 655f 7479 7065  low_machine_type
-00028c20: 3a20 4f70 7469 6f6e 616c 5b73 7472 5d20  : Optional[str] 
-00028c30: 3d20 4e6f 6e65 2c0a 2020 2020 6461 7461  = None,.    data
-00028c40: 666c 6f77 5f6d 6178 5f6e 756d 5f77 6f72  flow_max_num_wor
-00028c50: 6b65 7273 3a20 4f70 7469 6f6e 616c 5b69  kers: Optional[i
-00028c60: 6e74 5d20 3d20 4e6f 6e65 2c0a 2020 2020  nt] = None,.    
-00028c70: 6461 7461 666c 6f77 5f64 6973 6b5f 7369  dataflow_disk_si
-00028c80: 7a65 5f67 623a 204f 7074 696f 6e61 6c5b  ze_gb: Optional[
-00028c90: 696e 745d 203d 204e 6f6e 652c 0a20 2020  int] = None,.   
-00028ca0: 2064 6174 6166 6c6f 775f 7375 626e 6574   dataflow_subnet
-00028cb0: 776f 726b 3a20 4f70 7469 6f6e 616c 5b73  work: Optional[s
-00028cc0: 7472 5d20 3d20 4e6f 6e65 2c0a 2020 2020  tr] = None,.    
-00028cd0: 6461 7461 666c 6f77 5f75 7365 5f70 7562  dataflow_use_pub
-00028ce0: 6c69 635f 6970 733a 204f 7074 696f 6e61  lic_ips: Optiona
-00028cf0: 6c5b 626f 6f6c 5d20 3d20 4e6f 6e65 2c0a  l[bool] = None,.
-00028d00: 2020 2020 656e 6372 7970 7469 6f6e 5f73      encryption_s
-00028d10: 7065 635f 6b65 795f 6e61 6d65 3a20 4f70  pec_key_name: Op
-00028d20: 7469 6f6e 616c 5b73 7472 5d20 3d20 4e6f  tional[str] = No
-00028d30: 6e65 2c0a 2020 2020 7374 6167 655f 315f  ne,.    stage_1_
-00028d40: 6465 6164 6c69 6e65 5f68 6f75 7273 3a20  deadline_hours: 
-00028d50: 4f70 7469 6f6e 616c 5b66 6c6f 6174 5d20  Optional[float] 
-00028d60: 3d20 4e6f 6e65 2c0a 2020 2020 7374 6167  = None,.    stag
-00028d70: 655f 325f 6465 6164 6c69 6e65 5f68 6f75  e_2_deadline_hou
-00028d80: 7273 3a20 4f70 7469 6f6e 616c 5b66 6c6f  rs: Optional[flo
-00028d90: 6174 5d20 3d20 4e6f 6e65 2c0a 293a 0a20  at] = None,.):. 
-00028da0: 2022 2222 5265 7475 726e 7320 6665 6174   """Returns feat
-00028db0: 7572 6520 7472 616e 7366 6f72 6d20 656e  ure transform en
-00028dc0: 6769 6e65 2070 6970 656c 696e 6520 616e  gine pipeline an
-00028dd0: 6420 666f 726d 6174 7465 6420 7061 7261  d formatted para
-00028de0: 6d65 7465 7273 2e22 2222 0a0a 2020 6966  meters."""..  if
-00028df0: 2069 7369 6e73 7461 6e63 6528 7466 5f61   isinstance(tf_a
-00028e00: 7574 6f5f 7472 616e 7366 6f72 6d5f 6665  uto_transform_fe
-00028e10: 6174 7572 6573 2c20 6c69 7374 293a 0a20  atures, list):. 
-00028e20: 2020 2074 665f 6175 746f 5f74 7261 6e73     tf_auto_trans
-00028e30: 666f 726d 5f66 6561 7475 7265 7320 3d20  form_features = 
-00028e40: 7b27 6175 746f 273a 2074 665f 6175 746f  {'auto': tf_auto
-00028e50: 5f74 7261 6e73 666f 726d 5f66 6561 7475  _transform_featu
-00028e60: 7265 737d 0a0a 2020 7069 7065 6c69 6e65  res}..  pipeline
-00028e70: 5f64 6566 696e 6974 696f 6e5f 7061 7468  _definition_path
-00028e80: 203d 206f 732e 7061 7468 2e6a 6f69 6e28   = os.path.join(
-00028e90: 0a20 2020 2020 2070 6174 686c 6962 2e50  .      pathlib.P
-00028ea0: 6174 6828 5f5f 6669 6c65 5f5f 292e 7061  ath(__file__).pa
-00028eb0: 7265 6e74 2e72 6573 6f6c 7665 2829 2c20  rent.resolve(), 
-00028ec0: 2766 6561 7475 7265 5f73 656c 6563 7469  'feature_selecti
-00028ed0: 6f6e 5f70 6970 656c 696e 652e 7961 6d6c  on_pipeline.yaml
-00028ee0: 270a 2020 290a 0a20 2070 6172 616d 6574  '.  )..  paramet
-00028ef0: 6572 5f76 616c 7565 7320 3d20 7b0a 2020  er_values = {.  
-00028f00: 2020 2020 2772 6f6f 745f 6469 7227 3a20      'root_dir': 
-00028f10: 726f 6f74 5f64 6972 2c0a 2020 2020 2020  root_dir,.      
-00028f20: 2770 726f 6a65 6374 273a 2070 726f 6a65  'project': proje
-00028f30: 6374 2c0a 2020 2020 2020 276c 6f63 6174  ct,.      'locat
-00028f40: 696f 6e27 3a20 6c6f 6361 7469 6f6e 2c0a  ion': location,.
-00028f50: 2020 2020 2020 2774 6172 6765 745f 636f        'target_co
-00028f60: 6c75 6d6e 273a 2074 6172 6765 745f 636f  lumn': target_co
-00028f70: 6c75 6d6e 2c0a 2020 2020 2020 2777 6569  lumn,.      'wei
-00028f80: 6768 745f 636f 6c75 6d6e 273a 2077 6569  ght_column': wei
-00028f90: 6768 745f 636f 6c75 6d6e 2c0a 2020 2020  ght_column,.    
-00028fa0: 2020 2770 7265 6469 6374 696f 6e5f 7479    'prediction_ty
-00028fb0: 7065 273a 2070 7265 6469 6374 696f 6e5f  pe': prediction_
-00028fc0: 7479 7065 2c0a 2020 2020 2020 2764 6174  type,.      'dat
-00028fd0: 6173 6574 5f6c 6576 656c 5f63 7573 746f  aset_level_custo
-00028fe0: 6d5f 7472 616e 7366 6f72 6d61 7469 6f6e  m_transformation
-00028ff0: 5f64 6566 696e 6974 696f 6e73 273a 2028  _definitions': (
-00029000: 0a20 2020 2020 2020 2020 2064 6174 6173  .          datas
-00029010: 6574 5f6c 6576 656c 5f63 7573 746f 6d5f  et_level_custom_
-00029020: 7472 616e 7366 6f72 6d61 7469 6f6e 5f64  transformation_d
-00029030: 6566 696e 6974 696f 6e73 0a20 2020 2020  efinitions.     
-00029040: 2020 2020 2069 6620 6461 7461 7365 745f       if dataset_
-00029050: 6c65 7665 6c5f 6375 7374 6f6d 5f74 7261  level_custom_tra
-00029060: 6e73 666f 726d 6174 696f 6e5f 6465 6669  nsformation_defi
-00029070: 6e69 7469 6f6e 730a 2020 2020 2020 2020  nitions.        
-00029080: 2020 656c 7365 205b 5d0a 2020 2020 2020    else [].      
-00029090: 292c 0a20 2020 2020 2027 6461 7461 7365  ),.      'datase
-000290a0: 745f 6c65 7665 6c5f 7472 616e 7366 6f72  t_level_transfor
-000290b0: 6d61 7469 6f6e 7327 3a20 280a 2020 2020  mations': (.    
-000290c0: 2020 2020 2020 6461 7461 7365 745f 6c65        dataset_le
-000290d0: 7665 6c5f 7472 616e 7366 6f72 6d61 7469  vel_transformati
-000290e0: 6f6e 7320 6966 2064 6174 6173 6574 5f6c  ons if dataset_l
-000290f0: 6576 656c 5f74 7261 6e73 666f 726d 6174  evel_transformat
-00029100: 696f 6e73 2065 6c73 6520 5b5d 0a20 2020  ions else [].   
-00029110: 2020 2029 2c0a 2020 2020 2020 2772 756e     ),.      'run
-00029120: 5f66 6561 7475 7265 5f73 656c 6563 7469  _feature_selecti
-00029130: 6f6e 273a 2072 756e 5f66 6561 7475 7265  on': run_feature
-00029140: 5f73 656c 6563 7469 6f6e 2c0a 2020 2020  _selection,.    
-00029150: 2020 2766 6561 7475 7265 5f73 656c 6563    'feature_selec
-00029160: 7469 6f6e 5f61 6c67 6f72 6974 686d 273a  tion_algorithm':
-00029170: 2066 6561 7475 7265 5f73 656c 6563 7469   feature_selecti
-00029180: 6f6e 5f61 6c67 6f72 6974 686d 2c0a 2020  on_algorithm,.  
-00029190: 2020 2020 2766 6561 7475 7265 5f73 656c      'feature_sel
-000291a0: 6563 7469 6f6e 5f65 7865 6375 7469 6f6e  ection_execution
-000291b0: 5f65 6e67 696e 6527 3a20 6665 6174 7572  _engine': featur
-000291c0: 655f 7365 6c65 6374 696f 6e5f 6578 6563  e_selection_exec
-000291d0: 7574 696f 6e5f 656e 6769 6e65 2c0a 2020  ution_engine,.  
-000291e0: 2020 2020 276d 6178 5f73 656c 6563 7465      'max_selecte
-000291f0: 645f 6665 6174 7572 6573 273a 206d 6178  d_features': max
-00029200: 5f73 656c 6563 7465 645f 6665 6174 7572  _selected_featur
-00029210: 6573 2c0a 2020 2020 2020 2770 7265 6465  es,.      'prede
-00029220: 6669 6e65 645f 7370 6c69 745f 6b65 7927  fined_split_key'
-00029230: 3a20 7072 6564 6566 696e 6564 5f73 706c  : predefined_spl
-00029240: 6974 5f6b 6579 2c0a 2020 2020 2020 2773  it_key,.      's
-00029250: 7472 6174 6966 6965 645f 7370 6c69 745f  tratified_split_
-00029260: 6b65 7927 3a20 7374 7261 7469 6669 6564  key': stratified
-00029270: 5f73 706c 6974 5f6b 6579 2c0a 2020 2020  _split_key,.    
-00029280: 2020 2774 7261 696e 696e 675f 6672 6163    'training_frac
-00029290: 7469 6f6e 273a 2074 7261 696e 696e 675f  tion': training_
-000292a0: 6672 6163 7469 6f6e 2c0a 2020 2020 2020  fraction,.      
-000292b0: 2776 616c 6964 6174 696f 6e5f 6672 6163  'validation_frac
-000292c0: 7469 6f6e 273a 2076 616c 6964 6174 696f  tion': validatio
-000292d0: 6e5f 6672 6163 7469 6f6e 2c0a 2020 2020  n_fraction,.    
-000292e0: 2020 2774 6573 745f 6672 6163 7469 6f6e    'test_fraction
-000292f0: 273a 2074 6573 745f 6672 6163 7469 6f6e  ': test_fraction
-00029300: 2c0a 2020 2020 2020 2774 665f 6175 746f  ,.      'tf_auto
-00029310: 5f74 7261 6e73 666f 726d 5f66 6561 7475  _transform_featu
-00029320: 7265 7327 3a20 7466 5f61 7574 6f5f 7472  res': tf_auto_tr
-00029330: 616e 7366 6f72 6d5f 6665 6174 7572 6573  ansform_features
-00029340: 2c0a 2020 2020 2020 276f 7074 696d 697a  ,.      'optimiz
-00029350: 6174 696f 6e5f 6f62 6a65 6374 6976 6527  ation_objective'
-00029360: 3a20 6f70 7469 6d69 7a61 7469 6f6e 5f6f  : optimization_o
-00029370: 626a 6563 7469 7665 2c0a 2020 2020 2020  bjective,.      
-00029380: 2764 6174 615f 736f 7572 6365 5f63 7376  'data_source_csv
-00029390: 5f66 696c 656e 616d 6573 273a 2064 6174  _filenames': dat
-000293a0: 615f 736f 7572 6365 5f63 7376 5f66 696c  a_source_csv_fil
-000293b0: 656e 616d 6573 2c0a 2020 2020 2020 2764  enames,.      'd
-000293c0: 6174 615f 736f 7572 6365 5f62 6967 7175  ata_source_bigqu
-000293d0: 6572 795f 7461 626c 655f 7061 7468 273a  ery_table_path':
-000293e0: 2064 6174 615f 736f 7572 6365 5f62 6967   data_source_big
-000293f0: 7175 6572 795f 7461 626c 655f 7061 7468  query_table_path
-00029400: 2c0a 2020 2020 2020 2762 6967 7175 6572  ,.      'bigquer
-00029410: 795f 7374 6167 696e 675f 6675 6c6c 5f64  y_staging_full_d
-00029420: 6174 6173 6574 5f69 6427 3a20 6269 6771  ataset_id': bigq
-00029430: 7565 7279 5f73 7461 6769 6e67 5f66 756c  uery_staging_ful
-00029440: 6c5f 6461 7461 7365 745f 6964 2c0a 2020  l_dataset_id,.  
-00029450: 2020 2020 2764 6174 6166 6c6f 775f 6d61      'dataflow_ma
-00029460: 6368 696e 655f 7479 7065 273a 2064 6174  chine_type': dat
-00029470: 6166 6c6f 775f 6d61 6368 696e 655f 7479  aflow_machine_ty
-00029480: 7065 2c0a 2020 2020 2020 2764 6174 6166  pe,.      'dataf
-00029490: 6c6f 775f 6d61 785f 6e75 6d5f 776f 726b  low_max_num_work
-000294a0: 6572 7327 3a20 6461 7461 666c 6f77 5f6d  ers': dataflow_m
-000294b0: 6178 5f6e 756d 5f77 6f72 6b65 7273 2c0a  ax_num_workers,.
-000294c0: 2020 2020 2020 2764 6174 6166 6c6f 775f        'dataflow_
-000294d0: 6469 736b 5f73 697a 655f 6762 273a 2064  disk_size_gb': d
-000294e0: 6174 6166 6c6f 775f 6469 736b 5f73 697a  ataflow_disk_siz
-000294f0: 655f 6762 2c0a 2020 2020 2020 2764 6174  e_gb,.      'dat
-00029500: 6166 6c6f 775f 7375 626e 6574 776f 726b  aflow_subnetwork
-00029510: 273a 2064 6174 6166 6c6f 775f 7375 626e  ': dataflow_subn
-00029520: 6574 776f 726b 2c0a 2020 2020 2020 2764  etwork,.      'd
-00029530: 6174 6166 6c6f 775f 7573 655f 7075 626c  ataflow_use_publ
-00029540: 6963 5f69 7073 273a 2064 6174 6166 6c6f  ic_ips': dataflo
-00029550: 775f 7573 655f 7075 626c 6963 5f69 7073  w_use_public_ips
-00029560: 2c0a 2020 2020 2020 2765 6e63 7279 7074  ,.      'encrypt
-00029570: 696f 6e5f 7370 6563 5f6b 6579 5f6e 616d  ion_spec_key_nam
-00029580: 6527 3a20 656e 6372 7970 7469 6f6e 5f73  e': encryption_s
-00029590: 7065 635f 6b65 795f 6e61 6d65 2c0a 2020  pec_key_name,.  
-000295a0: 2020 2020 2773 7461 6765 5f31 5f64 6561      'stage_1_dea
-000295b0: 646c 696e 655f 686f 7572 7327 3a20 7374  dline_hours': st
-000295c0: 6167 655f 315f 6465 6164 6c69 6e65 5f68  age_1_deadline_h
-000295d0: 6f75 7273 2c0a 2020 2020 2020 2773 7461  ours,.      'sta
-000295e0: 6765 5f32 5f64 6561 646c 696e 655f 686f  ge_2_deadline_ho
-000295f0: 7572 7327 3a20 7374 6167 655f 325f 6465  urs': stage_2_de
-00029600: 6164 6c69 6e65 5f68 6f75 7273 2c0a 2020  adline_hours,.  
-00029610: 7d0a 0a20 2070 6172 616d 6574 6572 5f76  }..  parameter_v
-00029620: 616c 7565 7320 3d20 7b0a 2020 2020 2020  alues = {.      
-00029630: 7061 7261 6d3a 2076 616c 7565 0a20 2020  param: value.   
-00029640: 2020 2066 6f72 2070 6172 616d 2c20 7661     for param, va
-00029650: 6c75 6520 696e 2070 6172 616d 6574 6572  lue in parameter
-00029660: 5f76 616c 7565 732e 6974 656d 7328 290a  _values.items().
-00029670: 2020 2020 2020 6966 2076 616c 7565 2069        if value i
-00029680: 7320 6e6f 7420 4e6f 6e65 0a20 207d 0a0a  s not None.  }..
-00029690: 2020 7265 7475 726e 2070 6970 656c 696e    return pipelin
-000296a0: 655f 6465 6669 6e69 7469 6f6e 5f70 6174  e_definition_pat
-000296b0: 682c 2070 6172 616d 6574 6572 5f76 616c  h, parameter_val
-000296c0: 7565 730a                                ues.
+0001ce90: 733a 2069 6e74 203d 205f 4556 414c 5541  s: int = _EVALUA
+0001cea0: 5449 4f4e 5f44 4154 4146 4c4f 575f 4d41  TION_DATAFLOW_MA
+0001ceb0: 585f 4e55 4d5f 574f 524b 4552 532c 0a20  X_NUM_WORKERS,. 
+0001cec0: 2020 2065 7661 6c75 6174 696f 6e5f 6461     evaluation_da
+0001ced0: 7461 666c 6f77 5f64 6973 6b5f 7369 7a65  taflow_disk_size
+0001cee0: 5f67 623a 2069 6e74 203d 205f 4556 414c  _gb: int = _EVAL
+0001cef0: 5541 5449 4f4e 5f44 4154 4146 4c4f 575f  UATION_DATAFLOW_
+0001cf00: 4449 534b 5f53 495a 455f 4742 2c0a 2020  DISK_SIZE_GB,.  
+0001cf10: 2020 6461 7461 666c 6f77 5f73 6572 7669    dataflow_servi
+0001cf20: 6365 5f61 6363 6f75 6e74 3a20 7374 7220  ce_account: str 
+0001cf30: 3d20 2727 2c0a 2020 2020 6461 7461 666c  = '',.    datafl
+0001cf40: 6f77 5f73 7562 6e65 7477 6f72 6b3a 2073  ow_subnetwork: s
+0001cf50: 7472 203d 2027 272c 0a20 2020 2064 6174  tr = '',.    dat
+0001cf60: 6166 6c6f 775f 7573 655f 7075 626c 6963  aflow_use_public
+0001cf70: 5f69 7073 3a20 626f 6f6c 203d 2054 7275  _ips: bool = Tru
+0001cf80: 652c 0a20 2020 2065 6e63 7279 7074 696f  e,.    encryptio
+0001cf90: 6e5f 7370 6563 5f6b 6579 5f6e 616d 653a  n_spec_key_name:
+0001cfa0: 2073 7472 203d 2027 272c 0a29 202d 3e20   str = '',.) -> 
+0001cfb0: 5475 706c 655b 7374 722c 2044 6963 745b  Tuple[str, Dict[
+0001cfc0: 7374 722c 2041 6e79 5d5d 3a0a 2020 2320  str, Any]]:.  # 
+0001cfd0: 666d 743a 206f 6666 0a20 2022 2222 4765  fmt: off.  """Ge
+0001cfe0: 7420 7468 6520 5461 624e 6574 2074 7261  t the TabNet tra
+0001cff0: 696e 696e 6720 7069 7065 6c69 6e65 2e0a  ining pipeline..
+0001d000: 0a20 2041 7267 733a 0a20 2020 2070 726f  .  Args:.    pro
+0001d010: 6a65 6374 3a20 5468 6520 4743 5020 7072  ject: The GCP pr
+0001d020: 6f6a 6563 7420 7468 6174 2072 756e 7320  oject that runs 
+0001d030: 7468 6520 7069 7065 6c69 6e65 2063 6f6d  the pipeline com
+0001d040: 706f 6e65 6e74 732e 0a20 2020 206c 6f63  ponents..    loc
+0001d050: 6174 696f 6e3a 2054 6865 2047 4350 2072  ation: The GCP r
+0001d060: 6567 696f 6e20 7468 6174 2072 756e 7320  egion that runs 
+0001d070: 7468 6520 7069 7065 6c69 6e65 2063 6f6d  the pipeline com
+0001d080: 706f 6e65 6e74 732e 0a20 2020 2072 6f6f  ponents..    roo
+0001d090: 745f 6469 723a 2054 6865 2072 6f6f 7420  t_dir: The root 
+0001d0a0: 4743 5320 6469 7265 6374 6f72 7920 666f  GCS directory fo
+0001d0b0: 7220 7468 6520 7069 7065 6c69 6e65 2063  r the pipeline c
+0001d0c0: 6f6d 706f 6e65 6e74 732e 0a20 2020 2074  omponents..    t
+0001d0d0: 6172 6765 745f 636f 6c75 6d6e 3a20 5468  arget_column: Th
+0001d0e0: 6520 7461 7267 6574 2063 6f6c 756d 6e20  e target column 
+0001d0f0: 6e61 6d65 2e0a 2020 2020 7072 6564 6963  name..    predic
+0001d100: 7469 6f6e 5f74 7970 653a 2054 6865 2074  tion_type: The t
+0001d110: 7970 6520 6f66 2070 7265 6469 6374 696f  ype of predictio
+0001d120: 6e20 7468 6520 6d6f 6465 6c20 6973 2074  n the model is t
+0001d130: 6f20 7072 6f64 7563 652e 2020 2263 6c61  o produce.  "cla
+0001d140: 7373 6966 6963 6174 696f 6e22 206f 7220  ssification" or 
+0001d150: 2272 6567 7265 7373 696f 6e22 2e0a 2020  "regression"..  
+0001d160: 2020 6c65 6172 6e69 6e67 5f72 6174 653a    learning_rate:
+0001d170: 2054 6865 206c 6561 726e 696e 6720 7261   The learning ra
+0001d180: 7465 2075 7365 6420 6279 2074 6865 206c  te used by the l
+0001d190: 696e 6561 7220 6f70 7469 6d69 7a65 722e  inear optimizer.
+0001d1a0: 0a20 2020 2074 7261 6e73 666f 726d 5f63  .    transform_c
+0001d1b0: 6f6e 6669 673a 2050 6174 6820 746f 2076  onfig: Path to v
+0001d1c0: 3120 5446 2074 7261 6e73 666f 726d 6174  1 TF transformat
+0001d1d0: 696f 6e20 636f 6e66 6967 7572 6174 696f  ion configuratio
+0001d1e0: 6e2e 0a20 2020 2064 6174 6173 6574 5f6c  n..    dataset_l
+0001d1f0: 6576 656c 5f63 7573 746f 6d5f 7472 616e  evel_custom_tran
+0001d200: 7366 6f72 6d61 7469 6f6e 5f64 6566 696e  sformation_defin
+0001d210: 6974 696f 6e73 3a20 4461 7461 7365 742d  itions: Dataset-
+0001d220: 6c65 7665 6c20 6375 7374 6f6d 2074 7261  level custom tra
+0001d230: 6e73 666f 726d 6174 696f 6e20 6465 6669  nsformation defi
+0001d240: 6e69 7469 6f6e 7320 696e 2073 7472 696e  nitions in strin
+0001d250: 6720 666f 726d 6174 2e0a 2020 2020 6461  g format..    da
+0001d260: 7461 7365 745f 6c65 7665 6c5f 7472 616e  taset_level_tran
+0001d270: 7366 6f72 6d61 7469 6f6e 733a 2044 6174  sformations: Dat
+0001d280: 6173 6574 2d6c 6576 656c 2074 7261 6e73  aset-level trans
+0001d290: 666f 726d 6174 696f 6e20 636f 6e66 6967  formation config
+0001d2a0: 7572 6174 696f 6e20 696e 2073 7472 696e  uration in strin
+0001d2b0: 6720 666f 726d 6174 2e0a 2020 2020 7275  g format..    ru
+0001d2c0: 6e5f 6665 6174 7572 655f 7365 6c65 6374  n_feature_select
+0001d2d0: 696f 6e3a 2057 6865 7468 6572 2074 6f20  ion: Whether to 
+0001d2e0: 656e 6162 6c65 2066 6561 7475 7265 2073  enable feature s
+0001d2f0: 656c 6563 7469 6f6e 2e0a 2020 2020 6665  election..    fe
+0001d300: 6174 7572 655f 7365 6c65 6374 696f 6e5f  ature_selection_
+0001d310: 616c 676f 7269 7468 6d3a 2046 6561 7475  algorithm: Featu
+0001d320: 7265 2073 656c 6563 7469 6f6e 2061 6c67  re selection alg
+0001d330: 6f72 6974 686d 2e0a 2020 2020 6d61 7465  orithm..    mate
+0001d340: 7269 616c 697a 6564 5f65 7861 6d70 6c65  rialized_example
+0001d350: 735f 666f 726d 6174 3a20 5468 6520 666f  s_format: The fo
+0001d360: 726d 6174 2066 6f72 2074 6865 206d 6174  rmat for the mat
+0001d370: 6572 6961 6c69 7a65 6420 6578 616d 706c  erialized exampl
+0001d380: 6573 2e0a 2020 2020 6d61 785f 7365 6c65  es..    max_sele
+0001d390: 6374 6564 5f66 6561 7475 7265 733a 204d  cted_features: M
+0001d3a0: 6178 696d 756d 206e 756d 6265 7220 6f66  aximum number of
+0001d3b0: 2066 6561 7475 7265 7320 746f 2073 656c   features to sel
+0001d3c0: 6563 742e 0a20 2020 2070 7265 6465 6669  ect..    predefi
+0001d3d0: 6e65 645f 7370 6c69 745f 6b65 793a 2050  ned_split_key: P
+0001d3e0: 7265 6465 6669 6e65 6420 7370 6c69 7420  redefined split 
+0001d3f0: 6b65 792e 0a20 2020 2073 7472 6174 6966  key..    stratif
+0001d400: 6965 645f 7370 6c69 745f 6b65 793a 2053  ied_split_key: S
+0001d410: 7472 6174 6966 6965 6420 7370 6c69 7420  tratified split 
+0001d420: 6b65 792e 0a20 2020 2074 7261 696e 696e  key..    trainin
+0001d430: 675f 6672 6163 7469 6f6e 3a20 5472 6169  g_fraction: Trai
+0001d440: 6e69 6e67 2066 7261 6374 696f 6e2e 0a20  ning fraction.. 
+0001d450: 2020 2076 616c 6964 6174 696f 6e5f 6672     validation_fr
+0001d460: 6163 7469 6f6e 3a20 5661 6c69 6461 7469  action: Validati
+0001d470: 6f6e 2066 7261 6374 696f 6e2e 0a20 2020  on fraction..   
+0001d480: 2074 6573 745f 6672 6163 7469 6f6e 3a20   test_fraction: 
+0001d490: 5465 7374 2066 7261 6374 696f 6e2e 0a20  Test fraction.. 
+0001d4a0: 2020 2074 665f 7472 616e 7366 6f72 6d5f     tf_transform_
+0001d4b0: 6578 6563 7574 696f 6e5f 656e 6769 6e65  execution_engine
+0001d4c0: 3a20 5468 6520 6578 6563 7574 696f 6e20  : The execution 
+0001d4d0: 656e 6769 6e65 2075 7365 6420 746f 2065  engine used to e
+0001d4e0: 7865 6375 7465 2054 462d 6261 7365 6420  xecute TF-based 
+0001d4f0: 7472 616e 7366 6f72 6d61 7469 6f6e 732e  transformations.
+0001d500: 0a20 2020 2074 665f 6175 746f 5f74 7261  .    tf_auto_tra
+0001d510: 6e73 666f 726d 5f66 6561 7475 7265 733a  nsform_features:
+0001d520: 204c 6973 7420 6f66 2061 7574 6f20 7472   List of auto tr
+0001d530: 616e 7366 6f72 6d20 6665 6174 7572 6573  ansform features
+0001d540: 2069 6e20 7468 6520 636f 6d6d 612d 7365   in the comma-se
+0001d550: 7061 7261 7465 6420 7374 7269 6e67 2066  parated string f
+0001d560: 6f72 6d61 742e 0a20 2020 2074 665f 6375  ormat..    tf_cu
+0001d570: 7374 6f6d 5f74 7261 6e73 666f 726d 6174  stom_transformat
+0001d580: 696f 6e5f 6465 6669 6e69 7469 6f6e 733a  ion_definitions:
+0001d590: 2054 4620 6375 7374 6f6d 2074 7261 6e73   TF custom trans
+0001d5a0: 666f 726d 6174 696f 6e20 6465 6669 6e69  formation defini
+0001d5b0: 7469 6f6e 7320 696e 2073 7472 696e 6720  tions in string 
+0001d5c0: 666f 726d 6174 2e0a 2020 2020 7466 5f74  format..    tf_t
+0001d5d0: 7261 6e73 666f 726d 6174 696f 6e73 5f70  ransformations_p
+0001d5e0: 6174 683a 2050 6174 6820 746f 2054 4620  ath: Path to TF 
+0001d5f0: 7472 616e 7366 6f72 6d61 7469 6f6e 2063  transformation c
+0001d600: 6f6e 6669 6775 7261 7469 6f6e 2e0a 2020  onfiguration..  
+0001d610: 2020 6d61 785f 7374 6570 733a 204e 756d    max_steps: Num
+0001d620: 6265 7220 6f66 2073 7465 7073 2074 6f20  ber of steps to 
+0001d630: 7275 6e20 7468 6520 7472 6169 6e65 7220  run the trainer 
+0001d640: 666f 722e 0a20 2020 206d 6178 5f74 7261  for..    max_tra
+0001d650: 696e 5f73 6563 733a 2041 6d6f 756e 7420  in_secs: Amount 
+0001d660: 6f66 2074 696d 6520 696e 2073 6563 6f6e  of time in secon
+0001d670: 6473 2074 6f20 7275 6e20 7468 6520 7472  ds to run the tr
+0001d680: 6169 6e65 7220 666f 722e 0a20 2020 206c  ainer for..    l
+0001d690: 6172 6765 5f63 6174 6567 6f72 795f 6469  arge_category_di
+0001d6a0: 6d3a 2045 6d62 6564 6469 6e67 2064 696d  m: Embedding dim
+0001d6b0: 656e 7369 6f6e 2066 6f72 2063 6174 6567  ension for categ
+0001d6c0: 6f72 6963 616c 2066 6561 7475 7265 2077  orical feature w
+0001d6d0: 6974 6820 6c61 7267 6520 6e75 6d62 6572  ith large number
+0001d6e0: 206f 6620 6361 7465 676f 7269 6573 2e0a   of categories..
+0001d6f0: 2020 2020 6c61 7267 655f 6361 7465 676f      large_catego
+0001d700: 7279 5f74 6872 6573 683a 2054 6872 6573  ry_thresh: Thres
+0001d710: 686f 6c64 2066 6f72 206e 756d 6265 7220  hold for number 
+0001d720: 6f66 2063 6174 6567 6f72 6965 7320 746f  of categories to
+0001d730: 2061 7070 6c79 206c 6172 6765 5f63 6174   apply large_cat
+0001d740: 6567 6f72 795f 6469 6d20 656d 6265 6464  egory_dim embedd
+0001d750: 696e 6720 6469 6d65 6e73 696f 6e20 746f  ing dimension to
+0001d760: 2e0a 2020 2020 7965 6f5f 6a6f 686e 736f  ..    yeo_johnso
+0001d770: 6e5f 7472 616e 7366 6f72 6d3a 2045 6e61  n_transform: Ena
+0001d780: 626c 6573 2074 7261 696e 6162 6c65 2059  bles trainable Y
+0001d790: 656f 2d4a 6f68 6e73 6f6e 2070 6f77 6572  eo-Johnson power
+0001d7a0: 2074 7261 6e73 666f 726d 2e0a 2020 2020   transform..    
+0001d7b0: 6665 6174 7572 655f 6469 6d3a 2044 696d  feature_dim: Dim
+0001d7c0: 656e 7369 6f6e 616c 6974 7920 6f66 2074  ensionality of t
+0001d7d0: 6865 2068 6964 6465 6e20 7265 7072 6573  he hidden repres
+0001d7e0: 656e 7461 7469 6f6e 2069 6e20 6665 6174  entation in feat
+0001d7f0: 7572 6520 7472 616e 7366 6f72 6d61 7469  ure transformati
+0001d800: 6f6e 2062 6c6f 636b 2e0a 2020 2020 6665  on block..    fe
+0001d810: 6174 7572 655f 6469 6d5f 7261 7469 6f3a  ature_dim_ratio:
+0001d820: 2054 6865 2072 6174 696f 206f 6620 6f75   The ratio of ou
+0001d830: 7470 7574 2064 696d 656e 7369 6f6e 2028  tput dimension (
+0001d840: 6469 6d65 6e73 696f 6e61 6c69 7479 206f  dimensionality o
+0001d850: 6620 7468 6520 6f75 7470 7574 7320 6f66  f the outputs of
+0001d860: 2065 6163 6820 6465 6369 7369 6f6e 2073   each decision s
+0001d870: 7465 7029 2074 6f20 6665 6174 7572 6520  tep) to feature 
+0001d880: 6469 6d65 6e73 696f 6e2e 0a20 2020 206e  dimension..    n
+0001d890: 756d 5f64 6563 6973 696f 6e5f 7374 6570  um_decision_step
+0001d8a0: 733a 204e 756d 6265 7220 6f66 2073 6571  s: Number of seq
+0001d8b0: 7565 6e74 6961 6c20 6465 6369 7369 6f6e  uential decision
+0001d8c0: 2073 7465 7073 2e0a 2020 2020 7265 6c61   steps..    rela
+0001d8d0: 7861 7469 6f6e 5f66 6163 746f 723a 2052  xation_factor: R
+0001d8e0: 656c 6178 6174 696f 6e20 6661 6374 6f72  elaxation factor
+0001d8f0: 2074 6861 7420 7072 6f6d 6f74 6573 2074   that promotes t
+0001d900: 6865 2072 6575 7365 206f 6620 6561 6368  he reuse of each
+0001d910: 2066 6561 7475 7265 2061 7420 6469 6666   feature at diff
+0001d920: 6572 656e 7420 6465 6369 7369 6f6e 2073  erent decision s
+0001d930: 7465 7073 2e20 5768 656e 2069 7420 6973  teps. When it is
+0001d940: 2031 2c20 6120 6665 6174 7572 6520 6973   1, a feature is
+0001d950: 2065 6e66 6f72 6365 6420 746f 2062 6520   enforced to be 
+0001d960: 7573 6564 206f 6e6c 7920 6174 206f 6e65  used only at one
+0001d970: 2064 6563 6973 696f 6e20 7374 6570 2061   decision step a
+0001d980: 6e64 2061 7320 6974 2069 6e63 7265 6173  nd as it increas
+0001d990: 6573 2c20 6d6f 7265 2066 6c65 7869 6269  es, more flexibi
+0001d9a0: 6c69 7479 2069 7320 7072 6f76 6964 6564  lity is provided
+0001d9b0: 2074 6f20 7573 6520 6120 6665 6174 7572   to use a featur
+0001d9c0: 6520 6174 206d 756c 7469 706c 6520 6465  e at multiple de
+0001d9d0: 6369 7369 6f6e 2073 7465 7073 2e0a 2020  cision steps..  
+0001d9e0: 2020 6465 6361 795f 6576 6572 793a 204e    decay_every: N
+0001d9f0: 756d 6265 7220 6f66 2069 7465 7261 7469  umber of iterati
+0001da00: 6f6e 7320 666f 7220 7065 7269 6f64 6963  ons for periodic
+0001da10: 616c 6c79 2061 7070 6c79 696e 6720 6c65  ally applying le
+0001da20: 6172 6e69 6e67 2072 6174 6520 6465 6361  arning rate deca
+0001da30: 7969 6e67 2e0a 2020 2020 6465 6361 795f  ying..    decay_
+0001da40: 7261 7465 3a20 4c65 6172 6e69 6e67 2072  rate: Learning r
+0001da50: 6174 6520 6465 6361 7969 6e67 2e0a 2020  ate decaying..  
+0001da60: 2020 6772 6164 6965 6e74 5f74 6872 6573    gradient_thres
+0001da70: 683a 2054 6872 6573 686f 6c64 2066 6f72  h: Threshold for
+0001da80: 2074 6865 206e 6f72 6d20 6f66 2067 7261   the norm of gra
+0001da90: 6469 656e 7473 2066 6f72 2063 6c69 7070  dients for clipp
+0001daa0: 696e 672e 0a20 2020 2073 7061 7273 6974  ing..    sparsit
+0001dab0: 795f 6c6f 7373 5f77 6569 6768 743a 2057  y_loss_weight: W
+0001dac0: 6569 6768 7420 6f66 2074 6865 206c 6f73  eight of the los
+0001dad0: 7320 666f 7220 7370 6172 7369 7479 2072  s for sparsity r
+0001dae0: 6567 756c 6172 697a 6174 696f 6e20 2869  egularization (i
+0001daf0: 6e63 7265 6173 696e 6720 6974 2077 696c  ncreasing it wil
+0001db00: 6c20 7969 656c 6420 6d6f 7265 2073 7061  l yield more spa
+0001db10: 7273 6520 6665 6174 7572 6520 7365 6c65  rse feature sele
+0001db20: 6374 696f 6e29 2e0a 2020 2020 6261 7463  ction)..    batc
+0001db30: 685f 6d6f 6d65 6e74 756d 3a20 4d6f 6d65  h_momentum: Mome
+0001db40: 6e74 756d 2069 6e20 6768 6f73 7420 6261  ntum in ghost ba
+0001db50: 7463 6820 6e6f 726d 616c 697a 6174 696f  tch normalizatio
+0001db60: 6e2e 0a20 2020 2062 6174 6368 5f73 697a  n..    batch_siz
+0001db70: 655f 7261 7469 6f3a 2054 6865 2072 6174  e_ratio: The rat
+0001db80: 696f 206f 6620 7669 7274 7561 6c20 6261  io of virtual ba
+0001db90: 7463 6820 7369 7a65 2028 7369 7a65 206f  tch size (size o
+0001dba0: 6620 7468 6520 6768 6f73 7420 6261 7463  f the ghost batc
+0001dbb0: 6820 6e6f 726d 616c 697a 6174 696f 6e29  h normalization)
+0001dbc0: 2074 6f20 6261 7463 6820 7369 7a65 2e0a   to batch size..
+0001dbd0: 2020 2020 6e75 6d5f 7472 616e 7366 6f72      num_transfor
+0001dbe0: 6d65 725f 6c61 7965 7273 3a20 5468 6520  mer_layers: The 
+0001dbf0: 6e75 6d62 6572 206f 6620 7472 616e 7366  number of transf
+0001dc00: 6f72 6d65 7220 6c61 7965 7273 2066 6f72  ormer layers for
+0001dc10: 2065 6163 6820 6465 6369 7369 6f6e 2073   each decision s
+0001dc20: 7465 702e 2075 7365 6420 6f6e 6c79 2061  tep. used only a
+0001dc30: 7420 6f6e 6520 6465 6369 7369 6f6e 2073  t one decision s
+0001dc40: 7465 7020 616e 6420 6173 2069 7420 696e  tep and as it in
+0001dc50: 6372 6561 7365 732c 206d 6f72 6520 666c  creases, more fl
+0001dc60: 6578 6962 696c 6974 7920 6973 2070 726f  exibility is pro
+0001dc70: 7669 6465 6420 746f 2075 7365 2061 2066  vided to use a f
+0001dc80: 6561 7475 7265 2061 7420 6d75 6c74 6970  eature at multip
+0001dc90: 6c65 2064 6563 6973 696f 6e20 7374 6570  le decision step
+0001dca0: 732e 0a20 2020 206e 756d 5f74 7261 6e73  s..    num_trans
+0001dcb0: 666f 726d 6572 5f6c 6179 6572 735f 7261  former_layers_ra
+0001dcc0: 7469 6f3a 2054 6865 2072 6174 696f 206f  tio: The ratio o
+0001dcd0: 6620 7368 6172 6564 2074 7261 6e73 666f  f shared transfo
+0001dce0: 726d 6572 206c 6179 6572 2074 6f20 7472  rmer layer to tr
+0001dcf0: 616e 7366 6f72 6d65 7220 6c61 7965 7273  ansformer layers
+0001dd00: 2e0a 2020 2020 636c 6173 735f 7765 6967  ..    class_weig
+0001dd10: 6874 3a20 5468 6520 636c 6173 7320 7765  ht: The class we
+0001dd20: 6967 6874 2069 7320 7573 6564 2074 6f20  ight is used to 
+0001dd30: 636f 6d70 7574 6573 2061 2077 6569 6768  computes a weigh
+0001dd40: 7465 6420 6372 6f73 7320 656e 7472 6f70  ted cross entrop
+0001dd50: 7920 7768 6963 6820 6973 2068 656c 7066  y which is helpf
+0001dd60: 756c 2069 6e20 636c 6173 7369 6679 2069  ul in classify i
+0001dd70: 6d62 616c 616e 6365 6420 6461 7461 7365  mbalanced datase
+0001dd80: 742e 204f 6e6c 7920 7573 6564 2066 6f72  t. Only used for
+0001dd90: 2063 6c61 7373 6966 6963 6174 696f 6e2e   classification.
+0001dda0: 0a20 2020 206c 6f73 735f 6675 6e63 7469  .    loss_functi
+0001ddb0: 6f6e 5f74 7970 653a 204c 6f73 7320 6675  on_type: Loss fu
+0001ddc0: 6e63 7469 6f6e 2074 7970 652e 204c 6f73  nction type. Los
+0001ddd0: 7320 6675 6e63 7469 6f6e 2069 6e20 636c  s function in cl
+0001dde0: 6173 7369 6669 6361 7469 6f6e 205b 6372  assification [cr
+0001ddf0: 6f73 735f 656e 7472 6f70 792c 2077 6569  oss_entropy, wei
+0001de00: 6768 7465 645f 6372 6f73 735f 656e 7472  ghted_cross_entr
+0001de10: 6f70 792c 2066 6f63 616c 5f6c 6f73 735d  opy, focal_loss]
+0001de20: 2c20 6465 6661 756c 7420 6973 2063 726f  , default is cro
+0001de30: 7373 5f65 6e74 726f 7079 2e20 4c6f 7373  ss_entropy. Loss
+0001de40: 2066 756e 6374 696f 6e20 696e 2072 6567   function in reg
+0001de50: 7265 7373 696f 6e3a 205b 726d 7365 2c20  ression: [rmse, 
+0001de60: 6d61 652c 206d 7365 5d2c 2064 6566 6175  mae, mse], defau
+0001de70: 6c74 2069 7320 6d73 652e 0a20 2020 2061  lt is mse..    a
+0001de80: 6c70 6861 5f66 6f63 616c 5f6c 6f73 733a  lpha_focal_loss:
+0001de90: 2041 6c70 6861 2076 616c 7565 2028 6261   Alpha value (ba
+0001dea0: 6c61 6e63 696e 6720 6661 6374 6f72 2920  lancing factor) 
+0001deb0: 696e 2066 6f63 616c 5f6c 6f73 7320 6675  in focal_loss fu
+0001dec0: 6e63 7469 6f6e 2e20 204f 6e6c 7920 7573  nction.  Only us
+0001ded0: 6564 2066 6f72 2063 6c61 7373 6966 6963  ed for classific
+0001dee0: 6174 696f 6e2e 0a20 2020 2067 616d 6d61  ation..    gamma
+0001def0: 5f66 6f63 616c 5f6c 6f73 733a 2047 616d  _focal_loss: Gam
+0001df00: 6d61 2076 616c 7565 2028 6d6f 6475 6c61  ma value (modula
+0001df10: 7469 6e67 2066 6163 746f 7229 2066 6f72  ting factor) for
+0001df20: 2066 6f63 616c 206c 6f73 7320 666f 7220   focal loss for 
+0001df30: 666f 6361 6c20 6c6f 7373 2e20 4f6e 6c79  focal loss. Only
+0001df40: 2075 7365 6420 666f 7220 636c 6173 7369   used for classi
+0001df50: 6669 6361 7469 6f6e 2e0a 2020 2020 656e  fication..    en
+0001df60: 6162 6c65 5f70 726f 6669 6c65 723a 2045  able_profiler: E
+0001df70: 6e61 626c 6573 2070 726f 6669 6c69 6e67  nables profiling
+0001df80: 2061 6e64 2073 6176 6573 2061 2074 7261   and saves a tra
+0001df90: 6365 2064 7572 696e 6720 6576 616c 7561  ce during evalua
+0001dfa0: 7469 6f6e 2e0a 2020 2020 6361 6368 655f  tion..    cache_
+0001dfb0: 6461 7461 3a20 5768 6574 6865 7220 746f  data: Whether to
+0001dfc0: 2063 6163 6865 2064 6174 6120 6f72 206e   cache data or n
+0001dfd0: 6f74 2e20 4966 2073 6574 2074 6f20 2761  ot. If set to 'a
+0001dfe0: 7574 6f27 2c20 6361 6368 696e 6720 6973  uto', caching is
+0001dff0: 2064 6574 6572 6d69 6e65 6420 6261 7365   determined base
+0001e000: 6420 6f6e 2074 6865 2064 6174 6173 6574  d on the dataset
+0001e010: 2073 697a 652e 0a20 2020 2073 6565 643a   size..    seed:
+0001e020: 2053 6565 6420 746f 2062 6520 7573 6564   Seed to be used
+0001e030: 2066 6f72 2074 6869 7320 7275 6e2e 0a20   for this run.. 
+0001e040: 2020 2065 7661 6c5f 7374 6570 733a 204e     eval_steps: N
+0001e050: 756d 6265 7220 6f66 2073 7465 7073 2074  umber of steps t
+0001e060: 6f20 7275 6e20 6576 616c 7561 7469 6f6e  o run evaluation
+0001e070: 2066 6f72 2e20 4966 206e 6f74 2073 7065   for. If not spe
+0001e080: 6369 6669 6564 206f 7220 6e65 6761 7469  cified or negati
+0001e090: 7665 2c20 6974 206d 6561 6e73 2072 756e  ve, it means run
+0001e0a0: 2065 7661 6c75 6174 696f 6e20 6f6e 2074   evaluation on t
+0001e0b0: 6865 2077 686f 6c65 2076 616c 6964 6174  he whole validat
+0001e0c0: 696f 6e20 6461 7461 7365 742e 2049 6620  ion dataset. If 
+0001e0d0: 7365 7420 746f 2030 2c20 6974 206d 6561  set to 0, it mea
+0001e0e0: 6e73 2072 756e 2065 7661 6c75 6174 696f  ns run evaluatio
+0001e0f0: 6e20 666f 7220 6120 6669 7865 6420 6e75  n for a fixed nu
+0001e100: 6d62 6572 206f 6620 7361 6d70 6c65 732e  mber of samples.
+0001e110: 0a20 2020 2062 6174 6368 5f73 697a 653a  .    batch_size:
+0001e120: 2042 6174 6368 2073 697a 6520 666f 7220   Batch size for 
+0001e130: 7472 6169 6e69 6e67 2e0a 2020 2020 6d65  training..    me
+0001e140: 6173 7572 656d 656e 745f 7365 6c65 6374  asurement_select
+0001e150: 696f 6e5f 7479 7065 3a20 5768 6963 6820  ion_type: Which 
+0001e160: 6d65 6173 7572 656d 656e 7420 746f 2075  measurement to u
+0001e170: 7365 2069 662f 7768 656e 2074 6865 2073  se if/when the s
+0001e180: 6572 7669 6365 2061 7574 6f6d 6174 6963  ervice automatic
+0001e190: 616c 6c79 2073 656c 6563 7473 2074 6865  ally selects the
+0001e1a0: 2066 696e 616c 206d 6561 7375 7265 6d65   final measureme
+0001e1b0: 6e74 2066 726f 6d20 7072 6576 696f 7573  nt from previous
+0001e1c0: 6c79 2072 6570 6f72 7465 6420 696e 7465  ly reported inte
+0001e1d0: 726d 6564 6961 7465 206d 6561 7375 7265  rmediate measure
+0001e1e0: 6d65 6e74 732e 204f 6e65 206f 6620 2242  ments. One of "B
+0001e1f0: 4553 545f 4d45 4153 5552 454d 454e 5422  EST_MEASUREMENT"
+0001e200: 206f 7220 224c 4153 545f 4d45 4153 5552   or "LAST_MEASUR
+0001e210: 454d 454e 5422 2e0a 2020 2020 6f70 7469  EMENT"..    opti
+0001e220: 6d69 7a61 7469 6f6e 5f6d 6574 7269 633a  mization_metric:
+0001e230: 204f 7074 696d 697a 6174 696f 6e20 6d65   Optimization me
+0001e240: 7472 6963 2075 7365 6420 666f 7220 606d  tric used for `m
+0001e250: 6561 7375 7265 6d65 6e74 5f73 656c 6563  easurement_selec
+0001e260: 7469 6f6e 5f74 7970 6560 2e20 4465 6661  tion_type`. Defa
+0001e270: 756c 7420 6973 2022 726d 7365 2220 666f  ult is "rmse" fo
+0001e280: 7220 7265 6772 6573 7369 6f6e 2061 6e64  r regression and
+0001e290: 2022 6175 6322 2066 6f72 2063 6c61 7373   "auc" for class
+0001e2a0: 6966 6963 6174 696f 6e2e 0a20 2020 2065  ification..    e
+0001e2b0: 7661 6c5f 6672 6571 7565 6e63 795f 7365  val_frequency_se
+0001e2c0: 6373 3a20 4672 6571 7565 6e63 7920 6174  cs: Frequency at
+0001e2d0: 2077 6869 6368 2065 7661 6c75 6174 696f   which evaluatio
+0001e2e0: 6e20 616e 6420 6368 6563 6b70 6f69 6e74  n and checkpoint
+0001e2f0: 696e 6720 7769 6c6c 2074 616b 6520 706c  ing will take pl
+0001e300: 6163 652e 0a20 2020 2064 6174 615f 736f  ace..    data_so
+0001e310: 7572 6365 5f63 7376 5f66 696c 656e 616d  urce_csv_filenam
+0001e320: 6573 3a20 5468 6520 4353 5620 6461 7461  es: The CSV data
+0001e330: 2073 6f75 7263 652e 0a20 2020 2064 6174   source..    dat
+0001e340: 615f 736f 7572 6365 5f62 6967 7175 6572  a_source_bigquer
+0001e350: 795f 7461 626c 655f 7061 7468 3a20 5468  y_table_path: Th
+0001e360: 6520 4269 6751 7565 7279 2064 6174 6120  e BigQuery data 
+0001e370: 736f 7572 6365 2e0a 2020 2020 6269 6771  source..    bigq
+0001e380: 7565 7279 5f73 7461 6769 6e67 5f66 756c  uery_staging_ful
+0001e390: 6c5f 6461 7461 7365 745f 6964 3a20 5468  l_dataset_id: Th
+0001e3a0: 6520 4269 6751 7565 7279 2073 7461 6769  e BigQuery stagi
+0001e3b0: 6e67 2066 756c 6c20 6461 7461 7365 7420  ng full dataset 
+0001e3c0: 6964 2066 6f72 2073 746f 7269 6e67 2069  id for storing i
+0001e3d0: 6e74 6572 6d65 6469 6174 6520 7461 626c  ntermediate tabl
+0001e3e0: 6573 2e0a 2020 2020 7765 6967 6874 5f63  es..    weight_c
+0001e3f0: 6f6c 756d 6e3a 2054 6865 2077 6569 6768  olumn: The weigh
+0001e400: 7420 636f 6c75 6d6e 206e 616d 652e 0a20  t column name.. 
+0001e410: 2020 2074 7261 6e73 666f 726d 5f64 6174     transform_dat
+0001e420: 6166 6c6f 775f 6d61 6368 696e 655f 7479  aflow_machine_ty
+0001e430: 7065 3a20 5468 6520 6461 7461 666c 6f77  pe: The dataflow
+0001e440: 206d 6163 6869 6e65 2074 7970 6520 666f   machine type fo
+0001e450: 7220 7472 616e 7366 6f72 6d20 636f 6d70  r transform comp
+0001e460: 6f6e 656e 742e 0a20 2020 2074 7261 6e73  onent..    trans
+0001e470: 666f 726d 5f64 6174 6166 6c6f 775f 6d61  form_dataflow_ma
+0001e480: 785f 6e75 6d5f 776f 726b 6572 733a 2054  x_num_workers: T
+0001e490: 6865 206d 6178 206e 756d 6265 7220 6f66  he max number of
+0001e4a0: 2044 6174 6166 6c6f 7720 776f 726b 6572   Dataflow worker
+0001e4b0: 7320 666f 7220 7472 616e 7366 6f72 6d20  s for transform 
+0001e4c0: 636f 6d70 6f6e 656e 742e 0a20 2020 2074  component..    t
+0001e4d0: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
+0001e4e0: 775f 6469 736b 5f73 697a 655f 6762 3a20  w_disk_size_gb: 
+0001e4f0: 4461 7461 666c 6f77 2077 6f72 6b65 7227  Dataflow worker'
+0001e500: 7320 6469 736b 2073 697a 6520 696e 2047  s disk size in G
+0001e510: 4220 666f 7220 7472 616e 7366 6f72 6d20  B for transform 
+0001e520: 636f 6d70 6f6e 656e 742e 0a20 2020 2077  component..    w
+0001e530: 6f72 6b65 725f 706f 6f6c 5f73 7065 6373  orker_pool_specs
+0001e540: 5f6f 7665 7272 6964 653a 2054 6865 2064  _override: The d
+0001e550: 6963 7469 6f6e 6172 7920 666f 7220 6f76  ictionary for ov
+0001e560: 6572 7269 6469 6e67 2074 7261 696e 696e  erriding trainin
+0001e570: 6720 616e 6420 6576 616c 7561 7469 6f6e  g and evaluation
+0001e580: 2077 6f72 6b65 7220 706f 6f6c 2073 7065   worker pool spe
+0001e590: 6373 2e20 5468 6520 6469 6374 696f 6e61  cs. The dictiona
+0001e5a0: 7279 2073 686f 756c 6420 6265 206f 6620  ry should be of 
+0001e5b0: 666f 726d 6174 2068 7474 7073 3a2f 2f67  format https://g
+0001e5c0: 6974 6875 622e 636f 6d2f 676f 6f67 6c65  ithub.com/google
+0001e5d0: 6170 6973 2f67 6f6f 676c 6561 7069 732f  apis/googleapis/
+0001e5e0: 626c 6f62 2f34 6538 3336 6337 6332 3537  blob/4e836c7c257
+0001e5f0: 6533 6532 3062 3164 6531 3464 3437 3039  e3e20b1de14d4709
+0001e600: 3933 6132 6231 6634 3733 3661 382f 676f  93a2b1f4736a8/go
+0001e610: 6f67 6c65 2f63 6c6f 7564 2f61 6970 6c61  ogle/cloud/aipla
+0001e620: 7466 6f72 6d2f 7631 6265 7461 312f 6375  tform/v1beta1/cu
+0001e630: 7374 6f6d 5f6a 6f62 2e70 726f 746f 234c  stom_job.proto#L
+0001e640: 3137 322e 0a20 2020 2072 756e 5f65 7661  172..    run_eva
+0001e650: 6c75 6174 696f 6e3a 2057 6865 7468 6572  luation: Whether
+0001e660: 2074 6f20 7275 6e20 6576 616c 7561 7469   to run evaluati
+0001e670: 6f6e 2073 7465 7073 2064 7572 696e 6720  on steps during 
+0001e680: 7472 6169 6e69 6e67 2e0a 2020 2020 6576  training..    ev
+0001e690: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+0001e6a0: 7265 6469 6374 5f6d 6163 6869 6e65 5f74  redict_machine_t
+0001e6b0: 7970 653a 2054 6865 2070 7265 6469 6374  ype: The predict
+0001e6c0: 696f 6e20 7365 7276 6572 206d 6163 6869  ion server machi
+0001e6d0: 6e65 2074 7970 6520 666f 7220 6261 7463  ne type for batc
+0001e6e0: 6820 7072 6564 6963 7420 636f 6d70 6f6e  h predict compon
+0001e6f0: 656e 7473 2064 7572 696e 6720 6576 616c  ents during eval
+0001e700: 7561 7469 6f6e 2e0a 2020 2020 6576 616c  uation..    eval
+0001e710: 7561 7469 6f6e 5f62 6174 6368 5f70 7265  uation_batch_pre
+0001e720: 6469 6374 5f73 7461 7274 696e 675f 7265  dict_starting_re
+0001e730: 706c 6963 615f 636f 756e 743a 2054 6865  plica_count: The
+0001e740: 2069 6e69 7469 616c 206e 756d 6265 7220   initial number 
+0001e750: 6f66 2070 7265 6469 6374 696f 6e20 7365  of prediction se
+0001e760: 7276 6572 2066 6f72 2062 6174 6368 2070  rver for batch p
+0001e770: 7265 6469 6374 2063 6f6d 706f 6e65 6e74  redict component
+0001e780: 7320 6475 7269 6e67 2065 7661 6c75 6174  s during evaluat
+0001e790: 696f 6e2e 0a20 2020 2065 7661 6c75 6174  ion..    evaluat
+0001e7a0: 696f 6e5f 6261 7463 685f 7072 6564 6963  ion_batch_predic
+0001e7b0: 745f 6d61 785f 7265 706c 6963 615f 636f  t_max_replica_co
+0001e7c0: 756e 743a 2054 6865 206d 6178 206e 756d  unt: The max num
+0001e7d0: 6265 7220 6f66 2070 7265 6469 6374 696f  ber of predictio
+0001e7e0: 6e20 7365 7276 6572 2066 6f72 2062 6174  n server for bat
+0001e7f0: 6368 2070 7265 6469 6374 2063 6f6d 706f  ch predict compo
+0001e800: 6e65 6e74 7320 6475 7269 6e67 2065 7661  nents during eva
+0001e810: 6c75 6174 696f 6e2e 0a20 2020 2065 7661  luation..    eva
+0001e820: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
+0001e830: 5f6d 6163 6869 6e65 5f74 7970 653a 2054  _machine_type: T
+0001e840: 6865 2064 6174 6166 6c6f 7720 6d61 6368  he dataflow mach
+0001e850: 696e 6520 7479 7065 2066 6f72 2065 7661  ine type for eva
+0001e860: 6c75 6174 696f 6e20 636f 6d70 6f6e 656e  luation componen
+0001e870: 7473 2e0a 2020 2020 6576 616c 7561 7469  ts..    evaluati
+0001e880: 6f6e 5f64 6174 6166 6c6f 775f 7374 6172  on_dataflow_star
+0001e890: 7469 6e67 5f6e 756d 5f77 6f72 6b65 7273  ting_num_workers
+0001e8a0: 3a20 5468 6520 696e 6974 6961 6c20 6e75  : The initial nu
+0001e8b0: 6d62 6572 206f 6620 4461 7461 666c 6f77  mber of Dataflow
+0001e8c0: 2077 6f72 6b65 7273 2066 6f72 2065 7661   workers for eva
+0001e8d0: 6c75 6174 696f 6e20 636f 6d70 6f6e 656e  luation componen
+0001e8e0: 7473 2e0a 2020 2020 6576 616c 7561 7469  ts..    evaluati
+0001e8f0: 6f6e 5f64 6174 6166 6c6f 775f 6d61 785f  on_dataflow_max_
+0001e900: 6e75 6d5f 776f 726b 6572 733a 2054 6865  num_workers: The
+0001e910: 206d 6178 206e 756d 6265 7220 6f66 2044   max number of D
+0001e920: 6174 6166 6c6f 7720 776f 726b 6572 7320  ataflow workers 
+0001e930: 666f 7220 6576 616c 7561 7469 6f6e 2063  for evaluation c
+0001e940: 6f6d 706f 6e65 6e74 732e 0a20 2020 2065  omponents..    e
+0001e950: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
+0001e960: 6f77 5f64 6973 6b5f 7369 7a65 5f67 623a  ow_disk_size_gb:
+0001e970: 2044 6174 6166 6c6f 7720 776f 726b 6572   Dataflow worker
+0001e980: 2773 2064 6973 6b20 7369 7a65 2069 6e20  's disk size in 
+0001e990: 4742 2066 6f72 2065 7661 6c75 6174 696f  GB for evaluatio
+0001e9a0: 6e20 636f 6d70 6f6e 656e 7473 2e0a 2020  n components..  
+0001e9b0: 2020 6461 7461 666c 6f77 5f73 6572 7669    dataflow_servi
+0001e9c0: 6365 5f61 6363 6f75 6e74 3a20 4375 7374  ce_account: Cust
+0001e9d0: 6f6d 2073 6572 7669 6365 2061 6363 6f75  om service accou
+0001e9e0: 6e74 2074 6f20 7275 6e20 6461 7461 666c  nt to run datafl
+0001e9f0: 6f77 206a 6f62 732e 0a20 2020 2064 6174  ow jobs..    dat
+0001ea00: 6166 6c6f 775f 7375 626e 6574 776f 726b  aflow_subnetwork
+0001ea10: 3a20 4461 7461 666c 6f77 2773 2066 756c  : Dataflow's ful
+0001ea20: 6c79 2071 7561 6c69 6669 6564 2073 7562  ly qualified sub
+0001ea30: 6e65 7477 6f72 6b20 6e61 6d65 2c20 7768  network name, wh
+0001ea40: 656e 2065 6d70 7479 2074 6865 2064 6566  en empty the def
+0001ea50: 6175 6c74 2073 7562 6e65 7477 6f72 6b20  ault subnetwork 
+0001ea60: 7769 6c6c 2062 6520 7573 6564 2e20 4578  will be used. Ex
+0001ea70: 616d 706c 653a 2068 7474 7073 3a2f 2f63  ample: https://c
+0001ea80: 6c6f 7564 2e67 6f6f 676c 652e 636f 6d2f  loud.google.com/
+0001ea90: 6461 7461 666c 6f77 2f64 6f63 732f 6775  dataflow/docs/gu
+0001eaa0: 6964 6573 2f73 7065 6369 6679 696e 672d  ides/specifying-
+0001eab0: 6e65 7477 6f72 6b73 2365 7861 6d70 6c65  networks#example
+0001eac0: 5f6e 6574 776f 726b 5f61 6e64 5f73 7562  _network_and_sub
+0001ead0: 6e65 7477 6f72 6b5f 7370 6563 6966 6963  network_specific
+0001eae0: 6174 696f 6e73 0a20 2020 2064 6174 6166  ations.    dataf
+0001eaf0: 6c6f 775f 7573 655f 7075 626c 6963 5f69  low_use_public_i
+0001eb00: 7073 3a20 5370 6563 6966 6965 7320 7768  ps: Specifies wh
+0001eb10: 6574 6865 7220 4461 7461 666c 6f77 2077  ether Dataflow w
+0001eb20: 6f72 6b65 7273 2075 7365 2070 7562 6c69  orkers use publi
+0001eb30: 6320 4950 2061 6464 7265 7373 6573 2e0a  c IP addresses..
+0001eb40: 2020 2020 656e 6372 7970 7469 6f6e 5f73      encryption_s
+0001eb50: 7065 635f 6b65 795f 6e61 6d65 3a20 5468  pec_key_name: Th
+0001eb60: 6520 4b4d 5320 6b65 7920 6e61 6d65 2e0a  e KMS key name..
+0001eb70: 0a20 2052 6574 7572 6e73 3a0a 2020 2020  .  Returns:.    
+0001eb80: 5475 706c 6520 6f66 2070 6970 656c 696e  Tuple of pipelin
+0001eb90: 655f 6465 6669 6e69 7469 6f6e 5f70 6174  e_definition_pat
+0001eba0: 6820 616e 6420 7061 7261 6d65 7465 725f  h and parameter_
+0001ebb0: 7661 6c75 6573 2e0a 2020 2222 220a 2020  values..  """.  
+0001ebc0: 2320 666d 743a 206f 6e0a 2020 6966 2069  # fmt: on.  if i
+0001ebd0: 7369 6e73 7461 6e63 6528 7466 5f61 7574  sinstance(tf_aut
+0001ebe0: 6f5f 7472 616e 7366 6f72 6d5f 6665 6174  o_transform_feat
+0001ebf0: 7572 6573 2c20 6c69 7374 293a 0a20 2020  ures, list):.   
+0001ec00: 2074 665f 6175 746f 5f74 7261 6e73 666f   tf_auto_transfo
+0001ec10: 726d 5f66 6561 7475 7265 7320 3d20 7b27  rm_features = {'
+0001ec20: 6175 746f 273a 2074 665f 6175 746f 5f74  auto': tf_auto_t
+0001ec30: 7261 6e73 666f 726d 5f66 6561 7475 7265  ransform_feature
+0001ec40: 737d 0a0a 2020 6966 2074 7261 6e73 666f  s}..  if transfo
+0001ec50: 726d 5f63 6f6e 6669 6720 616e 6420 7466  rm_config and tf
+0001ec60: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
+0001ec70: 5f70 6174 683a 0a20 2020 2072 6169 7365  _path:.    raise
+0001ec80: 2056 616c 7565 4572 726f 7228 0a20 2020   ValueError(.   
+0001ec90: 2020 2020 2027 4f6e 6c79 206f 6e65 206f       'Only one o
+0001eca0: 6620 7472 616e 7366 6f72 6d5f 636f 6e66  f transform_conf
+0001ecb0: 6967 2061 6e64 2074 665f 7472 616e 7366  ig and tf_transf
+0001ecc0: 6f72 6d61 7469 6f6e 735f 7061 7468 2063  ormations_path c
+0001ecd0: 616e 2027 0a20 2020 2020 2020 2027 6265  an '.        'be
+0001ece0: 2073 7065 6369 6669 6564 2e27 0a20 2020   specified.'.   
+0001ecf0: 2029 0a0a 2020 656c 6966 2074 7261 6e73   )..  elif trans
+0001ed00: 666f 726d 5f63 6f6e 6669 673a 0a20 2020  form_config:.   
+0001ed10: 2077 6172 6e69 6e67 732e 7761 726e 280a   warnings.warn(.
+0001ed20: 2020 2020 2020 2020 2774 7261 6e73 666f          'transfo
+0001ed30: 726d 5f63 6f6e 6669 6720 7061 7261 6d65  rm_config parame
+0001ed40: 7465 7220 6973 2064 6570 7265 6361 7465  ter is deprecate
+0001ed50: 642e 2027 0a20 2020 2020 2020 2027 506c  d. '.        'Pl
+0001ed60: 6561 7365 2075 7365 2074 6865 2066 6c61  ease use the fla
+0001ed70: 7474 656e 6564 2074 7261 6e73 666f 726d  ttened transform
+0001ed80: 2063 6f6e 6669 6720 6172 6775 6d65 6e74   config argument
+0001ed90: 7320 696e 7374 6561 642e 270a 2020 2020  s instead.'.    
+0001eda0: 290a 2020 2020 7466 5f74 7261 6e73 666f  ).    tf_transfo
+0001edb0: 726d 6174 696f 6e73 5f70 6174 6820 3d20  rmations_path = 
+0001edc0: 7472 616e 7366 6f72 6d5f 636f 6e66 6967  transform_config
+0001edd0: 0a0a 2020 6966 206e 6f74 2077 6f72 6b65  ..  if not worke
+0001ede0: 725f 706f 6f6c 5f73 7065 6373 5f6f 7665  r_pool_specs_ove
+0001edf0: 7272 6964 653a 0a20 2020 2077 6f72 6b65  rride:.    worke
+0001ee00: 725f 706f 6f6c 5f73 7065 6373 5f6f 7665  r_pool_specs_ove
+0001ee10: 7272 6964 6520 3d20 5b5d 0a0a 2020 7061  rride = []..  pa
+0001ee20: 7261 6d65 7465 725f 7661 6c75 6573 203d  rameter_values =
+0001ee30: 207b 7d0a 2020 7472 6169 6e69 6e67 5f61   {}.  training_a
+0001ee40: 6e64 5f65 7661 6c5f 7061 7261 6d65 7465  nd_eval_paramete
+0001ee50: 7273 203d 207b 0a20 2020 2020 2027 7072  rs = {.      'pr
+0001ee60: 6f6a 6563 7427 3a20 7072 6f6a 6563 742c  oject': project,
+0001ee70: 0a20 2020 2020 2027 6c6f 6361 7469 6f6e  .      'location
+0001ee80: 273a 206c 6f63 6174 696f 6e2c 0a20 2020  ': location,.   
+0001ee90: 2020 2027 726f 6f74 5f64 6972 273a 2072     'root_dir': r
+0001eea0: 6f6f 745f 6469 722c 0a20 2020 2020 2027  oot_dir,.      '
+0001eeb0: 7461 7267 6574 5f63 6f6c 756d 6e27 3a20  target_column': 
+0001eec0: 7461 7267 6574 5f63 6f6c 756d 6e2c 0a20  target_column,. 
+0001eed0: 2020 2020 2027 7072 6564 6963 7469 6f6e       'prediction
+0001eee0: 5f74 7970 6527 3a20 7072 6564 6963 7469  _type': predicti
+0001eef0: 6f6e 5f74 7970 652c 0a20 2020 2020 2027  on_type,.      '
+0001ef00: 6c65 6172 6e69 6e67 5f72 6174 6527 3a20  learning_rate': 
+0001ef10: 6c65 6172 6e69 6e67 5f72 6174 652c 0a20  learning_rate,. 
+0001ef20: 2020 2020 2027 6d61 785f 7374 6570 7327       'max_steps'
+0001ef30: 3a20 6d61 785f 7374 6570 732c 0a20 2020  : max_steps,.   
+0001ef40: 2020 2027 6d61 785f 7472 6169 6e5f 7365     'max_train_se
+0001ef50: 6373 273a 206d 6178 5f74 7261 696e 5f73  cs': max_train_s
+0001ef60: 6563 732c 0a20 2020 2020 2027 6c61 7267  ecs,.      'larg
+0001ef70: 655f 6361 7465 676f 7279 5f64 696d 273a  e_category_dim':
+0001ef80: 206c 6172 6765 5f63 6174 6567 6f72 795f   large_category_
+0001ef90: 6469 6d2c 0a20 2020 2020 2027 6c61 7267  dim,.      'larg
+0001efa0: 655f 6361 7465 676f 7279 5f74 6872 6573  e_category_thres
+0001efb0: 6827 3a20 6c61 7267 655f 6361 7465 676f  h': large_catego
+0001efc0: 7279 5f74 6872 6573 682c 0a20 2020 2020  ry_thresh,.     
+0001efd0: 2027 7965 6f5f 6a6f 686e 736f 6e5f 7472   'yeo_johnson_tr
+0001efe0: 616e 7366 6f72 6d27 3a20 7965 6f5f 6a6f  ansform': yeo_jo
+0001eff0: 686e 736f 6e5f 7472 616e 7366 6f72 6d2c  hnson_transform,
+0001f000: 0a20 2020 2020 2027 6665 6174 7572 655f  .      'feature_
+0001f010: 6469 6d27 3a20 6665 6174 7572 655f 6469  dim': feature_di
+0001f020: 6d2c 0a20 2020 2020 2027 6665 6174 7572  m,.      'featur
+0001f030: 655f 6469 6d5f 7261 7469 6f27 3a20 6665  e_dim_ratio': fe
+0001f040: 6174 7572 655f 6469 6d5f 7261 7469 6f2c  ature_dim_ratio,
+0001f050: 0a20 2020 2020 2027 6e75 6d5f 6465 6369  .      'num_deci
+0001f060: 7369 6f6e 5f73 7465 7073 273a 206e 756d  sion_steps': num
+0001f070: 5f64 6563 6973 696f 6e5f 7374 6570 732c  _decision_steps,
+0001f080: 0a20 2020 2020 2027 7265 6c61 7861 7469  .      'relaxati
+0001f090: 6f6e 5f66 6163 746f 7227 3a20 7265 6c61  on_factor': rela
+0001f0a0: 7861 7469 6f6e 5f66 6163 746f 722c 0a20  xation_factor,. 
+0001f0b0: 2020 2020 2027 6465 6361 795f 6576 6572       'decay_ever
+0001f0c0: 7927 3a20 6465 6361 795f 6576 6572 792c  y': decay_every,
+0001f0d0: 0a20 2020 2020 2027 6465 6361 795f 7261  .      'decay_ra
+0001f0e0: 7465 273a 2064 6563 6179 5f72 6174 652c  te': decay_rate,
+0001f0f0: 0a20 2020 2020 2027 6772 6164 6965 6e74  .      'gradient
+0001f100: 5f74 6872 6573 6827 3a20 6772 6164 6965  _thresh': gradie
+0001f110: 6e74 5f74 6872 6573 682c 0a20 2020 2020  nt_thresh,.     
+0001f120: 2027 7370 6172 7369 7479 5f6c 6f73 735f   'sparsity_loss_
+0001f130: 7765 6967 6874 273a 2073 7061 7273 6974  weight': sparsit
+0001f140: 795f 6c6f 7373 5f77 6569 6768 742c 0a20  y_loss_weight,. 
+0001f150: 2020 2020 2027 6261 7463 685f 6d6f 6d65       'batch_mome
+0001f160: 6e74 756d 273a 2062 6174 6368 5f6d 6f6d  ntum': batch_mom
+0001f170: 656e 7475 6d2c 0a20 2020 2020 2027 6261  entum,.      'ba
+0001f180: 7463 685f 7369 7a65 5f72 6174 696f 273a  tch_size_ratio':
+0001f190: 2062 6174 6368 5f73 697a 655f 7261 7469   batch_size_rati
+0001f1a0: 6f2c 0a20 2020 2020 2027 6e75 6d5f 7472  o,.      'num_tr
+0001f1b0: 616e 7366 6f72 6d65 725f 6c61 7965 7273  ansformer_layers
+0001f1c0: 273a 206e 756d 5f74 7261 6e73 666f 726d  ': num_transform
+0001f1d0: 6572 5f6c 6179 6572 732c 0a20 2020 2020  er_layers,.     
+0001f1e0: 2027 6e75 6d5f 7472 616e 7366 6f72 6d65   'num_transforme
+0001f1f0: 725f 6c61 7965 7273 5f72 6174 696f 273a  r_layers_ratio':
+0001f200: 206e 756d 5f74 7261 6e73 666f 726d 6572   num_transformer
+0001f210: 5f6c 6179 6572 735f 7261 7469 6f2c 0a20  _layers_ratio,. 
+0001f220: 2020 2020 2027 636c 6173 735f 7765 6967       'class_weig
+0001f230: 6874 273a 2063 6c61 7373 5f77 6569 6768  ht': class_weigh
+0001f240: 742c 0a20 2020 2020 2027 6c6f 7373 5f66  t,.      'loss_f
+0001f250: 756e 6374 696f 6e5f 7479 7065 273a 206c  unction_type': l
+0001f260: 6f73 735f 6675 6e63 7469 6f6e 5f74 7970  oss_function_typ
+0001f270: 652c 0a20 2020 2020 2027 616c 7068 615f  e,.      'alpha_
+0001f280: 666f 6361 6c5f 6c6f 7373 273a 2061 6c70  focal_loss': alp
+0001f290: 6861 5f66 6f63 616c 5f6c 6f73 732c 0a20  ha_focal_loss,. 
+0001f2a0: 2020 2020 2027 6761 6d6d 615f 666f 6361       'gamma_foca
+0001f2b0: 6c5f 6c6f 7373 273a 2067 616d 6d61 5f66  l_loss': gamma_f
+0001f2c0: 6f63 616c 5f6c 6f73 732c 0a20 2020 2020  ocal_loss,.     
+0001f2d0: 2027 656e 6162 6c65 5f70 726f 6669 6c65   'enable_profile
+0001f2e0: 7227 3a20 656e 6162 6c65 5f70 726f 6669  r': enable_profi
+0001f2f0: 6c65 722c 0a20 2020 2020 2027 6361 6368  ler,.      'cach
+0001f300: 655f 6461 7461 273a 2063 6163 6865 5f64  e_data': cache_d
+0001f310: 6174 612c 0a20 2020 2020 2027 7365 6564  ata,.      'seed
+0001f320: 273a 2073 6565 642c 0a20 2020 2020 2027  ': seed,.      '
+0001f330: 6576 616c 5f73 7465 7073 273a 2065 7661  eval_steps': eva
+0001f340: 6c5f 7374 6570 732c 0a20 2020 2020 2027  l_steps,.      '
+0001f350: 6261 7463 685f 7369 7a65 273a 2062 6174  batch_size': bat
+0001f360: 6368 5f73 697a 652c 0a20 2020 2020 2027  ch_size,.      '
+0001f370: 6d65 6173 7572 656d 656e 745f 7365 6c65  measurement_sele
+0001f380: 6374 696f 6e5f 7479 7065 273a 206d 6561  ction_type': mea
+0001f390: 7375 7265 6d65 6e74 5f73 656c 6563 7469  surement_selecti
+0001f3a0: 6f6e 5f74 7970 652c 0a20 2020 2020 2027  on_type,.      '
+0001f3b0: 6f70 7469 6d69 7a61 7469 6f6e 5f6d 6574  optimization_met
+0001f3c0: 7269 6327 3a20 6f70 7469 6d69 7a61 7469  ric': optimizati
+0001f3d0: 6f6e 5f6d 6574 7269 632c 0a20 2020 2020  on_metric,.     
+0001f3e0: 2027 6576 616c 5f66 7265 7175 656e 6379   'eval_frequency
+0001f3f0: 5f73 6563 7327 3a20 6576 616c 5f66 7265  _secs': eval_fre
+0001f400: 7175 656e 6379 5f73 6563 732c 0a20 2020  quency_secs,.   
+0001f410: 2020 2027 7765 6967 6874 5f63 6f6c 756d     'weight_colum
+0001f420: 6e27 3a20 7765 6967 6874 5f63 6f6c 756d  n': weight_colum
+0001f430: 6e2c 0a20 2020 2020 2027 7472 616e 7366  n,.      'transf
+0001f440: 6f72 6d5f 6461 7461 666c 6f77 5f6d 6163  orm_dataflow_mac
+0001f450: 6869 6e65 5f74 7970 6527 3a20 7472 616e  hine_type': tran
+0001f460: 7366 6f72 6d5f 6461 7461 666c 6f77 5f6d  sform_dataflow_m
+0001f470: 6163 6869 6e65 5f74 7970 652c 0a20 2020  achine_type,.   
+0001f480: 2020 2027 7472 616e 7366 6f72 6d5f 6461     'transform_da
+0001f490: 7461 666c 6f77 5f6d 6178 5f6e 756d 5f77  taflow_max_num_w
+0001f4a0: 6f72 6b65 7273 273a 2074 7261 6e73 666f  orkers': transfo
+0001f4b0: 726d 5f64 6174 6166 6c6f 775f 6d61 785f  rm_dataflow_max_
+0001f4c0: 6e75 6d5f 776f 726b 6572 732c 0a20 2020  num_workers,.   
+0001f4d0: 2020 2027 7472 616e 7366 6f72 6d5f 6461     'transform_da
+0001f4e0: 7461 666c 6f77 5f64 6973 6b5f 7369 7a65  taflow_disk_size
+0001f4f0: 5f67 6227 3a20 7472 616e 7366 6f72 6d5f  _gb': transform_
+0001f500: 6461 7461 666c 6f77 5f64 6973 6b5f 7369  dataflow_disk_si
+0001f510: 7a65 5f67 622c 0a20 2020 2020 2027 776f  ze_gb,.      'wo
+0001f520: 726b 6572 5f70 6f6f 6c5f 7370 6563 735f  rker_pool_specs_
+0001f530: 6f76 6572 7269 6465 273a 2077 6f72 6b65  override': worke
+0001f540: 725f 706f 6f6c 5f73 7065 6373 5f6f 7665  r_pool_specs_ove
+0001f550: 7272 6964 652c 0a20 2020 2020 2027 7275  rride,.      'ru
+0001f560: 6e5f 6576 616c 7561 7469 6f6e 273a 2072  n_evaluation': r
+0001f570: 756e 5f65 7661 6c75 6174 696f 6e2c 0a20  un_evaluation,. 
+0001f580: 2020 2020 2027 6576 616c 7561 7469 6f6e       'evaluation
+0001f590: 5f62 6174 6368 5f70 7265 6469 6374 5f6d  _batch_predict_m
+0001f5a0: 6163 6869 6e65 5f74 7970 6527 3a20 280a  achine_type': (.
+0001f5b0: 2020 2020 2020 2020 2020 6576 616c 7561            evalua
+0001f5c0: 7469 6f6e 5f62 6174 6368 5f70 7265 6469  tion_batch_predi
+0001f5d0: 6374 5f6d 6163 6869 6e65 5f74 7970 650a  ct_machine_type.
+0001f5e0: 2020 2020 2020 292c 0a20 2020 2020 2027        ),.      '
+0001f5f0: 6576 616c 7561 7469 6f6e 5f62 6174 6368  evaluation_batch
+0001f600: 5f70 7265 6469 6374 5f73 7461 7274 696e  _predict_startin
+0001f610: 675f 7265 706c 6963 615f 636f 756e 7427  g_replica_count'
+0001f620: 3a20 280a 2020 2020 2020 2020 2020 6576  : (.          ev
+0001f630: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+0001f640: 7265 6469 6374 5f73 7461 7274 696e 675f  redict_starting_
+0001f650: 7265 706c 6963 615f 636f 756e 740a 2020  replica_count.  
+0001f660: 2020 2020 292c 0a20 2020 2020 2027 6576      ),.      'ev
+0001f670: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+0001f680: 7265 6469 6374 5f6d 6178 5f72 6570 6c69  redict_max_repli
+0001f690: 6361 5f63 6f75 6e74 273a 2028 0a20 2020  ca_count': (.   
+0001f6a0: 2020 2020 2020 2065 7661 6c75 6174 696f         evaluatio
+0001f6b0: 6e5f 6261 7463 685f 7072 6564 6963 745f  n_batch_predict_
+0001f6c0: 6d61 785f 7265 706c 6963 615f 636f 756e  max_replica_coun
+0001f6d0: 740a 2020 2020 2020 292c 0a20 2020 2020  t.      ),.     
+0001f6e0: 2027 6576 616c 7561 7469 6f6e 5f64 6174   'evaluation_dat
+0001f6f0: 6166 6c6f 775f 6d61 6368 696e 655f 7479  aflow_machine_ty
+0001f700: 7065 273a 2065 7661 6c75 6174 696f 6e5f  pe': evaluation_
+0001f710: 6461 7461 666c 6f77 5f6d 6163 6869 6e65  dataflow_machine
+0001f720: 5f74 7970 652c 0a20 2020 2020 2027 6576  _type,.      'ev
+0001f730: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
+0001f740: 775f 7374 6172 7469 6e67 5f6e 756d 5f77  w_starting_num_w
+0001f750: 6f72 6b65 7273 273a 2028 0a20 2020 2020  orkers': (.     
+0001f760: 2020 2020 2065 7661 6c75 6174 696f 6e5f       evaluation_
+0001f770: 6461 7461 666c 6f77 5f73 7461 7274 696e  dataflow_startin
+0001f780: 675f 6e75 6d5f 776f 726b 6572 730a 2020  g_num_workers.  
+0001f790: 2020 2020 292c 0a20 2020 2020 2027 6576      ),.      'ev
+0001f7a0: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
+0001f7b0: 775f 6d61 785f 6e75 6d5f 776f 726b 6572  w_max_num_worker
+0001f7c0: 7327 3a20 280a 2020 2020 2020 2020 2020  s': (.          
+0001f7d0: 6576 616c 7561 7469 6f6e 5f64 6174 6166  evaluation_dataf
+0001f7e0: 6c6f 775f 6d61 785f 6e75 6d5f 776f 726b  low_max_num_work
+0001f7f0: 6572 730a 2020 2020 2020 292c 0a20 2020  ers.      ),.   
+0001f800: 2020 2027 6576 616c 7561 7469 6f6e 5f64     'evaluation_d
+0001f810: 6174 6166 6c6f 775f 6469 736b 5f73 697a  ataflow_disk_siz
+0001f820: 655f 6762 273a 2065 7661 6c75 6174 696f  e_gb': evaluatio
+0001f830: 6e5f 6461 7461 666c 6f77 5f64 6973 6b5f  n_dataflow_disk_
+0001f840: 7369 7a65 5f67 622c 0a20 2020 2020 2027  size_gb,.      '
+0001f850: 6461 7461 666c 6f77 5f73 6572 7669 6365  dataflow_service
+0001f860: 5f61 6363 6f75 6e74 273a 2064 6174 6166  _account': dataf
+0001f870: 6c6f 775f 7365 7276 6963 655f 6163 636f  low_service_acco
+0001f880: 756e 742c 0a20 2020 2020 2027 6461 7461  unt,.      'data
+0001f890: 666c 6f77 5f73 7562 6e65 7477 6f72 6b27  flow_subnetwork'
+0001f8a0: 3a20 6461 7461 666c 6f77 5f73 7562 6e65  : dataflow_subne
+0001f8b0: 7477 6f72 6b2c 0a20 2020 2020 2027 6461  twork,.      'da
+0001f8c0: 7461 666c 6f77 5f75 7365 5f70 7562 6c69  taflow_use_publi
+0001f8d0: 635f 6970 7327 3a20 6461 7461 666c 6f77  c_ips': dataflow
+0001f8e0: 5f75 7365 5f70 7562 6c69 635f 6970 732c  _use_public_ips,
+0001f8f0: 0a20 2020 2020 2027 656e 6372 7970 7469  .      'encrypti
+0001f900: 6f6e 5f73 7065 635f 6b65 795f 6e61 6d65  on_spec_key_name
+0001f910: 273a 2065 6e63 7279 7074 696f 6e5f 7370  ': encryption_sp
+0001f920: 6563 5f6b 6579 5f6e 616d 652c 0a20 207d  ec_key_name,.  }
+0001f930: 0a20 205f 7570 6461 7465 5f70 6172 616d  .  _update_param
+0001f940: 6574 6572 7328 7061 7261 6d65 7465 725f  eters(parameter_
+0001f950: 7661 6c75 6573 2c20 7472 6169 6e69 6e67  values, training
+0001f960: 5f61 6e64 5f65 7661 6c5f 7061 7261 6d65  _and_eval_parame
+0001f970: 7465 7273 290a 0a20 2066 7465 5f70 6172  ters)..  fte_par
+0001f980: 616d 7320 3d20 7b0a 2020 2020 2020 2764  ams = {.      'd
+0001f990: 6174 6173 6574 5f6c 6576 656c 5f63 7573  ataset_level_cus
+0001f9a0: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
+0001f9b0: 6f6e 5f64 6566 696e 6974 696f 6e73 273a  on_definitions':
+0001f9c0: 2028 0a20 2020 2020 2020 2020 2064 6174   (.          dat
+0001f9d0: 6173 6574 5f6c 6576 656c 5f63 7573 746f  aset_level_custo
+0001f9e0: 6d5f 7472 616e 7366 6f72 6d61 7469 6f6e  m_transformation
+0001f9f0: 5f64 6566 696e 6974 696f 6e73 0a20 2020  _definitions.   
+0001fa00: 2020 2020 2020 2069 6620 6461 7461 7365         if datase
+0001fa10: 745f 6c65 7665 6c5f 6375 7374 6f6d 5f74  t_level_custom_t
+0001fa20: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
+0001fa30: 6669 6e69 7469 6f6e 730a 2020 2020 2020  finitions.      
+0001fa40: 2020 2020 656c 7365 205b 5d0a 2020 2020      else [].    
+0001fa50: 2020 292c 0a20 2020 2020 2027 6461 7461    ),.      'data
+0001fa60: 7365 745f 6c65 7665 6c5f 7472 616e 7366  set_level_transf
+0001fa70: 6f72 6d61 7469 6f6e 7327 3a20 280a 2020  ormations': (.  
+0001fa80: 2020 2020 2020 2020 6461 7461 7365 745f          dataset_
+0001fa90: 6c65 7665 6c5f 7472 616e 7366 6f72 6d61  level_transforma
+0001faa0: 7469 6f6e 7320 6966 2064 6174 6173 6574  tions if dataset
+0001fab0: 5f6c 6576 656c 5f74 7261 6e73 666f 726d  _level_transform
+0001fac0: 6174 696f 6e73 2065 6c73 6520 5b5d 0a20  ations else []. 
+0001fad0: 2020 2020 2029 2c0a 2020 2020 2020 2772       ),.      'r
+0001fae0: 756e 5f66 6561 7475 7265 5f73 656c 6563  un_feature_selec
+0001faf0: 7469 6f6e 273a 2072 756e 5f66 6561 7475  tion': run_featu
+0001fb00: 7265 5f73 656c 6563 7469 6f6e 2c0a 2020  re_selection,.  
+0001fb10: 2020 2020 2766 6561 7475 7265 5f73 656c      'feature_sel
+0001fb20: 6563 7469 6f6e 5f61 6c67 6f72 6974 686d  ection_algorithm
+0001fb30: 273a 2066 6561 7475 7265 5f73 656c 6563  ': feature_selec
+0001fb40: 7469 6f6e 5f61 6c67 6f72 6974 686d 2c0a  tion_algorithm,.
+0001fb50: 2020 2020 2020 276d 6178 5f73 656c 6563        'max_selec
+0001fb60: 7465 645f 6665 6174 7572 6573 273a 206d  ted_features': m
+0001fb70: 6178 5f73 656c 6563 7465 645f 6665 6174  ax_selected_feat
+0001fb80: 7572 6573 2c0a 2020 2020 2020 2770 7265  ures,.      'pre
+0001fb90: 6465 6669 6e65 645f 7370 6c69 745f 6b65  defined_split_ke
+0001fba0: 7927 3a20 7072 6564 6566 696e 6564 5f73  y': predefined_s
+0001fbb0: 706c 6974 5f6b 6579 2c0a 2020 2020 2020  plit_key,.      
+0001fbc0: 2773 7472 6174 6966 6965 645f 7370 6c69  'stratified_spli
+0001fbd0: 745f 6b65 7927 3a20 7374 7261 7469 6669  t_key': stratifi
+0001fbe0: 6564 5f73 706c 6974 5f6b 6579 2c0a 2020  ed_split_key,.  
+0001fbf0: 2020 2020 2774 7261 696e 696e 675f 6672      'training_fr
+0001fc00: 6163 7469 6f6e 273a 2074 7261 696e 696e  action': trainin
+0001fc10: 675f 6672 6163 7469 6f6e 2c0a 2020 2020  g_fraction,.    
+0001fc20: 2020 2776 616c 6964 6174 696f 6e5f 6672    'validation_fr
+0001fc30: 6163 7469 6f6e 273a 2076 616c 6964 6174  action': validat
+0001fc40: 696f 6e5f 6672 6163 7469 6f6e 2c0a 2020  ion_fraction,.  
+0001fc50: 2020 2020 2774 6573 745f 6672 6163 7469      'test_fracti
+0001fc60: 6f6e 273a 2074 6573 745f 6672 6163 7469  on': test_fracti
+0001fc70: 6f6e 2c0a 2020 2020 2020 2774 665f 6175  on,.      'tf_au
+0001fc80: 746f 5f74 7261 6e73 666f 726d 5f66 6561  to_transform_fea
+0001fc90: 7475 7265 7327 3a20 280a 2020 2020 2020  tures': (.      
+0001fca0: 2020 2020 7466 5f61 7574 6f5f 7472 616e      tf_auto_tran
+0001fcb0: 7366 6f72 6d5f 6665 6174 7572 6573 2069  sform_features i
+0001fcc0: 6620 7466 5f61 7574 6f5f 7472 616e 7366  f tf_auto_transf
+0001fcd0: 6f72 6d5f 6665 6174 7572 6573 2065 6c73  orm_features els
+0001fce0: 6520 7b7d 0a20 2020 2020 2029 2c0a 2020  e {}.      ),.  
+0001fcf0: 2020 2020 2774 665f 6375 7374 6f6d 5f74      'tf_custom_t
+0001fd00: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
+0001fd10: 6669 6e69 7469 6f6e 7327 3a20 280a 2020  finitions': (.  
+0001fd20: 2020 2020 2020 2020 7466 5f63 7573 746f          tf_custo
+0001fd30: 6d5f 7472 616e 7366 6f72 6d61 7469 6f6e  m_transformation
+0001fd40: 5f64 6566 696e 6974 696f 6e73 0a20 2020  _definitions.   
+0001fd50: 2020 2020 2020 2069 6620 7466 5f63 7573         if tf_cus
+0001fd60: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
+0001fd70: 6f6e 5f64 6566 696e 6974 696f 6e73 0a20  on_definitions. 
+0001fd80: 2020 2020 2020 2020 2065 6c73 6520 5b5d           else []
+0001fd90: 0a20 2020 2020 2029 2c0a 2020 2020 2020  .      ),.      
+0001fda0: 2774 665f 7472 616e 7366 6f72 6d61 7469  'tf_transformati
+0001fdb0: 6f6e 735f 7061 7468 273a 2074 665f 7472  ons_path': tf_tr
+0001fdc0: 616e 7366 6f72 6d61 7469 6f6e 735f 7061  ansformations_pa
+0001fdd0: 7468 2c0a 2020 2020 2020 276d 6174 6572  th,.      'mater
+0001fde0: 6961 6c69 7a65 645f 6578 616d 706c 6573  ialized_examples
+0001fdf0: 5f66 6f72 6d61 7427 3a20 280a 2020 2020  _format': (.    
+0001fe00: 2020 2020 2020 6d61 7465 7269 616c 697a        materializ
+0001fe10: 6564 5f65 7861 6d70 6c65 735f 666f 726d  ed_examples_form
+0001fe20: 6174 0a20 2020 2020 2020 2020 2069 6620  at.          if 
+0001fe30: 6d61 7465 7269 616c 697a 6564 5f65 7861  materialized_exa
+0001fe40: 6d70 6c65 735f 666f 726d 6174 0a20 2020  mples_format.   
+0001fe50: 2020 2020 2020 2065 6c73 6520 2774 6672         else 'tfr
+0001fe60: 6563 6f72 6473 5f67 7a69 7027 0a20 2020  ecords_gzip'.   
+0001fe70: 2020 2029 2c0a 2020 2020 2020 2774 665f     ),.      'tf_
+0001fe80: 7472 616e 7366 6f72 6d5f 6578 6563 7574  transform_execut
+0001fe90: 696f 6e5f 656e 6769 6e65 273a 2028 0a20  ion_engine': (. 
+0001fea0: 2020 2020 2020 2020 2074 665f 7472 616e           tf_tran
+0001feb0: 7366 6f72 6d5f 6578 6563 7574 696f 6e5f  sform_execution_
+0001fec0: 656e 6769 6e65 0a20 2020 2020 2020 2020  engine.         
+0001fed0: 2069 6620 7466 5f74 7261 6e73 666f 726d   if tf_transform
+0001fee0: 5f65 7865 6375 7469 6f6e 5f65 6e67 696e  _execution_engin
+0001fef0: 650a 2020 2020 2020 2020 2020 656c 7365  e.          else
+0001ff00: 2027 6461 7461 666c 6f77 270a 2020 2020   'dataflow'.    
+0001ff10: 2020 292c 0a20 207d 0a20 205f 7570 6461    ),.  }.  _upda
+0001ff20: 7465 5f70 6172 616d 6574 6572 7328 7061  te_parameters(pa
+0001ff30: 7261 6d65 7465 725f 7661 6c75 6573 2c20  rameter_values, 
+0001ff40: 6674 655f 7061 7261 6d73 290a 0a20 2064  fte_params)..  d
+0001ff50: 6174 615f 736f 7572 6365 5f61 6e64 5f73  ata_source_and_s
+0001ff60: 706c 6974 5f70 6172 616d 6574 6572 7320  plit_parameters 
+0001ff70: 3d20 7b0a 2020 2020 2020 2764 6174 615f  = {.      'data_
+0001ff80: 736f 7572 6365 5f63 7376 5f66 696c 656e  source_csv_filen
+0001ff90: 616d 6573 273a 2064 6174 615f 736f 7572  ames': data_sour
+0001ffa0: 6365 5f63 7376 5f66 696c 656e 616d 6573  ce_csv_filenames
+0001ffb0: 2c0a 2020 2020 2020 2764 6174 615f 736f  ,.      'data_so
+0001ffc0: 7572 6365 5f62 6967 7175 6572 795f 7461  urce_bigquery_ta
+0001ffd0: 626c 655f 7061 7468 273a 2064 6174 615f  ble_path': data_
+0001ffe0: 736f 7572 6365 5f62 6967 7175 6572 795f  source_bigquery_
+0001fff0: 7461 626c 655f 7061 7468 2c0a 2020 2020  table_path,.    
+00020000: 2020 2762 6967 7175 6572 795f 7374 6167    'bigquery_stag
+00020010: 696e 675f 6675 6c6c 5f64 6174 6173 6574  ing_full_dataset
+00020020: 5f69 6427 3a20 6269 6771 7565 7279 5f73  _id': bigquery_s
+00020030: 7461 6769 6e67 5f66 756c 6c5f 6461 7461  taging_full_data
+00020040: 7365 745f 6964 2c0a 2020 7d0a 2020 5f75  set_id,.  }.  _u
+00020050: 7064 6174 655f 7061 7261 6d65 7465 7273  pdate_parameters
+00020060: 2870 6172 616d 6574 6572 5f76 616c 7565  (parameter_value
+00020070: 732c 2064 6174 615f 736f 7572 6365 5f61  s, data_source_a
+00020080: 6e64 5f73 706c 6974 5f70 6172 616d 6574  nd_split_paramet
+00020090: 6572 7329 0a0a 2020 7069 7065 6c69 6e65  ers)..  pipeline
+000200a0: 5f64 6566 696e 6974 696f 6e5f 7061 7468  _definition_path
+000200b0: 203d 206f 732e 7061 7468 2e6a 6f69 6e28   = os.path.join(
+000200c0: 0a20 2020 2020 2070 6174 686c 6962 2e50  .      pathlib.P
+000200d0: 6174 6828 5f5f 6669 6c65 5f5f 292e 7061  ath(__file__).pa
+000200e0: 7265 6e74 2e72 6573 6f6c 7665 2829 2c20  rent.resolve(), 
+000200f0: 2774 6162 6e65 745f 7472 6169 6e65 725f  'tabnet_trainer_
+00020100: 7069 7065 6c69 6e65 2e79 616d 6c27 0a20  pipeline.yaml'. 
+00020110: 2029 0a0a 2020 7265 7475 726e 2070 6970   )..  return pip
+00020120: 656c 696e 655f 6465 6669 6e69 7469 6f6e  eline_definition
+00020130: 5f70 6174 682c 2070 6172 616d 6574 6572  _path, parameter
+00020140: 5f76 616c 7565 730a 0a0a 6465 6620 6765  _values...def ge
+00020150: 745f 7461 626e 6574 5f73 7475 6479 5f73  t_tabnet_study_s
+00020160: 7065 635f 7061 7261 6d65 7465 7273 5f6f  pec_parameters_o
+00020170: 7665 7272 6964 6528 0a20 2020 2064 6174  verride(.    dat
+00020180: 6173 6574 5f73 697a 655f 6275 636b 6574  aset_size_bucket
+00020190: 3a20 7374 722c 2070 7265 6469 6374 696f  : str, predictio
+000201a0: 6e5f 7479 7065 3a20 7374 722c 2074 7261  n_type: str, tra
+000201b0: 696e 696e 675f 6275 6467 6574 5f62 7563  ining_budget_buc
+000201c0: 6b65 743a 2073 7472 0a29 202d 3e20 4c69  ket: str.) -> Li
+000201d0: 7374 5b44 6963 745b 7374 722c 2041 6e79  st[Dict[str, Any
+000201e0: 5d5d 3a0a 2020 2222 2247 6574 2073 7475  ]]:.  """Get stu
+000201f0: 6479 5f73 7065 635f 7061 7261 6d65 7465  dy_spec_paramete
+00020200: 7273 5f6f 7665 7272 6964 6520 666f 7220  rs_override for 
+00020210: 6120 5461 624e 6574 2068 7970 6572 7061  a TabNet hyperpa
+00020220: 7261 6d65 7465 7220 7475 6e69 6e67 206a  rameter tuning j
+00020230: 6f62 2e0a 0a20 2041 7267 733a 0a20 2020  ob...  Args:.   
+00020240: 2064 6174 6173 6574 5f73 697a 655f 6275   dataset_size_bu
+00020250: 636b 6574 3a20 5369 7a65 206f 6620 7468  cket: Size of th
+00020260: 6520 6461 7461 7365 742e 204f 6e65 206f  e dataset. One o
+00020270: 6620 2273 6d61 6c6c 2220 283c 2031 4d20  f "small" (< 1M 
+00020280: 726f 7773 292c 0a20 2020 2020 2022 6d65  rows),.      "me
+00020290: 6469 756d 2220 2831 4d20 2d20 3130 304d  dium" (1M - 100M
+000202a0: 2072 6f77 7329 2c20 6f72 2022 6c61 7267   rows), or "larg
+000202b0: 6522 2028 3e20 3130 304d 2072 6f77 7329  e" (> 100M rows)
+000202c0: 2e0a 2020 2020 7072 6564 6963 7469 6f6e  ..    prediction
+000202d0: 5f74 7970 653a 2054 6865 2074 7970 6520  _type: The type 
+000202e0: 6f66 2070 7265 6469 6374 696f 6e20 7468  of prediction th
+000202f0: 6520 6d6f 6465 6c20 6973 2074 6f20 7072  e model is to pr
+00020300: 6f64 7563 652e 0a20 2020 2020 2022 636c  oduce..      "cl
+00020310: 6173 7369 6669 6361 7469 6f6e 2220 6f72  assification" or
+00020320: 2022 7265 6772 6573 7369 6f6e 222e 0a20   "regression".. 
+00020330: 2020 2074 7261 696e 696e 675f 6275 6467     training_budg
+00020340: 6574 5f62 7563 6b65 743a 2042 7563 6b65  et_bucket: Bucke
+00020350: 7420 6f66 2074 6865 2065 7374 696d 6174  t of the estimat
+00020360: 6564 2074 7261 696e 696e 6720 6275 6467  ed training budg
+00020370: 6574 2e20 4f6e 6520 6f66 0a20 2020 2020  et. One of.     
+00020380: 2022 736d 616c 6c22 2028 3c20 2436 3030   "small" (< $600
+00020390: 292c 2022 6d65 6469 756d 2220 2824 3630  ), "medium" ($60
+000203a0: 3020 2d20 2432 3430 3029 2c20 6f72 2022  0 - $2400), or "
+000203b0: 6c61 7267 6522 2028 3e20 2432 3430 3029  large" (> $2400)
+000203c0: 2e20 5468 6973 0a20 2020 2020 2070 6172  . This.      par
+000203d0: 616d 6574 6572 2069 7320 6f6e 6c79 2075  ameter is only u
+000203e0: 7365 6420 6173 2061 2068 696e 7420 666f  sed as a hint fo
+000203f0: 7220 7468 6520 6879 7065 7270 6172 616d  r the hyperparam
+00020400: 6574 6572 2073 6561 7263 6820 7370 6163  eter search spac
+00020410: 652c 0a20 2020 2020 2075 6e72 656c 6174  e,.      unrelat
+00020420: 6564 2074 6f20 7468 6520 7265 616c 2063  ed to the real c
+00020430: 6f73 742e 0a0a 2020 5265 7475 726e 733a  ost...  Returns:
+00020440: 0a20 2020 204c 6973 7420 6f66 2073 7475  .    List of stu
+00020450: 6479 5f73 7065 635f 7061 7261 6d65 7465  dy_spec_paramete
+00020460: 7273 5f6f 7665 7272 6964 652e 0a20 2022  rs_override..  "
+00020470: 2222 0a0a 2020 6966 2064 6174 6173 6574  ""..  if dataset
+00020480: 5f73 697a 655f 6275 636b 6574 206e 6f74  _size_bucket not
+00020490: 2069 6e20 5b27 736d 616c 6c27 2c20 276d   in ['small', 'm
+000204a0: 6564 6975 6d27 2c20 276c 6172 6765 275d  edium', 'large']
+000204b0: 3a0a 2020 2020 7261 6973 6520 5661 6c75  :.    raise Valu
+000204c0: 6545 7272 6f72 280a 2020 2020 2020 2020  eError(.        
+000204d0: 2749 6e76 616c 6964 2064 6174 6173 6574  'Invalid dataset
+000204e0: 5f73 697a 655f 6275 636b 6574 2070 726f  _size_bucket pro
+000204f0: 7669 6465 642e 2053 7570 706f 7274 6564  vided. Supported
+00020500: 2076 616c 7565 7320 270a 2020 2020 2020   values '.      
+00020510: 2020 2720 6172 6520 2273 6d61 6c6c 222c    ' are "small",
+00020520: 2022 6d65 6469 756d 2220 6f72 2022 6c61   "medium" or "la
+00020530: 7267 6522 2e27 0a20 2020 2029 0a20 2069  rge".'.    ).  i
+00020540: 6620 7472 6169 6e69 6e67 5f62 7564 6765  f training_budge
+00020550: 745f 6275 636b 6574 206e 6f74 2069 6e20  t_bucket not in 
+00020560: 5b27 736d 616c 6c27 2c20 276d 6564 6975  ['small', 'mediu
+00020570: 6d27 2c20 276c 6172 6765 275d 3a0a 2020  m', 'large']:.  
+00020580: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
+00020590: 6f72 280a 2020 2020 2020 2020 2749 6e76  or(.        'Inv
+000205a0: 616c 6964 2074 7261 696e 696e 675f 6275  alid training_bu
+000205b0: 6467 6574 5f62 7563 6b65 7420 7072 6f76  dget_bucket prov
+000205c0: 6964 6564 2e20 5375 7070 6f72 7465 6420  ided. Supported 
+000205d0: 7661 6c75 6573 2027 0a20 2020 2020 2020  values '.       
+000205e0: 2027 6172 6520 2273 6d61 6c6c 222c 2022   'are "small", "
+000205f0: 6d65 6469 756d 2220 6f72 2022 6c61 7267  medium" or "larg
+00020600: 6522 2e27 0a20 2020 2029 0a0a 2020 7061  e".'.    )..  pa
+00020610: 7261 6d5f 7061 7468 203d 206f 732e 7061  ram_path = os.pa
+00020620: 7468 2e6a 6f69 6e28 0a20 2020 2020 2070  th.join(.      p
+00020630: 6174 686c 6962 2e50 6174 6828 5f5f 6669  athlib.Path(__fi
+00020640: 6c65 5f5f 292e 7061 7265 6e74 2e72 6573  le__).parent.res
+00020650: 6f6c 7665 2829 2c0a 2020 2020 2020 6627  olve(),.      f'
+00020660: 636f 6e66 6967 732f 7461 626e 6574 5f70  configs/tabnet_p
+00020670: 6172 616d 735f 7b64 6174 6173 6574 5f73  arams_{dataset_s
+00020680: 697a 655f 6275 636b 6574 7d5f 6461 7461  ize_bucket}_data
+00020690: 5f7b 7472 6169 6e69 6e67 5f62 7564 6765  _{training_budge
+000206a0: 745f 6275 636b 6574 7d5f 7365 6172 6368  t_bucket}_search
+000206b0: 5f73 7061 6365 2e6a 736f 6e27 2c0a 2020  _space.json',.  
+000206c0: 290a 2020 7769 7468 206f 7065 6e28 7061  ).  with open(pa
+000206d0: 7261 6d5f 7061 7468 2c20 2772 2729 2061  ram_path, 'r') a
+000206e0: 7320 663a 0a20 2020 2070 6172 616d 5f63  s f:.    param_c
+000206f0: 6f6e 7465 6e74 203d 2066 2e72 6561 6428  ontent = f.read(
+00020700: 290a 2020 2020 7061 7261 6d73 203d 206a  ).    params = j
+00020710: 736f 6e2e 6c6f 6164 7328 7061 7261 6d5f  son.loads(param_
+00020720: 636f 6e74 656e 7429 0a0a 2020 6966 2070  content)..  if p
+00020730: 7265 6469 6374 696f 6e5f 7479 7065 203d  rediction_type =
+00020740: 3d20 2772 6567 7265 7373 696f 6e27 3a0a  = 'regression':.
+00020750: 2020 2020 7265 7475 726e 205f 666f 726d      return _form
+00020760: 6174 5f74 6162 6e65 745f 7265 6772 6573  at_tabnet_regres
+00020770: 7369 6f6e 5f73 7475 6479 5f73 7065 635f  sion_study_spec_
+00020780: 7061 7261 6d65 7465 7273 5f6f 7665 7272  parameters_overr
+00020790: 6964 6528 0a20 2020 2020 2020 2070 6172  ide(.        par
+000207a0: 616d 732c 2074 7261 696e 696e 675f 6275  ams, training_bu
+000207b0: 6467 6574 5f62 7563 6b65 740a 2020 2020  dget_bucket.    
+000207c0: 290a 2020 7265 7475 726e 2070 6172 616d  ).  return param
+000207d0: 730a 0a0a 6465 6620 5f66 6f72 6d61 745f  s...def _format_
+000207e0: 7461 626e 6574 5f72 6567 7265 7373 696f  tabnet_regressio
+000207f0: 6e5f 7374 7564 795f 7370 6563 5f70 6172  n_study_spec_par
+00020800: 616d 6574 6572 735f 6f76 6572 7269 6465  ameters_override
+00020810: 280a 2020 2020 7061 7261 6d73 3a20 4c69  (.    params: Li
+00020820: 7374 5b44 6963 745b 7374 722c 2041 6e79  st[Dict[str, Any
+00020830: 5d5d 2c20 7472 6169 6e69 6e67 5f62 7564  ]], training_bud
+00020840: 6765 745f 6275 636b 6574 3a20 7374 720a  get_bucket: str.
+00020850: 2920 2d3e 204c 6973 745b 4469 6374 5b73  ) -> List[Dict[s
+00020860: 7472 2c20 416e 795d 5d3a 0a20 2022 2222  tr, Any]]:.  """
+00020870: 4765 7420 7265 6772 6573 7369 6f6e 2073  Get regression s
+00020880: 7475 6479 5f73 7065 635f 7061 7261 6d65  tudy_spec_parame
+00020890: 7465 7273 5f6f 7665 7272 6964 6520 666f  ters_override fo
+000208a0: 7220 6120 5461 624e 6574 2068 7970 6572  r a TabNet hyper
+000208b0: 7061 7261 6d65 7465 7220 7475 6e69 6e67  parameter tuning
+000208c0: 206a 6f62 2e0a 0a20 2041 7267 733a 0a20   job...  Args:. 
+000208d0: 2020 2070 6172 616d 733a 204c 6973 7420     params: List 
+000208e0: 6f66 2064 6963 7469 6f6e 6172 6965 7320  of dictionaries 
+000208f0: 7265 7072 6573 656e 7469 6e67 2070 6172  representing par
+00020900: 616d 6574 6572 7320 746f 206f 7074 696d  ameters to optim
+00020910: 697a 652e 2054 6865 0a20 2020 2020 2064  ize. The.      d
+00020920: 6963 7469 6f6e 6172 7920 6b65 7920 6973  ictionary key is
+00020930: 2074 6865 2070 6172 616d 6574 6572 5f69   the parameter_i
+00020940: 642c 2077 6869 6368 2069 7320 7061 7373  d, which is pass
+00020950: 6564 2074 6f20 7472 6169 6e69 6e67 206a  ed to training j
+00020960: 6f62 2061 7320 610a 2020 2020 2020 636f  ob as a.      co
+00020970: 6d6d 616e 6420 6c69 6e65 2061 7267 756d  mmand line argum
+00020980: 656e 742c 2061 6e64 2074 6865 2064 6963  ent, and the dic
+00020990: 7469 6f6e 6172 7920 7661 6c75 6520 6973  tionary value is
+000209a0: 2074 6865 2070 6172 616d 6574 6572 0a20   the parameter. 
+000209b0: 2020 2020 2073 7065 6369 6669 6361 7469       specificati
+000209c0: 6f6e 206f 6620 7468 6520 6d65 7472 6963  on of the metric
+000209d0: 2e0a 2020 2020 7472 6169 6e69 6e67 5f62  ..    training_b
+000209e0: 7564 6765 745f 6275 636b 6574 3a20 4275  udget_bucket: Bu
+000209f0: 636b 6574 206f 6620 7468 6520 6573 7469  cket of the esti
+00020a00: 6d61 7465 6420 7472 6169 6e69 6e67 2062  mated training b
+00020a10: 7564 6765 742e 204f 6e65 206f 660a 2020  udget. One of.  
+00020a20: 2020 2020 2273 6d61 6c6c 2220 283c 2024      "small" (< $
+00020a30: 3630 3029 2c20 226d 6564 6975 6d22 2028  600), "medium" (
+00020a40: 2436 3030 202d 2024 3234 3030 292c 206f  $600 - $2400), o
+00020a50: 7220 226c 6172 6765 2220 283e 2024 3234  r "large" (> $24
+00020a60: 3030 292e 2054 6869 730a 2020 2020 2020  00). This.      
+00020a70: 7061 7261 6d65 7465 7220 6973 206f 6e6c  parameter is onl
+00020a80: 7920 7573 6564 2061 7320 6120 6869 6e74  y used as a hint
+00020a90: 2066 6f72 2074 6865 2068 7970 6572 7061   for the hyperpa
+00020aa0: 7261 6d65 7465 7220 7365 6172 6368 2073  rameter search s
+00020ab0: 7061 6365 2c0a 2020 2020 2020 756e 7265  pace,.      unre
+00020ac0: 6c61 7465 6420 746f 2074 6865 2072 6561  lated to the rea
+00020ad0: 6c20 636f 7374 2e0a 0a20 2052 6574 7572  l cost...  Retur
+00020ae0: 6e73 3a0a 2020 2020 4c69 7374 206f 6620  ns:.    List of 
+00020af0: 7374 7564 795f 7370 6563 5f70 6172 616d  study_spec_param
+00020b00: 6574 6572 735f 6f76 6572 7269 6465 2066  eters_override f
+00020b10: 6f72 2072 6567 7265 7373 696f 6e2e 0a20  or regression.. 
+00020b20: 2022 2222 0a0a 2020 2320 546f 2067 6574   """..  # To get
+00020b30: 2072 6567 7265 7373 696f 6e20 7374 7564   regression stud
+00020b40: 795f 7370 6563 5f70 6172 616d 6574 6572  y_spec_parameter
+00020b50: 732c 2077 6520 6e65 6564 2074 6f20 7365  s, we need to se
+00020b60: 740a 2020 2320 606c 6f73 735f 6675 6e63  t.  # `loss_func
+00020b70: 7469 6f6e 5f74 7970 6560 2074 6f20 e280  tion_type` to ..
+00020b80: 986d 6165 e280 9920 28e2 8098 6d61 65e2  .mae... (...mae.
+00020b90: 8099 2061 6e64 20e2 8098 6d73 65e2 8099  .. and ...mse...
+00020ba0: 2066 6f72 2022 6c61 7267 6522 2073 6561   for "large" sea
+00020bb0: 7263 6820 7370 6163 6529 2c0a 2020 2320  rch space),.  # 
+00020bc0: 7265 6d6f 7665 2074 6865 2060 616c 7068  remove the `alph
+00020bd0: 615f 666f 6361 6c5f 6c6f 7373 602c 2060  a_focal_loss`, `
+00020be0: 6761 6d6d 615f 666f 6361 6c5f 6c6f 7373  gamma_focal_loss
+00020bf0: 600a 2020 2320 616e 6420 6063 6c61 7373  `.  # and `class
+00020c00: 5f77 6569 6768 7460 2070 6172 616d 6574  _weight` paramet
+00020c10: 6572 7320 616e 6420 696e 6372 6561 7365  ers and increase
+00020c20: 2074 6865 206d 6178 2066 6f72 0a20 2023   the max for.  #
+00020c30: 2060 7370 6172 7369 7479 5f6c 6f73 735f   `sparsity_loss_
+00020c40: 7765 6967 6874 6020 746f 2031 3030 2e0a  weight` to 100..
+00020c50: 2020 666f 726d 6174 7465 645f 7061 7261    formatted_para
+00020c60: 6d73 203d 205b 5d0a 2020 666f 7220 7061  ms = [].  for pa
+00020c70: 7261 6d20 696e 2070 6172 616d 733a 0a20  ram in params:. 
+00020c80: 2020 2069 6620 7061 7261 6d5b 2770 6172     if param['par
+00020c90: 616d 6574 6572 5f69 6427 5d20 696e 205b  ameter_id'] in [
+00020ca0: 0a20 2020 2020 2020 2027 616c 7068 615f  .        'alpha_
+00020cb0: 666f 6361 6c5f 6c6f 7373 272c 0a20 2020  focal_loss',.   
+00020cc0: 2020 2020 2027 6761 6d6d 615f 666f 6361       'gamma_foca
+00020cd0: 6c5f 6c6f 7373 272c 0a20 2020 2020 2020  l_loss',.       
+00020ce0: 2027 636c 6173 735f 7765 6967 6874 272c   'class_weight',
+00020cf0: 0a20 2020 205d 3a0a 2020 2020 2020 636f  .    ]:.      co
+00020d00: 6e74 696e 7565 0a20 2020 2065 6c69 6620  ntinue.    elif 
+00020d10: 7061 7261 6d5b 2770 6172 616d 6574 6572  param['parameter
+00020d20: 5f69 6427 5d20 3d3d 2027 7370 6172 7369  _id'] == 'sparsi
+00020d30: 7479 5f6c 6f73 735f 7765 6967 6874 273a  ty_loss_weight':
+00020d40: 0a20 2020 2020 2070 6172 616d 5b27 646f  .      param['do
+00020d50: 7562 6c65 5f76 616c 7565 5f73 7065 6327  uble_value_spec'
+00020d60: 5d5b 276d 6178 5f76 616c 7565 275d 203d  ]['max_value'] =
+00020d70: 2031 3030 0a20 2020 2065 6c69 6620 7061   100.    elif pa
+00020d80: 7261 6d5b 2770 6172 616d 6574 6572 5f69  ram['parameter_i
+00020d90: 6427 5d20 3d3d 2027 6c6f 7373 5f66 756e  d'] == 'loss_fun
+00020da0: 6374 696f 6e5f 7479 7065 273a 0a20 2020  ction_type':.   
+00020db0: 2020 2069 6620 7472 6169 6e69 6e67 5f62     if training_b
+00020dc0: 7564 6765 745f 6275 636b 6574 203d 3d20  udget_bucket == 
+00020dd0: 276c 6172 6765 273a 0a20 2020 2020 2020  'large':.       
+00020de0: 2070 6172 616d 5b27 6361 7465 676f 7269   param['categori
+00020df0: 6361 6c5f 7661 6c75 655f 7370 6563 275d  cal_value_spec']
+00020e00: 5b27 7661 6c75 6573 275d 203d 205b 276d  ['values'] = ['m
+00020e10: 6165 272c 2027 6d73 6527 5d0a 2020 2020  ae', 'mse'].    
+00020e20: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
+00020e30: 7061 7261 6d5b 2763 6174 6567 6f72 6963  param['categoric
+00020e40: 616c 5f76 616c 7565 5f73 7065 6327 5d5b  al_value_spec'][
+00020e50: 2776 616c 7565 7327 5d20 3d20 5b27 6d61  'values'] = ['ma
+00020e60: 6527 5d0a 0a20 2020 2066 6f72 6d61 7474  e']..    formatt
+00020e70: 6564 5f70 6172 616d 732e 6170 7065 6e64  ed_params.append
+00020e80: 2870 6172 616d 290a 0a20 2072 6574 7572  (param)..  retur
+00020e90: 6e20 666f 726d 6174 7465 645f 7061 7261  n formatted_para
+00020ea0: 6d73 0a0a 0a64 6566 2067 6574 5f77 6964  ms...def get_wid
+00020eb0: 655f 616e 645f 6465 6570 5f73 7475 6479  e_and_deep_study
+00020ec0: 5f73 7065 635f 7061 7261 6d65 7465 7273  _spec_parameters
+00020ed0: 5f6f 7665 7272 6964 6528 2920 2d3e 204c  _override() -> L
+00020ee0: 6973 745b 4469 6374 5b73 7472 2c20 416e  ist[Dict[str, An
+00020ef0: 795d 5d3a 0a20 2022 2222 4765 7420 7374  y]]:.  """Get st
+00020f00: 7564 795f 7370 6563 5f70 6172 616d 6574  udy_spec_paramet
+00020f10: 6572 735f 6f76 6572 7269 6465 2066 6f72  ers_override for
+00020f20: 2061 2057 6964 6520 2620 4465 6570 2068   a Wide & Deep h
+00020f30: 7970 6572 7061 7261 6d65 7465 7220 7475  yperparameter tu
+00020f40: 6e69 6e67 206a 6f62 2e0a 0a20 2052 6574  ning job...  Ret
+00020f50: 7572 6e73 3a0a 2020 2020 4c69 7374 206f  urns:.    List o
+00020f60: 6620 7374 7564 795f 7370 6563 5f70 6172  f study_spec_par
+00020f70: 616d 6574 6572 735f 6f76 6572 7269 6465  ameters_override
+00020f80: 2e0a 2020 2222 220a 2020 7061 7261 6d5f  ..  """.  param_
+00020f90: 7061 7468 203d 206f 732e 7061 7468 2e6a  path = os.path.j
+00020fa0: 6f69 6e28 0a20 2020 2020 2070 6174 686c  oin(.      pathl
+00020fb0: 6962 2e50 6174 6828 5f5f 6669 6c65 5f5f  ib.Path(__file__
+00020fc0: 292e 7061 7265 6e74 2e72 6573 6f6c 7665  ).parent.resolve
+00020fd0: 2829 2c0a 2020 2020 2020 2763 6f6e 6669  (),.      'confi
+00020fe0: 6773 2f77 6964 655f 616e 645f 6465 6570  gs/wide_and_deep
+00020ff0: 5f70 6172 616d 732e 6a73 6f6e 272c 0a20  _params.json',. 
+00021000: 2029 0a20 2077 6974 6820 6f70 656e 2870   ).  with open(p
+00021010: 6172 616d 5f70 6174 682c 2027 7227 2920  aram_path, 'r') 
+00021020: 6173 2066 3a0a 2020 2020 7061 7261 6d5f  as f:.    param_
+00021030: 636f 6e74 656e 7420 3d20 662e 7265 6164  content = f.read
+00021040: 2829 0a20 2020 2070 6172 616d 7320 3d20  ().    params = 
+00021050: 6a73 6f6e 2e6c 6f61 6473 2870 6172 616d  json.loads(param
+00021060: 5f63 6f6e 7465 6e74 290a 0a20 2072 6574  _content)..  ret
+00021070: 7572 6e20 7061 7261 6d73 0a0a 0a64 6566  urn params...def
+00021080: 2067 6574 5f78 6762 6f6f 7374 5f73 7475   get_xgboost_stu
+00021090: 6479 5f73 7065 635f 7061 7261 6d65 7465  dy_spec_paramete
+000210a0: 7273 5f6f 7665 7272 6964 6528 2920 2d3e  rs_override() ->
+000210b0: 204c 6973 745b 4469 6374 5b73 7472 2c20   List[Dict[str, 
+000210c0: 416e 795d 5d3a 0a20 2022 2222 4765 7420  Any]]:.  """Get 
+000210d0: 7374 7564 795f 7370 6563 5f70 6172 616d  study_spec_param
+000210e0: 6574 6572 735f 6f76 6572 7269 6465 2066  eters_override f
+000210f0: 6f72 2061 6e20 5847 426f 6f73 7420 6879  or an XGBoost hy
+00021100: 7065 7270 6172 616d 6574 6572 2074 756e  perparameter tun
+00021110: 696e 6720 6a6f 622e 0a0a 2020 5265 7475  ing job...  Retu
+00021120: 726e 733a 0a20 2020 204c 6973 7420 6f66  rns:.    List of
+00021130: 2073 7475 6479 5f73 7065 635f 7061 7261   study_spec_para
+00021140: 6d65 7465 7273 5f6f 7665 7272 6964 652e  meters_override.
+00021150: 0a20 2022 2222 0a20 2070 6172 616d 5f70  .  """.  param_p
+00021160: 6174 6820 3d20 6f73 2e70 6174 682e 6a6f  ath = os.path.jo
+00021170: 696e 280a 2020 2020 2020 7061 7468 6c69  in(.      pathli
+00021180: 622e 5061 7468 285f 5f66 696c 655f 5f29  b.Path(__file__)
+00021190: 2e70 6172 656e 742e 7265 736f 6c76 6528  .parent.resolve(
+000211a0: 292c 2027 636f 6e66 6967 732f 7867 626f  ), 'configs/xgbo
+000211b0: 6f73 745f 7061 7261 6d73 2e6a 736f 6e27  ost_params.json'
+000211c0: 0a20 2029 0a20 2077 6974 6820 6f70 656e  .  ).  with open
+000211d0: 2870 6172 616d 5f70 6174 682c 2027 7227  (param_path, 'r'
+000211e0: 2920 6173 2066 3a0a 2020 2020 7061 7261  ) as f:.    para
+000211f0: 6d5f 636f 6e74 656e 7420 3d20 662e 7265  m_content = f.re
+00021200: 6164 2829 0a20 2020 2070 6172 616d 7320  ad().    params 
+00021210: 3d20 6a73 6f6e 2e6c 6f61 6473 2870 6172  = json.loads(par
+00021220: 616d 5f63 6f6e 7465 6e74 290a 0a20 2072  am_content)..  r
+00021230: 6574 7572 6e20 7061 7261 6d73 0a0a 0a64  eturn params...d
+00021240: 6566 2067 6574 5f78 6762 6f6f 7374 5f74  ef get_xgboost_t
+00021250: 7261 696e 6572 5f70 6970 656c 696e 655f  rainer_pipeline_
+00021260: 616e 645f 7061 7261 6d65 7465 7273 280a  and_parameters(.
+00021270: 2020 2020 7072 6f6a 6563 743a 2073 7472      project: str
+00021280: 2c0a 2020 2020 6c6f 6361 7469 6f6e 3a20  ,.    location: 
+00021290: 7374 722c 0a20 2020 2072 6f6f 745f 6469  str,.    root_di
+000212a0: 723a 2073 7472 2c0a 2020 2020 7461 7267  r: str,.    targ
+000212b0: 6574 5f63 6f6c 756d 6e3a 2073 7472 2c0a  et_column: str,.
+000212c0: 2020 2020 6f62 6a65 6374 6976 653a 2073      objective: s
+000212d0: 7472 2c0a 2020 2020 6576 616c 5f6d 6574  tr,.    eval_met
+000212e0: 7269 633a 204f 7074 696f 6e61 6c5b 7374  ric: Optional[st
+000212f0: 725d 203d 204e 6f6e 652c 0a20 2020 206e  r] = None,.    n
+00021300: 756d 5f62 6f6f 7374 5f72 6f75 6e64 3a20  um_boost_round: 
+00021310: 4f70 7469 6f6e 616c 5b69 6e74 5d20 3d20  Optional[int] = 
+00021320: 4e6f 6e65 2c0a 2020 2020 6561 726c 795f  None,.    early_
+00021330: 7374 6f70 7069 6e67 5f72 6f75 6e64 733a  stopping_rounds:
+00021340: 204f 7074 696f 6e61 6c5b 696e 745d 203d   Optional[int] =
+00021350: 204e 6f6e 652c 0a20 2020 2062 6173 655f   None,.    base_
+00021360: 7363 6f72 653a 204f 7074 696f 6e61 6c5b  score: Optional[
+00021370: 666c 6f61 745d 203d 204e 6f6e 652c 0a20  float] = None,. 
+00021380: 2020 2064 6973 6162 6c65 5f64 6566 6175     disable_defau
+00021390: 6c74 5f65 7661 6c5f 6d65 7472 6963 3a20  lt_eval_metric: 
+000213a0: 4f70 7469 6f6e 616c 5b69 6e74 5d20 3d20  Optional[int] = 
+000213b0: 4e6f 6e65 2c0a 2020 2020 7365 6564 3a20  None,.    seed: 
+000213c0: 4f70 7469 6f6e 616c 5b69 6e74 5d20 3d20  Optional[int] = 
+000213d0: 4e6f 6e65 2c0a 2020 2020 7365 6564 5f70  None,.    seed_p
+000213e0: 6572 5f69 7465 7261 7469 6f6e 3a20 4f70  er_iteration: Op
+000213f0: 7469 6f6e 616c 5b62 6f6f 6c5d 203d 204e  tional[bool] = N
+00021400: 6f6e 652c 0a20 2020 2062 6f6f 7374 6572  one,.    booster
+00021410: 3a20 4f70 7469 6f6e 616c 5b73 7472 5d20  : Optional[str] 
+00021420: 3d20 4e6f 6e65 2c0a 2020 2020 6574 613a  = None,.    eta:
+00021430: 204f 7074 696f 6e61 6c5b 666c 6f61 745d   Optional[float]
+00021440: 203d 204e 6f6e 652c 0a20 2020 2067 616d   = None,.    gam
+00021450: 6d61 3a20 4f70 7469 6f6e 616c 5b66 6c6f  ma: Optional[flo
+00021460: 6174 5d20 3d20 4e6f 6e65 2c0a 2020 2020  at] = None,.    
+00021470: 6d61 785f 6465 7074 683a 204f 7074 696f  max_depth: Optio
+00021480: 6e61 6c5b 696e 745d 203d 204e 6f6e 652c  nal[int] = None,
+00021490: 0a20 2020 206d 696e 5f63 6869 6c64 5f77  .    min_child_w
+000214a0: 6569 6768 743a 204f 7074 696f 6e61 6c5b  eight: Optional[
+000214b0: 666c 6f61 745d 203d 204e 6f6e 652c 0a20  float] = None,. 
+000214c0: 2020 206d 6178 5f64 656c 7461 5f73 7465     max_delta_ste
+000214d0: 703a 204f 7074 696f 6e61 6c5b 666c 6f61  p: Optional[floa
+000214e0: 745d 203d 204e 6f6e 652c 0a20 2020 2073  t] = None,.    s
+000214f0: 7562 7361 6d70 6c65 3a20 4f70 7469 6f6e  ubsample: Option
+00021500: 616c 5b66 6c6f 6174 5d20 3d20 4e6f 6e65  al[float] = None
+00021510: 2c0a 2020 2020 636f 6c73 616d 706c 655f  ,.    colsample_
+00021520: 6279 7472 6565 3a20 4f70 7469 6f6e 616c  bytree: Optional
+00021530: 5b66 6c6f 6174 5d20 3d20 4e6f 6e65 2c0a  [float] = None,.
+00021540: 2020 2020 636f 6c73 616d 706c 655f 6279      colsample_by
+00021550: 6c65 7665 6c3a 204f 7074 696f 6e61 6c5b  level: Optional[
+00021560: 666c 6f61 745d 203d 204e 6f6e 652c 0a20  float] = None,. 
+00021570: 2020 2063 6f6c 7361 6d70 6c65 5f62 796e     colsample_byn
+00021580: 6f64 653a 204f 7074 696f 6e61 6c5b 666c  ode: Optional[fl
+00021590: 6f61 745d 203d 204e 6f6e 652c 0a20 2020  oat] = None,.   
+000215a0: 2072 6567 5f6c 616d 6264 613a 204f 7074   reg_lambda: Opt
+000215b0: 696f 6e61 6c5b 666c 6f61 745d 203d 204e  ional[float] = N
+000215c0: 6f6e 652c 0a20 2020 2072 6567 5f61 6c70  one,.    reg_alp
+000215d0: 6861 3a20 4f70 7469 6f6e 616c 5b66 6c6f  ha: Optional[flo
+000215e0: 6174 5d20 3d20 4e6f 6e65 2c0a 2020 2020  at] = None,.    
+000215f0: 7472 6565 5f6d 6574 686f 643a 204f 7074  tree_method: Opt
+00021600: 696f 6e61 6c5b 7374 725d 203d 204e 6f6e  ional[str] = Non
+00021610: 652c 0a20 2020 2073 6361 6c65 5f70 6f73  e,.    scale_pos
+00021620: 5f77 6569 6768 743a 204f 7074 696f 6e61  _weight: Optiona
+00021630: 6c5b 666c 6f61 745d 203d 204e 6f6e 652c  l[float] = None,
+00021640: 0a20 2020 2075 7064 6174 6572 3a20 4f70  .    updater: Op
+00021650: 7469 6f6e 616c 5b73 7472 5d20 3d20 4e6f  tional[str] = No
+00021660: 6e65 2c0a 2020 2020 7265 6672 6573 685f  ne,.    refresh_
+00021670: 6c65 6166 3a20 4f70 7469 6f6e 616c 5b69  leaf: Optional[i
+00021680: 6e74 5d20 3d20 4e6f 6e65 2c0a 2020 2020  nt] = None,.    
+00021690: 7072 6f63 6573 735f 7479 7065 3a20 4f70  process_type: Op
+000216a0: 7469 6f6e 616c 5b73 7472 5d20 3d20 4e6f  tional[str] = No
+000216b0: 6e65 2c0a 2020 2020 6772 6f77 5f70 6f6c  ne,.    grow_pol
+000216c0: 6963 793a 204f 7074 696f 6e61 6c5b 7374  icy: Optional[st
+000216d0: 725d 203d 204e 6f6e 652c 0a20 2020 2073  r] = None,.    s
+000216e0: 616d 706c 696e 675f 6d65 7468 6f64 3a20  ampling_method: 
+000216f0: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
+00021700: 4e6f 6e65 2c0a 2020 2020 6d6f 6e6f 746f  None,.    monoto
+00021710: 6e65 5f63 6f6e 7374 7261 696e 7473 3a20  ne_constraints: 
+00021720: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
+00021730: 4e6f 6e65 2c0a 2020 2020 696e 7465 7261  None,.    intera
+00021740: 6374 696f 6e5f 636f 6e73 7472 6169 6e74  ction_constraint
+00021750: 733a 204f 7074 696f 6e61 6c5b 7374 725d  s: Optional[str]
+00021760: 203d 204e 6f6e 652c 0a20 2020 2073 616d   = None,.    sam
+00021770: 706c 655f 7479 7065 3a20 4f70 7469 6f6e  ple_type: Option
+00021780: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
+00021790: 2020 2020 6e6f 726d 616c 697a 655f 7479      normalize_ty
+000217a0: 7065 3a20 4f70 7469 6f6e 616c 5b73 7472  pe: Optional[str
+000217b0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7261  ] = None,.    ra
+000217c0: 7465 5f64 726f 703a 204f 7074 696f 6e61  te_drop: Optiona
+000217d0: 6c5b 666c 6f61 745d 203d 204e 6f6e 652c  l[float] = None,
+000217e0: 0a20 2020 206f 6e65 5f64 726f 703a 204f  .    one_drop: O
+000217f0: 7074 696f 6e61 6c5b 696e 745d 203d 204e  ptional[int] = N
+00021800: 6f6e 652c 0a20 2020 2073 6b69 705f 6472  one,.    skip_dr
+00021810: 6f70 3a20 4f70 7469 6f6e 616c 5b66 6c6f  op: Optional[flo
+00021820: 6174 5d20 3d20 4e6f 6e65 2c0a 2020 2020  at] = None,.    
+00021830: 6e75 6d5f 7061 7261 6c6c 656c 5f74 7265  num_parallel_tre
+00021840: 653a 204f 7074 696f 6e61 6c5b 696e 745d  e: Optional[int]
+00021850: 203d 204e 6f6e 652c 0a20 2020 2066 6561   = None,.    fea
+00021860: 7475 7265 5f73 656c 6563 746f 723a 204f  ture_selector: O
+00021870: 7074 696f 6e61 6c5b 7374 725d 203d 204e  ptional[str] = N
+00021880: 6f6e 652c 0a20 2020 2074 6f70 5f6b 3a20  one,.    top_k: 
+00021890: 4f70 7469 6f6e 616c 5b69 6e74 5d20 3d20  Optional[int] = 
+000218a0: 4e6f 6e65 2c0a 2020 2020 6d61 785f 6361  None,.    max_ca
+000218b0: 745f 746f 5f6f 6e65 686f 743a 204f 7074  t_to_onehot: Opt
+000218c0: 696f 6e61 6c5b 696e 745d 203d 204e 6f6e  ional[int] = Non
+000218d0: 652c 0a20 2020 206d 6178 5f6c 6561 7665  e,.    max_leave
+000218e0: 733a 204f 7074 696f 6e61 6c5b 696e 745d  s: Optional[int]
+000218f0: 203d 204e 6f6e 652c 0a20 2020 206d 6178   = None,.    max
+00021900: 5f62 696e 3a20 4f70 7469 6f6e 616c 5b69  _bin: Optional[i
+00021910: 6e74 5d20 3d20 4e6f 6e65 2c0a 2020 2020  nt] = None,.    
+00021920: 7477 6565 6469 655f 7661 7269 616e 6365  tweedie_variance
+00021930: 5f70 6f77 6572 3a20 4f70 7469 6f6e 616c  _power: Optional
+00021940: 5b66 6c6f 6174 5d20 3d20 4e6f 6e65 2c0a  [float] = None,.
+00021950: 2020 2020 6875 6265 725f 736c 6f70 653a      huber_slope:
+00021960: 204f 7074 696f 6e61 6c5b 666c 6f61 745d   Optional[float]
+00021970: 203d 204e 6f6e 652c 0a20 2020 2064 6174   = None,.    dat
+00021980: 6173 6574 5f6c 6576 656c 5f63 7573 746f  aset_level_custo
+00021990: 6d5f 7472 616e 7366 6f72 6d61 7469 6f6e  m_transformation
+000219a0: 5f64 6566 696e 6974 696f 6e73 3a20 4f70  _definitions: Op
+000219b0: 7469 6f6e 616c 5b0a 2020 2020 2020 2020  tional[.        
+000219c0: 4c69 7374 5b44 6963 745b 7374 722c 2041  List[Dict[str, A
+000219d0: 6e79 5d5d 0a20 2020 205d 203d 204e 6f6e  ny]].    ] = Non
+000219e0: 652c 0a20 2020 2064 6174 6173 6574 5f6c  e,.    dataset_l
+000219f0: 6576 656c 5f74 7261 6e73 666f 726d 6174  evel_transformat
+00021a00: 696f 6e73 3a20 4f70 7469 6f6e 616c 5b4c  ions: Optional[L
+00021a10: 6973 745b 4469 6374 5b73 7472 2c20 416e  ist[Dict[str, An
+00021a20: 795d 5d5d 203d 204e 6f6e 652c 0a20 2020  y]]] = None,.   
+00021a30: 2072 756e 5f66 6561 7475 7265 5f73 656c   run_feature_sel
+00021a40: 6563 7469 6f6e 3a20 4f70 7469 6f6e 616c  ection: Optional
+00021a50: 5b62 6f6f 6c5d 203d 204e 6f6e 652c 0a20  [bool] = None,. 
+00021a60: 2020 2066 6561 7475 7265 5f73 656c 6563     feature_selec
+00021a70: 7469 6f6e 5f61 6c67 6f72 6974 686d 3a20  tion_algorithm: 
+00021a80: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
+00021a90: 4e6f 6e65 2c0a 2020 2020 6d61 785f 7365  None,.    max_se
+00021aa0: 6c65 6374 6564 5f66 6561 7475 7265 733a  lected_features:
+00021ab0: 204f 7074 696f 6e61 6c5b 696e 745d 203d   Optional[int] =
+00021ac0: 204e 6f6e 652c 0a20 2020 2070 7265 6465   None,.    prede
+00021ad0: 6669 6e65 645f 7370 6c69 745f 6b65 793a  fined_split_key:
+00021ae0: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
+00021af0: 204e 6f6e 652c 0a20 2020 2073 7472 6174   None,.    strat
+00021b00: 6966 6965 645f 7370 6c69 745f 6b65 793a  ified_split_key:
+00021b10: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
+00021b20: 204e 6f6e 652c 0a20 2020 2074 7261 696e   None,.    train
+00021b30: 696e 675f 6672 6163 7469 6f6e 3a20 4f70  ing_fraction: Op
+00021b40: 7469 6f6e 616c 5b66 6c6f 6174 5d20 3d20  tional[float] = 
+00021b50: 4e6f 6e65 2c0a 2020 2020 7661 6c69 6461  None,.    valida
+00021b60: 7469 6f6e 5f66 7261 6374 696f 6e3a 204f  tion_fraction: O
+00021b70: 7074 696f 6e61 6c5b 666c 6f61 745d 203d  ptional[float] =
+00021b80: 204e 6f6e 652c 0a20 2020 2074 6573 745f   None,.    test_
+00021b90: 6672 6163 7469 6f6e 3a20 4f70 7469 6f6e  fraction: Option
+00021ba0: 616c 5b66 6c6f 6174 5d20 3d20 4e6f 6e65  al[float] = None
+00021bb0: 2c0a 2020 2020 7466 5f61 7574 6f5f 7472  ,.    tf_auto_tr
+00021bc0: 616e 7366 6f72 6d5f 6665 6174 7572 6573  ansform_features
+00021bd0: 3a20 4f70 7469 6f6e 616c 5b0a 2020 2020  : Optional[.    
+00021be0: 2020 2020 556e 696f 6e5b 4c69 7374 5b73      Union[List[s
+00021bf0: 7472 5d2c 2044 6963 745b 7374 722c 204c  tr], Dict[str, L
+00021c00: 6973 745b 7374 725d 5d5d 0a20 2020 205d  ist[str]]].    ]
+00021c10: 203d 204e 6f6e 652c 0a20 2020 2074 665f   = None,.    tf_
+00021c20: 6375 7374 6f6d 5f74 7261 6e73 666f 726d  custom_transform
+00021c30: 6174 696f 6e5f 6465 6669 6e69 7469 6f6e  ation_definition
+00021c40: 733a 204f 7074 696f 6e61 6c5b 4c69 7374  s: Optional[List
+00021c50: 5b44 6963 745b 7374 722c 2041 6e79 5d5d  [Dict[str, Any]]
+00021c60: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7466  ] = None,.    tf
+00021c70: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
+00021c80: 5f70 6174 683a 204f 7074 696f 6e61 6c5b  _path: Optional[
+00021c90: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
+00021ca0: 2064 6174 615f 736f 7572 6365 5f63 7376   data_source_csv
+00021cb0: 5f66 696c 656e 616d 6573 3a20 4f70 7469  _filenames: Opti
+00021cc0: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
+00021cd0: 2c0a 2020 2020 6461 7461 5f73 6f75 7263  ,.    data_sourc
+00021ce0: 655f 6269 6771 7565 7279 5f74 6162 6c65  e_bigquery_table
+00021cf0: 5f70 6174 683a 204f 7074 696f 6e61 6c5b  _path: Optional[
+00021d00: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
+00021d10: 2062 6967 7175 6572 795f 7374 6167 696e   bigquery_stagin
+00021d20: 675f 6675 6c6c 5f64 6174 6173 6574 5f69  g_full_dataset_i
+00021d30: 643a 204f 7074 696f 6e61 6c5b 7374 725d  d: Optional[str]
+00021d40: 203d 204e 6f6e 652c 0a20 2020 2077 6569   = None,.    wei
+00021d50: 6768 745f 636f 6c75 6d6e 3a20 4f70 7469  ght_column: Opti
+00021d60: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
+00021d70: 2c0a 2020 2020 7472 6169 6e69 6e67 5f6d  ,.    training_m
+00021d80: 6163 6869 6e65 5f74 7970 653a 204f 7074  achine_type: Opt
+00021d90: 696f 6e61 6c5b 7374 725d 203d 204e 6f6e  ional[str] = Non
+00021da0: 652c 0a20 2020 2074 7261 696e 696e 675f  e,.    training_
+00021db0: 746f 7461 6c5f 7265 706c 6963 615f 636f  total_replica_co
+00021dc0: 756e 743a 204f 7074 696f 6e61 6c5b 696e  unt: Optional[in
+00021dd0: 745d 203d 204e 6f6e 652c 0a20 2020 2074  t] = None,.    t
+00021de0: 7261 696e 696e 675f 6163 6365 6c65 7261  raining_accelera
+00021df0: 746f 725f 7479 7065 3a20 4f70 7469 6f6e  tor_type: Option
+00021e00: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
+00021e10: 2020 2020 7472 6169 6e69 6e67 5f61 6363      training_acc
+00021e20: 656c 6572 6174 6f72 5f63 6f75 6e74 3a20  elerator_count: 
+00021e30: 4f70 7469 6f6e 616c 5b69 6e74 5d20 3d20  Optional[int] = 
+00021e40: 4e6f 6e65 2c0a 2020 2020 7472 616e 7366  None,.    transf
+00021e50: 6f72 6d5f 6461 7461 666c 6f77 5f6d 6163  orm_dataflow_mac
+00021e60: 6869 6e65 5f74 7970 653a 204f 7074 696f  hine_type: Optio
+00021e70: 6e61 6c5b 7374 725d 203d 204e 6f6e 652c  nal[str] = None,
+00021e80: 0a20 2020 2074 7261 6e73 666f 726d 5f64  .    transform_d
+00021e90: 6174 6166 6c6f 775f 6d61 785f 6e75 6d5f  ataflow_max_num_
+00021ea0: 776f 726b 6572 733a 204f 7074 696f 6e61  workers: Optiona
+00021eb0: 6c5b 696e 745d 203d 204e 6f6e 652c 0a20  l[int] = None,. 
+00021ec0: 2020 2074 7261 6e73 666f 726d 5f64 6174     transform_dat
+00021ed0: 6166 6c6f 775f 6469 736b 5f73 697a 655f  aflow_disk_size_
+00021ee0: 6762 3a20 4f70 7469 6f6e 616c 5b69 6e74  gb: Optional[int
+00021ef0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7275  ] = None,.    ru
+00021f00: 6e5f 6576 616c 7561 7469 6f6e 3a20 4f70  n_evaluation: Op
+00021f10: 7469 6f6e 616c 5b62 6f6f 6c5d 203d 204e  tional[bool] = N
+00021f20: 6f6e 652c 0a20 2020 2065 7661 6c75 6174  one,.    evaluat
+00021f30: 696f 6e5f 6261 7463 685f 7072 6564 6963  ion_batch_predic
+00021f40: 745f 6d61 6368 696e 655f 7479 7065 3a20  t_machine_type: 
+00021f50: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
+00021f60: 4e6f 6e65 2c0a 2020 2020 6576 616c 7561  None,.    evalua
+00021f70: 7469 6f6e 5f62 6174 6368 5f70 7265 6469  tion_batch_predi
+00021f80: 6374 5f73 7461 7274 696e 675f 7265 706c  ct_starting_repl
+00021f90: 6963 615f 636f 756e 743a 204f 7074 696f  ica_count: Optio
+00021fa0: 6e61 6c5b 696e 745d 203d 204e 6f6e 652c  nal[int] = None,
+00021fb0: 0a20 2020 2065 7661 6c75 6174 696f 6e5f  .    evaluation_
+00021fc0: 6261 7463 685f 7072 6564 6963 745f 6d61  batch_predict_ma
+00021fd0: 785f 7265 706c 6963 615f 636f 756e 743a  x_replica_count:
+00021fe0: 204f 7074 696f 6e61 6c5b 696e 745d 203d   Optional[int] =
+00021ff0: 204e 6f6e 652c 0a20 2020 2065 7661 6c75   None,.    evalu
+00022000: 6174 696f 6e5f 6461 7461 666c 6f77 5f6d  ation_dataflow_m
+00022010: 6163 6869 6e65 5f74 7970 653a 204f 7074  achine_type: Opt
+00022020: 696f 6e61 6c5b 7374 725d 203d 204e 6f6e  ional[str] = Non
+00022030: 652c 0a20 2020 2065 7661 6c75 6174 696f  e,.    evaluatio
+00022040: 6e5f 6461 7461 666c 6f77 5f73 7461 7274  n_dataflow_start
+00022050: 696e 675f 6e75 6d5f 776f 726b 6572 733a  ing_num_workers:
+00022060: 204f 7074 696f 6e61 6c5b 696e 745d 203d   Optional[int] =
+00022070: 204e 6f6e 652c 0a20 2020 2065 7661 6c75   None,.    evalu
+00022080: 6174 696f 6e5f 6461 7461 666c 6f77 5f6d  ation_dataflow_m
+00022090: 6178 5f6e 756d 5f77 6f72 6b65 7273 3a20  ax_num_workers: 
+000220a0: 4f70 7469 6f6e 616c 5b69 6e74 5d20 3d20  Optional[int] = 
+000220b0: 4e6f 6e65 2c0a 2020 2020 6576 616c 7561  None,.    evalua
+000220c0: 7469 6f6e 5f64 6174 6166 6c6f 775f 6469  tion_dataflow_di
+000220d0: 736b 5f73 697a 655f 6762 3a20 4f70 7469  sk_size_gb: Opti
+000220e0: 6f6e 616c 5b69 6e74 5d20 3d20 4e6f 6e65  onal[int] = None
+000220f0: 2c0a 2020 2020 6461 7461 666c 6f77 5f73  ,.    dataflow_s
+00022100: 6572 7669 6365 5f61 6363 6f75 6e74 3a20  ervice_account: 
+00022110: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
+00022120: 4e6f 6e65 2c0a 2020 2020 6461 7461 666c  None,.    datafl
+00022130: 6f77 5f73 7562 6e65 7477 6f72 6b3a 204f  ow_subnetwork: O
+00022140: 7074 696f 6e61 6c5b 7374 725d 203d 204e  ptional[str] = N
+00022150: 6f6e 652c 0a20 2020 2064 6174 6166 6c6f  one,.    dataflo
+00022160: 775f 7573 655f 7075 626c 6963 5f69 7073  w_use_public_ips
+00022170: 3a20 4f70 7469 6f6e 616c 5b62 6f6f 6c5d  : Optional[bool]
+00022180: 203d 204e 6f6e 652c 0a20 2020 2065 6e63   = None,.    enc
+00022190: 7279 7074 696f 6e5f 7370 6563 5f6b 6579  ryption_spec_key
+000221a0: 5f6e 616d 653a 204f 7074 696f 6e61 6c5b  _name: Optional[
+000221b0: 7374 725d 203d 204e 6f6e 652c 0a29 3a0a  str] = None,.):.
+000221c0: 2020 2320 666d 743a 206f 6666 0a20 2022    # fmt: off.  "
+000221d0: 2222 4765 7420 7468 6520 5847 426f 6f73  ""Get the XGBoos
+000221e0: 7420 7472 6169 6e69 6e67 2070 6970 656c  t training pipel
+000221f0: 696e 652e 0a0a 2020 4172 6773 3a0a 2020  ine...  Args:.  
+00022200: 2020 7072 6f6a 6563 743a 2054 6865 2047    project: The G
+00022210: 4350 2070 726f 6a65 6374 2074 6861 7420  CP project that 
+00022220: 7275 6e73 2074 6865 2070 6970 656c 696e  runs the pipelin
+00022230: 6520 636f 6d70 6f6e 656e 7473 2e0a 2020  e components..  
+00022240: 2020 6c6f 6361 7469 6f6e 3a20 5468 6520    location: The 
+00022250: 4743 5020 7265 6769 6f6e 2074 6861 7420  GCP region that 
+00022260: 7275 6e73 2074 6865 2070 6970 656c 696e  runs the pipelin
+00022270: 6520 636f 6d70 6f6e 656e 7473 2e0a 2020  e components..  
+00022280: 2020 726f 6f74 5f64 6972 3a20 5468 6520    root_dir: The 
+00022290: 726f 6f74 2047 4353 2064 6972 6563 746f  root GCS directo
+000222a0: 7279 2066 6f72 2074 6865 2070 6970 656c  ry for the pipel
+000222b0: 696e 6520 636f 6d70 6f6e 656e 7473 2e0a  ine components..
+000222c0: 2020 2020 7461 7267 6574 5f63 6f6c 756d      target_colum
+000222d0: 6e3a 2054 6865 2074 6172 6765 7420 636f  n: The target co
+000222e0: 6c75 6d6e 206e 616d 652e 0a20 2020 206f  lumn name..    o
+000222f0: 626a 6563 7469 7665 3a20 5370 6563 6966  bjective: Specif
+00022300: 6965 7320 7468 6520 6c65 6172 6e69 6e67  ies the learning
+00022310: 2074 6173 6b20 616e 6420 7468 6520 6c65   task and the le
+00022320: 6172 6e69 6e67 206f 626a 6563 7469 7665  arning objective
+00022330: 2e20 4d75 7374 2062 6520 6f6e 6520 6f66  . Must be one of
+00022340: 205b 7265 673a 7371 7561 7265 6465 7272   [reg:squarederr
+00022350: 6f72 2c20 7265 673a 7371 7561 7265 646c  or, reg:squaredl
+00022360: 6f67 6572 726f 722c 2072 6567 3a6c 6f67  ogerror, reg:log
+00022370: 6973 7469 632c 2072 6567 3a67 616d 6d61  istic, reg:gamma
+00022380: 2c20 7265 673a 7477 6565 6469 652c 2072  , reg:tweedie, r
+00022390: 6567 3a70 7365 7564 6f68 7562 6572 6572  eg:pseudohuberer
+000223a0: 726f 722c 2062 696e 6172 793a 6c6f 6769  ror, binary:logi
+000223b0: 7374 6963 2c20 6d75 6c74 693a 736f 6674  stic, multi:soft
+000223c0: 7072 6f62 5d2e 0a20 2020 2065 7661 6c5f  prob]..    eval_
+000223d0: 6d65 7472 6963 3a20 4576 616c 7561 7469  metric: Evaluati
+000223e0: 6f6e 206d 6574 7269 6373 2066 6f72 2076  on metrics for v
+000223f0: 616c 6964 6174 696f 6e20 6461 7461 2072  alidation data r
+00022400: 6570 7265 7365 6e74 6564 2061 7320 6120  epresented as a 
+00022410: 636f 6d6d 612d 7365 7061 7261 7465 6420  comma-separated 
+00022420: 7374 7269 6e67 2e0a 2020 2020 6e75 6d5f  string..    num_
+00022430: 626f 6f73 745f 726f 756e 643a 204e 756d  boost_round: Num
+00022440: 6265 7220 6f66 2062 6f6f 7374 696e 6720  ber of boosting 
+00022450: 6974 6572 6174 696f 6e73 2e0a 2020 2020  iterations..    
+00022460: 6561 726c 795f 7374 6f70 7069 6e67 5f72  early_stopping_r
+00022470: 6f75 6e64 733a 2041 6374 6976 6174 6573  ounds: Activates
+00022480: 2065 6172 6c79 2073 746f 7070 696e 672e   early stopping.
+00022490: 2056 616c 6964 6174 696f 6e20 6572 726f   Validation erro
+000224a0: 7220 6e65 6564 7320 746f 2064 6563 7265  r needs to decre
+000224b0: 6173 6520 6174 206c 6561 7374 2065 7665  ase at least eve
+000224c0: 7279 2065 6172 6c79 5f73 746f 7070 696e  ry early_stoppin
+000224d0: 675f 726f 756e 6473 2072 6f75 6e64 2873  g_rounds round(s
+000224e0: 2920 746f 2063 6f6e 7469 6e75 6520 7472  ) to continue tr
+000224f0: 6169 6e69 6e67 2e0a 2020 2020 6261 7365  aining..    base
+00022500: 5f73 636f 7265 3a20 5468 6520 696e 6974  _score: The init
+00022510: 6961 6c20 7072 6564 6963 7469 6f6e 2073  ial prediction s
+00022520: 636f 7265 206f 6620 616c 6c20 696e 7374  core of all inst
+00022530: 616e 6365 732c 2067 6c6f 6261 6c20 6269  ances, global bi
+00022540: 6173 2e0a 2020 2020 6469 7361 626c 655f  as..    disable_
+00022550: 6465 6661 756c 745f 6576 616c 5f6d 6574  default_eval_met
+00022560: 7269 633a 2046 6c61 6720 746f 2064 6973  ric: Flag to dis
+00022570: 6162 6c65 2064 6566 6175 6c74 206d 6574  able default met
+00022580: 7269 632e 2053 6574 2074 6f20 3e30 2074  ric. Set to >0 t
+00022590: 6f20 6469 7361 626c 652e 2044 6566 6175  o disable. Defau
+000225a0: 6c74 2074 6f20 302e 0a20 2020 2073 6565  lt to 0..    see
+000225b0: 643a 2052 616e 646f 6d20 7365 6564 2e0a  d: Random seed..
+000225c0: 2020 2020 7365 6564 5f70 6572 5f69 7465      seed_per_ite
+000225d0: 7261 7469 6f6e 3a20 5365 6564 2050 524e  ration: Seed PRN
+000225e0: 4720 6465 7465 726d 6e69 7374 6963 6c79  G determnisticly
+000225f0: 2076 6961 2069 7465 7261 746f 7220 6e75   via iterator nu
+00022600: 6d62 6572 2e0a 2020 2020 626f 6f73 7465  mber..    booste
+00022610: 723a 2057 6869 6368 2062 6f6f 7374 6572  r: Which booster
+00022620: 2074 6f20 7573 652c 2063 616e 2062 6520   to use, can be 
+00022630: 6762 7472 6565 2c20 6762 6c69 6e65 6172  gbtree, gblinear
+00022640: 206f 7220 6461 7274 2e20 6762 7472 6565   or dart. gbtree
+00022650: 2061 6e64 2064 6172 7420 7573 6520 7472   and dart use tr
+00022660: 6565 2062 6173 6564 206d 6f64 656c 2077  ee based model w
+00022670: 6869 6c65 2067 626c 696e 6561 7220 7573  hile gblinear us
+00022680: 6573 206c 696e 6561 7220 6675 6e63 7469  es linear functi
+00022690: 6f6e 2e0a 2020 2020 6574 613a 204c 6561  on..    eta: Lea
+000226a0: 726e 696e 6720 7261 7465 2e0a 2020 2020  rning rate..    
+000226b0: 6761 6d6d 613a 204d 696e 696d 756d 206c  gamma: Minimum l
+000226c0: 6f73 7320 7265 6475 6374 696f 6e20 7265  oss reduction re
+000226d0: 7175 6972 6564 2074 6f20 6d61 6b65 2061  quired to make a
+000226e0: 2066 7572 7468 6572 2070 6172 7469 7469   further partiti
+000226f0: 6f6e 206f 6e20 6120 6c65 6166 206e 6f64  on on a leaf nod
+00022700: 6520 6f66 2074 6865 2074 7265 652e 0a20  e of the tree.. 
+00022710: 2020 206d 6178 5f64 6570 7468 3a20 4d61     max_depth: Ma
+00022720: 7869 6d75 6d20 6465 7074 6820 6f66 2061  ximum depth of a
+00022730: 2074 7265 652e 0a20 2020 206d 696e 5f63   tree..    min_c
+00022740: 6869 6c64 5f77 6569 6768 743a 204d 696e  hild_weight: Min
+00022750: 696d 756d 2073 756d 206f 6620 696e 7374  imum sum of inst
+00022760: 616e 6365 2077 6569 6768 7428 6865 7373  ance weight(hess
+00022770: 6961 6e29 206e 6565 6465 6420 696e 2061  ian) needed in a
+00022780: 2063 6869 6c64 2e0a 2020 2020 6d61 785f   child..    max_
+00022790: 6465 6c74 615f 7374 6570 3a20 4d61 7869  delta_step: Maxi
+000227a0: 6d75 6d20 6465 6c74 6120 7374 6570 2077  mum delta step w
+000227b0: 6520 616c 6c6f 7720 6561 6368 2074 7265  e allow each tre
+000227c0: 6527 7320 7765 6967 6874 2065 7374 696d  e's weight estim
+000227d0: 6174 696f 6e20 746f 2062 652e 0a20 2020  ation to be..   
+000227e0: 2073 7562 7361 6d70 6c65 3a20 5375 6273   subsample: Subs
+000227f0: 616d 706c 6520 7261 7469 6f20 6f66 2074  ample ratio of t
+00022800: 6865 2074 7261 696e 696e 6720 696e 7374  he training inst
+00022810: 616e 6365 2e0a 2020 2020 636f 6c73 616d  ance..    colsam
+00022820: 706c 655f 6279 7472 6565 3a20 5375 6273  ple_bytree: Subs
+00022830: 616d 706c 6520 7261 7469 6f20 6f66 2063  ample ratio of c
+00022840: 6f6c 756d 6e73 2077 6865 6e20 636f 6e73  olumns when cons
+00022850: 7472 7563 7469 6e67 2065 6163 6820 7472  tructing each tr
+00022860: 6565 2e0a 2020 2020 636f 6c73 616d 706c  ee..    colsampl
+00022870: 655f 6279 6c65 7665 6c3a 2053 7562 7361  e_bylevel: Subsa
+00022880: 6d70 6c65 2072 6174 696f 206f 6620 636f  mple ratio of co
+00022890: 6c75 6d6e 7320 666f 7220 6561 6368 2073  lumns for each s
+000228a0: 706c 6974 2c20 696e 2065 6163 6820 6c65  plit, in each le
+000228b0: 7665 6c2e 0a20 2020 2063 6f6c 7361 6d70  vel..    colsamp
+000228c0: 6c65 5f62 796e 6f64 653a 2053 7562 7361  le_bynode: Subsa
+000228d0: 6d70 6c65 2072 6174 696f 206f 6620 636f  mple ratio of co
+000228e0: 6c75 6d6e 7320 666f 7220 6561 6368 206e  lumns for each n
+000228f0: 6f64 6520 2873 706c 6974 292e 0a20 2020  ode (split)..   
+00022900: 2072 6567 5f6c 616d 6264 613a 204c 3220   reg_lambda: L2 
+00022910: 7265 6775 6c61 7269 7a61 7469 6f6e 2074  regularization t
+00022920: 6572 6d20 6f6e 2077 6569 6768 7473 2e0a  erm on weights..
+00022930: 2020 2020 7265 675f 616c 7068 613a 204c      reg_alpha: L
+00022940: 3120 7265 6775 6c61 7269 7a61 7469 6f6e  1 regularization
+00022950: 2074 6572 6d20 6f6e 2077 6569 6768 7473   term on weights
+00022960: 2e0a 2020 2020 7472 6565 5f6d 6574 686f  ..    tree_metho
+00022970: 643a 2054 6865 2074 7265 6520 636f 6e73  d: The tree cons
+00022980: 7472 7563 7469 6f6e 2061 6c67 6f72 6974  truction algorit
+00022990: 686d 2075 7365 6420 696e 2058 4742 6f6f  hm used in XGBoo
+000229a0: 7374 2e20 4368 6f69 6365 733a 205b 2261  st. Choices: ["a
+000229b0: 7574 6f22 2c20 2265 7861 6374 222c 2022  uto", "exact", "
+000229c0: 6170 7072 6f78 222c 2022 6869 7374 222c  approx", "hist",
+000229d0: 2022 6770 755f 6578 6163 7422 2c20 2267   "gpu_exact", "g
+000229e0: 7075 5f68 6973 7422 5d2e 0a20 2020 2073  pu_hist"]..    s
+000229f0: 6361 6c65 5f70 6f73 5f77 6569 6768 743a  cale_pos_weight:
+00022a00: 2043 6f6e 7472 6f6c 2074 6865 2062 616c   Control the bal
+00022a10: 616e 6365 206f 6620 706f 7369 7469 7665  ance of positive
+00022a20: 2061 6e64 206e 6567 6174 6976 6520 7765   and negative we
+00022a30: 6967 6874 732e 0a20 2020 2075 7064 6174  ights..    updat
+00022a40: 6572 3a20 4120 636f 6d6d 6120 7365 7061  er: A comma sepa
+00022a50: 7261 7465 6420 7374 7269 6e67 2064 6566  rated string def
+00022a60: 696e 696e 6720 7468 6520 7365 7175 656e  ining the sequen
+00022a70: 6365 206f 6620 7472 6565 2075 7064 6174  ce of tree updat
+00022a80: 6572 7320 746f 2072 756e 2e0a 2020 2020  ers to run..    
+00022a90: 7265 6672 6573 685f 6c65 6166 3a20 5265  refresh_leaf: Re
+00022aa0: 6672 6573 6820 7570 6461 7465 7220 706c  fresh updater pl
+00022ab0: 7567 696e 2e20 5570 6461 7465 2074 7265  ugin. Update tre
+00022ac0: 6520 6c65 6166 2061 6e64 206e 6f64 6573  e leaf and nodes
+00022ad0: 2773 2073 7461 7473 2069 6620 5472 7565  's stats if True
+00022ae0: 2e20 5768 656e 2069 7420 6973 2046 616c  . When it is Fal
+00022af0: 7365 2c20 6f6e 6c79 206e 6f64 6520 7374  se, only node st
+00022b00: 6174 7320 6172 6520 7570 6461 7465 642e  ats are updated.
+00022b10: 0a20 2020 2070 726f 6365 7373 5f74 7970  .    process_typ
+00022b20: 653a 2041 2074 7970 6520 6f66 2062 6f6f  e: A type of boo
+00022b30: 7374 696e 6720 7072 6f63 6573 7320 746f  sting process to
+00022b40: 2072 756e 2e20 4368 6f69 6365 733a 5b22   run. Choices:["
+00022b50: 6465 6661 756c 7422 2c20 2275 7064 6174  default", "updat
+00022b60: 6522 5d0a 2020 2020 6772 6f77 5f70 6f6c  e"].    grow_pol
+00022b70: 6963 793a 2043 6f6e 7472 6f6c 7320 6120  icy: Controls a 
+00022b80: 7761 7920 6e65 7720 6e6f 6465 7320 6172  way new nodes ar
+00022b90: 6520 6164 6465 6420 746f 2074 6865 2074  e added to the t
+00022ba0: 7265 652e 204f 6e6c 7920 7375 7070 6f72  ree. Only suppor
+00022bb0: 7465 6420 6966 2074 7265 655f 6d65 7468  ted if tree_meth
+00022bc0: 6f64 2069 7320 6869 7374 2e20 4368 6f69  od is hist. Choi
+00022bd0: 6365 733a 5b22 6465 7074 6877 6973 6522  ces:["depthwise"
+00022be0: 2c20 226c 6f73 7367 7569 6465 225d 0a20  , "lossguide"]. 
+00022bf0: 2020 2073 616d 706c 696e 675f 6d65 7468     sampling_meth
+00022c00: 6f64 3a20 5468 6520 6d65 7468 6f64 2074  od: The method t
+00022c10: 6f20 7573 6520 746f 2073 616d 706c 6520  o use to sample 
+00022c20: 7468 6520 7472 6169 6e69 6e67 2069 6e73  the training ins
+00022c30: 7461 6e63 6573 2e0a 2020 2020 6d6f 6e6f  tances..    mono
+00022c40: 746f 6e65 5f63 6f6e 7374 7261 696e 7473  tone_constraints
+00022c50: 3a20 436f 6e73 7472 6169 6e74 206f 6620  : Constraint of 
+00022c60: 7661 7269 6162 6c65 206d 6f6e 6f74 6f6e  variable monoton
+00022c70: 6963 6974 792e 0a20 2020 2069 6e74 6572  icity..    inter
+00022c80: 6163 7469 6f6e 5f63 6f6e 7374 7261 696e  action_constrain
+00022c90: 7473 3a20 436f 6e73 7472 6169 6e74 7320  ts: Constraints 
+00022ca0: 666f 7220 696e 7465 7261 6374 696f 6e20  for interaction 
+00022cb0: 7265 7072 6573 656e 7469 6e67 2070 6572  representing per
+00022cc0: 6d69 7474 6564 2069 6e74 6572 6163 7469  mitted interacti
+00022cd0: 6f6e 732e 0a20 2020 2073 616d 706c 655f  ons..    sample_
+00022ce0: 7479 7065 3a20 5b64 6172 7420 626f 6f73  type: [dart boos
+00022cf0: 7465 7220 6f6e 6c79 5d20 5479 7065 206f  ter only] Type o
+00022d00: 6620 7361 6d70 6c69 6e67 2061 6c67 6f72  f sampling algor
+00022d10: 6974 686d 2e20 2043 686f 6963 6573 3a5b  ithm.  Choices:[
+00022d20: 2275 6e69 666f 726d 222c 2022 7765 6967  "uniform", "weig
+00022d30: 6874 6564 225d 0a20 2020 206e 6f72 6d61  hted"].    norma
+00022d40: 6c69 7a65 5f74 7970 653a 205b 6461 7274  lize_type: [dart
+00022d50: 2062 6f6f 7374 6572 206f 6e6c 795d 2054   booster only] T
+00022d60: 7970 6520 6f66 206e 6f72 6d61 6c69 7a61  ype of normaliza
+00022d70: 7469 6f6e 2061 6c67 6f72 6974 686d 2c20  tion algorithm, 
+00022d80: 4368 6f69 6365 733a 5b22 7472 6565 222c  Choices:["tree",
+00022d90: 2022 666f 7265 7374 225d 0a20 2020 2072   "forest"].    r
+00022da0: 6174 655f 6472 6f70 3a20 5b64 6172 7420  ate_drop: [dart 
+00022db0: 626f 6f73 7465 7220 6f6e 6c79 5d20 4472  booster only] Dr
+00022dc0: 6f70 6f75 7420 7261 7465 2e27 0a20 2020  opout rate.'.   
+00022dd0: 206f 6e65 5f64 726f 703a 205b 6461 7274   one_drop: [dart
+00022de0: 2062 6f6f 7374 6572 206f 6e6c 795d 2057   booster only] W
+00022df0: 6865 6e20 7468 6973 2066 6c61 6720 6973  hen this flag is
+00022e00: 2065 6e61 626c 6564 2c20 6174 206c 6561   enabled, at lea
+00022e10: 7374 206f 6e65 2074 7265 6520 6973 2061  st one tree is a
+00022e20: 6c77 6179 7320 6472 6f70 7065 6420 6475  lways dropped du
+00022e30: 7269 6e67 2074 6865 2064 726f 706f 7574  ring the dropout
+00022e40: 2028 616c 6c6f 7773 2042 696e 6f6d 6961   (allows Binomia
+00022e50: 6c2d 706c 7573 2d6f 6e65 206f 7220 6570  l-plus-one or ep
+00022e60: 7369 6c6f 6e2d 6472 6f70 6f75 7420 6672  silon-dropout fr
+00022e70: 6f6d 2074 6865 206f 7269 6769 6e61 6c20  om the original 
+00022e80: 4441 5254 2070 6170 6572 292e 0a20 2020  DART paper)..   
+00022e90: 2073 6b69 705f 6472 6f70 3a20 5b64 6172   skip_drop: [dar
+00022ea0: 7420 626f 6f73 7465 7220 6f6e 6c79 5d20  t booster only] 
+00022eb0: 5072 6f62 6162 696c 6974 7920 6f66 2073  Probability of s
+00022ec0: 6b69 7070 696e 6720 7468 6520 6472 6f70  kipping the drop
+00022ed0: 6f75 7420 7072 6f63 6564 7572 6520 6475  out procedure du
+00022ee0: 7269 6e67 2061 2062 6f6f 7374 696e 6720  ring a boosting 
+00022ef0: 6974 6572 6174 696f 6e2e 0a20 2020 206e  iteration..    n
+00022f00: 756d 5f70 6172 616c 6c65 6c5f 7472 6565  um_parallel_tree
+00022f10: 3a20 4e75 6d62 6572 206f 6620 7061 7261  : Number of para
+00022f20: 6c6c 656c 2074 7265 6573 2063 6f6e 7374  llel trees const
+00022f30: 7275 6374 6564 2064 7572 696e 6720 6561  ructed during ea
+00022f40: 6368 2069 7465 7261 7469 6f6e 2e20 5468  ch iteration. Th
+00022f50: 6973 206f 7074 696f 6e20 6973 2075 7365  is option is use
+00022f60: 6420 746f 2073 7570 706f 7274 2062 6f6f  d to support boo
+00022f70: 7374 6564 2072 616e 646f 6d20 666f 7265  sted random fore
+00022f80: 7374 2e0a 2020 2020 6665 6174 7572 655f  st..    feature_
+00022f90: 7365 6c65 6374 6f72 3a20 5b6c 696e 6561  selector: [linea
+00022fa0: 7220 626f 6f73 7465 7220 6f6e 6c79 5d20  r booster only] 
+00022fb0: 4665 6174 7572 6520 7365 6c65 6374 696f  Feature selectio
+00022fc0: 6e20 616e 6420 6f72 6465 7269 6e67 206d  n and ordering m
+00022fd0: 6574 686f 642e 0a20 2020 2074 6f70 5f6b  ethod..    top_k
+00022fe0: 3a20 5468 6520 6e75 6d62 6572 206f 6620  : The number of 
+00022ff0: 746f 7020 6665 6174 7572 6573 2074 6f20  top features to 
+00023000: 7365 6c65 6374 2069 6e20 6772 6565 6479  select in greedy
+00023010: 2061 6e64 2074 6872 6966 7479 2066 6561   and thrifty fea
+00023020: 7475 7265 2073 656c 6563 746f 722e 2054  ture selector. T
+00023030: 6865 2076 616c 7565 206f 6620 3020 6d65  he value of 0 me
+00023040: 616e 7320 7573 696e 6720 616c 6c20 7468  ans using all th
+00023050: 6520 6665 6174 7572 6573 2e0a 2020 2020  e features..    
+00023060: 6d61 785f 6361 745f 746f 5f6f 6e65 686f  max_cat_to_oneho
+00023070: 743a 2041 2074 6872 6573 686f 6c64 2066  t: A threshold f
+00023080: 6f72 2064 6563 6964 696e 6720 7768 6574  or deciding whet
+00023090: 6865 7220 5847 426f 6f73 7420 7368 6f75  her XGBoost shou
+000230a0: 6c64 2075 7365 206f 6e65 2d68 6f74 2065  ld use one-hot e
+000230b0: 6e63 6f64 696e 6720 6261 7365 6420 7370  ncoding based sp
+000230c0: 6c69 7420 666f 7220 6361 7465 676f 7269  lit for categori
+000230d0: 6361 6c20 6461 7461 2e0a 2020 2020 6d61  cal data..    ma
+000230e0: 785f 6c65 6176 6573 3a20 4d61 7869 6d75  x_leaves: Maximu
+000230f0: 6d20 6e75 6d62 6572 206f 6620 6e6f 6465  m number of node
+00023100: 7320 746f 2062 6520 6164 6465 642e 0a20  s to be added.. 
+00023110: 2020 206d 6178 5f62 696e 3a20 4d61 7869     max_bin: Maxi
+00023120: 6d75 6d20 6e75 6d62 6572 206f 6620 6469  mum number of di
+00023130: 7363 7265 7465 2062 696e 7320 746f 2062  screte bins to b
+00023140: 7563 6b65 7420 636f 6e74 696e 756f 7573  ucket continuous
+00023150: 2066 6561 7475 7265 732e 0a20 2020 2074   features..    t
+00023160: 7765 6564 6965 5f76 6172 6961 6e63 655f  weedie_variance_
+00023170: 706f 7765 723a 2050 6172 616d 6574 6572  power: Parameter
+00023180: 2074 6861 7420 636f 6e74 726f 6c73 2074   that controls t
+00023190: 6865 2076 6172 6961 6e63 6520 6f66 2074  he variance of t
+000231a0: 6865 2054 7765 6564 6965 2064 6973 7472  he Tweedie distr
+000231b0: 6962 7574 696f 6e2e 0a20 2020 2068 7562  ibution..    hub
+000231c0: 6572 5f73 6c6f 7065 3a20 4120 7061 7261  er_slope: A para
+000231d0: 6d65 7465 7220 7573 6564 2066 6f72 2050  meter used for P
+000231e0: 7365 7564 6f2d 4875 6265 7220 6c6f 7373  seudo-Huber loss
+000231f0: 2074 6f20 6465 6669 6e65 2074 6865 2064   to define the d
+00023200: 656c 7461 2074 6572 6d2e 0a20 2020 2064  elta term..    d
+00023210: 6174 6173 6574 5f6c 6576 656c 5f63 7573  ataset_level_cus
+00023220: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
+00023230: 6f6e 5f64 6566 696e 6974 696f 6e73 3a20  on_definitions: 
+00023240: 4461 7461 7365 742d 6c65 7665 6c20 6375  Dataset-level cu
+00023250: 7374 6f6d 2074 7261 6e73 666f 726d 6174  stom transformat
+00023260: 696f 6e20 6465 6669 6e69 7469 6f6e 7320  ion definitions 
+00023270: 696e 2073 7472 696e 6720 666f 726d 6174  in string format
+00023280: 2e0a 2020 2020 6461 7461 7365 745f 6c65  ..    dataset_le
+00023290: 7665 6c5f 7472 616e 7366 6f72 6d61 7469  vel_transformati
+000232a0: 6f6e 733a 2044 6174 6173 6574 2d6c 6576  ons: Dataset-lev
+000232b0: 656c 2074 7261 6e73 666f 726d 6174 696f  el transformatio
+000232c0: 6e20 636f 6e66 6967 7572 6174 696f 6e20  n configuration 
+000232d0: 696e 2073 7472 696e 6720 666f 726d 6174  in string format
+000232e0: 2e0a 2020 2020 7275 6e5f 6665 6174 7572  ..    run_featur
+000232f0: 655f 7365 6c65 6374 696f 6e3a 2057 6865  e_selection: Whe
+00023300: 7468 6572 2074 6f20 656e 6162 6c65 2066  ther to enable f
+00023310: 6561 7475 7265 2073 656c 6563 7469 6f6e  eature selection
+00023320: 2e0a 2020 2020 6665 6174 7572 655f 7365  ..    feature_se
+00023330: 6c65 6374 696f 6e5f 616c 676f 7269 7468  lection_algorith
+00023340: 6d3a 2046 6561 7475 7265 2073 656c 6563  m: Feature selec
+00023350: 7469 6f6e 2061 6c67 6f72 6974 686d 2e0a  tion algorithm..
+00023360: 2020 2020 6d61 785f 7365 6c65 6374 6564      max_selected
+00023370: 5f66 6561 7475 7265 733a 204d 6178 696d  _features: Maxim
+00023380: 756d 206e 756d 6265 7220 6f66 2066 6561  um number of fea
+00023390: 7475 7265 7320 746f 2073 656c 6563 742e  tures to select.
+000233a0: 0a20 2020 2070 7265 6465 6669 6e65 645f  .    predefined_
+000233b0: 7370 6c69 745f 6b65 793a 2050 7265 6465  split_key: Prede
+000233c0: 6669 6e65 6420 7370 6c69 7420 6b65 792e  fined split key.
+000233d0: 0a20 2020 2073 7472 6174 6966 6965 645f  .    stratified_
+000233e0: 7370 6c69 745f 6b65 793a 2053 7472 6174  split_key: Strat
+000233f0: 6966 6965 6420 7370 6c69 7420 6b65 792e  ified split key.
+00023400: 0a20 2020 2074 7261 696e 696e 675f 6672  .    training_fr
+00023410: 6163 7469 6f6e 3a20 5472 6169 6e69 6e67  action: Training
+00023420: 2066 7261 6374 696f 6e2e 0a20 2020 2076   fraction..    v
+00023430: 616c 6964 6174 696f 6e5f 6672 6163 7469  alidation_fracti
+00023440: 6f6e 3a20 5661 6c69 6461 7469 6f6e 2066  on: Validation f
+00023450: 7261 6374 696f 6e2e 0a20 2020 2074 6573  raction..    tes
+00023460: 745f 6672 6163 7469 6f6e 3a20 5465 7374  t_fraction: Test
+00023470: 2066 7261 6374 696f 6e2e 0a20 2020 2074   fraction..    t
+00023480: 665f 6175 746f 5f74 7261 6e73 666f 726d  f_auto_transform
+00023490: 5f66 6561 7475 7265 733a 204c 6973 7420  _features: List 
+000234a0: 6f66 2061 7574 6f20 7472 616e 7366 6f72  of auto transfor
+000234b0: 6d20 6665 6174 7572 6573 2069 6e20 7468  m features in th
+000234c0: 6520 636f 6d6d 612d 7365 7061 7261 7465  e comma-separate
+000234d0: 6420 7374 7269 6e67 2066 6f72 6d61 742e  d string format.
+000234e0: 0a20 2020 2074 665f 6375 7374 6f6d 5f74  .    tf_custom_t
+000234f0: 7261 6e73 666f 726d 6174 696f 6e5f 6465  ransformation_de
+00023500: 6669 6e69 7469 6f6e 733a 2054 4620 6375  finitions: TF cu
+00023510: 7374 6f6d 2074 7261 6e73 666f 726d 6174  stom transformat
+00023520: 696f 6e20 6465 6669 6e69 7469 6f6e 7320  ion definitions 
+00023530: 696e 2073 7472 696e 6720 666f 726d 6174  in string format
+00023540: 2e0a 2020 2020 7466 5f74 7261 6e73 666f  ..    tf_transfo
+00023550: 726d 6174 696f 6e73 5f70 6174 683a 2050  rmations_path: P
+00023560: 6174 6820 746f 2054 4620 7472 616e 7366  ath to TF transf
+00023570: 6f72 6d61 7469 6f6e 2063 6f6e 6669 6775  ormation configu
+00023580: 7261 7469 6f6e 2e0a 2020 2020 6461 7461  ration..    data
+00023590: 5f73 6f75 7263 655f 6373 765f 6669 6c65  _source_csv_file
+000235a0: 6e61 6d65 733a 2054 6865 2043 5356 2064  names: The CSV d
+000235b0: 6174 6120 736f 7572 6365 2e0a 2020 2020  ata source..    
+000235c0: 6461 7461 5f73 6f75 7263 655f 6269 6771  data_source_bigq
+000235d0: 7565 7279 5f74 6162 6c65 5f70 6174 683a  uery_table_path:
+000235e0: 2054 6865 2042 6967 5175 6572 7920 6461   The BigQuery da
+000235f0: 7461 2073 6f75 7263 652e 0a20 2020 2062  ta source..    b
+00023600: 6967 7175 6572 795f 7374 6167 696e 675f  igquery_staging_
+00023610: 6675 6c6c 5f64 6174 6173 6574 5f69 643a  full_dataset_id:
+00023620: 2054 6865 2042 6967 5175 6572 7920 7374   The BigQuery st
+00023630: 6167 696e 6720 6675 6c6c 2064 6174 6173  aging full datas
+00023640: 6574 2069 6420 666f 7220 7374 6f72 696e  et id for storin
+00023650: 6720 696e 7465 726d 6564 6961 7465 2074  g intermediate t
+00023660: 6162 6c65 732e 0a20 2020 2077 6569 6768  ables..    weigh
+00023670: 745f 636f 6c75 6d6e 3a20 5468 6520 7765  t_column: The we
+00023680: 6967 6874 2063 6f6c 756d 6e20 6e61 6d65  ight column name
+00023690: 2e0a 2020 2020 7472 6169 6e69 6e67 5f6d  ..    training_m
+000236a0: 6163 6869 6e65 5f74 7970 653a 204d 6163  achine_type: Mac
+000236b0: 6869 6e65 2074 7970 652e 0a20 2020 2074  hine type..    t
+000236c0: 7261 696e 696e 675f 746f 7461 6c5f 7265  raining_total_re
+000236d0: 706c 6963 615f 636f 756e 743a 204e 756d  plica_count: Num
+000236e0: 6265 7220 6f66 2077 6f72 6b65 7273 2e0a  ber of workers..
+000236f0: 2020 2020 7472 6169 6e69 6e67 5f61 6363      training_acc
+00023700: 656c 6572 6174 6f72 5f74 7970 653a 2041  elerator_type: A
+00023710: 6363 656c 6572 6174 6f72 2074 7970 652e  ccelerator type.
+00023720: 0a20 2020 2074 7261 696e 696e 675f 6163  .    training_ac
+00023730: 6365 6c65 7261 746f 725f 636f 756e 743a  celerator_count:
+00023740: 2041 6363 656c 6572 6174 6f72 2063 6f75   Accelerator cou
+00023750: 6e74 2e0a 2020 2020 7472 616e 7366 6f72  nt..    transfor
+00023760: 6d5f 6461 7461 666c 6f77 5f6d 6163 6869  m_dataflow_machi
+00023770: 6e65 5f74 7970 653a 2054 6865 2064 6174  ne_type: The dat
+00023780: 6166 6c6f 7720 6d61 6368 696e 6520 7479  aflow machine ty
+00023790: 7065 2066 6f72 2074 7261 6e73 666f 726d  pe for transform
+000237a0: 2063 6f6d 706f 6e65 6e74 2e0a 2020 2020   component..    
+000237b0: 7472 616e 7366 6f72 6d5f 6461 7461 666c  transform_datafl
+000237c0: 6f77 5f6d 6178 5f6e 756d 5f77 6f72 6b65  ow_max_num_worke
+000237d0: 7273 3a20 5468 6520 6d61 7820 6e75 6d62  rs: The max numb
+000237e0: 6572 206f 6620 4461 7461 666c 6f77 2077  er of Dataflow w
+000237f0: 6f72 6b65 7273 2066 6f72 2074 7261 6e73  orkers for trans
+00023800: 666f 726d 2063 6f6d 706f 6e65 6e74 2e0a  form component..
+00023810: 2020 2020 7472 616e 7366 6f72 6d5f 6461      transform_da
+00023820: 7461 666c 6f77 5f64 6973 6b5f 7369 7a65  taflow_disk_size
+00023830: 5f67 623a 2044 6174 6166 6c6f 7720 776f  _gb: Dataflow wo
+00023840: 726b 6572 2773 2064 6973 6b20 7369 7a65  rker's disk size
+00023850: 2069 6e20 4742 2066 6f72 2074 7261 6e73   in GB for trans
+00023860: 666f 726d 2063 6f6d 706f 6e65 6e74 2e0a  form component..
+00023870: 2020 2020 7275 6e5f 6576 616c 7561 7469      run_evaluati
+00023880: 6f6e 3a20 5768 6574 6865 7220 746f 2072  on: Whether to r
+00023890: 756e 2065 7661 6c75 6174 696f 6e20 7374  un evaluation st
+000238a0: 6570 7320 6475 7269 6e67 2074 7261 696e  eps during train
+000238b0: 696e 672e 0a20 2020 2065 7661 6c75 6174  ing..    evaluat
+000238c0: 696f 6e5f 6261 7463 685f 7072 6564 6963  ion_batch_predic
+000238d0: 745f 6d61 6368 696e 655f 7479 7065 3a20  t_machine_type: 
+000238e0: 5468 6520 7072 6564 6963 7469 6f6e 2073  The prediction s
+000238f0: 6572 7665 7220 6d61 6368 696e 6520 7479  erver machine ty
+00023900: 7065 2066 6f72 2062 6174 6368 2070 7265  pe for batch pre
+00023910: 6469 6374 2063 6f6d 706f 6e65 6e74 7320  dict components 
+00023920: 6475 7269 6e67 2065 7661 6c75 6174 696f  during evaluatio
+00023930: 6e2e 0a20 2020 2065 7661 6c75 6174 696f  n..    evaluatio
+00023940: 6e5f 6261 7463 685f 7072 6564 6963 745f  n_batch_predict_
+00023950: 7374 6172 7469 6e67 5f72 6570 6c69 6361  starting_replica
+00023960: 5f63 6f75 6e74 3a20 5468 6520 696e 6974  _count: The init
+00023970: 6961 6c20 6e75 6d62 6572 206f 6620 7072  ial number of pr
+00023980: 6564 6963 7469 6f6e 2073 6572 7665 7220  ediction server 
+00023990: 666f 7220 6261 7463 6820 7072 6564 6963  for batch predic
+000239a0: 7420 636f 6d70 6f6e 656e 7473 2064 7572  t components dur
+000239b0: 696e 6720 6576 616c 7561 7469 6f6e 2e0a  ing evaluation..
+000239c0: 2020 2020 6576 616c 7561 7469 6f6e 5f62      evaluation_b
+000239d0: 6174 6368 5f70 7265 6469 6374 5f6d 6178  atch_predict_max
+000239e0: 5f72 6570 6c69 6361 5f63 6f75 6e74 3a20  _replica_count: 
+000239f0: 5468 6520 6d61 7820 6e75 6d62 6572 206f  The max number o
+00023a00: 6620 7072 6564 6963 7469 6f6e 2073 6572  f prediction ser
+00023a10: 7665 7220 666f 7220 6261 7463 6820 7072  ver for batch pr
+00023a20: 6564 6963 7420 636f 6d70 6f6e 656e 7473  edict components
+00023a30: 2064 7572 696e 6720 6576 616c 7561 7469   during evaluati
+00023a40: 6f6e 2e0a 2020 2020 6576 616c 7561 7469  on..    evaluati
+00023a50: 6f6e 5f64 6174 6166 6c6f 775f 6d61 6368  on_dataflow_mach
+00023a60: 696e 655f 7479 7065 3a20 5468 6520 6461  ine_type: The da
+00023a70: 7461 666c 6f77 206d 6163 6869 6e65 2074  taflow machine t
+00023a80: 7970 6520 666f 7220 6576 616c 7561 7469  ype for evaluati
+00023a90: 6f6e 2063 6f6d 706f 6e65 6e74 732e 0a20  on components.. 
+00023aa0: 2020 2065 7661 6c75 6174 696f 6e5f 6461     evaluation_da
+00023ab0: 7461 666c 6f77 5f73 7461 7274 696e 675f  taflow_starting_
+00023ac0: 6e75 6d5f 776f 726b 6572 733a 2054 6865  num_workers: The
+00023ad0: 2069 6e69 7469 616c 206e 756d 6265 7220   initial number 
+00023ae0: 6f66 2044 6174 6166 6c6f 7720 776f 726b  of Dataflow work
+00023af0: 6572 7320 666f 7220 6576 616c 7561 7469  ers for evaluati
+00023b00: 6f6e 2063 6f6d 706f 6e65 6e74 732e 0a20  on components.. 
+00023b10: 2020 2065 7661 6c75 6174 696f 6e5f 6461     evaluation_da
+00023b20: 7461 666c 6f77 5f6d 6178 5f6e 756d 5f77  taflow_max_num_w
+00023b30: 6f72 6b65 7273 3a20 5468 6520 6d61 7820  orkers: The max 
+00023b40: 6e75 6d62 6572 206f 6620 4461 7461 666c  number of Datafl
+00023b50: 6f77 2077 6f72 6b65 7273 2066 6f72 2065  ow workers for e
+00023b60: 7661 6c75 6174 696f 6e20 636f 6d70 6f6e  valuation compon
+00023b70: 656e 7473 2e0a 2020 2020 6576 616c 7561  ents..    evalua
+00023b80: 7469 6f6e 5f64 6174 6166 6c6f 775f 6469  tion_dataflow_di
+00023b90: 736b 5f73 697a 655f 6762 3a20 4461 7461  sk_size_gb: Data
+00023ba0: 666c 6f77 2077 6f72 6b65 7227 7320 6469  flow worker's di
+00023bb0: 736b 2073 697a 6520 696e 2047 4220 666f  sk size in GB fo
+00023bc0: 7220 6576 616c 7561 7469 6f6e 2063 6f6d  r evaluation com
+00023bd0: 706f 6e65 6e74 732e 0a20 2020 2064 6174  ponents..    dat
+00023be0: 6166 6c6f 775f 7365 7276 6963 655f 6163  aflow_service_ac
+00023bf0: 636f 756e 743a 2043 7573 746f 6d20 7365  count: Custom se
+00023c00: 7276 6963 6520 6163 636f 756e 7420 746f  rvice account to
+00023c10: 2072 756e 2064 6174 6166 6c6f 7720 6a6f   run dataflow jo
+00023c20: 6273 2e0a 2020 2020 6461 7461 666c 6f77  bs..    dataflow
+00023c30: 5f73 7562 6e65 7477 6f72 6b3a 2044 6174  _subnetwork: Dat
+00023c40: 6166 6c6f 7727 7320 6675 6c6c 7920 7175  aflow's fully qu
+00023c50: 616c 6966 6965 6420 7375 626e 6574 776f  alified subnetwo
+00023c60: 726b 206e 616d 652c 2077 6865 6e20 656d  rk name, when em
+00023c70: 7074 7920 7468 6520 6465 6661 756c 7420  pty the default 
+00023c80: 7375 626e 6574 776f 726b 2077 696c 6c20  subnetwork will 
+00023c90: 6265 2075 7365 642e 2045 7861 6d70 6c65  be used. Example
+00023ca0: 3a20 6874 7470 733a 2f2f 636c 6f75 642e  : https://cloud.
+00023cb0: 676f 6f67 6c65 2e63 6f6d 2f64 6174 6166  google.com/dataf
+00023cc0: 6c6f 772f 646f 6373 2f67 7569 6465 732f  low/docs/guides/
+00023cd0: 7370 6563 6966 7969 6e67 2d6e 6574 776f  specifying-netwo
+00023ce0: 726b 7323 6578 616d 706c 655f 6e65 7477  rks#example_netw
+00023cf0: 6f72 6b5f 616e 645f 7375 626e 6574 776f  ork_and_subnetwo
+00023d00: 726b 5f73 7065 6369 6669 6361 7469 6f6e  rk_specification
+00023d10: 730a 2020 2020 6461 7461 666c 6f77 5f75  s.    dataflow_u
+00023d20: 7365 5f70 7562 6c69 635f 6970 733a 2053  se_public_ips: S
+00023d30: 7065 6369 6669 6573 2077 6865 7468 6572  pecifies whether
+00023d40: 2044 6174 6166 6c6f 7720 776f 726b 6572   Dataflow worker
+00023d50: 7320 7573 6520 7075 626c 6963 2049 5020  s use public IP 
+00023d60: 6164 6472 6573 7365 732e 0a20 2020 2065  addresses..    e
+00023d70: 6e63 7279 7074 696f 6e5f 7370 6563 5f6b  ncryption_spec_k
+00023d80: 6579 5f6e 616d 653a 2054 6865 204b 4d53  ey_name: The KMS
+00023d90: 206b 6579 206e 616d 652e 0a0a 2020 5265   key name...  Re
+00023da0: 7475 726e 733a 0a20 2020 2054 7570 6c65  turns:.    Tuple
+00023db0: 206f 6620 7069 7065 6c69 6e65 5f64 6566   of pipeline_def
+00023dc0: 696e 6974 696f 6e5f 7061 7468 2061 6e64  inition_path and
+00023dd0: 2070 6172 616d 6574 6572 5f76 616c 7565   parameter_value
+00023de0: 732e 0a20 2022 2222 0a20 2023 2066 6d74  s..  """.  # fmt
+00023df0: 3a20 6f6e 0a20 2070 6172 616d 6574 6572  : on.  parameter
+00023e00: 5f76 616c 7565 7320 3d20 7b7d 0a20 2069  _values = {}.  i
+00023e10: 6620 6973 696e 7374 616e 6365 2874 665f  f isinstance(tf_
+00023e20: 6175 746f 5f74 7261 6e73 666f 726d 5f66  auto_transform_f
+00023e30: 6561 7475 7265 732c 206c 6973 7429 3a0a  eatures, list):.
+00023e40: 2020 2020 7466 5f61 7574 6f5f 7472 616e      tf_auto_tran
+00023e50: 7366 6f72 6d5f 6665 6174 7572 6573 203d  sform_features =
+00023e60: 207b 2761 7574 6f27 3a20 7466 5f61 7574   {'auto': tf_aut
+00023e70: 6f5f 7472 616e 7366 6f72 6d5f 6665 6174  o_transform_feat
+00023e80: 7572 6573 7d0a 0a20 2074 7261 696e 696e  ures}..  trainin
+00023e90: 675f 616e 645f 6576 616c 5f70 6172 616d  g_and_eval_param
+00023ea0: 6574 6572 7320 3d20 7b0a 2020 2020 2020  eters = {.      
+00023eb0: 2770 726f 6a65 6374 273a 2070 726f 6a65  'project': proje
+00023ec0: 6374 2c0a 2020 2020 2020 276c 6f63 6174  ct,.      'locat
+00023ed0: 696f 6e27 3a20 6c6f 6361 7469 6f6e 2c0a  ion': location,.
+00023ee0: 2020 2020 2020 2772 6f6f 745f 6469 7227        'root_dir'
+00023ef0: 3a20 726f 6f74 5f64 6972 2c0a 2020 2020  : root_dir,.    
+00023f00: 2020 2774 6172 6765 745f 636f 6c75 6d6e    'target_column
+00023f10: 273a 2074 6172 6765 745f 636f 6c75 6d6e  ': target_column
+00023f20: 2c0a 2020 2020 2020 276f 626a 6563 7469  ,.      'objecti
+00023f30: 7665 273a 206f 626a 6563 7469 7665 2c0a  ve': objective,.
+00023f40: 2020 2020 2020 2765 7661 6c5f 6d65 7472        'eval_metr
+00023f50: 6963 273a 2065 7661 6c5f 6d65 7472 6963  ic': eval_metric
+00023f60: 2c0a 2020 2020 2020 276e 756d 5f62 6f6f  ,.      'num_boo
+00023f70: 7374 5f72 6f75 6e64 273a 206e 756d 5f62  st_round': num_b
+00023f80: 6f6f 7374 5f72 6f75 6e64 2c0a 2020 2020  oost_round,.    
+00023f90: 2020 2765 6172 6c79 5f73 746f 7070 696e    'early_stoppin
+00023fa0: 675f 726f 756e 6473 273a 2065 6172 6c79  g_rounds': early
+00023fb0: 5f73 746f 7070 696e 675f 726f 756e 6473  _stopping_rounds
+00023fc0: 2c0a 2020 2020 2020 2762 6173 655f 7363  ,.      'base_sc
+00023fd0: 6f72 6527 3a20 6261 7365 5f73 636f 7265  ore': base_score
+00023fe0: 2c0a 2020 2020 2020 2764 6973 6162 6c65  ,.      'disable
+00023ff0: 5f64 6566 6175 6c74 5f65 7661 6c5f 6d65  _default_eval_me
+00024000: 7472 6963 273a 2064 6973 6162 6c65 5f64  tric': disable_d
+00024010: 6566 6175 6c74 5f65 7661 6c5f 6d65 7472  efault_eval_metr
+00024020: 6963 2c0a 2020 2020 2020 2773 6565 6427  ic,.      'seed'
+00024030: 3a20 7365 6564 2c0a 2020 2020 2020 2773  : seed,.      's
+00024040: 6565 645f 7065 725f 6974 6572 6174 696f  eed_per_iteratio
+00024050: 6e27 3a20 7365 6564 5f70 6572 5f69 7465  n': seed_per_ite
+00024060: 7261 7469 6f6e 2c0a 2020 2020 2020 2762  ration,.      'b
+00024070: 6f6f 7374 6572 273a 2062 6f6f 7374 6572  ooster': booster
+00024080: 2c0a 2020 2020 2020 2765 7461 273a 2065  ,.      'eta': e
+00024090: 7461 2c0a 2020 2020 2020 2767 616d 6d61  ta,.      'gamma
+000240a0: 273a 2067 616d 6d61 2c0a 2020 2020 2020  ': gamma,.      
+000240b0: 276d 6178 5f64 6570 7468 273a 206d 6178  'max_depth': max
+000240c0: 5f64 6570 7468 2c0a 2020 2020 2020 276d  _depth,.      'm
+000240d0: 696e 5f63 6869 6c64 5f77 6569 6768 7427  in_child_weight'
+000240e0: 3a20 6d69 6e5f 6368 696c 645f 7765 6967  : min_child_weig
+000240f0: 6874 2c0a 2020 2020 2020 276d 6178 5f64  ht,.      'max_d
+00024100: 656c 7461 5f73 7465 7027 3a20 6d61 785f  elta_step': max_
+00024110: 6465 6c74 615f 7374 6570 2c0a 2020 2020  delta_step,.    
+00024120: 2020 2773 7562 7361 6d70 6c65 273a 2073    'subsample': s
+00024130: 7562 7361 6d70 6c65 2c0a 2020 2020 2020  ubsample,.      
+00024140: 2763 6f6c 7361 6d70 6c65 5f62 7974 7265  'colsample_bytre
+00024150: 6527 3a20 636f 6c73 616d 706c 655f 6279  e': colsample_by
+00024160: 7472 6565 2c0a 2020 2020 2020 2763 6f6c  tree,.      'col
+00024170: 7361 6d70 6c65 5f62 796c 6576 656c 273a  sample_bylevel':
+00024180: 2063 6f6c 7361 6d70 6c65 5f62 796c 6576   colsample_bylev
+00024190: 656c 2c0a 2020 2020 2020 2763 6f6c 7361  el,.      'colsa
+000241a0: 6d70 6c65 5f62 796e 6f64 6527 3a20 636f  mple_bynode': co
+000241b0: 6c73 616d 706c 655f 6279 6e6f 6465 2c0a  lsample_bynode,.
+000241c0: 2020 2020 2020 2772 6567 5f6c 616d 6264        'reg_lambd
+000241d0: 6127 3a20 7265 675f 6c61 6d62 6461 2c0a  a': reg_lambda,.
+000241e0: 2020 2020 2020 2772 6567 5f61 6c70 6861        'reg_alpha
+000241f0: 273a 2072 6567 5f61 6c70 6861 2c0a 2020  ': reg_alpha,.  
+00024200: 2020 2020 2774 7265 655f 6d65 7468 6f64      'tree_method
+00024210: 273a 2074 7265 655f 6d65 7468 6f64 2c0a  ': tree_method,.
+00024220: 2020 2020 2020 2773 6361 6c65 5f70 6f73        'scale_pos
+00024230: 5f77 6569 6768 7427 3a20 7363 616c 655f  _weight': scale_
+00024240: 706f 735f 7765 6967 6874 2c0a 2020 2020  pos_weight,.    
+00024250: 2020 2775 7064 6174 6572 273a 2075 7064    'updater': upd
+00024260: 6174 6572 2c0a 2020 2020 2020 2772 6566  ater,.      'ref
+00024270: 7265 7368 5f6c 6561 6627 3a20 7265 6672  resh_leaf': refr
+00024280: 6573 685f 6c65 6166 2c0a 2020 2020 2020  esh_leaf,.      
+00024290: 2770 726f 6365 7373 5f74 7970 6527 3a20  'process_type': 
+000242a0: 7072 6f63 6573 735f 7479 7065 2c0a 2020  process_type,.  
+000242b0: 2020 2020 2767 726f 775f 706f 6c69 6379      'grow_policy
+000242c0: 273a 2067 726f 775f 706f 6c69 6379 2c0a  ': grow_policy,.
+000242d0: 2020 2020 2020 2773 616d 706c 696e 675f        'sampling_
+000242e0: 6d65 7468 6f64 273a 2073 616d 706c 696e  method': samplin
+000242f0: 675f 6d65 7468 6f64 2c0a 2020 2020 2020  g_method,.      
+00024300: 276d 6f6e 6f74 6f6e 655f 636f 6e73 7472  'monotone_constr
+00024310: 6169 6e74 7327 3a20 6d6f 6e6f 746f 6e65  aints': monotone
+00024320: 5f63 6f6e 7374 7261 696e 7473 2c0a 2020  _constraints,.  
+00024330: 2020 2020 2769 6e74 6572 6163 7469 6f6e      'interaction
+00024340: 5f63 6f6e 7374 7261 696e 7473 273a 2069  _constraints': i
+00024350: 6e74 6572 6163 7469 6f6e 5f63 6f6e 7374  nteraction_const
+00024360: 7261 696e 7473 2c0a 2020 2020 2020 2773  raints,.      's
+00024370: 616d 706c 655f 7479 7065 273a 2073 616d  ample_type': sam
+00024380: 706c 655f 7479 7065 2c0a 2020 2020 2020  ple_type,.      
+00024390: 276e 6f72 6d61 6c69 7a65 5f74 7970 6527  'normalize_type'
+000243a0: 3a20 6e6f 726d 616c 697a 655f 7479 7065  : normalize_type
+000243b0: 2c0a 2020 2020 2020 2772 6174 655f 6472  ,.      'rate_dr
+000243c0: 6f70 273a 2072 6174 655f 6472 6f70 2c0a  op': rate_drop,.
+000243d0: 2020 2020 2020 276f 6e65 5f64 726f 7027        'one_drop'
+000243e0: 3a20 6f6e 655f 6472 6f70 2c0a 2020 2020  : one_drop,.    
+000243f0: 2020 2773 6b69 705f 6472 6f70 273a 2073    'skip_drop': s
+00024400: 6b69 705f 6472 6f70 2c0a 2020 2020 2020  kip_drop,.      
+00024410: 276e 756d 5f70 6172 616c 6c65 6c5f 7472  'num_parallel_tr
+00024420: 6565 273a 206e 756d 5f70 6172 616c 6c65  ee': num_paralle
+00024430: 6c5f 7472 6565 2c0a 2020 2020 2020 2766  l_tree,.      'f
+00024440: 6561 7475 7265 5f73 656c 6563 746f 7227  eature_selector'
+00024450: 3a20 6665 6174 7572 655f 7365 6c65 6374  : feature_select
+00024460: 6f72 2c0a 2020 2020 2020 2774 6f70 5f6b  or,.      'top_k
+00024470: 273a 2074 6f70 5f6b 2c0a 2020 2020 2020  ': top_k,.      
+00024480: 276d 6178 5f63 6174 5f74 6f5f 6f6e 6568  'max_cat_to_oneh
+00024490: 6f74 273a 206d 6178 5f63 6174 5f74 6f5f  ot': max_cat_to_
+000244a0: 6f6e 6568 6f74 2c0a 2020 2020 2020 276d  onehot,.      'm
+000244b0: 6178 5f6c 6561 7665 7327 3a20 6d61 785f  ax_leaves': max_
+000244c0: 6c65 6176 6573 2c0a 2020 2020 2020 276d  leaves,.      'm
+000244d0: 6178 5f62 696e 273a 206d 6178 5f62 696e  ax_bin': max_bin
+000244e0: 2c0a 2020 2020 2020 2774 7765 6564 6965  ,.      'tweedie
+000244f0: 5f76 6172 6961 6e63 655f 706f 7765 7227  _variance_power'
+00024500: 3a20 7477 6565 6469 655f 7661 7269 616e  : tweedie_varian
+00024510: 6365 5f70 6f77 6572 2c0a 2020 2020 2020  ce_power,.      
+00024520: 2768 7562 6572 5f73 6c6f 7065 273a 2068  'huber_slope': h
+00024530: 7562 6572 5f73 6c6f 7065 2c0a 2020 2020  uber_slope,.    
+00024540: 2020 2777 6569 6768 745f 636f 6c75 6d6e    'weight_column
+00024550: 273a 2077 6569 6768 745f 636f 6c75 6d6e  ': weight_column
+00024560: 2c0a 2020 2020 2020 2774 7261 696e 696e  ,.      'trainin
+00024570: 675f 6d61 6368 696e 655f 7479 7065 273a  g_machine_type':
+00024580: 2074 7261 696e 696e 675f 6d61 6368 696e   training_machin
+00024590: 655f 7479 7065 2c0a 2020 2020 2020 2774  e_type,.      't
+000245a0: 7261 696e 696e 675f 746f 7461 6c5f 7265  raining_total_re
+000245b0: 706c 6963 615f 636f 756e 7427 3a20 7472  plica_count': tr
+000245c0: 6169 6e69 6e67 5f74 6f74 616c 5f72 6570  aining_total_rep
+000245d0: 6c69 6361 5f63 6f75 6e74 2c0a 2020 2020  lica_count,.    
+000245e0: 2020 2774 7261 696e 696e 675f 6163 6365    'training_acce
+000245f0: 6c65 7261 746f 725f 7479 7065 273a 2074  lerator_type': t
+00024600: 7261 696e 696e 675f 6163 6365 6c65 7261  raining_accelera
+00024610: 746f 725f 7479 7065 2c0a 2020 2020 2020  tor_type,.      
+00024620: 2774 7261 696e 696e 675f 6163 6365 6c65  'training_accele
+00024630: 7261 746f 725f 636f 756e 7427 3a20 7472  rator_count': tr
+00024640: 6169 6e69 6e67 5f61 6363 656c 6572 6174  aining_accelerat
+00024650: 6f72 5f63 6f75 6e74 2c0a 2020 2020 2020  or_count,.      
+00024660: 2774 7261 6e73 666f 726d 5f64 6174 6166  'transform_dataf
+00024670: 6c6f 775f 6d61 6368 696e 655f 7479 7065  low_machine_type
+00024680: 273a 2074 7261 6e73 666f 726d 5f64 6174  ': transform_dat
+00024690: 6166 6c6f 775f 6d61 6368 696e 655f 7479  aflow_machine_ty
+000246a0: 7065 2c0a 2020 2020 2020 2774 7261 6e73  pe,.      'trans
+000246b0: 666f 726d 5f64 6174 6166 6c6f 775f 6d61  form_dataflow_ma
+000246c0: 785f 6e75 6d5f 776f 726b 6572 7327 3a20  x_num_workers': 
+000246d0: 7472 616e 7366 6f72 6d5f 6461 7461 666c  transform_datafl
+000246e0: 6f77 5f6d 6178 5f6e 756d 5f77 6f72 6b65  ow_max_num_worke
+000246f0: 7273 2c0a 2020 2020 2020 2774 7261 6e73  rs,.      'trans
+00024700: 666f 726d 5f64 6174 6166 6c6f 775f 6469  form_dataflow_di
+00024710: 736b 5f73 697a 655f 6762 273a 2074 7261  sk_size_gb': tra
+00024720: 6e73 666f 726d 5f64 6174 6166 6c6f 775f  nsform_dataflow_
+00024730: 6469 736b 5f73 697a 655f 6762 2c0a 2020  disk_size_gb,.  
+00024740: 2020 2020 2772 756e 5f65 7661 6c75 6174      'run_evaluat
+00024750: 696f 6e27 3a20 7275 6e5f 6576 616c 7561  ion': run_evalua
+00024760: 7469 6f6e 2c0a 2020 2020 2020 2765 7661  tion,.      'eva
+00024770: 6c75 6174 696f 6e5f 6261 7463 685f 7072  luation_batch_pr
+00024780: 6564 6963 745f 6d61 6368 696e 655f 7479  edict_machine_ty
+00024790: 7065 273a 2028 0a20 2020 2020 2020 2020  pe': (.         
+000247a0: 2065 7661 6c75 6174 696f 6e5f 6261 7463   evaluation_batc
+000247b0: 685f 7072 6564 6963 745f 6d61 6368 696e  h_predict_machin
+000247c0: 655f 7479 7065 0a20 2020 2020 2029 2c0a  e_type.      ),.
+000247d0: 2020 2020 2020 2765 7661 6c75 6174 696f        'evaluatio
+000247e0: 6e5f 6261 7463 685f 7072 6564 6963 745f  n_batch_predict_
+000247f0: 7374 6172 7469 6e67 5f72 6570 6c69 6361  starting_replica
+00024800: 5f63 6f75 6e74 273a 2028 0a20 2020 2020  _count': (.     
+00024810: 2020 2020 2065 7661 6c75 6174 696f 6e5f       evaluation_
+00024820: 6261 7463 685f 7072 6564 6963 745f 7374  batch_predict_st
+00024830: 6172 7469 6e67 5f72 6570 6c69 6361 5f63  arting_replica_c
+00024840: 6f75 6e74 0a20 2020 2020 2029 2c0a 2020  ount.      ),.  
+00024850: 2020 2020 2765 7661 6c75 6174 696f 6e5f      'evaluation_
+00024860: 6261 7463 685f 7072 6564 6963 745f 6d61  batch_predict_ma
+00024870: 785f 7265 706c 6963 615f 636f 756e 7427  x_replica_count'
+00024880: 3a20 280a 2020 2020 2020 2020 2020 6576  : (.          ev
+00024890: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+000248a0: 7265 6469 6374 5f6d 6178 5f72 6570 6c69  redict_max_repli
+000248b0: 6361 5f63 6f75 6e74 0a20 2020 2020 2029  ca_count.      )
+000248c0: 2c0a 2020 2020 2020 2765 7661 6c75 6174  ,.      'evaluat
+000248d0: 696f 6e5f 6461 7461 666c 6f77 5f6d 6163  ion_dataflow_mac
+000248e0: 6869 6e65 5f74 7970 6527 3a20 6576 616c  hine_type': eval
+000248f0: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
+00024900: 6d61 6368 696e 655f 7479 7065 2c0a 2020  machine_type,.  
+00024910: 2020 2020 2765 7661 6c75 6174 696f 6e5f      'evaluation_
+00024920: 6461 7461 666c 6f77 5f73 7461 7274 696e  dataflow_startin
+00024930: 675f 6e75 6d5f 776f 726b 6572 7327 3a20  g_num_workers': 
+00024940: 280a 2020 2020 2020 2020 2020 6576 616c  (.          eval
+00024950: 7561 7469 6f6e 5f64 6174 6166 6c6f 775f  uation_dataflow_
+00024960: 7374 6172 7469 6e67 5f6e 756d 5f77 6f72  starting_num_wor
+00024970: 6b65 7273 0a20 2020 2020 2029 2c0a 2020  kers.      ),.  
+00024980: 2020 2020 2765 7661 6c75 6174 696f 6e5f      'evaluation_
+00024990: 6461 7461 666c 6f77 5f6d 6178 5f6e 756d  dataflow_max_num
+000249a0: 5f77 6f72 6b65 7273 273a 2028 0a20 2020  _workers': (.   
+000249b0: 2020 2020 2020 2065 7661 6c75 6174 696f         evaluatio
+000249c0: 6e5f 6461 7461 666c 6f77 5f6d 6178 5f6e  n_dataflow_max_n
+000249d0: 756d 5f77 6f72 6b65 7273 0a20 2020 2020  um_workers.     
+000249e0: 2029 2c0a 2020 2020 2020 2765 7661 6c75   ),.      'evalu
+000249f0: 6174 696f 6e5f 6461 7461 666c 6f77 5f64  ation_dataflow_d
+00024a00: 6973 6b5f 7369 7a65 5f67 6227 3a20 6576  isk_size_gb': ev
+00024a10: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
+00024a20: 775f 6469 736b 5f73 697a 655f 6762 2c0a  w_disk_size_gb,.
+00024a30: 2020 2020 2020 2764 6174 6166 6c6f 775f        'dataflow_
+00024a40: 7365 7276 6963 655f 6163 636f 756e 7427  service_account'
+00024a50: 3a20 6461 7461 666c 6f77 5f73 6572 7669  : dataflow_servi
+00024a60: 6365 5f61 6363 6f75 6e74 2c0a 2020 2020  ce_account,.    
+00024a70: 2020 2764 6174 6166 6c6f 775f 7375 626e    'dataflow_subn
+00024a80: 6574 776f 726b 273a 2064 6174 6166 6c6f  etwork': dataflo
+00024a90: 775f 7375 626e 6574 776f 726b 2c0a 2020  w_subnetwork,.  
+00024aa0: 2020 2020 2764 6174 6166 6c6f 775f 7573      'dataflow_us
+00024ab0: 655f 7075 626c 6963 5f69 7073 273a 2064  e_public_ips': d
+00024ac0: 6174 6166 6c6f 775f 7573 655f 7075 626c  ataflow_use_publ
+00024ad0: 6963 5f69 7073 2c0a 2020 2020 2020 2765  ic_ips,.      'e
+00024ae0: 6e63 7279 7074 696f 6e5f 7370 6563 5f6b  ncryption_spec_k
+00024af0: 6579 5f6e 616d 6527 3a20 656e 6372 7970  ey_name': encryp
+00024b00: 7469 6f6e 5f73 7065 635f 6b65 795f 6e61  tion_spec_key_na
+00024b10: 6d65 2c0a 2020 7d0a 2020 5f75 7064 6174  me,.  }.  _updat
+00024b20: 655f 7061 7261 6d65 7465 7273 2870 6172  e_parameters(par
+00024b30: 616d 6574 6572 5f76 616c 7565 732c 2074  ameter_values, t
+00024b40: 7261 696e 696e 675f 616e 645f 6576 616c  raining_and_eval
+00024b50: 5f70 6172 616d 6574 6572 7329 0a0a 2020  _parameters)..  
+00024b60: 6674 655f 7061 7261 6d73 203d 207b 0a20  fte_params = {. 
+00024b70: 2020 2020 2027 6461 7461 7365 745f 6c65       'dataset_le
+00024b80: 7665 6c5f 6375 7374 6f6d 5f74 7261 6e73  vel_custom_trans
+00024b90: 666f 726d 6174 696f 6e5f 6465 6669 6e69  formation_defini
+00024ba0: 7469 6f6e 7327 3a20 280a 2020 2020 2020  tions': (.      
+00024bb0: 2020 2020 6461 7461 7365 745f 6c65 7665      dataset_leve
+00024bc0: 6c5f 6375 7374 6f6d 5f74 7261 6e73 666f  l_custom_transfo
+00024bd0: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
+00024be0: 6f6e 730a 2020 2020 2020 2020 2020 6966  ons.          if
+00024bf0: 2064 6174 6173 6574 5f6c 6576 656c 5f63   dataset_level_c
+00024c00: 7573 746f 6d5f 7472 616e 7366 6f72 6d61  ustom_transforma
+00024c10: 7469 6f6e 5f64 6566 696e 6974 696f 6e73  tion_definitions
+00024c20: 0a20 2020 2020 2020 2020 2065 6c73 6520  .          else 
+00024c30: 5b5d 0a20 2020 2020 2029 2c0a 2020 2020  [].      ),.    
+00024c40: 2020 2764 6174 6173 6574 5f6c 6576 656c    'dataset_level
+00024c50: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
+00024c60: 273a 2028 0a20 2020 2020 2020 2020 2064  ': (.          d
+00024c70: 6174 6173 6574 5f6c 6576 656c 5f74 7261  ataset_level_tra
+00024c80: 6e73 666f 726d 6174 696f 6e73 2069 6620  nsformations if 
+00024c90: 6461 7461 7365 745f 6c65 7665 6c5f 7472  dataset_level_tr
+00024ca0: 616e 7366 6f72 6d61 7469 6f6e 7320 656c  ansformations el
+00024cb0: 7365 205b 5d0a 2020 2020 2020 292c 0a20  se [].      ),. 
+00024cc0: 2020 2020 2027 7275 6e5f 6665 6174 7572       'run_featur
+00024cd0: 655f 7365 6c65 6374 696f 6e27 3a20 7275  e_selection': ru
+00024ce0: 6e5f 6665 6174 7572 655f 7365 6c65 6374  n_feature_select
+00024cf0: 696f 6e2c 0a20 2020 2020 2027 6665 6174  ion,.      'feat
+00024d00: 7572 655f 7365 6c65 6374 696f 6e5f 616c  ure_selection_al
+00024d10: 676f 7269 7468 6d27 3a20 6665 6174 7572  gorithm': featur
+00024d20: 655f 7365 6c65 6374 696f 6e5f 616c 676f  e_selection_algo
+00024d30: 7269 7468 6d2c 0a20 2020 2020 2027 6d61  rithm,.      'ma
+00024d40: 785f 7365 6c65 6374 6564 5f66 6561 7475  x_selected_featu
+00024d50: 7265 7327 3a20 6d61 785f 7365 6c65 6374  res': max_select
+00024d60: 6564 5f66 6561 7475 7265 732c 0a20 2020  ed_features,.   
+00024d70: 2020 2027 7072 6564 6566 696e 6564 5f73     'predefined_s
+00024d80: 706c 6974 5f6b 6579 273a 2070 7265 6465  plit_key': prede
+00024d90: 6669 6e65 645f 7370 6c69 745f 6b65 792c  fined_split_key,
+00024da0: 0a20 2020 2020 2027 7374 7261 7469 6669  .      'stratifi
+00024db0: 6564 5f73 706c 6974 5f6b 6579 273a 2073  ed_split_key': s
+00024dc0: 7472 6174 6966 6965 645f 7370 6c69 745f  tratified_split_
+00024dd0: 6b65 792c 0a20 2020 2020 2027 7472 6169  key,.      'trai
+00024de0: 6e69 6e67 5f66 7261 6374 696f 6e27 3a20  ning_fraction': 
+00024df0: 7472 6169 6e69 6e67 5f66 7261 6374 696f  training_fractio
+00024e00: 6e2c 0a20 2020 2020 2027 7661 6c69 6461  n,.      'valida
+00024e10: 7469 6f6e 5f66 7261 6374 696f 6e27 3a20  tion_fraction': 
+00024e20: 7661 6c69 6461 7469 6f6e 5f66 7261 6374  validation_fract
+00024e30: 696f 6e2c 0a20 2020 2020 2027 7465 7374  ion,.      'test
+00024e40: 5f66 7261 6374 696f 6e27 3a20 7465 7374  _fraction': test
+00024e50: 5f66 7261 6374 696f 6e2c 0a20 2020 2020  _fraction,.     
+00024e60: 2027 7466 5f61 7574 6f5f 7472 616e 7366   'tf_auto_transf
+00024e70: 6f72 6d5f 6665 6174 7572 6573 273a 2028  orm_features': (
+00024e80: 0a20 2020 2020 2020 2020 2074 665f 6175  .          tf_au
+00024e90: 746f 5f74 7261 6e73 666f 726d 5f66 6561  to_transform_fea
+00024ea0: 7475 7265 7320 6966 2074 665f 6175 746f  tures if tf_auto
+00024eb0: 5f74 7261 6e73 666f 726d 5f66 6561 7475  _transform_featu
+00024ec0: 7265 7320 656c 7365 207b 7d0a 2020 2020  res else {}.    
+00024ed0: 2020 292c 0a20 2020 2020 2027 7466 5f63    ),.      'tf_c
+00024ee0: 7573 746f 6d5f 7472 616e 7366 6f72 6d61  ustom_transforma
+00024ef0: 7469 6f6e 5f64 6566 696e 6974 696f 6e73  tion_definitions
+00024f00: 273a 2028 0a20 2020 2020 2020 2020 2074  ': (.          t
+00024f10: 665f 6375 7374 6f6d 5f74 7261 6e73 666f  f_custom_transfo
+00024f20: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
+00024f30: 6f6e 730a 2020 2020 2020 2020 2020 6966  ons.          if
+00024f40: 2074 665f 6375 7374 6f6d 5f74 7261 6e73   tf_custom_trans
+00024f50: 666f 726d 6174 696f 6e5f 6465 6669 6e69  formation_defini
+00024f60: 7469 6f6e 730a 2020 2020 2020 2020 2020  tions.          
+00024f70: 656c 7365 205b 5d0a 2020 2020 2020 292c  else [].      ),
+00024f80: 0a20 2020 2020 2027 7466 5f74 7261 6e73  .      'tf_trans
+00024f90: 666f 726d 6174 696f 6e73 5f70 6174 6827  formations_path'
+00024fa0: 3a20 7466 5f74 7261 6e73 666f 726d 6174  : tf_transformat
+00024fb0: 696f 6e73 5f70 6174 682c 0a20 207d 0a20  ions_path,.  }. 
+00024fc0: 205f 7570 6461 7465 5f70 6172 616d 6574   _update_paramet
+00024fd0: 6572 7328 7061 7261 6d65 7465 725f 7661  ers(parameter_va
+00024fe0: 6c75 6573 2c20 6674 655f 7061 7261 6d73  lues, fte_params
+00024ff0: 290a 0a20 2064 6174 615f 736f 7572 6365  )..  data_source
+00025000: 5f61 6e64 5f73 706c 6974 5f70 6172 616d  _and_split_param
+00025010: 6574 6572 7320 3d20 7b0a 2020 2020 2020  eters = {.      
+00025020: 2764 6174 615f 736f 7572 6365 5f63 7376  'data_source_csv
+00025030: 5f66 696c 656e 616d 6573 273a 2064 6174  _filenames': dat
+00025040: 615f 736f 7572 6365 5f63 7376 5f66 696c  a_source_csv_fil
+00025050: 656e 616d 6573 2c0a 2020 2020 2020 2764  enames,.      'd
+00025060: 6174 615f 736f 7572 6365 5f62 6967 7175  ata_source_bigqu
+00025070: 6572 795f 7461 626c 655f 7061 7468 273a  ery_table_path':
+00025080: 2064 6174 615f 736f 7572 6365 5f62 6967   data_source_big
+00025090: 7175 6572 795f 7461 626c 655f 7061 7468  query_table_path
+000250a0: 2c0a 2020 2020 2020 2762 6967 7175 6572  ,.      'bigquer
+000250b0: 795f 7374 6167 696e 675f 6675 6c6c 5f64  y_staging_full_d
+000250c0: 6174 6173 6574 5f69 6427 3a20 6269 6771  ataset_id': bigq
+000250d0: 7565 7279 5f73 7461 6769 6e67 5f66 756c  uery_staging_ful
+000250e0: 6c5f 6461 7461 7365 745f 6964 2c0a 2020  l_dataset_id,.  
+000250f0: 7d0a 2020 5f75 7064 6174 655f 7061 7261  }.  _update_para
+00025100: 6d65 7465 7273 2870 6172 616d 6574 6572  meters(parameter
+00025110: 5f76 616c 7565 732c 2064 6174 615f 736f  _values, data_so
+00025120: 7572 6365 5f61 6e64 5f73 706c 6974 5f70  urce_and_split_p
+00025130: 6172 616d 6574 6572 7329 0a0a 2020 7069  arameters)..  pi
+00025140: 7065 6c69 6e65 5f64 6566 696e 6974 696f  peline_definitio
+00025150: 6e5f 7061 7468 203d 206f 732e 7061 7468  n_path = os.path
+00025160: 2e6a 6f69 6e28 0a20 2020 2020 2070 6174  .join(.      pat
+00025170: 686c 6962 2e50 6174 6828 5f5f 6669 6c65  hlib.Path(__file
+00025180: 5f5f 292e 7061 7265 6e74 2e72 6573 6f6c  __).parent.resol
+00025190: 7665 2829 2c20 2778 6762 6f6f 7374 5f74  ve(), 'xgboost_t
+000251a0: 7261 696e 6572 5f70 6970 656c 696e 652e  rainer_pipeline.
+000251b0: 7961 6d6c 270a 2020 290a 0a20 2072 6574  yaml'.  )..  ret
+000251c0: 7572 6e20 7069 7065 6c69 6e65 5f64 6566  urn pipeline_def
+000251d0: 696e 6974 696f 6e5f 7061 7468 2c20 7061  inition_path, pa
+000251e0: 7261 6d65 7465 725f 7661 6c75 6573 0a0a  rameter_values..
+000251f0: 0a64 6566 2067 6574 5f78 6762 6f6f 7374  .def get_xgboost
+00025200: 5f68 7970 6572 7061 7261 6d65 7465 725f  _hyperparameter_
+00025210: 7475 6e69 6e67 5f6a 6f62 5f70 6970 656c  tuning_job_pipel
+00025220: 696e 655f 616e 645f 7061 7261 6d65 7465  ine_and_paramete
+00025230: 7273 280a 2020 2020 7072 6f6a 6563 743a  rs(.    project:
+00025240: 2073 7472 2c0a 2020 2020 6c6f 6361 7469   str,.    locati
+00025250: 6f6e 3a20 7374 722c 0a20 2020 2072 6f6f  on: str,.    roo
+00025260: 745f 6469 723a 2073 7472 2c0a 2020 2020  t_dir: str,.    
+00025270: 7461 7267 6574 5f63 6f6c 756d 6e3a 2073  target_column: s
+00025280: 7472 2c0a 2020 2020 6f62 6a65 6374 6976  tr,.    objectiv
+00025290: 653a 2073 7472 2c0a 2020 2020 7374 7564  e: str,.    stud
+000252a0: 795f 7370 6563 5f6d 6574 7269 635f 6964  y_spec_metric_id
+000252b0: 3a20 7374 722c 0a20 2020 2073 7475 6479  : str,.    study
+000252c0: 5f73 7065 635f 6d65 7472 6963 5f67 6f61  _spec_metric_goa
+000252d0: 6c3a 2073 7472 2c0a 2020 2020 6d61 785f  l: str,.    max_
+000252e0: 7472 6961 6c5f 636f 756e 743a 2069 6e74  trial_count: int
+000252f0: 2c0a 2020 2020 7061 7261 6c6c 656c 5f74  ,.    parallel_t
+00025300: 7269 616c 5f63 6f75 6e74 3a20 696e 742c  rial_count: int,
+00025310: 0a20 2020 2073 7475 6479 5f73 7065 635f  .    study_spec_
+00025320: 7061 7261 6d65 7465 7273 5f6f 7665 7272  parameters_overr
+00025330: 6964 653a 204f 7074 696f 6e61 6c5b 4c69  ide: Optional[Li
+00025340: 7374 5b44 6963 745b 7374 722c 2041 6e79  st[Dict[str, Any
+00025350: 5d5d 5d20 3d20 4e6f 6e65 2c0a 2020 2020  ]]] = None,.    
+00025360: 6576 616c 5f6d 6574 7269 633a 204f 7074  eval_metric: Opt
+00025370: 696f 6e61 6c5b 7374 725d 203d 204e 6f6e  ional[str] = Non
+00025380: 652c 0a20 2020 2064 6973 6162 6c65 5f64  e,.    disable_d
+00025390: 6566 6175 6c74 5f65 7661 6c5f 6d65 7472  efault_eval_metr
+000253a0: 6963 3a20 4f70 7469 6f6e 616c 5b69 6e74  ic: Optional[int
+000253b0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7365  ] = None,.    se
+000253c0: 6564 3a20 4f70 7469 6f6e 616c 5b69 6e74  ed: Optional[int
+000253d0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7365  ] = None,.    se
+000253e0: 6564 5f70 6572 5f69 7465 7261 7469 6f6e  ed_per_iteration
+000253f0: 3a20 4f70 7469 6f6e 616c 5b62 6f6f 6c5d  : Optional[bool]
+00025400: 203d 204e 6f6e 652c 0a20 2020 2064 6174   = None,.    dat
+00025410: 6173 6574 5f6c 6576 656c 5f63 7573 746f  aset_level_custo
+00025420: 6d5f 7472 616e 7366 6f72 6d61 7469 6f6e  m_transformation
+00025430: 5f64 6566 696e 6974 696f 6e73 3a20 4f70  _definitions: Op
+00025440: 7469 6f6e 616c 5b0a 2020 2020 2020 2020  tional[.        
+00025450: 4c69 7374 5b44 6963 745b 7374 722c 2041  List[Dict[str, A
+00025460: 6e79 5d5d 0a20 2020 205d 203d 204e 6f6e  ny]].    ] = Non
+00025470: 652c 0a20 2020 2064 6174 6173 6574 5f6c  e,.    dataset_l
+00025480: 6576 656c 5f74 7261 6e73 666f 726d 6174  evel_transformat
+00025490: 696f 6e73 3a20 4f70 7469 6f6e 616c 5b4c  ions: Optional[L
+000254a0: 6973 745b 4469 6374 5b73 7472 2c20 416e  ist[Dict[str, An
+000254b0: 795d 5d5d 203d 204e 6f6e 652c 0a20 2020  y]]] = None,.   
+000254c0: 2072 756e 5f66 6561 7475 7265 5f73 656c   run_feature_sel
+000254d0: 6563 7469 6f6e 3a20 4f70 7469 6f6e 616c  ection: Optional
+000254e0: 5b62 6f6f 6c5d 203d 204e 6f6e 652c 0a20  [bool] = None,. 
+000254f0: 2020 2066 6561 7475 7265 5f73 656c 6563     feature_selec
+00025500: 7469 6f6e 5f61 6c67 6f72 6974 686d 3a20  tion_algorithm: 
+00025510: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
+00025520: 4e6f 6e65 2c0a 2020 2020 6d61 785f 7365  None,.    max_se
+00025530: 6c65 6374 6564 5f66 6561 7475 7265 733a  lected_features:
+00025540: 204f 7074 696f 6e61 6c5b 696e 745d 203d   Optional[int] =
+00025550: 204e 6f6e 652c 0a20 2020 2070 7265 6465   None,.    prede
+00025560: 6669 6e65 645f 7370 6c69 745f 6b65 793a  fined_split_key:
+00025570: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
+00025580: 204e 6f6e 652c 0a20 2020 2073 7472 6174   None,.    strat
+00025590: 6966 6965 645f 7370 6c69 745f 6b65 793a  ified_split_key:
+000255a0: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
+000255b0: 204e 6f6e 652c 0a20 2020 2074 7261 696e   None,.    train
+000255c0: 696e 675f 6672 6163 7469 6f6e 3a20 4f70  ing_fraction: Op
+000255d0: 7469 6f6e 616c 5b66 6c6f 6174 5d20 3d20  tional[float] = 
+000255e0: 4e6f 6e65 2c0a 2020 2020 7661 6c69 6461  None,.    valida
+000255f0: 7469 6f6e 5f66 7261 6374 696f 6e3a 204f  tion_fraction: O
+00025600: 7074 696f 6e61 6c5b 666c 6f61 745d 203d  ptional[float] =
+00025610: 204e 6f6e 652c 0a20 2020 2074 6573 745f   None,.    test_
+00025620: 6672 6163 7469 6f6e 3a20 4f70 7469 6f6e  fraction: Option
+00025630: 616c 5b66 6c6f 6174 5d20 3d20 4e6f 6e65  al[float] = None
+00025640: 2c0a 2020 2020 7466 5f61 7574 6f5f 7472  ,.    tf_auto_tr
+00025650: 616e 7366 6f72 6d5f 6665 6174 7572 6573  ansform_features
+00025660: 3a20 4f70 7469 6f6e 616c 5b0a 2020 2020  : Optional[.    
+00025670: 2020 2020 556e 696f 6e5b 4c69 7374 5b73      Union[List[s
+00025680: 7472 5d2c 2044 6963 745b 7374 722c 204c  tr], Dict[str, L
+00025690: 6973 745b 7374 725d 5d5d 0a20 2020 205d  ist[str]]].    ]
+000256a0: 203d 204e 6f6e 652c 0a20 2020 2074 665f   = None,.    tf_
+000256b0: 6375 7374 6f6d 5f74 7261 6e73 666f 726d  custom_transform
+000256c0: 6174 696f 6e5f 6465 6669 6e69 7469 6f6e  ation_definition
+000256d0: 733a 204f 7074 696f 6e61 6c5b 4c69 7374  s: Optional[List
+000256e0: 5b44 6963 745b 7374 722c 2041 6e79 5d5d  [Dict[str, Any]]
+000256f0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 7466  ] = None,.    tf
+00025700: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
+00025710: 5f70 6174 683a 204f 7074 696f 6e61 6c5b  _path: Optional[
+00025720: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
+00025730: 2064 6174 615f 736f 7572 6365 5f63 7376   data_source_csv
+00025740: 5f66 696c 656e 616d 6573 3a20 4f70 7469  _filenames: Opti
+00025750: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
+00025760: 2c0a 2020 2020 6461 7461 5f73 6f75 7263  ,.    data_sourc
+00025770: 655f 6269 6771 7565 7279 5f74 6162 6c65  e_bigquery_table
+00025780: 5f70 6174 683a 204f 7074 696f 6e61 6c5b  _path: Optional[
+00025790: 7374 725d 203d 204e 6f6e 652c 0a20 2020  str] = None,.   
+000257a0: 2062 6967 7175 6572 795f 7374 6167 696e   bigquery_stagin
+000257b0: 675f 6675 6c6c 5f64 6174 6173 6574 5f69  g_full_dataset_i
+000257c0: 643a 204f 7074 696f 6e61 6c5b 7374 725d  d: Optional[str]
+000257d0: 203d 204e 6f6e 652c 0a20 2020 2077 6569   = None,.    wei
+000257e0: 6768 745f 636f 6c75 6d6e 3a20 4f70 7469  ght_column: Opti
+000257f0: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
+00025800: 2c0a 2020 2020 6d61 785f 6661 696c 6564  ,.    max_failed
+00025810: 5f74 7269 616c 5f63 6f75 6e74 3a20 4f70  _trial_count: Op
+00025820: 7469 6f6e 616c 5b69 6e74 5d20 3d20 4e6f  tional[int] = No
+00025830: 6e65 2c0a 2020 2020 7472 6169 6e69 6e67  ne,.    training
+00025840: 5f6d 6163 6869 6e65 5f74 7970 653a 204f  _machine_type: O
+00025850: 7074 696f 6e61 6c5b 7374 725d 203d 204e  ptional[str] = N
+00025860: 6f6e 652c 0a20 2020 2074 7261 696e 696e  one,.    trainin
+00025870: 675f 746f 7461 6c5f 7265 706c 6963 615f  g_total_replica_
+00025880: 636f 756e 743a 204f 7074 696f 6e61 6c5b  count: Optional[
+00025890: 696e 745d 203d 204e 6f6e 652c 0a20 2020  int] = None,.   
+000258a0: 2074 7261 696e 696e 675f 6163 6365 6c65   training_accele
+000258b0: 7261 746f 725f 7479 7065 3a20 4f70 7469  rator_type: Opti
+000258c0: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
+000258d0: 2c0a 2020 2020 7472 6169 6e69 6e67 5f61  ,.    training_a
+000258e0: 6363 656c 6572 6174 6f72 5f63 6f75 6e74  ccelerator_count
+000258f0: 3a20 4f70 7469 6f6e 616c 5b69 6e74 5d20  : Optional[int] 
+00025900: 3d20 4e6f 6e65 2c0a 2020 2020 7374 7564  = None,.    stud
+00025910: 795f 7370 6563 5f61 6c67 6f72 6974 686d  y_spec_algorithm
+00025920: 3a20 4f70 7469 6f6e 616c 5b73 7472 5d20  : Optional[str] 
+00025930: 3d20 4e6f 6e65 2c0a 2020 2020 7374 7564  = None,.    stud
+00025940: 795f 7370 6563 5f6d 6561 7375 7265 6d65  y_spec_measureme
+00025950: 6e74 5f73 656c 6563 7469 6f6e 5f74 7970  nt_selection_typ
+00025960: 653a 204f 7074 696f 6e61 6c5b 7374 725d  e: Optional[str]
+00025970: 203d 204e 6f6e 652c 0a20 2020 2074 7261   = None,.    tra
+00025980: 6e73 666f 726d 5f64 6174 6166 6c6f 775f  nsform_dataflow_
+00025990: 6d61 6368 696e 655f 7479 7065 3a20 4f70  machine_type: Op
+000259a0: 7469 6f6e 616c 5b73 7472 5d20 3d20 4e6f  tional[str] = No
+000259b0: 6e65 2c0a 2020 2020 7472 616e 7366 6f72  ne,.    transfor
+000259c0: 6d5f 6461 7461 666c 6f77 5f6d 6178 5f6e  m_dataflow_max_n
+000259d0: 756d 5f77 6f72 6b65 7273 3a20 4f70 7469  um_workers: Opti
+000259e0: 6f6e 616c 5b69 6e74 5d20 3d20 4e6f 6e65  onal[int] = None
+000259f0: 2c0a 2020 2020 7472 616e 7366 6f72 6d5f  ,.    transform_
+00025a00: 6461 7461 666c 6f77 5f64 6973 6b5f 7369  dataflow_disk_si
+00025a10: 7a65 5f67 623a 204f 7074 696f 6e61 6c5b  ze_gb: Optional[
+00025a20: 696e 745d 203d 204e 6f6e 652c 0a20 2020  int] = None,.   
+00025a30: 2072 756e 5f65 7661 6c75 6174 696f 6e3a   run_evaluation:
+00025a40: 204f 7074 696f 6e61 6c5b 626f 6f6c 5d20   Optional[bool] 
+00025a50: 3d20 4e6f 6e65 2c0a 2020 2020 6576 616c  = None,.    eval
+00025a60: 7561 7469 6f6e 5f62 6174 6368 5f70 7265  uation_batch_pre
+00025a70: 6469 6374 5f6d 6163 6869 6e65 5f74 7970  dict_machine_typ
+00025a80: 653a 204f 7074 696f 6e61 6c5b 7374 725d  e: Optional[str]
+00025a90: 203d 204e 6f6e 652c 0a20 2020 2065 7661   = None,.    eva
+00025aa0: 6c75 6174 696f 6e5f 6261 7463 685f 7072  luation_batch_pr
+00025ab0: 6564 6963 745f 7374 6172 7469 6e67 5f72  edict_starting_r
+00025ac0: 6570 6c69 6361 5f63 6f75 6e74 3a20 4f70  eplica_count: Op
+00025ad0: 7469 6f6e 616c 5b69 6e74 5d20 3d20 4e6f  tional[int] = No
+00025ae0: 6e65 2c0a 2020 2020 6576 616c 7561 7469  ne,.    evaluati
+00025af0: 6f6e 5f62 6174 6368 5f70 7265 6469 6374  on_batch_predict
+00025b00: 5f6d 6178 5f72 6570 6c69 6361 5f63 6f75  _max_replica_cou
+00025b10: 6e74 3a20 4f70 7469 6f6e 616c 5b69 6e74  nt: Optional[int
+00025b20: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6576  ] = None,.    ev
+00025b30: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
+00025b40: 775f 6d61 6368 696e 655f 7479 7065 3a20  w_machine_type: 
+00025b50: 4f70 7469 6f6e 616c 5b73 7472 5d20 3d20  Optional[str] = 
+00025b60: 4e6f 6e65 2c0a 2020 2020 6576 616c 7561  None,.    evalua
+00025b70: 7469 6f6e 5f64 6174 6166 6c6f 775f 7374  tion_dataflow_st
+00025b80: 6172 7469 6e67 5f6e 756d 5f77 6f72 6b65  arting_num_worke
+00025b90: 7273 3a20 4f70 7469 6f6e 616c 5b69 6e74  rs: Optional[int
+00025ba0: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6576  ] = None,.    ev
+00025bb0: 616c 7561 7469 6f6e 5f64 6174 6166 6c6f  aluation_dataflo
+00025bc0: 775f 6d61 785f 6e75 6d5f 776f 726b 6572  w_max_num_worker
+00025bd0: 733a 204f 7074 696f 6e61 6c5b 696e 745d  s: Optional[int]
+00025be0: 203d 204e 6f6e 652c 0a20 2020 2065 7661   = None,.    eva
+00025bf0: 6c75 6174 696f 6e5f 6461 7461 666c 6f77  luation_dataflow
+00025c00: 5f64 6973 6b5f 7369 7a65 5f67 623a 204f  _disk_size_gb: O
+00025c10: 7074 696f 6e61 6c5b 696e 745d 203d 204e  ptional[int] = N
+00025c20: 6f6e 652c 0a20 2020 2064 6174 6166 6c6f  one,.    dataflo
+00025c30: 775f 7365 7276 6963 655f 6163 636f 756e  w_service_accoun
+00025c40: 743a 204f 7074 696f 6e61 6c5b 7374 725d  t: Optional[str]
+00025c50: 203d 204e 6f6e 652c 0a20 2020 2064 6174   = None,.    dat
+00025c60: 6166 6c6f 775f 7375 626e 6574 776f 726b  aflow_subnetwork
+00025c70: 3a20 4f70 7469 6f6e 616c 5b73 7472 5d20  : Optional[str] 
+00025c80: 3d20 4e6f 6e65 2c0a 2020 2020 6461 7461  = None,.    data
+00025c90: 666c 6f77 5f75 7365 5f70 7562 6c69 635f  flow_use_public_
+00025ca0: 6970 733a 204f 7074 696f 6e61 6c5b 626f  ips: Optional[bo
+00025cb0: 6f6c 5d20 3d20 4e6f 6e65 2c0a 2020 2020  ol] = None,.    
+00025cc0: 656e 6372 7970 7469 6f6e 5f73 7065 635f  encryption_spec_
+00025cd0: 6b65 795f 6e61 6d65 3a20 4f70 7469 6f6e  key_name: Option
+00025ce0: 616c 5b73 7472 5d20 3d20 4e6f 6e65 2c0a  al[str] = None,.
+00025cf0: 293a 0a20 2023 2066 6d74 3a20 6f66 660a  ):.  # fmt: off.
+00025d00: 2020 2222 2247 6574 2074 6865 2058 4742    """Get the XGB
+00025d10: 6f6f 7374 2048 7970 6572 7061 7261 6d65  oost Hyperparame
+00025d20: 7465 7254 756e 696e 674a 6f62 2070 6970  terTuningJob pip
+00025d30: 656c 696e 652e 0a0a 2020 4172 6773 3a0a  eline...  Args:.
+00025d40: 2020 2020 7072 6f6a 6563 743a 2054 6865      project: The
+00025d50: 2047 4350 2070 726f 6a65 6374 2074 6861   GCP project tha
+00025d60: 7420 7275 6e73 2074 6865 2070 6970 656c  t runs the pipel
+00025d70: 696e 6520 636f 6d70 6f6e 656e 7473 2e0a  ine components..
+00025d80: 2020 2020 6c6f 6361 7469 6f6e 3a20 5468      location: Th
+00025d90: 6520 4743 5020 7265 6769 6f6e 2074 6861  e GCP region tha
+00025da0: 7420 7275 6e73 2074 6865 2070 6970 656c  t runs the pipel
+00025db0: 696e 6520 636f 6d70 6f6e 656e 7473 2e0a  ine components..
+00025dc0: 2020 2020 726f 6f74 5f64 6972 3a20 5468      root_dir: Th
+00025dd0: 6520 726f 6f74 2047 4353 2064 6972 6563  e root GCS direc
+00025de0: 746f 7279 2066 6f72 2074 6865 2070 6970  tory for the pip
+00025df0: 656c 696e 6520 636f 6d70 6f6e 656e 7473  eline components
+00025e00: 2e0a 2020 2020 7461 7267 6574 5f63 6f6c  ..    target_col
+00025e10: 756d 6e3a 2054 6865 2074 6172 6765 7420  umn: The target 
+00025e20: 636f 6c75 6d6e 206e 616d 652e 0a20 2020  column name..   
+00025e30: 206f 626a 6563 7469 7665 3a20 5370 6563   objective: Spec
+00025e40: 6966 6965 7320 7468 6520 6c65 6172 6e69  ifies the learni
+00025e50: 6e67 2074 6173 6b20 616e 6420 7468 6520  ng task and the 
+00025e60: 6c65 6172 6e69 6e67 206f 626a 6563 7469  learning objecti
+00025e70: 7665 2e20 4d75 7374 2062 6520 6f6e 6520  ve. Must be one 
+00025e80: 6f66 205b 7265 673a 7371 7561 7265 6465  of [reg:squarede
+00025e90: 7272 6f72 2c20 7265 673a 7371 7561 7265  rror, reg:square
+00025ea0: 646c 6f67 6572 726f 722c 2072 6567 3a6c  dlogerror, reg:l
+00025eb0: 6f67 6973 7469 632c 2072 6567 3a67 616d  ogistic, reg:gam
+00025ec0: 6d61 2c20 7265 673a 7477 6565 6469 652c  ma, reg:tweedie,
+00025ed0: 2072 6567 3a70 7365 7564 6f68 7562 6572   reg:pseudohuber
+00025ee0: 6572 726f 722c 2062 696e 6172 793a 6c6f  error, binary:lo
+00025ef0: 6769 7374 6963 2c20 6d75 6c74 693a 736f  gistic, multi:so
+00025f00: 6674 7072 6f62 5d2e 0a20 2020 2073 7475  ftprob]..    stu
+00025f10: 6479 5f73 7065 635f 6d65 7472 6963 5f69  dy_spec_metric_i
+00025f20: 643a 204d 6574 7269 6320 746f 206f 7074  d: Metric to opt
+00025f30: 696d 697a 652e 2046 6f72 206f 7074 696f  imize. For optio
+00025f40: 6e73 2c20 706c 6561 7365 206c 6f6f 6b20  ns, please look 
+00025f50: 756e 6465 7220 2765 7661 6c5f 6d65 7472  under 'eval_metr
+00025f60: 6963 2720 6174 2068 7474 7073 3a2f 2f78  ic' at https://x
+00025f70: 6762 6f6f 7374 2e72 6561 6474 6865 646f  gboost.readthedo
+00025f80: 6373 2e69 6f2f 656e 2f73 7461 626c 652f  cs.io/en/stable/
+00025f90: 7061 7261 6d65 7465 722e 6874 6d6c 236c  parameter.html#l
+00025fa0: 6561 726e 696e 672d 7461 736b 2d70 6172  earning-task-par
+00025fb0: 616d 6574 6572 732e 0a20 2020 2073 7475  ameters..    stu
+00025fc0: 6479 5f73 7065 635f 6d65 7472 6963 5f67  dy_spec_metric_g
+00025fd0: 6f61 6c3a 204f 7074 696d 697a 6174 696f  oal: Optimizatio
+00025fe0: 6e20 676f 616c 206f 6620 7468 6520 6d65  n goal of the me
+00025ff0: 7472 6963 2c20 706f 7373 6962 6c65 2076  tric, possible v
+00026000: 616c 7565 733a 2022 4d41 5849 4d49 5a45  alues: "MAXIMIZE
+00026010: 222c 2022 4d49 4e49 4d49 5a45 222e 0a20  ", "MINIMIZE".. 
+00026020: 2020 206d 6178 5f74 7269 616c 5f63 6f75     max_trial_cou
+00026030: 6e74 3a20 5468 6520 6465 7369 7265 6420  nt: The desired 
+00026040: 746f 7461 6c20 6e75 6d62 6572 206f 6620  total number of 
+00026050: 7472 6961 6c73 2e0a 2020 2020 7061 7261  trials..    para
+00026060: 6c6c 656c 5f74 7269 616c 5f63 6f75 6e74  llel_trial_count
+00026070: 3a20 5468 6520 6465 7369 7265 6420 6e75  : The desired nu
+00026080: 6d62 6572 206f 6620 7472 6961 6c73 2074  mber of trials t
+00026090: 6f20 7275 6e20 696e 2070 6172 616c 6c65  o run in paralle
+000260a0: 6c2e 0a20 2020 2073 7475 6479 5f73 7065  l..    study_spe
+000260b0: 635f 7061 7261 6d65 7465 7273 5f6f 7665  c_parameters_ove
+000260c0: 7272 6964 653a 204c 6973 7420 6f66 2064  rride: List of d
+000260d0: 6963 7469 6f6e 6172 6965 7320 7265 7072  ictionaries repr
+000260e0: 6573 656e 7469 6e67 2070 6172 616d 6574  esenting paramet
+000260f0: 6572 7320 746f 206f 7074 696d 697a 652e  ers to optimize.
+00026100: 2054 6865 2064 6963 7469 6f6e 6172 7920   The dictionary 
+00026110: 6b65 7920 6973 2074 6865 2070 6172 616d  key is the param
+00026120: 6574 6572 5f69 642c 2077 6869 6368 2069  eter_id, which i
+00026130: 7320 7061 7373 6564 2074 6f20 7472 6169  s passed to trai
+00026140: 6e69 6e67 206a 6f62 2061 7320 6120 636f  ning job as a co
+00026150: 6d6d 616e 6420 6c69 6e65 2061 7267 756d  mmand line argum
+00026160: 656e 742c 2061 6e64 2074 6865 2064 6963  ent, and the dic
+00026170: 7469 6f6e 6172 7920 7661 6c75 6520 6973  tionary value is
+00026180: 2074 6865 2070 6172 616d 6574 6572 2073   the parameter s
+00026190: 7065 6369 6669 6361 7469 6f6e 206f 6620  pecification of 
+000261a0: 7468 6520 6d65 7472 6963 2e0a 2020 2020  the metric..    
+000261b0: 6576 616c 5f6d 6574 7269 633a 2045 7661  eval_metric: Eva
+000261c0: 6c75 6174 696f 6e20 6d65 7472 6963 7320  luation metrics 
+000261d0: 666f 7220 7661 6c69 6461 7469 6f6e 2064  for validation d
+000261e0: 6174 6120 7265 7072 6573 656e 7465 6420  ata represented 
+000261f0: 6173 2061 2063 6f6d 6d61 2d73 6570 6172  as a comma-separ
+00026200: 6174 6564 2073 7472 696e 672e 0a20 2020  ated string..   
+00026210: 2064 6973 6162 6c65 5f64 6566 6175 6c74   disable_default
+00026220: 5f65 7661 6c5f 6d65 7472 6963 3a20 466c  _eval_metric: Fl
+00026230: 6167 2074 6f20 6469 7361 626c 6520 6465  ag to disable de
+00026240: 6661 756c 7420 6d65 7472 6963 2e20 5365  fault metric. Se
+00026250: 7420 746f 203e 3020 746f 2064 6973 6162  t to >0 to disab
+00026260: 6c65 2e20 4465 6661 756c 7420 746f 2030  le. Default to 0
+00026270: 2e0a 2020 2020 7365 6564 3a20 5261 6e64  ..    seed: Rand
+00026280: 6f6d 2073 6565 642e 0a20 2020 2073 6565  om seed..    see
+00026290: 645f 7065 725f 6974 6572 6174 696f 6e3a  d_per_iteration:
+000262a0: 2053 6565 6420 5052 4e47 2064 6574 6572   Seed PRNG deter
+000262b0: 6d6e 6973 7469 636c 7920 7669 6120 6974  mnisticly via it
+000262c0: 6572 6174 6f72 206e 756d 6265 722e 0a20  erator number.. 
+000262d0: 2020 2064 6174 6173 6574 5f6c 6576 656c     dataset_level
+000262e0: 5f63 7573 746f 6d5f 7472 616e 7366 6f72  _custom_transfor
+000262f0: 6d61 7469 6f6e 5f64 6566 696e 6974 696f  mation_definitio
+00026300: 6e73 3a20 4461 7461 7365 742d 6c65 7665  ns: Dataset-leve
+00026310: 6c20 6375 7374 6f6d 2074 7261 6e73 666f  l custom transfo
+00026320: 726d 6174 696f 6e20 6465 6669 6e69 7469  rmation definiti
+00026330: 6f6e 7320 696e 2073 7472 696e 6720 666f  ons in string fo
+00026340: 726d 6174 2e0a 2020 2020 6461 7461 7365  rmat..    datase
+00026350: 745f 6c65 7665 6c5f 7472 616e 7366 6f72  t_level_transfor
+00026360: 6d61 7469 6f6e 733a 2044 6174 6173 6574  mations: Dataset
+00026370: 2d6c 6576 656c 2074 7261 6e73 666f 726d  -level transform
+00026380: 6174 696f 6e20 636f 6e66 6967 7572 6174  ation configurat
+00026390: 696f 6e20 696e 2073 7472 696e 6720 666f  ion in string fo
+000263a0: 726d 6174 2e0a 2020 2020 7275 6e5f 6665  rmat..    run_fe
+000263b0: 6174 7572 655f 7365 6c65 6374 696f 6e3a  ature_selection:
+000263c0: 2057 6865 7468 6572 2074 6f20 656e 6162   Whether to enab
+000263d0: 6c65 2066 6561 7475 7265 2073 656c 6563  le feature selec
+000263e0: 7469 6f6e 2e0a 2020 2020 6665 6174 7572  tion..    featur
+000263f0: 655f 7365 6c65 6374 696f 6e5f 616c 676f  e_selection_algo
+00026400: 7269 7468 6d3a 2046 6561 7475 7265 2073  rithm: Feature s
+00026410: 656c 6563 7469 6f6e 2061 6c67 6f72 6974  election algorit
+00026420: 686d 2e0a 2020 2020 6d61 785f 7365 6c65  hm..    max_sele
+00026430: 6374 6564 5f66 6561 7475 7265 733a 204d  cted_features: M
+00026440: 6178 696d 756d 206e 756d 6265 7220 6f66  aximum number of
+00026450: 2066 6561 7475 7265 7320 746f 2073 656c   features to sel
+00026460: 6563 742e 0a20 2020 2070 7265 6465 6669  ect..    predefi
+00026470: 6e65 645f 7370 6c69 745f 6b65 793a 2050  ned_split_key: P
+00026480: 7265 6465 6669 6e65 6420 7370 6c69 7420  redefined split 
+00026490: 6b65 792e 0a20 2020 2073 7472 6174 6966  key..    stratif
+000264a0: 6965 645f 7370 6c69 745f 6b65 793a 2053  ied_split_key: S
+000264b0: 7472 6174 6966 6965 6420 7370 6c69 7420  tratified split 
+000264c0: 6b65 792e 0a20 2020 2074 7261 696e 696e  key..    trainin
+000264d0: 675f 6672 6163 7469 6f6e 3a20 5472 6169  g_fraction: Trai
+000264e0: 6e69 6e67 2066 7261 6374 696f 6e2e 0a20  ning fraction.. 
+000264f0: 2020 2076 616c 6964 6174 696f 6e5f 6672     validation_fr
+00026500: 6163 7469 6f6e 3a20 5661 6c69 6461 7469  action: Validati
+00026510: 6f6e 2066 7261 6374 696f 6e2e 0a20 2020  on fraction..   
+00026520: 2074 6573 745f 6672 6163 7469 6f6e 3a20   test_fraction: 
+00026530: 5465 7374 2066 7261 6374 696f 6e2e 0a20  Test fraction.. 
+00026540: 2020 2074 665f 6175 746f 5f74 7261 6e73     tf_auto_trans
+00026550: 666f 726d 5f66 6561 7475 7265 733a 204c  form_features: L
+00026560: 6973 7420 6f66 2061 7574 6f20 7472 616e  ist of auto tran
+00026570: 7366 6f72 6d20 6665 6174 7572 6573 2069  sform features i
+00026580: 6e20 7468 6520 636f 6d6d 612d 7365 7061  n the comma-sepa
+00026590: 7261 7465 6420 7374 7269 6e67 2066 6f72  rated string for
+000265a0: 6d61 742e 0a20 2020 2074 665f 6375 7374  mat..    tf_cust
+000265b0: 6f6d 5f74 7261 6e73 666f 726d 6174 696f  om_transformatio
+000265c0: 6e5f 6465 6669 6e69 7469 6f6e 733a 2054  n_definitions: T
+000265d0: 4620 6375 7374 6f6d 2074 7261 6e73 666f  F custom transfo
+000265e0: 726d 6174 696f 6e20 6465 6669 6e69 7469  rmation definiti
+000265f0: 6f6e 7320 696e 2073 7472 696e 6720 666f  ons in string fo
+00026600: 726d 6174 2e0a 2020 2020 7466 5f74 7261  rmat..    tf_tra
+00026610: 6e73 666f 726d 6174 696f 6e73 5f70 6174  nsformations_pat
+00026620: 683a 2050 6174 6820 746f 2054 4620 7472  h: Path to TF tr
+00026630: 616e 7366 6f72 6d61 7469 6f6e 2063 6f6e  ansformation con
+00026640: 6669 6775 7261 7469 6f6e 2e0a 2020 2020  figuration..    
+00026650: 6461 7461 5f73 6f75 7263 655f 6373 765f  data_source_csv_
+00026660: 6669 6c65 6e61 6d65 733a 2054 6865 2043  filenames: The C
+00026670: 5356 2064 6174 6120 736f 7572 6365 2e0a  SV data source..
+00026680: 2020 2020 6461 7461 5f73 6f75 7263 655f      data_source_
+00026690: 6269 6771 7565 7279 5f74 6162 6c65 5f70  bigquery_table_p
+000266a0: 6174 683a 2054 6865 2042 6967 5175 6572  ath: The BigQuer
+000266b0: 7920 6461 7461 2073 6f75 7263 652e 0a20  y data source.. 
+000266c0: 2020 2062 6967 7175 6572 795f 7374 6167     bigquery_stag
+000266d0: 696e 675f 6675 6c6c 5f64 6174 6173 6574  ing_full_dataset
+000266e0: 5f69 643a 2054 6865 2042 6967 5175 6572  _id: The BigQuer
+000266f0: 7920 7374 6167 696e 6720 6675 6c6c 2064  y staging full d
+00026700: 6174 6173 6574 2069 6420 666f 7220 7374  ataset id for st
+00026710: 6f72 696e 6720 696e 7465 726d 6564 6961  oring intermedia
+00026720: 7465 2074 6162 6c65 732e 0a20 2020 2077  te tables..    w
+00026730: 6569 6768 745f 636f 6c75 6d6e 3a20 5468  eight_column: Th
+00026740: 6520 7765 6967 6874 2063 6f6c 756d 6e20  e weight column 
+00026750: 6e61 6d65 2e0a 2020 2020 6d61 785f 6661  name..    max_fa
+00026760: 696c 6564 5f74 7269 616c 5f63 6f75 6e74  iled_trial_count
+00026770: 3a20 5468 6520 6e75 6d62 6572 206f 6620  : The number of 
+00026780: 6661 696c 6564 2074 7269 616c 7320 7468  failed trials th
+00026790: 6174 206e 6565 6420 746f 2062 6520 7365  at need to be se
+000267a0: 656e 2062 6566 6f72 6520 6661 696c 696e  en before failin
+000267b0: 6720 7468 6520 4879 7065 7270 6172 616d  g the Hyperparam
+000267c0: 6574 6572 5475 6e69 6e67 4a6f 622e 2049  eterTuningJob. I
+000267d0: 6620 7365 7420 746f 2030 2c20 5665 7274  f set to 0, Vert
+000267e0: 6578 2041 4920 6465 6369 6465 7320 686f  ex AI decides ho
+000267f0: 7720 6d61 6e79 2074 7269 616c 7320 6d75  w many trials mu
+00026800: 7374 2066 6169 6c20 6265 666f 7265 2074  st fail before t
+00026810: 6865 2077 686f 6c65 206a 6f62 2066 6169  he whole job fai
+00026820: 6c73 2e0a 2020 2020 7472 6169 6e69 6e67  ls..    training
+00026830: 5f6d 6163 6869 6e65 5f74 7970 653a 204d  _machine_type: M
+00026840: 6163 6869 6e65 2074 7970 652e 0a20 2020  achine type..   
+00026850: 2074 7261 696e 696e 675f 746f 7461 6c5f   training_total_
+00026860: 7265 706c 6963 615f 636f 756e 743a 204e  replica_count: N
+00026870: 756d 6265 7220 6f66 2077 6f72 6b65 7273  umber of workers
+00026880: 2e0a 2020 2020 7472 6169 6e69 6e67 5f61  ..    training_a
+00026890: 6363 656c 6572 6174 6f72 5f74 7970 653a  ccelerator_type:
+000268a0: 2041 6363 656c 6572 6174 6f72 2074 7970   Accelerator typ
+000268b0: 652e 0a20 2020 2074 7261 696e 696e 675f  e..    training_
+000268c0: 6163 6365 6c65 7261 746f 725f 636f 756e  accelerator_coun
+000268d0: 743a 2041 6363 656c 6572 6174 6f72 2063  t: Accelerator c
+000268e0: 6f75 6e74 2e0a 2020 2020 7374 7564 795f  ount..    study_
+000268f0: 7370 6563 5f61 6c67 6f72 6974 686d 3a20  spec_algorithm: 
+00026900: 5468 6520 7365 6172 6368 2061 6c67 6f72  The search algor
+00026910: 6974 686d 2073 7065 6369 6669 6564 2066  ithm specified f
+00026920: 6f72 2074 6865 2073 7475 6479 2e20 4f6e  or the study. On
+00026930: 6520 6f66 2027 414c 474f 5249 5448 4d5f  e of 'ALGORITHM_
+00026940: 554e 5350 4543 4946 4945 4427 2c20 2747  UNSPECIFIED', 'G
+00026950: 5249 445f 5345 4152 4348 272c 206f 7220  RID_SEARCH', or 
+00026960: 2752 414e 444f 4d5f 5345 4152 4348 272e  'RANDOM_SEARCH'.
+00026970: 0a20 2020 2073 7475 6479 5f73 7065 635f  .    study_spec_
+00026980: 6d65 6173 7572 656d 656e 745f 7365 6c65  measurement_sele
+00026990: 6374 696f 6e5f 7479 7065 3a20 2057 6869  ction_type:  Whi
+000269a0: 6368 206d 6561 7375 7265 6d65 6e74 2074  ch measurement t
+000269b0: 6f20 7573 6520 6966 2f77 6865 6e20 7468  o use if/when th
+000269c0: 6520 7365 7276 6963 6520 6175 746f 6d61  e service automa
+000269d0: 7469 6361 6c6c 7920 7365 6c65 6374 7320  tically selects 
+000269e0: 7468 6520 6669 6e61 6c20 6d65 6173 7572  the final measur
+000269f0: 656d 656e 7420 6672 6f6d 2070 7265 7669  ement from previ
+00026a00: 6f75 736c 7920 7265 706f 7274 6564 2069  ously reported i
+00026a10: 6e74 6572 6d65 6469 6174 6520 6d65 6173  ntermediate meas
+00026a20: 7572 656d 656e 7473 2e20 4f6e 6520 6f66  urements. One of
+00026a30: 2022 4245 5354 5f4d 4541 5355 5245 4d45   "BEST_MEASUREME
+00026a40: 4e54 2220 6f72 2022 4c41 5354 5f4d 4541  NT" or "LAST_MEA
+00026a50: 5355 5245 4d45 4e54 222e 0a20 2020 2074  SUREMENT"..    t
+00026a60: 7261 6e73 666f 726d 5f64 6174 6166 6c6f  ransform_dataflo
+00026a70: 775f 6d61 6368 696e 655f 7479 7065 3a20  w_machine_type: 
+00026a80: 5468 6520 6461 7461 666c 6f77 206d 6163  The dataflow mac
+00026a90: 6869 6e65 2074 7970 6520 666f 7220 7472  hine type for tr
+00026aa0: 616e 7366 6f72 6d20 636f 6d70 6f6e 656e  ansform componen
+00026ab0: 742e 0a20 2020 2074 7261 6e73 666f 726d  t..    transform
+00026ac0: 5f64 6174 6166 6c6f 775f 6d61 785f 6e75  _dataflow_max_nu
+00026ad0: 6d5f 776f 726b 6572 733a 2054 6865 206d  m_workers: The m
+00026ae0: 6178 206e 756d 6265 7220 6f66 2044 6174  ax number of Dat
+00026af0: 6166 6c6f 7720 776f 726b 6572 7320 666f  aflow workers fo
+00026b00: 7220 7472 616e 7366 6f72 6d20 636f 6d70  r transform comp
+00026b10: 6f6e 656e 742e 0a20 2020 2074 7261 6e73  onent..    trans
+00026b20: 666f 726d 5f64 6174 6166 6c6f 775f 6469  form_dataflow_di
+00026b30: 736b 5f73 697a 655f 6762 3a20 4461 7461  sk_size_gb: Data
+00026b40: 666c 6f77 2077 6f72 6b65 7227 7320 6469  flow worker's di
+00026b50: 736b 2073 697a 6520 696e 2047 4220 666f  sk size in GB fo
+00026b60: 7220 7472 616e 7366 6f72 6d20 636f 6d70  r transform comp
+00026b70: 6f6e 656e 742e 0a20 2020 2072 756e 5f65  onent..    run_e
+00026b80: 7661 6c75 6174 696f 6e3a 2057 6865 7468  valuation: Wheth
+00026b90: 6572 2074 6f20 7275 6e20 6576 616c 7561  er to run evalua
+00026ba0: 7469 6f6e 2073 7465 7073 2064 7572 696e  tion steps durin
+00026bb0: 6720 7472 6169 6e69 6e67 2e0a 2020 2020  g training..    
+00026bc0: 6576 616c 7561 7469 6f6e 5f62 6174 6368  evaluation_batch
+00026bd0: 5f70 7265 6469 6374 5f6d 6163 6869 6e65  _predict_machine
+00026be0: 5f74 7970 653a 2054 6865 2070 7265 6469  _type: The predi
+00026bf0: 6374 696f 6e20 7365 7276 6572 206d 6163  ction server mac
+00026c00: 6869 6e65 2074 7970 6520 666f 7220 6261  hine type for ba
+00026c10: 7463 6820 7072 6564 6963 7420 636f 6d70  tch predict comp
+00026c20: 6f6e 656e 7473 2064 7572 696e 6720 6576  onents during ev
+00026c30: 616c 7561 7469 6f6e 2e0a 2020 2020 6576  aluation..    ev
+00026c40: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+00026c50: 7265 6469 6374 5f73 7461 7274 696e 675f  redict_starting_
+00026c60: 7265 706c 6963 615f 636f 756e 743a 2054  replica_count: T
+00026c70: 6865 2069 6e69 7469 616c 206e 756d 6265  he initial numbe
+00026c80: 7220 6f66 2070 7265 6469 6374 696f 6e20  r of prediction 
+00026c90: 7365 7276 6572 2066 6f72 2062 6174 6368  server for batch
+00026ca0: 2070 7265 6469 6374 2063 6f6d 706f 6e65   predict compone
+00026cb0: 6e74 7320 6475 7269 6e67 2065 7661 6c75  nts during evalu
+00026cc0: 6174 696f 6e2e 0a20 2020 2065 7661 6c75  ation..    evalu
+00026cd0: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
+00026ce0: 6963 745f 6d61 785f 7265 706c 6963 615f  ict_max_replica_
+00026cf0: 636f 756e 743a 2054 6865 206d 6178 206e  count: The max n
+00026d00: 756d 6265 7220 6f66 2070 7265 6469 6374  umber of predict
+00026d10: 696f 6e20 7365 7276 6572 2066 6f72 2062  ion server for b
+00026d20: 6174 6368 2070 7265 6469 6374 2063 6f6d  atch predict com
+00026d30: 706f 6e65 6e74 7320 6475 7269 6e67 2065  ponents during e
+00026d40: 7661 6c75 6174 696f 6e2e 0a20 2020 2065  valuation..    e
+00026d50: 7661 6c75 6174 696f 6e5f 6461 7461 666c  valuation_datafl
+00026d60: 6f77 5f6d 6163 6869 6e65 5f74 7970 653a  ow_machine_type:
+00026d70: 2054 6865 2064 6174 6166 6c6f 7720 6d61   The dataflow ma
+00026d80: 6368 696e 6520 7479 7065 2066 6f72 2065  chine type for e
+00026d90: 7661 6c75 6174 696f 6e20 636f 6d70 6f6e  valuation compon
+00026da0: 656e 7473 2e0a 2020 2020 6576 616c 7561  ents..    evalua
+00026db0: 7469 6f6e 5f64 6174 6166 6c6f 775f 7374  tion_dataflow_st
+00026dc0: 6172 7469 6e67 5f6e 756d 5f77 6f72 6b65  arting_num_worke
+00026dd0: 7273 3a20 5468 6520 696e 6974 6961 6c20  rs: The initial 
+00026de0: 6e75 6d62 6572 206f 6620 4461 7461 666c  number of Datafl
+00026df0: 6f77 2077 6f72 6b65 7273 2066 6f72 2065  ow workers for e
+00026e00: 7661 6c75 6174 696f 6e20 636f 6d70 6f6e  valuation compon
+00026e10: 656e 7473 2e0a 2020 2020 6576 616c 7561  ents..    evalua
+00026e20: 7469 6f6e 5f64 6174 6166 6c6f 775f 6d61  tion_dataflow_ma
+00026e30: 785f 6e75 6d5f 776f 726b 6572 733a 2054  x_num_workers: T
+00026e40: 6865 206d 6178 206e 756d 6265 7220 6f66  he max number of
+00026e50: 2044 6174 6166 6c6f 7720 776f 726b 6572   Dataflow worker
+00026e60: 7320 666f 7220 6576 616c 7561 7469 6f6e  s for evaluation
+00026e70: 2063 6f6d 706f 6e65 6e74 732e 0a20 2020   components..   
+00026e80: 2065 7661 6c75 6174 696f 6e5f 6461 7461   evaluation_data
+00026e90: 666c 6f77 5f64 6973 6b5f 7369 7a65 5f67  flow_disk_size_g
+00026ea0: 623a 2044 6174 6166 6c6f 7720 776f 726b  b: Dataflow work
+00026eb0: 6572 2773 2064 6973 6b20 7369 7a65 2069  er's disk size i
+00026ec0: 6e20 4742 2066 6f72 2065 7661 6c75 6174  n GB for evaluat
+00026ed0: 696f 6e20 636f 6d70 6f6e 656e 7473 2e0a  ion components..
+00026ee0: 2020 2020 6461 7461 666c 6f77 5f73 6572      dataflow_ser
+00026ef0: 7669 6365 5f61 6363 6f75 6e74 3a20 4375  vice_account: Cu
+00026f00: 7374 6f6d 2073 6572 7669 6365 2061 6363  stom service acc
+00026f10: 6f75 6e74 2074 6f20 7275 6e20 6461 7461  ount to run data
+00026f20: 666c 6f77 206a 6f62 732e 0a20 2020 2064  flow jobs..    d
+00026f30: 6174 6166 6c6f 775f 7375 626e 6574 776f  ataflow_subnetwo
+00026f40: 726b 3a20 4461 7461 666c 6f77 2773 2066  rk: Dataflow's f
+00026f50: 756c 6c79 2071 7561 6c69 6669 6564 2073  ully qualified s
+00026f60: 7562 6e65 7477 6f72 6b20 6e61 6d65 2c20  ubnetwork name, 
+00026f70: 7768 656e 2065 6d70 7479 2074 6865 2064  when empty the d
+00026f80: 6566 6175 6c74 2073 7562 6e65 7477 6f72  efault subnetwor
+00026f90: 6b20 7769 6c6c 2062 6520 7573 6564 2e20  k will be used. 
+00026fa0: 4578 616d 706c 653a 2068 7474 7073 3a2f  Example: https:/
+00026fb0: 2f63 6c6f 7564 2e67 6f6f 676c 652e 636f  /cloud.google.co
+00026fc0: 6d2f 6461 7461 666c 6f77 2f64 6f63 732f  m/dataflow/docs/
+00026fd0: 6775 6964 6573 2f73 7065 6369 6679 696e  guides/specifyin
+00026fe0: 672d 6e65 7477 6f72 6b73 2365 7861 6d70  g-networks#examp
+00026ff0: 6c65 5f6e 6574 776f 726b 5f61 6e64 5f73  le_network_and_s
+00027000: 7562 6e65 7477 6f72 6b5f 7370 6563 6966  ubnetwork_specif
+00027010: 6963 6174 696f 6e73 0a20 2020 2064 6174  ications.    dat
+00027020: 6166 6c6f 775f 7573 655f 7075 626c 6963  aflow_use_public
+00027030: 5f69 7073 3a20 5370 6563 6966 6965 7320  _ips: Specifies 
+00027040: 7768 6574 6865 7220 4461 7461 666c 6f77  whether Dataflow
+00027050: 2077 6f72 6b65 7273 2075 7365 2070 7562   workers use pub
+00027060: 6c69 6320 4950 2061 6464 7265 7373 6573  lic IP addresses
+00027070: 2e0a 2020 2020 656e 6372 7970 7469 6f6e  ..    encryption
+00027080: 5f73 7065 635f 6b65 795f 6e61 6d65 3a20  _spec_key_name: 
+00027090: 5468 6520 4b4d 5320 6b65 7920 6e61 6d65  The KMS key name
+000270a0: 2e0a 0a20 2052 6574 7572 6e73 3a0a 2020  ...  Returns:.  
+000270b0: 2020 5475 706c 6520 6f66 2070 6970 656c    Tuple of pipel
+000270c0: 696e 655f 6465 6669 6e69 7469 6f6e 5f70  ine_definition_p
+000270d0: 6174 6820 616e 6420 7061 7261 6d65 7465  ath and paramete
+000270e0: 725f 7661 6c75 6573 2e0a 2020 2222 220a  r_values..  """.
+000270f0: 2020 2320 666d 743a 206f 6e0a 2020 7061    # fmt: on.  pa
+00027100: 7261 6d65 7465 725f 7661 6c75 6573 203d  rameter_values =
+00027110: 207b 7d0a 2020 6966 2069 7369 6e73 7461   {}.  if isinsta
+00027120: 6e63 6528 7466 5f61 7574 6f5f 7472 616e  nce(tf_auto_tran
+00027130: 7366 6f72 6d5f 6665 6174 7572 6573 2c20  sform_features, 
+00027140: 6c69 7374 293a 0a20 2020 2074 665f 6175  list):.    tf_au
+00027150: 746f 5f74 7261 6e73 666f 726d 5f66 6561  to_transform_fea
+00027160: 7475 7265 7320 3d20 7b27 6175 746f 273a  tures = {'auto':
+00027170: 2074 665f 6175 746f 5f74 7261 6e73 666f   tf_auto_transfo
+00027180: 726d 5f66 6561 7475 7265 737d 0a0a 2020  rm_features}..  
+00027190: 7472 6169 6e69 6e67 5f61 6e64 5f65 7661  training_and_eva
+000271a0: 6c5f 7061 7261 6d65 7465 7273 203d 207b  l_parameters = {
+000271b0: 0a20 2020 2020 2027 7072 6f6a 6563 7427  .      'project'
+000271c0: 3a20 7072 6f6a 6563 742c 0a20 2020 2020  : project,.     
+000271d0: 2027 6c6f 6361 7469 6f6e 273a 206c 6f63   'location': loc
+000271e0: 6174 696f 6e2c 0a20 2020 2020 2027 726f  ation,.      'ro
+000271f0: 6f74 5f64 6972 273a 2072 6f6f 745f 6469  ot_dir': root_di
+00027200: 722c 0a20 2020 2020 2027 7461 7267 6574  r,.      'target
+00027210: 5f63 6f6c 756d 6e27 3a20 7461 7267 6574  _column': target
+00027220: 5f63 6f6c 756d 6e2c 0a20 2020 2020 2027  _column,.      '
+00027230: 6f62 6a65 6374 6976 6527 3a20 6f62 6a65  objective': obje
+00027240: 6374 6976 652c 0a20 2020 2020 2027 6576  ctive,.      'ev
+00027250: 616c 5f6d 6574 7269 6327 3a20 6576 616c  al_metric': eval
+00027260: 5f6d 6574 7269 632c 0a20 2020 2020 2027  _metric,.      '
+00027270: 7374 7564 795f 7370 6563 5f6d 6574 7269  study_spec_metri
+00027280: 635f 6964 273a 2073 7475 6479 5f73 7065  c_id': study_spe
+00027290: 635f 6d65 7472 6963 5f69 642c 0a20 2020  c_metric_id,.   
+000272a0: 2020 2027 7374 7564 795f 7370 6563 5f6d     'study_spec_m
+000272b0: 6574 7269 635f 676f 616c 273a 2073 7475  etric_goal': stu
+000272c0: 6479 5f73 7065 635f 6d65 7472 6963 5f67  dy_spec_metric_g
+000272d0: 6f61 6c2c 0a20 2020 2020 2027 6d61 785f  oal,.      'max_
+000272e0: 7472 6961 6c5f 636f 756e 7427 3a20 6d61  trial_count': ma
+000272f0: 785f 7472 6961 6c5f 636f 756e 742c 0a20  x_trial_count,. 
+00027300: 2020 2020 2027 7061 7261 6c6c 656c 5f74       'parallel_t
+00027310: 7269 616c 5f63 6f75 6e74 273a 2070 6172  rial_count': par
+00027320: 616c 6c65 6c5f 7472 6961 6c5f 636f 756e  allel_trial_coun
+00027330: 742c 0a20 2020 2020 2027 7374 7564 795f  t,.      'study_
+00027340: 7370 6563 5f70 6172 616d 6574 6572 735f  spec_parameters_
+00027350: 6f76 6572 7269 6465 273a 2028 0a20 2020  override': (.   
+00027360: 2020 2020 2020 2073 7475 6479 5f73 7065         study_spe
+00027370: 635f 7061 7261 6d65 7465 7273 5f6f 7665  c_parameters_ove
+00027380: 7272 6964 650a 2020 2020 2020 2020 2020  rride.          
+00027390: 6966 2073 7475 6479 5f73 7065 635f 7061  if study_spec_pa
+000273a0: 7261 6d65 7465 7273 5f6f 7665 7272 6964  rameters_overrid
+000273b0: 650a 2020 2020 2020 2020 2020 656c 7365  e.          else
+000273c0: 205b 5d0a 2020 2020 2020 292c 0a20 2020   [].      ),.   
+000273d0: 2020 2027 6469 7361 626c 655f 6465 6661     'disable_defa
+000273e0: 756c 745f 6576 616c 5f6d 6574 7269 6327  ult_eval_metric'
+000273f0: 3a20 6469 7361 626c 655f 6465 6661 756c  : disable_defaul
+00027400: 745f 6576 616c 5f6d 6574 7269 632c 0a20  t_eval_metric,. 
+00027410: 2020 2020 2027 7365 6564 273a 2073 6565       'seed': see
+00027420: 642c 0a20 2020 2020 2027 7365 6564 5f70  d,.      'seed_p
+00027430: 6572 5f69 7465 7261 7469 6f6e 273a 2073  er_iteration': s
+00027440: 6565 645f 7065 725f 6974 6572 6174 696f  eed_per_iteratio
+00027450: 6e2c 0a20 2020 2020 2027 7765 6967 6874  n,.      'weight
+00027460: 5f63 6f6c 756d 6e27 3a20 7765 6967 6874  _column': weight
+00027470: 5f63 6f6c 756d 6e2c 0a20 2020 2020 2027  _column,.      '
+00027480: 6d61 785f 6661 696c 6564 5f74 7269 616c  max_failed_trial
+00027490: 5f63 6f75 6e74 273a 206d 6178 5f66 6169  _count': max_fai
+000274a0: 6c65 645f 7472 6961 6c5f 636f 756e 742c  led_trial_count,
+000274b0: 0a20 2020 2020 2027 7472 6169 6e69 6e67  .      'training
+000274c0: 5f6d 6163 6869 6e65 5f74 7970 6527 3a20  _machine_type': 
+000274d0: 7472 6169 6e69 6e67 5f6d 6163 6869 6e65  training_machine
+000274e0: 5f74 7970 652c 0a20 2020 2020 2027 7472  _type,.      'tr
+000274f0: 6169 6e69 6e67 5f74 6f74 616c 5f72 6570  aining_total_rep
+00027500: 6c69 6361 5f63 6f75 6e74 273a 2074 7261  lica_count': tra
+00027510: 696e 696e 675f 746f 7461 6c5f 7265 706c  ining_total_repl
+00027520: 6963 615f 636f 756e 742c 0a20 2020 2020  ica_count,.     
+00027530: 2027 7472 6169 6e69 6e67 5f61 6363 656c   'training_accel
+00027540: 6572 6174 6f72 5f74 7970 6527 3a20 7472  erator_type': tr
+00027550: 6169 6e69 6e67 5f61 6363 656c 6572 6174  aining_accelerat
+00027560: 6f72 5f74 7970 652c 0a20 2020 2020 2027  or_type,.      '
+00027570: 7472 6169 6e69 6e67 5f61 6363 656c 6572  training_acceler
+00027580: 6174 6f72 5f63 6f75 6e74 273a 2074 7261  ator_count': tra
+00027590: 696e 696e 675f 6163 6365 6c65 7261 746f  ining_accelerato
+000275a0: 725f 636f 756e 742c 0a20 2020 2020 2027  r_count,.      '
+000275b0: 7374 7564 795f 7370 6563 5f61 6c67 6f72  study_spec_algor
+000275c0: 6974 686d 273a 2073 7475 6479 5f73 7065  ithm': study_spe
+000275d0: 635f 616c 676f 7269 7468 6d2c 0a20 2020  c_algorithm,.   
+000275e0: 2020 2027 7374 7564 795f 7370 6563 5f6d     'study_spec_m
+000275f0: 6561 7375 7265 6d65 6e74 5f73 656c 6563  easurement_selec
+00027600: 7469 6f6e 5f74 7970 6527 3a20 280a 2020  tion_type': (.  
+00027610: 2020 2020 2020 2020 7374 7564 795f 7370          study_sp
+00027620: 6563 5f6d 6561 7375 7265 6d65 6e74 5f73  ec_measurement_s
+00027630: 656c 6563 7469 6f6e 5f74 7970 650a 2020  election_type.  
+00027640: 2020 2020 292c 0a20 2020 2020 2027 7472      ),.      'tr
+00027650: 616e 7366 6f72 6d5f 6461 7461 666c 6f77  ansform_dataflow
+00027660: 5f6d 6163 6869 6e65 5f74 7970 6527 3a20  _machine_type': 
+00027670: 7472 616e 7366 6f72 6d5f 6461 7461 666c  transform_datafl
+00027680: 6f77 5f6d 6163 6869 6e65 5f74 7970 652c  ow_machine_type,
+00027690: 0a20 2020 2020 2027 7472 616e 7366 6f72  .      'transfor
+000276a0: 6d5f 6461 7461 666c 6f77 5f6d 6178 5f6e  m_dataflow_max_n
+000276b0: 756d 5f77 6f72 6b65 7273 273a 2074 7261  um_workers': tra
+000276c0: 6e73 666f 726d 5f64 6174 6166 6c6f 775f  nsform_dataflow_
+000276d0: 6d61 785f 6e75 6d5f 776f 726b 6572 732c  max_num_workers,
+000276e0: 0a20 2020 2020 2027 7472 616e 7366 6f72  .      'transfor
+000276f0: 6d5f 6461 7461 666c 6f77 5f64 6973 6b5f  m_dataflow_disk_
+00027700: 7369 7a65 5f67 6227 3a20 7472 616e 7366  size_gb': transf
+00027710: 6f72 6d5f 6461 7461 666c 6f77 5f64 6973  orm_dataflow_dis
+00027720: 6b5f 7369 7a65 5f67 622c 0a20 2020 2020  k_size_gb,.     
+00027730: 2027 7275 6e5f 6576 616c 7561 7469 6f6e   'run_evaluation
+00027740: 273a 2072 756e 5f65 7661 6c75 6174 696f  ': run_evaluatio
+00027750: 6e2c 0a20 2020 2020 2027 6576 616c 7561  n,.      'evalua
+00027760: 7469 6f6e 5f62 6174 6368 5f70 7265 6469  tion_batch_predi
+00027770: 6374 5f6d 6163 6869 6e65 5f74 7970 6527  ct_machine_type'
+00027780: 3a20 280a 2020 2020 2020 2020 2020 6576  : (.          ev
+00027790: 616c 7561 7469 6f6e 5f62 6174 6368 5f70  aluation_batch_p
+000277a0: 7265 6469 6374 5f6d 6163 6869 6e65 5f74  redict_machine_t
+000277b0: 7970 650a 2020 2020 2020 292c 0a20 2020  ype.      ),.   
+000277c0: 2020 2027 6576 616c 7561 7469 6f6e 5f62     'evaluation_b
+000277d0: 6174 6368 5f70 7265 6469 6374 5f73 7461  atch_predict_sta
+000277e0: 7274 696e 675f 7265 706c 6963 615f 636f  rting_replica_co
+000277f0: 756e 7427 3a20 280a 2020 2020 2020 2020  unt': (.        
+00027800: 2020 6576 616c 7561 7469 6f6e 5f62 6174    evaluation_bat
+00027810: 6368 5f70 7265 6469 6374 5f73 7461 7274  ch_predict_start
+00027820: 696e 675f 7265 706c 6963 615f 636f 756e  ing_replica_coun
+00027830: 740a 2020 2020 2020 292c 0a20 2020 2020  t.      ),.     
+00027840: 2027 6576 616c 7561 7469 6f6e 5f62 6174   'evaluation_bat
+00027850: 6368 5f70 7265 6469 6374 5f6d 6178 5f72  ch_predict_max_r
+00027860: 6570 6c69 6361 5f63 6f75 6e74 273a 2028  eplica_count': (
+00027870: 0a20 2020 2020 2020 2020 2065 7661 6c75  .          evalu
+00027880: 6174 696f 6e5f 6261 7463 685f 7072 6564  ation_batch_pred
+00027890: 6963 745f 6d61 785f 7265 706c 6963 615f  ict_max_replica_
+000278a0: 636f 756e 740a 2020 2020 2020 292c 0a20  count.      ),. 
+000278b0: 2020 2020 2027 6576 616c 7561 7469 6f6e       'evaluation
+000278c0: 5f64 6174 6166 6c6f 775f 6d61 6368 696e  _dataflow_machin
+000278d0: 655f 7479 7065 273a 2065 7661 6c75 6174  e_type': evaluat
+000278e0: 696f 6e5f 6461 7461 666c 6f77 5f6d 6163  ion_dataflow_mac
+000278f0: 6869 6e65 5f74 7970 652c 0a20 2020 2020  hine_type,.     
+00027900: 2027 6576 616c 7561 7469 6f6e 5f64 6174   'evaluation_dat
+00027910: 6166 6c6f 775f 7374 6172 7469 6e67 5f6e  aflow_starting_n
+00027920: 756d 5f77 6f72 6b65 7273 273a 2028 0a20  um_workers': (. 
+00027930: 2020 2020 2020 2020 2065 7661 6c75 6174           evaluat
+00027940: 696f 6e5f 6461 7461 666c 6f77 5f73 7461  ion_dataflow_sta
+00027950: 7274 696e 675f 6e75 6d5f 776f 726b 6572  rting_num_worker
+00027960: 730a 2020 2020 2020 292c 0a20 2020 2020  s.      ),.     
+00027970: 2027 6576 616c 7561 7469 6f6e 5f64 6174   'evaluation_dat
+00027980: 6166 6c6f 775f 6d61 785f 6e75 6d5f 776f  aflow_max_num_wo
+00027990: 726b 6572 7327 3a20 280a 2020 2020 2020  rkers': (.      
+000279a0: 2020 2020 6576 616c 7561 7469 6f6e 5f64      evaluation_d
+000279b0: 6174 6166 6c6f 775f 6d61 785f 6e75 6d5f  ataflow_max_num_
+000279c0: 776f 726b 6572 730a 2020 2020 2020 292c  workers.      ),
+000279d0: 0a20 2020 2020 2027 6576 616c 7561 7469  .      'evaluati
+000279e0: 6f6e 5f64 6174 6166 6c6f 775f 6469 736b  on_dataflow_disk
+000279f0: 5f73 697a 655f 6762 273a 2065 7661 6c75  _size_gb': evalu
+00027a00: 6174 696f 6e5f 6461 7461 666c 6f77 5f64  ation_dataflow_d
+00027a10: 6973 6b5f 7369 7a65 5f67 622c 0a20 2020  isk_size_gb,.   
+00027a20: 2020 2027 6461 7461 666c 6f77 5f73 6572     'dataflow_ser
+00027a30: 7669 6365 5f61 6363 6f75 6e74 273a 2064  vice_account': d
+00027a40: 6174 6166 6c6f 775f 7365 7276 6963 655f  ataflow_service_
+00027a50: 6163 636f 756e 742c 0a20 2020 2020 2027  account,.      '
+00027a60: 6461 7461 666c 6f77 5f73 7562 6e65 7477  dataflow_subnetw
+00027a70: 6f72 6b27 3a20 6461 7461 666c 6f77 5f73  ork': dataflow_s
+00027a80: 7562 6e65 7477 6f72 6b2c 0a20 2020 2020  ubnetwork,.     
+00027a90: 2027 6461 7461 666c 6f77 5f75 7365 5f70   'dataflow_use_p
+00027aa0: 7562 6c69 635f 6970 7327 3a20 6461 7461  ublic_ips': data
+00027ab0: 666c 6f77 5f75 7365 5f70 7562 6c69 635f  flow_use_public_
+00027ac0: 6970 732c 0a20 2020 2020 2027 656e 6372  ips,.      'encr
+00027ad0: 7970 7469 6f6e 5f73 7065 635f 6b65 795f  yption_spec_key_
+00027ae0: 6e61 6d65 273a 2065 6e63 7279 7074 696f  name': encryptio
+00027af0: 6e5f 7370 6563 5f6b 6579 5f6e 616d 652c  n_spec_key_name,
+00027b00: 0a20 207d 0a20 205f 7570 6461 7465 5f70  .  }.  _update_p
+00027b10: 6172 616d 6574 6572 7328 7061 7261 6d65  arameters(parame
+00027b20: 7465 725f 7661 6c75 6573 2c20 7472 6169  ter_values, trai
+00027b30: 6e69 6e67 5f61 6e64 5f65 7661 6c5f 7061  ning_and_eval_pa
+00027b40: 7261 6d65 7465 7273 290a 0a20 2066 7465  rameters)..  fte
+00027b50: 5f70 6172 616d 7320 3d20 7b0a 2020 2020  _params = {.    
+00027b60: 2020 2764 6174 6173 6574 5f6c 6576 656c    'dataset_level
+00027b70: 5f63 7573 746f 6d5f 7472 616e 7366 6f72  _custom_transfor
+00027b80: 6d61 7469 6f6e 5f64 6566 696e 6974 696f  mation_definitio
+00027b90: 6e73 273a 2028 0a20 2020 2020 2020 2020  ns': (.         
+00027ba0: 2064 6174 6173 6574 5f6c 6576 656c 5f63   dataset_level_c
+00027bb0: 7573 746f 6d5f 7472 616e 7366 6f72 6d61  ustom_transforma
+00027bc0: 7469 6f6e 5f64 6566 696e 6974 696f 6e73  tion_definitions
+00027bd0: 0a20 2020 2020 2020 2020 2069 6620 6461  .          if da
+00027be0: 7461 7365 745f 6c65 7665 6c5f 6375 7374  taset_level_cust
+00027bf0: 6f6d 5f74 7261 6e73 666f 726d 6174 696f  om_transformatio
+00027c00: 6e5f 6465 6669 6e69 7469 6f6e 730a 2020  n_definitions.  
+00027c10: 2020 2020 2020 2020 656c 7365 205b 5d0a          else [].
+00027c20: 2020 2020 2020 292c 0a20 2020 2020 2027        ),.      '
+00027c30: 6461 7461 7365 745f 6c65 7665 6c5f 7472  dataset_level_tr
+00027c40: 616e 7366 6f72 6d61 7469 6f6e 7327 3a20  ansformations': 
+00027c50: 280a 2020 2020 2020 2020 2020 6461 7461  (.          data
+00027c60: 7365 745f 6c65 7665 6c5f 7472 616e 7366  set_level_transf
+00027c70: 6f72 6d61 7469 6f6e 7320 6966 2064 6174  ormations if dat
+00027c80: 6173 6574 5f6c 6576 656c 5f74 7261 6e73  aset_level_trans
+00027c90: 666f 726d 6174 696f 6e73 2065 6c73 6520  formations else 
+00027ca0: 5b5d 0a20 2020 2020 2029 2c0a 2020 2020  [].      ),.    
+00027cb0: 2020 2772 756e 5f66 6561 7475 7265 5f73    'run_feature_s
+00027cc0: 656c 6563 7469 6f6e 273a 2072 756e 5f66  election': run_f
+00027cd0: 6561 7475 7265 5f73 656c 6563 7469 6f6e  eature_selection
+00027ce0: 2c0a 2020 2020 2020 2766 6561 7475 7265  ,.      'feature
+00027cf0: 5f73 656c 6563 7469 6f6e 5f61 6c67 6f72  _selection_algor
+00027d00: 6974 686d 273a 2066 6561 7475 7265 5f73  ithm': feature_s
+00027d10: 656c 6563 7469 6f6e 5f61 6c67 6f72 6974  election_algorit
+00027d20: 686d 2c0a 2020 2020 2020 276d 6178 5f73  hm,.      'max_s
+00027d30: 656c 6563 7465 645f 6665 6174 7572 6573  elected_features
+00027d40: 273a 206d 6178 5f73 656c 6563 7465 645f  ': max_selected_
+00027d50: 6665 6174 7572 6573 2c0a 2020 2020 2020  features,.      
+00027d60: 2770 7265 6465 6669 6e65 645f 7370 6c69  'predefined_spli
+00027d70: 745f 6b65 7927 3a20 7072 6564 6566 696e  t_key': predefin
+00027d80: 6564 5f73 706c 6974 5f6b 6579 2c0a 2020  ed_split_key,.  
+00027d90: 2020 2020 2773 7472 6174 6966 6965 645f      'stratified_
+00027da0: 7370 6c69 745f 6b65 7927 3a20 7374 7261  split_key': stra
+00027db0: 7469 6669 6564 5f73 706c 6974 5f6b 6579  tified_split_key
+00027dc0: 2c0a 2020 2020 2020 2774 7261 696e 696e  ,.      'trainin
+00027dd0: 675f 6672 6163 7469 6f6e 273a 2074 7261  g_fraction': tra
+00027de0: 696e 696e 675f 6672 6163 7469 6f6e 2c0a  ining_fraction,.
+00027df0: 2020 2020 2020 2776 616c 6964 6174 696f        'validatio
+00027e00: 6e5f 6672 6163 7469 6f6e 273a 2076 616c  n_fraction': val
+00027e10: 6964 6174 696f 6e5f 6672 6163 7469 6f6e  idation_fraction
+00027e20: 2c0a 2020 2020 2020 2774 6573 745f 6672  ,.      'test_fr
+00027e30: 6163 7469 6f6e 273a 2074 6573 745f 6672  action': test_fr
+00027e40: 6163 7469 6f6e 2c0a 2020 2020 2020 2774  action,.      't
+00027e50: 665f 6175 746f 5f74 7261 6e73 666f 726d  f_auto_transform
+00027e60: 5f66 6561 7475 7265 7327 3a20 280a 2020  _features': (.  
+00027e70: 2020 2020 2020 2020 7466 5f61 7574 6f5f          tf_auto_
+00027e80: 7472 616e 7366 6f72 6d5f 6665 6174 7572  transform_featur
+00027e90: 6573 2069 6620 7466 5f61 7574 6f5f 7472  es if tf_auto_tr
+00027ea0: 616e 7366 6f72 6d5f 6665 6174 7572 6573  ansform_features
+00027eb0: 2065 6c73 6520 7b7d 0a20 2020 2020 2029   else {}.      )
+00027ec0: 2c0a 2020 2020 2020 2774 665f 6375 7374  ,.      'tf_cust
+00027ed0: 6f6d 5f74 7261 6e73 666f 726d 6174 696f  om_transformatio
+00027ee0: 6e5f 6465 6669 6e69 7469 6f6e 7327 3a20  n_definitions': 
+00027ef0: 280a 2020 2020 2020 2020 2020 7466 5f63  (.          tf_c
+00027f00: 7573 746f 6d5f 7472 616e 7366 6f72 6d61  ustom_transforma
+00027f10: 7469 6f6e 5f64 6566 696e 6974 696f 6e73  tion_definitions
+00027f20: 0a20 2020 2020 2020 2020 2069 6620 7466  .          if tf
+00027f30: 5f63 7573 746f 6d5f 7472 616e 7366 6f72  _custom_transfor
+00027f40: 6d61 7469 6f6e 5f64 6566 696e 6974 696f  mation_definitio
+00027f50: 6e73 0a20 2020 2020 2020 2020 2065 6c73  ns.          els
+00027f60: 6520 5b5d 0a20 2020 2020 2029 2c0a 2020  e [].      ),.  
+00027f70: 2020 2020 2774 665f 7472 616e 7366 6f72      'tf_transfor
+00027f80: 6d61 7469 6f6e 735f 7061 7468 273a 2074  mations_path': t
+00027f90: 665f 7472 616e 7366 6f72 6d61 7469 6f6e  f_transformation
+00027fa0: 735f 7061 7468 2c0a 2020 7d0a 2020 5f75  s_path,.  }.  _u
+00027fb0: 7064 6174 655f 7061 7261 6d65 7465 7273  pdate_parameters
+00027fc0: 2870 6172 616d 6574 6572 5f76 616c 7565  (parameter_value
+00027fd0: 732c 2066 7465 5f70 6172 616d 7329 0a0a  s, fte_params)..
+00027fe0: 2020 6461 7461 5f73 6f75 7263 655f 616e    data_source_an
+00027ff0: 645f 7370 6c69 745f 7061 7261 6d65 7465  d_split_paramete
+00028000: 7273 203d 207b 0a20 2020 2020 2027 6461  rs = {.      'da
+00028010: 7461 5f73 6f75 7263 655f 6373 765f 6669  ta_source_csv_fi
+00028020: 6c65 6e61 6d65 7327 3a20 6461 7461 5f73  lenames': data_s
+00028030: 6f75 7263 655f 6373 765f 6669 6c65 6e61  ource_csv_filena
+00028040: 6d65 732c 0a20 2020 2020 2027 6461 7461  mes,.      'data
+00028050: 5f73 6f75 7263 655f 6269 6771 7565 7279  _source_bigquery
+00028060: 5f74 6162 6c65 5f70 6174 6827 3a20 6461  _table_path': da
+00028070: 7461 5f73 6f75 7263 655f 6269 6771 7565  ta_source_bigque
+00028080: 7279 5f74 6162 6c65 5f70 6174 682c 0a20  ry_table_path,. 
+00028090: 2020 2020 2027 6269 6771 7565 7279 5f73       'bigquery_s
+000280a0: 7461 6769 6e67 5f66 756c 6c5f 6461 7461  taging_full_data
+000280b0: 7365 745f 6964 273a 2062 6967 7175 6572  set_id': bigquer
+000280c0: 795f 7374 6167 696e 675f 6675 6c6c 5f64  y_staging_full_d
+000280d0: 6174 6173 6574 5f69 642c 0a20 207d 0a20  ataset_id,.  }. 
+000280e0: 205f 7570 6461 7465 5f70 6172 616d 6574   _update_paramet
+000280f0: 6572 7328 7061 7261 6d65 7465 725f 7661  ers(parameter_va
+00028100: 6c75 6573 2c20 6461 7461 5f73 6f75 7263  lues, data_sourc
+00028110: 655f 616e 645f 7370 6c69 745f 7061 7261  e_and_split_para
+00028120: 6d65 7465 7273 290a 0a20 2070 6970 656c  meters)..  pipel
+00028130: 696e 655f 6465 6669 6e69 7469 6f6e 5f70  ine_definition_p
+00028140: 6174 6820 3d20 6f73 2e70 6174 682e 6a6f  ath = os.path.jo
+00028150: 696e 280a 2020 2020 2020 7061 7468 6c69  in(.      pathli
+00028160: 622e 5061 7468 285f 5f66 696c 655f 5f29  b.Path(__file__)
+00028170: 2e70 6172 656e 742e 7265 736f 6c76 6528  .parent.resolve(
+00028180: 292c 0a20 2020 2020 2027 7867 626f 6f73  ),.      'xgboos
+00028190: 745f 6879 7065 7270 6172 616d 6574 6572  t_hyperparameter
+000281a0: 5f74 756e 696e 675f 6a6f 625f 7069 7065  _tuning_job_pipe
+000281b0: 6c69 6e65 2e79 616d 6c27 2c0a 2020 290a  line.yaml',.  ).
+000281c0: 0a20 2072 6574 7572 6e20 7069 7065 6c69  .  return pipeli
+000281d0: 6e65 5f64 6566 696e 6974 696f 6e5f 7061  ne_definition_pa
+000281e0: 7468 2c20 7061 7261 6d65 7465 725f 7661  th, parameter_va
+000281f0: 6c75 6573 0a0a 0a64 6566 2067 6574 5f66  lues...def get_f
+00028200: 6561 7475 7265 5f73 656c 6563 7469 6f6e  eature_selection
+00028210: 5f70 6970 656c 696e 655f 616e 645f 7061  _pipeline_and_pa
+00028220: 7261 6d65 7465 7273 280a 2020 2020 726f  rameters(.    ro
+00028230: 6f74 5f64 6972 3a20 7374 722c 0a20 2020  ot_dir: str,.   
+00028240: 2070 726f 6a65 6374 3a20 7374 722c 0a20   project: str,. 
+00028250: 2020 206c 6f63 6174 696f 6e3a 2073 7472     location: str
+00028260: 2c0a 2020 2020 7461 7267 6574 5f63 6f6c  ,.    target_col
+00028270: 756d 6e3a 2073 7472 2c0a 2020 2020 7072  umn: str,.    pr
+00028280: 6564 6963 7469 6f6e 5f74 7970 653a 2073  ediction_type: s
+00028290: 7472 2c0a 2020 2020 6f70 7469 6d69 7a61  tr,.    optimiza
+000282a0: 7469 6f6e 5f6f 626a 6563 7469 7665 3a20  tion_objective: 
+000282b0: 7374 722c 0a20 2020 2064 6174 6173 6574  str,.    dataset
+000282c0: 5f6c 6576 656c 5f63 7573 746f 6d5f 7472  _level_custom_tr
+000282d0: 616e 7366 6f72 6d61 7469 6f6e 5f64 6566  ansformation_def
+000282e0: 696e 6974 696f 6e73 3a20 4f70 7469 6f6e  initions: Option
+000282f0: 616c 5b0a 2020 2020 2020 2020 4c69 7374  al[.        List
+00028300: 5b44 6963 745b 7374 722c 2041 6e79 5d5d  [Dict[str, Any]]
+00028310: 0a20 2020 205d 203d 204e 6f6e 652c 0a20  .    ] = None,. 
+00028320: 2020 2064 6174 6173 6574 5f6c 6576 656c     dataset_level
+00028330: 5f74 7261 6e73 666f 726d 6174 696f 6e73  _transformations
+00028340: 3a20 4f70 7469 6f6e 616c 5b4c 6973 745b  : Optional[List[
+00028350: 4469 6374 5b73 7472 2c20 416e 795d 5d5d  Dict[str, Any]]]
+00028360: 203d 204e 6f6e 652c 0a20 2020 2072 756e   = None,.    run
+00028370: 5f66 6561 7475 7265 5f73 656c 6563 7469  _feature_selecti
+00028380: 6f6e 3a20 4f70 7469 6f6e 616c 5b62 6f6f  on: Optional[boo
+00028390: 6c5d 203d 204e 6f6e 652c 0a20 2020 2066  l] = None,.    f
+000283a0: 6561 7475 7265 5f73 656c 6563 7469 6f6e  eature_selection
+000283b0: 5f61 6c67 6f72 6974 686d 3a20 4f70 7469  _algorithm: Opti
+000283c0: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
+000283d0: 2c0a 2020 2020 6665 6174 7572 655f 7365  ,.    feature_se
+000283e0: 6c65 6374 696f 6e5f 6578 6563 7574 696f  lection_executio
+000283f0: 6e5f 656e 6769 6e65 3a20 4f70 7469 6f6e  n_engine: Option
+00028400: 616c 5b0a 2020 2020 2020 2020 7374 720a  al[.        str.
+00028410: 2020 2020 5d20 3d20 5f46 4541 5455 5245      ] = _FEATURE
+00028420: 5f53 454c 4543 5449 4f4e 5f45 5845 4355  _SELECTION_EXECU
+00028430: 5449 4f4e 5f45 4e47 494e 455f 4249 4751  TION_ENGINE_BIGQ
+00028440: 5545 5259 2c0a 2020 2020 6d61 785f 7365  UERY,.    max_se
+00028450: 6c65 6374 6564 5f66 6561 7475 7265 733a  lected_features:
+00028460: 204f 7074 696f 6e61 6c5b 696e 745d 203d   Optional[int] =
+00028470: 204e 6f6e 652c 0a20 2020 2070 7265 6465   None,.    prede
+00028480: 6669 6e65 645f 7370 6c69 745f 6b65 793a  fined_split_key:
+00028490: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
+000284a0: 204e 6f6e 652c 0a20 2020 2073 7472 6174   None,.    strat
+000284b0: 6966 6965 645f 7370 6c69 745f 6b65 793a  ified_split_key:
+000284c0: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
+000284d0: 204e 6f6e 652c 0a20 2020 2074 7261 696e   None,.    train
+000284e0: 696e 675f 6672 6163 7469 6f6e 3a20 4f70  ing_fraction: Op
+000284f0: 7469 6f6e 616c 5b66 6c6f 6174 5d20 3d20  tional[float] = 
+00028500: 4e6f 6e65 2c0a 2020 2020 7661 6c69 6461  None,.    valida
+00028510: 7469 6f6e 5f66 7261 6374 696f 6e3a 204f  tion_fraction: O
+00028520: 7074 696f 6e61 6c5b 666c 6f61 745d 203d  ptional[float] =
+00028530: 204e 6f6e 652c 0a20 2020 2074 6573 745f   None,.    test_
+00028540: 6672 6163 7469 6f6e 3a20 4f70 7469 6f6e  fraction: Option
+00028550: 616c 5b66 6c6f 6174 5d20 3d20 4e6f 6e65  al[float] = None
+00028560: 2c0a 2020 2020 7466 5f61 7574 6f5f 7472  ,.    tf_auto_tr
+00028570: 616e 7366 6f72 6d5f 6665 6174 7572 6573  ansform_features
+00028580: 3a20 4f70 7469 6f6e 616c 5b0a 2020 2020  : Optional[.    
+00028590: 2020 2020 556e 696f 6e5b 4c69 7374 5b73      Union[List[s
+000285a0: 7472 5d2c 2044 6963 745b 7374 722c 204c  tr], Dict[str, L
+000285b0: 6973 745b 7374 725d 5d5d 0a20 2020 205d  ist[str]]].    ]
+000285c0: 203d 204e 6f6e 652c 0a20 2020 2077 6569   = None,.    wei
+000285d0: 6768 745f 636f 6c75 6d6e 3a20 4f70 7469  ght_column: Opti
+000285e0: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
+000285f0: 2c0a 2020 2020 6461 7461 5f73 6f75 7263  ,.    data_sourc
+00028600: 655f 6373 765f 6669 6c65 6e61 6d65 733a  e_csv_filenames:
+00028610: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
+00028620: 204e 6f6e 652c 0a20 2020 2064 6174 615f   None,.    data_
+00028630: 736f 7572 6365 5f62 6967 7175 6572 795f  source_bigquery_
+00028640: 7461 626c 655f 7061 7468 3a20 4f70 7469  table_path: Opti
+00028650: 6f6e 616c 5b73 7472 5d20 3d20 4e6f 6e65  onal[str] = None
+00028660: 2c0a 2020 2020 6269 6771 7565 7279 5f73  ,.    bigquery_s
+00028670: 7461 6769 6e67 5f66 756c 6c5f 6461 7461  taging_full_data
+00028680: 7365 745f 6964 3a20 4f70 7469 6f6e 616c  set_id: Optional
+00028690: 5b73 7472 5d20 3d20 4e6f 6e65 2c0a 2020  [str] = None,.  
+000286a0: 2020 6461 7461 666c 6f77 5f6d 6163 6869    dataflow_machi
+000286b0: 6e65 5f74 7970 653a 204f 7074 696f 6e61  ne_type: Optiona
+000286c0: 6c5b 7374 725d 203d 204e 6f6e 652c 0a20  l[str] = None,. 
+000286d0: 2020 2064 6174 6166 6c6f 775f 6d61 785f     dataflow_max_
+000286e0: 6e75 6d5f 776f 726b 6572 733a 204f 7074  num_workers: Opt
+000286f0: 696f 6e61 6c5b 696e 745d 203d 204e 6f6e  ional[int] = Non
+00028700: 652c 0a20 2020 2064 6174 6166 6c6f 775f  e,.    dataflow_
+00028710: 6469 736b 5f73 697a 655f 6762 3a20 4f70  disk_size_gb: Op
+00028720: 7469 6f6e 616c 5b69 6e74 5d20 3d20 4e6f  tional[int] = No
+00028730: 6e65 2c0a 2020 2020 6461 7461 666c 6f77  ne,.    dataflow
+00028740: 5f73 7562 6e65 7477 6f72 6b3a 204f 7074  _subnetwork: Opt
+00028750: 696f 6e61 6c5b 7374 725d 203d 204e 6f6e  ional[str] = Non
+00028760: 652c 0a20 2020 2064 6174 6166 6c6f 775f  e,.    dataflow_
+00028770: 7573 655f 7075 626c 6963 5f69 7073 3a20  use_public_ips: 
+00028780: 4f70 7469 6f6e 616c 5b62 6f6f 6c5d 203d  Optional[bool] =
+00028790: 204e 6f6e 652c 0a20 2020 2065 6e63 7279   None,.    encry
+000287a0: 7074 696f 6e5f 7370 6563 5f6b 6579 5f6e  ption_spec_key_n
+000287b0: 616d 653a 204f 7074 696f 6e61 6c5b 7374  ame: Optional[st
+000287c0: 725d 203d 204e 6f6e 652c 0a20 2020 2073  r] = None,.    s
+000287d0: 7461 6765 5f31 5f64 6561 646c 696e 655f  tage_1_deadline_
+000287e0: 686f 7572 733a 204f 7074 696f 6e61 6c5b  hours: Optional[
+000287f0: 666c 6f61 745d 203d 204e 6f6e 652c 0a20  float] = None,. 
+00028800: 2020 2073 7461 6765 5f32 5f64 6561 646c     stage_2_deadl
+00028810: 696e 655f 686f 7572 733a 204f 7074 696f  ine_hours: Optio
+00028820: 6e61 6c5b 666c 6f61 745d 203d 204e 6f6e  nal[float] = Non
+00028830: 652c 0a29 3a0a 2020 2222 2252 6574 7572  e,.):.  """Retur
+00028840: 6e73 2066 6561 7475 7265 2074 7261 6e73  ns feature trans
+00028850: 666f 726d 2065 6e67 696e 6520 7069 7065  form engine pipe
+00028860: 6c69 6e65 2061 6e64 2066 6f72 6d61 7474  line and formatt
+00028870: 6564 2070 6172 616d 6574 6572 732e 2222  ed parameters.""
+00028880: 220a 0a20 2069 6620 6973 696e 7374 616e  "..  if isinstan
+00028890: 6365 2874 665f 6175 746f 5f74 7261 6e73  ce(tf_auto_trans
+000288a0: 666f 726d 5f66 6561 7475 7265 732c 206c  form_features, l
+000288b0: 6973 7429 3a0a 2020 2020 7466 5f61 7574  ist):.    tf_aut
+000288c0: 6f5f 7472 616e 7366 6f72 6d5f 6665 6174  o_transform_feat
+000288d0: 7572 6573 203d 207b 2761 7574 6f27 3a20  ures = {'auto': 
+000288e0: 7466 5f61 7574 6f5f 7472 616e 7366 6f72  tf_auto_transfor
+000288f0: 6d5f 6665 6174 7572 6573 7d0a 0a20 2070  m_features}..  p
+00028900: 6970 656c 696e 655f 6465 6669 6e69 7469  ipeline_definiti
+00028910: 6f6e 5f70 6174 6820 3d20 6f73 2e70 6174  on_path = os.pat
+00028920: 682e 6a6f 696e 280a 2020 2020 2020 7061  h.join(.      pa
+00028930: 7468 6c69 622e 5061 7468 285f 5f66 696c  thlib.Path(__fil
+00028940: 655f 5f29 2e70 6172 656e 742e 7265 736f  e__).parent.reso
+00028950: 6c76 6528 292c 2027 6665 6174 7572 655f  lve(), 'feature_
+00028960: 7365 6c65 6374 696f 6e5f 7069 7065 6c69  selection_pipeli
+00028970: 6e65 2e79 616d 6c27 0a20 2029 0a0a 2020  ne.yaml'.  )..  
+00028980: 7061 7261 6d65 7465 725f 7661 6c75 6573  parameter_values
+00028990: 203d 207b 0a20 2020 2020 2027 726f 6f74   = {.      'root
+000289a0: 5f64 6972 273a 2072 6f6f 745f 6469 722c  _dir': root_dir,
+000289b0: 0a20 2020 2020 2027 7072 6f6a 6563 7427  .      'project'
+000289c0: 3a20 7072 6f6a 6563 742c 0a20 2020 2020  : project,.     
+000289d0: 2027 6c6f 6361 7469 6f6e 273a 206c 6f63   'location': loc
+000289e0: 6174 696f 6e2c 0a20 2020 2020 2027 7461  ation,.      'ta
+000289f0: 7267 6574 5f63 6f6c 756d 6e27 3a20 7461  rget_column': ta
+00028a00: 7267 6574 5f63 6f6c 756d 6e2c 0a20 2020  rget_column,.   
+00028a10: 2020 2027 7765 6967 6874 5f63 6f6c 756d     'weight_colum
+00028a20: 6e27 3a20 7765 6967 6874 5f63 6f6c 756d  n': weight_colum
+00028a30: 6e2c 0a20 2020 2020 2027 7072 6564 6963  n,.      'predic
+00028a40: 7469 6f6e 5f74 7970 6527 3a20 7072 6564  tion_type': pred
+00028a50: 6963 7469 6f6e 5f74 7970 652c 0a20 2020  iction_type,.   
+00028a60: 2020 2027 6461 7461 7365 745f 6c65 7665     'dataset_leve
+00028a70: 6c5f 6375 7374 6f6d 5f74 7261 6e73 666f  l_custom_transfo
+00028a80: 726d 6174 696f 6e5f 6465 6669 6e69 7469  rmation_definiti
+00028a90: 6f6e 7327 3a20 280a 2020 2020 2020 2020  ons': (.        
+00028aa0: 2020 6461 7461 7365 745f 6c65 7665 6c5f    dataset_level_
+00028ab0: 6375 7374 6f6d 5f74 7261 6e73 666f 726d  custom_transform
+00028ac0: 6174 696f 6e5f 6465 6669 6e69 7469 6f6e  ation_definition
+00028ad0: 730a 2020 2020 2020 2020 2020 6966 2064  s.          if d
+00028ae0: 6174 6173 6574 5f6c 6576 656c 5f63 7573  ataset_level_cus
+00028af0: 746f 6d5f 7472 616e 7366 6f72 6d61 7469  tom_transformati
+00028b00: 6f6e 5f64 6566 696e 6974 696f 6e73 0a20  on_definitions. 
+00028b10: 2020 2020 2020 2020 2065 6c73 6520 5b5d           else []
+00028b20: 0a20 2020 2020 2029 2c0a 2020 2020 2020  .      ),.      
+00028b30: 2764 6174 6173 6574 5f6c 6576 656c 5f74  'dataset_level_t
+00028b40: 7261 6e73 666f 726d 6174 696f 6e73 273a  ransformations':
+00028b50: 2028 0a20 2020 2020 2020 2020 2064 6174   (.          dat
+00028b60: 6173 6574 5f6c 6576 656c 5f74 7261 6e73  aset_level_trans
+00028b70: 666f 726d 6174 696f 6e73 2069 6620 6461  formations if da
+00028b80: 7461 7365 745f 6c65 7665 6c5f 7472 616e  taset_level_tran
+00028b90: 7366 6f72 6d61 7469 6f6e 7320 656c 7365  sformations else
+00028ba0: 205b 5d0a 2020 2020 2020 292c 0a20 2020   [].      ),.   
+00028bb0: 2020 2027 7275 6e5f 6665 6174 7572 655f     'run_feature_
+00028bc0: 7365 6c65 6374 696f 6e27 3a20 7275 6e5f  selection': run_
+00028bd0: 6665 6174 7572 655f 7365 6c65 6374 696f  feature_selectio
+00028be0: 6e2c 0a20 2020 2020 2027 6665 6174 7572  n,.      'featur
+00028bf0: 655f 7365 6c65 6374 696f 6e5f 616c 676f  e_selection_algo
+00028c00: 7269 7468 6d27 3a20 6665 6174 7572 655f  rithm': feature_
+00028c10: 7365 6c65 6374 696f 6e5f 616c 676f 7269  selection_algori
+00028c20: 7468 6d2c 0a20 2020 2020 2027 6665 6174  thm,.      'feat
+00028c30: 7572 655f 7365 6c65 6374 696f 6e5f 6578  ure_selection_ex
+00028c40: 6563 7574 696f 6e5f 656e 6769 6e65 273a  ecution_engine':
+00028c50: 2066 6561 7475 7265 5f73 656c 6563 7469   feature_selecti
+00028c60: 6f6e 5f65 7865 6375 7469 6f6e 5f65 6e67  on_execution_eng
+00028c70: 696e 652c 0a20 2020 2020 2027 6d61 785f  ine,.      'max_
+00028c80: 7365 6c65 6374 6564 5f66 6561 7475 7265  selected_feature
+00028c90: 7327 3a20 6d61 785f 7365 6c65 6374 6564  s': max_selected
+00028ca0: 5f66 6561 7475 7265 732c 0a20 2020 2020  _features,.     
+00028cb0: 2027 7072 6564 6566 696e 6564 5f73 706c   'predefined_spl
+00028cc0: 6974 5f6b 6579 273a 2070 7265 6465 6669  it_key': predefi
+00028cd0: 6e65 645f 7370 6c69 745f 6b65 792c 0a20  ned_split_key,. 
+00028ce0: 2020 2020 2027 7374 7261 7469 6669 6564       'stratified
+00028cf0: 5f73 706c 6974 5f6b 6579 273a 2073 7472  _split_key': str
+00028d00: 6174 6966 6965 645f 7370 6c69 745f 6b65  atified_split_ke
+00028d10: 792c 0a20 2020 2020 2027 7472 6169 6e69  y,.      'traini
+00028d20: 6e67 5f66 7261 6374 696f 6e27 3a20 7472  ng_fraction': tr
+00028d30: 6169 6e69 6e67 5f66 7261 6374 696f 6e2c  aining_fraction,
+00028d40: 0a20 2020 2020 2027 7661 6c69 6461 7469  .      'validati
+00028d50: 6f6e 5f66 7261 6374 696f 6e27 3a20 7661  on_fraction': va
+00028d60: 6c69 6461 7469 6f6e 5f66 7261 6374 696f  lidation_fractio
+00028d70: 6e2c 0a20 2020 2020 2027 7465 7374 5f66  n,.      'test_f
+00028d80: 7261 6374 696f 6e27 3a20 7465 7374 5f66  raction': test_f
+00028d90: 7261 6374 696f 6e2c 0a20 2020 2020 2027  raction,.      '
+00028da0: 7466 5f61 7574 6f5f 7472 616e 7366 6f72  tf_auto_transfor
+00028db0: 6d5f 6665 6174 7572 6573 273a 2074 665f  m_features': tf_
+00028dc0: 6175 746f 5f74 7261 6e73 666f 726d 5f66  auto_transform_f
+00028dd0: 6561 7475 7265 732c 0a20 2020 2020 2027  eatures,.      '
+00028de0: 6f70 7469 6d69 7a61 7469 6f6e 5f6f 626a  optimization_obj
+00028df0: 6563 7469 7665 273a 206f 7074 696d 697a  ective': optimiz
+00028e00: 6174 696f 6e5f 6f62 6a65 6374 6976 652c  ation_objective,
+00028e10: 0a20 2020 2020 2027 6461 7461 5f73 6f75  .      'data_sou
+00028e20: 7263 655f 6373 765f 6669 6c65 6e61 6d65  rce_csv_filename
+00028e30: 7327 3a20 6461 7461 5f73 6f75 7263 655f  s': data_source_
+00028e40: 6373 765f 6669 6c65 6e61 6d65 732c 0a20  csv_filenames,. 
+00028e50: 2020 2020 2027 6461 7461 5f73 6f75 7263       'data_sourc
+00028e60: 655f 6269 6771 7565 7279 5f74 6162 6c65  e_bigquery_table
+00028e70: 5f70 6174 6827 3a20 6461 7461 5f73 6f75  _path': data_sou
+00028e80: 7263 655f 6269 6771 7565 7279 5f74 6162  rce_bigquery_tab
+00028e90: 6c65 5f70 6174 682c 0a20 2020 2020 2027  le_path,.      '
+00028ea0: 6269 6771 7565 7279 5f73 7461 6769 6e67  bigquery_staging
+00028eb0: 5f66 756c 6c5f 6461 7461 7365 745f 6964  _full_dataset_id
+00028ec0: 273a 2062 6967 7175 6572 795f 7374 6167  ': bigquery_stag
+00028ed0: 696e 675f 6675 6c6c 5f64 6174 6173 6574  ing_full_dataset
+00028ee0: 5f69 642c 0a20 2020 2020 2027 6461 7461  _id,.      'data
+00028ef0: 666c 6f77 5f6d 6163 6869 6e65 5f74 7970  flow_machine_typ
+00028f00: 6527 3a20 6461 7461 666c 6f77 5f6d 6163  e': dataflow_mac
+00028f10: 6869 6e65 5f74 7970 652c 0a20 2020 2020  hine_type,.     
+00028f20: 2027 6461 7461 666c 6f77 5f6d 6178 5f6e   'dataflow_max_n
+00028f30: 756d 5f77 6f72 6b65 7273 273a 2064 6174  um_workers': dat
+00028f40: 6166 6c6f 775f 6d61 785f 6e75 6d5f 776f  aflow_max_num_wo
+00028f50: 726b 6572 732c 0a20 2020 2020 2027 6461  rkers,.      'da
+00028f60: 7461 666c 6f77 5f64 6973 6b5f 7369 7a65  taflow_disk_size
+00028f70: 5f67 6227 3a20 6461 7461 666c 6f77 5f64  _gb': dataflow_d
+00028f80: 6973 6b5f 7369 7a65 5f67 622c 0a20 2020  isk_size_gb,.   
+00028f90: 2020 2027 6461 7461 666c 6f77 5f73 7562     'dataflow_sub
+00028fa0: 6e65 7477 6f72 6b27 3a20 6461 7461 666c  network': datafl
+00028fb0: 6f77 5f73 7562 6e65 7477 6f72 6b2c 0a20  ow_subnetwork,. 
+00028fc0: 2020 2020 2027 6461 7461 666c 6f77 5f75       'dataflow_u
+00028fd0: 7365 5f70 7562 6c69 635f 6970 7327 3a20  se_public_ips': 
+00028fe0: 6461 7461 666c 6f77 5f75 7365 5f70 7562  dataflow_use_pub
+00028ff0: 6c69 635f 6970 732c 0a20 2020 2020 2027  lic_ips,.      '
+00029000: 656e 6372 7970 7469 6f6e 5f73 7065 635f  encryption_spec_
+00029010: 6b65 795f 6e61 6d65 273a 2065 6e63 7279  key_name': encry
+00029020: 7074 696f 6e5f 7370 6563 5f6b 6579 5f6e  ption_spec_key_n
+00029030: 616d 652c 0a20 2020 2020 2027 7374 6167  ame,.      'stag
+00029040: 655f 315f 6465 6164 6c69 6e65 5f68 6f75  e_1_deadline_hou
+00029050: 7273 273a 2073 7461 6765 5f31 5f64 6561  rs': stage_1_dea
+00029060: 646c 696e 655f 686f 7572 732c 0a20 2020  dline_hours,.   
+00029070: 2020 2027 7374 6167 655f 325f 6465 6164     'stage_2_dead
+00029080: 6c69 6e65 5f68 6f75 7273 273a 2073 7461  line_hours': sta
+00029090: 6765 5f32 5f64 6561 646c 696e 655f 686f  ge_2_deadline_ho
+000290a0: 7572 732c 0a20 207d 0a0a 2020 7061 7261  urs,.  }..  para
+000290b0: 6d65 7465 725f 7661 6c75 6573 203d 207b  meter_values = {
+000290c0: 0a20 2020 2020 2070 6172 616d 3a20 7661  .      param: va
+000290d0: 6c75 650a 2020 2020 2020 666f 7220 7061  lue.      for pa
+000290e0: 7261 6d2c 2076 616c 7565 2069 6e20 7061  ram, value in pa
+000290f0: 7261 6d65 7465 725f 7661 6c75 6573 2e69  rameter_values.i
+00029100: 7465 6d73 2829 0a20 2020 2020 2069 6620  tems().      if 
+00029110: 7661 6c75 6520 6973 206e 6f74 204e 6f6e  value is not Non
+00029120: 650a 2020 7d0a 0a20 2072 6574 7572 6e20  e.  }..  return 
+00029130: 7069 7065 6c69 6e65 5f64 6566 696e 6974  pipeline_definit
+00029140: 696f 6e5f 7061 7468 2c20 7061 7261 6d65  ion_path, parame
+00029150: 7465 725f 7661 6c75 6573 0a              ter_values.
```

## google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job.py

```diff
@@ -154,23 +154,23 @@
                   ),
                   '1',
                   '", "machine_spec": ',
                   training_machine_spec,
                   ', "disk_spec": ',
                   training_disk_spec,
                   ', "container_spec": {"image_uri":"',
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/wide-and-deep-training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/wide-and-deep-training:20240119_0125',
                   '", "args": ["--target_column=',
                   target_column,
                   '", "--weight_column=',
                   weight_column,
                   '", "--model_type=',
                   prediction_type,
                   '", "--prediction_docker_uri=',
-                  'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125',
                   '", "--prediction_docker_uri_artifact_path=',
                   prediction_docker_uri_output,
                   '", "--baseline_path=',
                   instance_baseline.uri,
                   '", "--metadata_path=',
                   metadata.uri,
                   '", "--transform_output_path=',
```

## google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job_pipeline.yaml

```diff
@@ -79,30 +79,26 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-infra-validator:
     executorLabel: exec-automl-tabular-infra-validator
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-bool-identity:
     executorLabel: exec-bool-identity
     inputDefinitions:
       parameters:
         value:
           description: Boolean value to return
           parameterType: BOOLEAN
@@ -784,194 +780,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -979,42 +939,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -1056,38 +1008,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -1112,116 +1057,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -1237,272 +1155,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -1528,19 +1394,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -1553,44 +1417,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-get-best-hyperparameter-tuning-job-trial:
     executorLabel: exec-get-best-hyperparameter-tuning-job-trial
     inputDefinitions:
       parameters:
         gcp_resources:
           description: Proto tracking the hyperparameter tuning job.
@@ -2403,224 +2259,178 @@
     executorLabel: exec-training-configurator-and-validator
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -2668,146 +2478,113 @@
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The path to transform output.
       parameters:
         cache_data:
           defaultValue: auto
-          description: 'Whether to cache data or not. If set to
-
-            ''auto'', caching is determined based on the dataset size.'
+          description: Whether to cache data or not. If set to 'auto', caching is
+            determined based on the dataset size.
           isOptional: true
           parameterType: STRING
         enable_profiler:
           defaultValue: false
-          description: 'Enables profiling and saves a trace
-
-            during evaluation.'
+          description: Enables profiling and saves a trace during evaluation.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: The KMS key name.
           isOptional: true
           parameterType: STRING
         eval_frequency_secs:
           defaultValue: 600.0
-          description: 'Frequency at which evaluation and
-
-            checkpointing will take place.'
+          description: Frequency at which evaluation and checkpointing will take place.
           isOptional: true
           parameterType: NUMBER_INTEGER
         eval_steps:
           defaultValue: 0.0
-          description: 'Number of steps to run evaluation for. If not
-
-            specified or negative, it means run evaluation on the whole validation
-
-            dataset. If set to 0, it means run evaluation for a fixed number of
-
-            samples.'
+          description: Number of steps to run evaluation for. If not specified or
+            negative, it means run evaluation on the whole validation dataset. If
+            set to 0, it means run evaluation for a fixed number of samples.
           isOptional: true
           parameterType: NUMBER_INTEGER
         location:
           description: The GCP region that runs the pipeline components.
           parameterType: STRING
         max_failed_trial_count:
           defaultValue: 0.0
-          description: 'The number of failed trials that
-
-            need to be seen before failing the HyperparameterTuningJob. If set to
-            0,
-
-            Vertex AI decides how many trials must fail before the whole job fails.'
+          description: The number of failed trials that need to be seen before failing
+            the HyperparameterTuningJob. If set to 0, Vertex AI decides how many trials
+            must fail before the whole job fails.
           isOptional: true
           parameterType: NUMBER_INTEGER
         max_trial_count:
           description: The desired total number of trials.
           parameterType: NUMBER_INTEGER
         parallel_trial_count:
-          description: 'The desired number of trials to run
-
-            in parallel.'
+          description: The desired number of trials to run in parallel.
           parameterType: NUMBER_INTEGER
         prediction_type:
-          description: 'The type of prediction the model is to
-
-            produce. "classification" or "regression".'
+          description: The type of prediction the model is to produce. "classification"
+            or "regression".
           parameterType: STRING
         project:
           description: The GCP project that runs the pipeline components.
           parameterType: STRING
         root_dir:
           description: The root GCS directory for the pipeline components.
           parameterType: STRING
         seed:
           defaultValue: 1.0
           description: Seed to be used for this run.
           isOptional: true
           parameterType: NUMBER_INTEGER
         study_spec_algorithm:
           defaultValue: ALGORITHM_UNSPECIFIED
-          description: 'The search algorithm specified for
-
-            the study. One of ''ALGORITHM_UNSPECIFIED'', ''GRID_SEARCH'', or
-
-            ''RANDOM_SEARCH''.'
+          description: The search algorithm specified for the study. One of 'ALGORITHM_UNSPECIFIED',
+            'GRID_SEARCH', or 'RANDOM_SEARCH'.
           isOptional: true
           parameterType: STRING
         study_spec_measurement_selection_type:
           defaultValue: BEST_MEASUREMENT
-          description: 'Which measurement
-
-            to use if/when the service automatically selects the final measurement
-
-            from previously reported intermediate measurements. One of
-
-            "BEST_MEASUREMENT" or "LAST_MEASUREMENT".'
+          description: Which measurement to use if/when the service automatically
+            selects the final measurement from previously reported intermediate measurements.
+            One of "BEST_MEASUREMENT" or "LAST_MEASUREMENT".
           isOptional: true
           parameterType: STRING
         study_spec_metric_goal:
-          description: 'Optimization goal of the metric,
-
-            possible values: "MAXIMIZE", "MINIMIZE".'
+          description: 'Optimization goal of the metric, possible values: "MAXIMIZE",
+            "MINIMIZE".'
           parameterType: STRING
         study_spec_metric_id:
-          description: 'Metric to optimize, , possible
-
-            values: [ ''loss'', ''average_loss'', ''rmse'', ''mae'', ''mql'', ''accuracy'',
-            ''auc'', ''precision'', ''recall''].'
+          description: 'Metric to optimize, possible values: [ ''loss'', ''average_loss'',
+            ''rmse'', ''mae'', ''mql'', ''accuracy'', ''auc'', ''precision'', ''recall''].'
           parameterType: STRING
         study_spec_parameters_override:
-          description: 'List of dictionaries
-
-            representing parameters to optimize. The dictionary key is the
-
-            parameter_id, which is passed to training job as a command line
-
-            argument, and the dictionary value is the parameter specification of the
-
-            metric.'
+          description: List of dictionaries representing parameters to optimize. The
+            dictionary key is the parameter_id, which is passed to training job as
+            a command line argument, and the dictionary value is the parameter specification
+            of the metric.
           parameterType: LIST
         target_column:
           description: The target column name.
           parameterType: STRING
         training_disk_spec:
           defaultValue:
             boot_disk_size_gb: 100.0
             boot_disk_type: pd-ssd
           description: The training disk spec.
           isOptional: true
           parameterType: STRUCT
         training_machine_spec:
           defaultValue:
             machine_type: c2-standard-16
-          description: 'The training machine
-
-            spec. See https://cloud.google.com/compute/docs/machine-types for
-
-            options.'
+          description: The training machine spec. See https://cloud.google.com/compute/docs/machine-types
+            for options.
           isOptional: true
           parameterType: STRUCT
         weight_column:
           defaultValue: ''
           description: The weight column name.
           isOptional: true
           parameterType: STRING
@@ -2846,30 +2623,30 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
         - google_cloud_pipeline_components.container.v1.custom_job.launcher
         image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.44
     exec-automl-tabular-infra-validator:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-bool-identity:
       container:
         args:
         - --executor_input
@@ -2977,16 +2754,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -2995,15 +2772,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-get-best-hyperparameter-tuning-job-trial:
       container:
         args:
         - --executor_input
@@ -3069,15 +2846,15 @@
           \ = best_fn(trials_list, key=lambda trial: trial['objective_value'])\n\n\
           \  # Build unmanaged_container_model\n  unmanaged_container_model.metadata['containerSpec']\
           \ = {\n      'imageUri': prediction_docker_uri,\n      'healthRoute': '/health',\n\
           \      'predictRoute': '/predict',\n  }\n  unmanaged_container_model.metadata['predictSchemata']\
           \ = {\n      'instanceSchemaUri': instance_schema_uri,\n      'predictionSchemaUri':\
           \ prediction_schema_uri,\n  }\n  unmanaged_container_model.uri = os.path.join(\n\
           \      trials_dir, 'trial_{}'.format(best_trial['id']), 'model'\n  )\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-wide-and-deep-study-spec-parameters:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_wide_and_deep_study_spec_parameters
@@ -3409,15 +3186,15 @@
           \    raise ValueError(\n        'One of vertex_dataset, data_source_csv_filenames,'\n\
           \        ' data_source_bigquery_table_path must be specified'\n    )\n\n\
           \  return collections.namedtuple(\n      'Outputs',\n      [\n         \
           \ 'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
           \          'model_display_name',\n      ],\n  )(\n      data_source_csv_filenames,\n\
           \      data_source_bigquery_table_path,\n      model_display_name,\n  )\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-split-materialized-data:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _split_materialized_data
@@ -3455,15 +3232,15 @@
           \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
           \ data source: {materialized_data_json}')\n\n  # we map indices to file\
           \ patterns based on the ordering of insertion order\n  # in our transform_data\
           \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
           \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
           \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
           \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
     exec-training-configurator-and-validator:
       container:
         args:
         - training_configurator_and_validator
         - '{"Concat": ["--instance_schema_path=", "{{$.inputs.artifacts[''instance_schema''].uri}}"]}'
         - '{"Concat": ["--training_schema_path=", "{{$.inputs.artifacts[''training_schema''].uri}}"]}'
         - '{"Concat": ["--dataset_stats_path=", "{{$.inputs.artifacts[''dataset_stats''].uri}}"]}'
@@ -3500,15 +3277,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
     exec-wide-and-deep-hyperparameter-tuning-job:
       container:
         args:
         - --type
         - HyperparameterTuningJobWithMetrics
         - --project
         - '{{$.inputs.parameters[''project'']}}'
@@ -3528,19 +3305,19 @@
           "\", \"measurement_selection_type\": \"", "{{$.inputs.parameters[''study_spec_measurement_selection_type'']}}",
           "\"}, \"max_trial_count\": ", "{{$.inputs.parameters[''max_trial_count'']}}",
           ", \"parallel_trial_count\": ", "{{$.inputs.parameters[''parallel_trial_count'']}}",
           ", \"max_failed_trial_count\": ", "{{$.inputs.parameters[''max_failed_trial_count'']}}",
           ", \"trial_job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\"",
           "1", "\", \"machine_spec\": ", "{{$.inputs.parameters[''training_machine_spec'']}}",
           ", \"disk_spec\": ", "{{$.inputs.parameters[''training_disk_spec'']}}",
-          ", \"container_spec\": {\"image_uri\":\"", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/wide-and-deep-training:20231029_0125",
+          ", \"container_spec\": {\"image_uri\":\"", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/wide-and-deep-training:20240119_0125",
           "\", \"args\": [\"--target_column=", "{{$.inputs.parameters[''target_column'']}}",
           "\", \"--weight_column=", "{{$.inputs.parameters[''weight_column'']}}",
           "\", \"--model_type=", "{{$.inputs.parameters[''prediction_type'']}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--prediction_docker_uri_artifact_path=", "{{$.outputs.parameters[''prediction_docker_uri_output''].output_file}}",
           "\", \"--baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--training_schema_path=", "{{$.inputs.artifacts[''training_schema_uri''].uri}}",
           "\", \"--instance_schema_path=", "{{$.outputs.parameters[''instance_schema_uri''].output_file}}",
           "\", \"--prediction_schema_path=", "{{$.outputs.parameters[''prediction_schema_uri''].output_file}}",
```

## google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer.py

```diff
@@ -157,23 +157,23 @@
                   '"}, "job_spec": {"worker_pool_specs": [{"replica_count":"',
                   '1',
                   '", "machine_spec": ',
                   training_machine_spec,
                   ', "disk_spec": ',
                   training_disk_spec,
                   ', "container_spec": {"image_uri":"',
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/wide-and-deep-training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/wide-and-deep-training:20240119_0125',
                   '", "args": ["--target_column=',
                   target_column,
                   '", "--weight_column=',
                   weight_column,
                   '", "--model_type=',
                   prediction_type,
                   '", "--prediction_docker_uri=',
-                  'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125',
                   '", "--baseline_path=',
                   instance_baseline.uri,
                   '", "--metadata_path=',
                   metadata.uri,
                   '", "--transform_output_path=',
                   transform_output.uri,
                   '", "--training_schema_path=',
```

## google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer_pipeline.yaml

```diff
@@ -96,30 +96,26 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-infra-validator:
     executorLabel: exec-automl-tabular-infra-validator
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-bool-identity:
     executorLabel: exec-bool-identity
     inputDefinitions:
       parameters:
         value:
           description: Boolean value to return
           parameterType: BOOLEAN
@@ -814,194 +810,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -1009,42 +969,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -1086,38 +1038,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -1142,116 +1087,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -1267,272 +1185,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -1558,19 +1424,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -1583,44 +1447,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-model-batch-predict:
     executorLabel: exec-model-batch-predict
     inputDefinitions:
       artifacts:
         model:
           artifactType:
@@ -2373,224 +2229,178 @@
     executorLabel: exec-training-configurator-and-validator
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -2653,136 +2463,106 @@
         beta_2:
           defaultValue: 0.999
           description: Beta 2 value for optimizer_type="adam".
           isOptional: true
           parameterType: NUMBER_DOUBLE
         cache_data:
           defaultValue: auto
-          description: 'Whether to cache data or not. If set to
-
-            ''auto'', caching is determined based on the dataset size.'
+          description: Whether to cache data or not. If set to 'auto', caching is
+            determined based on the dataset size.
           isOptional: true
           parameterType: STRING
         dnn_beta_1:
           defaultValue: 0.9
           description: Beta 1 value for dnn_optimizer_type="adam".
           isOptional: true
           parameterType: NUMBER_DOUBLE
         dnn_beta_2:
           defaultValue: 0.999
           description: Beta 2 value for dnn_optimizer_type="adam".
           isOptional: true
           parameterType: NUMBER_DOUBLE
         dnn_dropout:
           defaultValue: 0.0
-          description: 'The probability we will drop out a given
-
-            coordinate.'
+          description: The probability we will drop out a given coordinate.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         dnn_l1_regularization_strength:
           defaultValue: 0.0
-          description: 'L1 regularization
-
-            strength for dnn_optimizer_type="ftrl".'
+          description: L1 regularization strength for dnn_optimizer_type="ftrl".
           isOptional: true
           parameterType: NUMBER_DOUBLE
         dnn_l2_regularization_strength:
           defaultValue: 0.0
-          description: 'L2 regularization
-
-            strength for dnn_optimizer_type="ftrl".'
+          description: L2 regularization strength for dnn_optimizer_type="ftrl".
           isOptional: true
           parameterType: NUMBER_DOUBLE
         dnn_l2_shrinkage_regularization_strength:
           defaultValue: 0.0
-          description: 'L2 shrinkage
-
-            regularization strength for dnn_optimizer_type="ftrl".'
+          description: L2 shrinkage regularization strength for dnn_optimizer_type="ftrl".
           isOptional: true
           parameterType: NUMBER_DOUBLE
         dnn_learning_rate:
-          description: 'The learning rate for training the
-
-            deep part of the model.'
+          description: The learning rate for training the deep part of the model.
           parameterType: NUMBER_DOUBLE
         dnn_optimizer_type:
           defaultValue: ftrl
-          description: 'The type of optimizer to use for the
-
-            deep part of the model. Choices are "adam", "ftrl" and "sgd". for the
-
-            Adam, FTRL, and Gradient Descent Optimizers, respectively.'
+          description: The type of optimizer to use for the deep part of the model.
+            Choices are "adam", "ftrl" and "sgd". for the Adam, FTRL, and Gradient
+            Descent Optimizers, respectively.
           isOptional: true
           parameterType: STRING
         embed_categories:
           defaultValue: true
-          description: 'If set to true, the categorical columns
-
-            will be used embedded and used in the deep part of the model. Embedding
-
-            size is the square root of the column cardinality.'
+          description: If set to true, the categorical columns will be used embedded
+            and used in the deep part of the model. Embedding size is the square root
+            of the column cardinality.
           isOptional: true
           parameterType: BOOLEAN
         enable_profiler:
           defaultValue: false
-          description: 'Enables profiling and saves a trace
-
-            during evaluation.'
+          description: Enables profiling and saves a trace during evaluation.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: The KMS key name.
           isOptional: true
           parameterType: STRING
         eval_frequency_secs:
           defaultValue: 600.0
-          description: 'Frequency at which evaluation and
-
-            checkpointing will take place.'
+          description: Frequency at which evaluation and checkpointing will take place.
           isOptional: true
           parameterType: NUMBER_INTEGER
         eval_steps:
           defaultValue: 0.0
-          description: 'Number of steps to run evaluation for. If not
-
-            specified or negative, it means run evaluation on the whole validation
-
-            dataset. If set to 0, it means run evaluation for a fixed number of
-
-            samples.'
+          description: Number of steps to run evaluation for. If not specified or
+            negative, it means run evaluation on the whole validation dataset. If
+            set to 0, it means run evaluation for a fixed number of samples.
           isOptional: true
           parameterType: NUMBER_INTEGER
         hidden_units:
           defaultValue: 30,30,30
-          description: 'Hidden layer sizes to use for DNN feature
-
-            columns, provided in comma-separated layers.'
+          description: Hidden layer sizes to use for DNN feature columns, provided
+            in comma-separated layers.
           isOptional: true
           parameterType: STRING
         l1_regularization_strength:
           defaultValue: 0.0
-          description: 'L1 regularization strength
-
-            for optimizer_type="ftrl".'
+          description: L1 regularization strength for optimizer_type="ftrl".
           isOptional: true
           parameterType: NUMBER_DOUBLE
         l2_regularization_strength:
           defaultValue: 0.0
-          description: 'L2 regularization strength
-
-            for optimizer_type="ftrl"'
+          description: L2 regularization strength for optimizer_type="ftrl"
           isOptional: true
           parameterType: NUMBER_DOUBLE
         l2_shrinkage_regularization_strength:
           defaultValue: 0.0
-          description: 'L2 shrinkage
-
-            regularization strength for optimizer_type="ftrl".'
+          description: L2 shrinkage regularization strength for optimizer_type="ftrl".
           isOptional: true
           parameterType: NUMBER_DOUBLE
         learning_rate:
           description: The learning rate used by the linear optimizer.
           parameterType: NUMBER_DOUBLE
         location:
           description: The GCP region that runs the pipeline components.
@@ -2790,52 +2570,39 @@
         max_steps:
           defaultValue: -1.0
           description: Number of steps to run the trainer for.
           isOptional: true
           parameterType: NUMBER_INTEGER
         max_train_secs:
           defaultValue: -1.0
-          description: 'Amount of time in seconds to run the
-
-            trainer for.'
+          description: Amount of time in seconds to run the trainer for.
           isOptional: true
           parameterType: NUMBER_INTEGER
         measurement_selection_type:
           defaultValue: BEST_MEASUREMENT
-          description: 'Which measurement to use
-
-            if/when the service automatically selects the final measurement from
-
-            previously reported intermediate measurements. One of "BEST_MEASUREMENT"
-
-            or "LAST_MEASUREMENT".'
+          description: Which measurement to use if/when the service automatically
+            selects the final measurement from previously reported intermediate measurements.
+            One of "BEST_MEASUREMENT" or "LAST_MEASUREMENT".
           isOptional: true
           parameterType: STRING
         optimization_metric:
           defaultValue: ''
-          description: 'Optimization metric used for
-
-            `measurement_selection_type`. Default is "rmse" for regression and "auc"
-
-            for classification.'
+          description: Optimization metric used for `measurement_selection_type`.
+            Default is "rmse" for regression and "auc" for classification.
           isOptional: true
           parameterType: STRING
         optimizer_type:
           defaultValue: adam
-          description: 'The type of optimizer to use. Choices are
-
-            "adam", "ftrl" and "sgd" for the Adam, FTRL, and Gradient Descent
-
-            Optimizers, respectively.'
+          description: The type of optimizer to use. Choices are "adam", "ftrl" and
+            "sgd" for the Adam, FTRL, and Gradient Descent Optimizers, respectively.
           isOptional: true
           parameterType: STRING
         prediction_type:
-          description: 'The type of prediction the model is to
-
-            produce. "classification" or "regression".'
+          description: The type of prediction the model is to produce. "classification"
+            or "regression".
           parameterType: STRING
         project:
           description: The GCP project that runs the pipeline components.
           parameterType: STRING
         root_dir:
           description: The root GCS directory for the pipeline components.
           parameterType: STRING
@@ -2853,26 +2620,22 @@
             boot_disk_type: pd-ssd
           description: The training disk spec.
           isOptional: true
           parameterType: STRUCT
         training_machine_spec:
           defaultValue:
             machine_type: c2-standard-16
-          description: 'The training machine
-
-            spec. See https://cloud.google.com/compute/docs/machine-types for
-
-            options.'
+          description: The training machine spec. See https://cloud.google.com/compute/docs/machine-types
+            for options.
           isOptional: true
           parameterType: STRUCT
         use_wide:
           defaultValue: true
-          description: 'If set to true, the categorical columns will be
-
-            used in the wide part of the DNN model.'
+          description: If set to true, the categorical columns will be used in the
+            wide part of the DNN model.
           isOptional: true
           parameterType: BOOLEAN
         weight_column:
           defaultValue: ''
           description: The weight column name.
           isOptional: true
           parameterType: STRING
@@ -2902,30 +2665,30 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
         - google_cloud_pipeline_components.container.v1.custom_job.launcher
         image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.44
     exec-automl-tabular-infra-validator:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-bool-identity:
       container:
         args:
         - --executor_input
@@ -3033,16 +2796,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -3051,15 +2814,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-model-batch-predict:
       container:
         args:
         - --type
@@ -3310,15 +3073,15 @@
           \    raise ValueError(\n        'One of vertex_dataset, data_source_csv_filenames,'\n\
           \        ' data_source_bigquery_table_path must be specified'\n    )\n\n\
           \  return collections.namedtuple(\n      'Outputs',\n      [\n         \
           \ 'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
           \          'model_display_name',\n      ],\n  )(\n      data_source_csv_filenames,\n\
           \      data_source_bigquery_table_path,\n      model_display_name,\n  )\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-split-materialized-data:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _split_materialized_data
@@ -3356,15 +3119,15 @@
           \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
           \ data source: {materialized_data_json}')\n\n  # we map indices to file\
           \ patterns based on the ordering of insertion order\n  # in our transform_data\
           \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
           \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
           \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
           \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
     exec-training-configurator-and-validator:
       container:
         args:
         - training_configurator_and_validator
         - '{"Concat": ["--instance_schema_path=", "{{$.inputs.artifacts[''instance_schema''].uri}}"]}'
         - '{"Concat": ["--training_schema_path=", "{{$.inputs.artifacts[''training_schema''].uri}}"]}'
         - '{"Concat": ["--dataset_stats_path=", "{{$.inputs.artifacts[''dataset_stats''].uri}}"]}'
@@ -3401,15 +3164,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
     exec-wide-and-deep-trainer:
       container:
         args:
         - --type
         - CustomJob
         - --project
         - '{{$.inputs.parameters[''project'']}}'
@@ -3419,19 +3182,19 @@
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"wide-and-deep-trainer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\"", "1",
           "\", \"machine_spec\": ", "{{$.inputs.parameters[''training_machine_spec'']}}",
           ", \"disk_spec\": ", "{{$.inputs.parameters[''training_disk_spec'']}}",
-          ", \"container_spec\": {\"image_uri\":\"", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/wide-and-deep-training:20231029_0125",
+          ", \"container_spec\": {\"image_uri\":\"", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/wide-and-deep-training:20240119_0125",
           "\", \"args\": [\"--target_column=", "{{$.inputs.parameters[''target_column'']}}",
           "\", \"--weight_column=", "{{$.inputs.parameters[''weight_column'']}}",
           "\", \"--model_type=", "{{$.inputs.parameters[''prediction_type'']}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--training_schema_path=", "{{$.inputs.artifacts[''training_schema_uri''].uri}}",
           "\", \"--job_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--training_data_path=", "{{$.inputs.artifacts[''materialized_train_split''].uri}}",
           "\", \"--validation_data_path=", "{{$.inputs.artifacts[''materialized_eval_split''].uri}}",
```

## google_cloud_pipeline_components/preview/automl/tabular/xgboost_hyperparameter_tuning_job_pipeline.yaml

```diff
@@ -79,18 +79,16 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-bool-identity:
     executorLabel: exec-bool-identity
     inputDefinitions:
       parameters:
         value:
           description: Boolean value to return
@@ -770,194 +768,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -965,42 +927,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -1042,38 +996,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -1098,116 +1045,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -1223,272 +1143,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -1514,19 +1382,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -1539,44 +1405,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-generate-xgboost-hyperparameter-tuning-worker-pool-specs:
     executorLabel: exec-generate-xgboost-hyperparameter-tuning-worker-pool-specs
     inputDefinitions:
       artifacts:
         instance_baseline:
           artifactType:
@@ -2489,224 +2347,178 @@
     executorLabel: exec-training-configurator-and-validator
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -2731,85 +2543,63 @@
           isOptional: true
           parameterType: STRING
         location:
           description: The GCP region that runs the pipeline components.
           parameterType: STRING
         max_failed_trial_count:
           defaultValue: 0.0
-          description: 'The number of failed trials that
-
-            need to be seen before failing the HyperparameterTuningJob. If set to
-            0,
-
-            Vertex AI decides how many trials must fail before the whole job fails.'
+          description: The number of failed trials that need to be seen before failing
+            the HyperparameterTuningJob. If set to 0, Vertex AI decides how many trials
+            must fail before the whole job fails.
           isOptional: true
           parameterType: NUMBER_INTEGER
         max_trial_count:
           description: The desired total number of trials.
           parameterType: NUMBER_INTEGER
         parallel_trial_count:
-          description: 'The desired number of trials to run
-
-            in parallel.'
+          description: The desired number of trials to run in parallel.
           parameterType: NUMBER_INTEGER
         project:
           description: The GCP project that runs the pipeline components.
           parameterType: STRING
         study_spec_algorithm:
           defaultValue: ALGORITHM_UNSPECIFIED
-          description: 'The search algorithm specified for
-
-            the study. One of ''ALGORITHM_UNSPECIFIED'', ''GRID_SEARCH'', or
-
-            ''RANDOM_SEARCH''.'
+          description: The search algorithm specified for the study. One of 'ALGORITHM_UNSPECIFIED',
+            'GRID_SEARCH', or 'RANDOM_SEARCH'.
           isOptional: true
           parameterType: STRING
         study_spec_measurement_selection_type:
           defaultValue: BEST_MEASUREMENT
-          description: 'Which measurement
-
-            to use if/when the service automatically selects the final measurement
-
-            from previously reported intermediate measurements. One of
-
-            "BEST_MEASUREMENT" or "LAST_MEASUREMENT".'
+          description: Which measurement to use if/when the service automatically
+            selects the final measurement from previously reported intermediate measurements.
+            One of "BEST_MEASUREMENT" or "LAST_MEASUREMENT".
           isOptional: true
           parameterType: STRING
         study_spec_metric_goal:
-          description: 'Optimization goal of the metric,
-
-            possible values: "MAXIMIZE", "MINIMIZE".'
+          description: 'Optimization goal of the metric, possible values: "MAXIMIZE",
+            "MINIMIZE".'
           parameterType: STRING
         study_spec_metric_id:
-          description: 'Metric to optimize. For options,
-
-            please look under ''eval_metric'' at
-
-            https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters.'
+          description: Metric to optimize. For options, please look under 'eval_metric'
+            at https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters.
           parameterType: STRING
         study_spec_parameters_override:
-          description: 'List of dictionaries
-
-            representing parameters to optimize. The dictionary key is the
-
-            parameter_id, which is passed to training job as a command line
-
-            argument, and the dictionary value is the parameter specification of the
-
-            metric.'
+          description: List of dictionaries representing parameters to optimize. The
+            dictionary key is the parameter_id, which is passed to training job as
+            a command line argument, and the dictionary value is the parameter specification
+            of the metric.
           parameterType: LIST
         worker_pool_specs:
           description: The worker pool specs.
           parameterType: LIST
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'Serialized gcp_resources proto tracking the custom training
-
-            job.'
+          description: Serialized gcp_resources proto tracking the custom training
+            job.
           parameterType: STRING
 deploymentSpec:
   executors:
     exec-automl-tabular-finalizer:
       container:
         args:
         - --type
@@ -2821,15 +2611,15 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
@@ -2943,16 +2733,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -2961,15 +2751,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-generate-xgboost-hyperparameter-tuning-worker-pool-specs:
       container:
         args:
         - --executor_input
@@ -3031,28 +2821,28 @@
           \ 0.\n    seed: Random seed.\n    seed_per_iteration: Seed PRNG determnisticly\
           \ via iterator number.\n\n  Raises:\n    ValueError: If accelerator_count\
           \ <= 0 and accelerator_type is specified.\n\n  Returns:\n    Output parameters.\n\
           \  \"\"\"\n  import copy\n  import collections\n  import re\n\n  def get_gcs_path(path):\n\
           \    return re.sub(r'^/gcs/', r'gs://', path)\n\n  master_worker_pool_spec\
           \ = {\n      'replica_count': 1,\n      'machine_spec': {\n          'machine_type':\
           \ machine_type,\n      },\n      'container_spec': {\n          'image_uri':\
-          \ 'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/xgboost-training:20231029_0125',\n\
+          \ 'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/xgboost-training:20240119_0125',\n\
           \          'args': [\n              f'--job_dir={get_gcs_path(job_dir)}',\n\
           \              f'--instance_schema_path={get_gcs_path(instance_schema_uri)}',\n\
           \              f'--prediction_schema_path={get_gcs_path(prediction_schema_uri)}',\n\
           \              f'--trials_path={get_gcs_path(trials)}',\n              f'--prediction_docker_uri_artifact_path={get_gcs_path(prediction_docker_uri_output)}',\n\
           \              f'--target_column={target_column}',\n              f'--objective={objective}',\n\
           \              f'--training_data_path={get_gcs_path(materialized_train_split)}',\n\
           \              f'--validation_data_path={get_gcs_path(materialized_eval_split)}',\n\
           \              f'--transform_output_path={get_gcs_path(transform_output)}',\n\
           \              f'--training_schema_path={get_gcs_path(training_schema_uri)}',\n\
           \              f'--baseline_path={get_gcs_path(instance_baseline)}',\n \
           \             f'--eval_metric={eval_metric}',\n              f'--disable_default_eval_metric={disable_default_eval_metric}',\n\
           \              f'--seed={seed}',\n              f'--seed_per_iteration={seed_per_iteration}',\n\
-          \              '--prediction_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/xgboost-prediction-server:20231029_0125',\n\
+          \              '--prediction_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/xgboost-prediction-server:20240119_0125',\n\
           \          ],\n      },\n  }\n\n  # Add optional arguments if set\n  if\
           \ weight_column:\n    master_worker_pool_spec['container_spec']['args'].append(\n\
           \        f'--weight_column={weight_column}'\n    )\n\n  # Add accelerator_type\
           \ and accelerator_count if set.\n  if accelerator_type:\n    if accelerator_count\
           \ <= 0:\n      raise ValueError(\n          'Accelerator count must be greator\
           \ than 0 when type is specified.'\n      )\n    master_worker_pool_spec['machine_spec'][\n\
           \        'accelerator_type'\n    ] = accelerator_type\n    master_worker_pool_spec['machine_spec'][\n\
@@ -3134,15 +2924,15 @@
           \ = best_fn(trials_list, key=lambda trial: trial['objective_value'])\n\n\
           \  # Build unmanaged_container_model\n  unmanaged_container_model.metadata['containerSpec']\
           \ = {\n      'imageUri': prediction_docker_uri,\n      'healthRoute': '/health',\n\
           \      'predictRoute': '/predict',\n  }\n  unmanaged_container_model.metadata['predictSchemata']\
           \ = {\n      'instanceSchemaUri': instance_schema_uri,\n      'predictionSchemaUri':\
           \ prediction_schema_uri,\n  }\n  unmanaged_container_model.uri = os.path.join(\n\
           \      trials_dir, 'trial_{}'.format(best_trial['id']), 'model'\n  )\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-prediction-type-for-xgboost:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _get_prediction_type_for_xgboost
@@ -3753,15 +3543,15 @@
           \    raise ValueError(\n        'One of vertex_dataset, data_source_csv_filenames,'\n\
           \        ' data_source_bigquery_table_path must be specified'\n    )\n\n\
           \  return collections.namedtuple(\n      'Outputs',\n      [\n         \
           \ 'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
           \          'model_display_name',\n      ],\n  )(\n      data_source_csv_filenames,\n\
           \      data_source_bigquery_table_path,\n      model_display_name,\n  )\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-split-materialized-data:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _split_materialized_data
@@ -3799,15 +3589,15 @@
           \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
           \ data source: {materialized_data_json}')\n\n  # we map indices to file\
           \ patterns based on the ordering of insertion order\n  # in our transform_data\
           \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
           \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
           \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
           \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
     exec-training-configurator-and-validator:
       container:
         args:
         - training_configurator_and_validator
         - '{"Concat": ["--instance_schema_path=", "{{$.inputs.artifacts[''instance_schema''].uri}}"]}'
         - '{"Concat": ["--training_schema_path=", "{{$.inputs.artifacts[''training_schema''].uri}}"]}'
         - '{"Concat": ["--dataset_stats_path=", "{{$.inputs.artifacts[''dataset_stats''].uri}}"]}'
@@ -3844,15 +3634,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
     exec-xgboost-hyperparameter-tuning-job:
       container:
         args:
         - --type
         - HyperparameterTuningJob
         - --project
         - '{{$.inputs.parameters[''project'']}}'
```

## google_cloud_pipeline_components/preview/automl/tabular/xgboost_trainer_pipeline.yaml

```diff
@@ -108,18 +108,16 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-bool-identity:
     executorLabel: exec-bool-identity
     inputDefinitions:
       parameters:
         value:
           description: Boolean value to return
@@ -870,194 +868,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -1065,42 +1027,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -1142,38 +1096,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -1198,116 +1145,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -1323,272 +1243,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -1614,19 +1482,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -1639,44 +1505,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-generate-xgboost-trainer-worker-pool-specs:
     executorLabel: exec-generate-xgboost-trainer-worker-pool-specs
     inputDefinitions:
       artifacts:
         instance_baseline:
           artifactType:
@@ -2753,224 +2611,178 @@
     executorLabel: exec-training-configurator-and-validator
     inputDefinitions:
       artifacts:
         dataset_stats:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Dataset stats generated by
-
-            feature transform engine.'
+          description: Dataset stats generated by feature transform engine.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'Schema of input data to the tf_model at
-
-            serving time.'
+          description: Schema of input data to the tf_model at serving time.
         training_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
       parameters:
         available_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            available at forecast time.'
+          description: The names of the columns that are available at forecast time.
           isOptional: true
           parameterType: LIST
         context_window:
           defaultValue: -1.0
           description: The length of the context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         enable_probabilistic_inference:
           defaultValue: false
-          description: 'If probabilistic inference is
-
-            enabled, the model will fit a distribution that captures the uncertainty
-
-            of a prediction. At inference time, the predictive distribution is used
-
-            to make a point prediction that minimizes the optimization objective.
-
-            For example, the mean of a predictive distribution is the point
-
-            prediction that minimizes RMSE loss. If quantiles are specified, then
-
-            the quantiles of the distribution are also returned.'
+          description: If probabilistic inference is enabled, the model will fit a
+            distribution that captures the uncertainty of a prediction. At inference
+            time, the predictive distribution is used to make a point prediction that
+            minimizes the optimization objective.  For example, the mean of a predictive
+            distribution is the point prediction that minimizes RMSE loss. If quantiles
+            are specified, then the quantiles of the distribution are also returned.
           isOptional: true
           parameterType: BOOLEAN
         forecast_horizon:
           defaultValue: -1.0
           description: The length of the forecast horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_model_type:
           defaultValue: ''
           description: The model types, e.g. l2l, seq2seq, tft.
           isOptional: true
           parameterType: STRING
         forecasting_transformations:
           defaultValue: {}
-          description: 'Dict mapping auto and/or type-resolutions to
-
-            feature columns. The supported types are auto, categorical, numeric,
-
-            text, and timestamp.'
+          description: Dict mapping auto and/or type-resolutions to feature columns.
+            The supported types are auto, categorical, numeric, text, and timestamp.
           isOptional: true
           parameterType: STRUCT
         group_columns:
-          description: 'A list of time series attribute column
-
-            names that define the time series hierarchy.'
+          description: A list of time series attribute column names that define the
+            time series hierarchy.
           isOptional: true
           parameterType: LIST
         group_temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over both the horizon and time series in the same
-
-            hierarchy group.'
+          description: The weight of the loss for predictions aggregated over both
+            the horizon and time series in the same hierarchy group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         group_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over time series in the same group.'
+          description: The weight of the loss for predictions aggregated over time
+            series in the same group.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of "classification",
-
-            "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         quantiles:
           defaultValue: []
           description: All quantiles that the model need to predict.
           isOptional: true
           parameterType: LIST
         run_distill:
           defaultValue: false
-          description: 'Whether the distillation should be applied to the
-
-            training.'
+          description: Whether the distillation should be applied to the training.
           isOptional: true
           parameterType: BOOLEAN
         run_evaluation:
           defaultValue: false
-          description: 'Whether we are running evaluation in the training
-
-            pipeline.'
+          description: Whether we are running evaluation in the training pipeline.
           isOptional: true
           parameterType: BOOLEAN
         split_example_counts:
-          description: 'JSON string of data split example counts for
-
-            train, validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
         stage_1_deadline_hours:
-          description: 'Stage 1 training budget in
-
-            hours.'
+          description: Stage 1 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         stage_2_deadline_hours:
-          description: 'Stage 2 training budget in
-
-            hours.'
+          description: Stage 2 training budget in hours.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         target_column:
           defaultValue: ''
           description: Target column of input data.
           isOptional: true
           parameterType: STRING
         temporal_total_weight:
           defaultValue: 0.0
-          description: 'The weight of the loss for
-
-            predictions aggregated over the horizon for a single time series.'
+          description: The weight of the loss for predictions aggregated over the
+            horizon for a single time series.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         time_column:
           defaultValue: ''
-          description: 'The column that indicates the time. Used by forecasting
-
-            only.'
+          description: The column that indicates the time. Used by forecasting only.
           isOptional: true
           parameterType: STRING
         time_series_attribute_columns:
           defaultValue: []
-          description: 'The column names of the time series
-
-            attributes.'
+          description: The column names of the time series attributes.
           isOptional: true
           parameterType: LIST
         time_series_identifier_column:
-          description: '[Deprecated] The time series identifier
-
-            column. Used by forecasting only. Raises exception if used -
-
-            use the "time_series_identifier_column" field instead.'
+          description: '[Deprecated] The time series identifier column. Used by forecasting
+            only. Raises exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         time_series_identifier_columns:
           defaultValue: []
-          description: 'The list of time series identifier columns.
-
-            Used by forecasting only.'
+          description: The list of time series identifier columns.  Used by forecasting
+            only.
           isOptional: true
           parameterType: LIST
         unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'The names of the columns that are
-
-            not available at forecast time.'
+          description: The names of the columns that are not available at forecast
+            time.
           isOptional: true
           parameterType: LIST
         weight_column:
           defaultValue: ''
           description: Weight column of input data.
           isOptional: true
           parameterType: STRING
@@ -3002,17 +2814,16 @@
           parameterType: STRING
         worker_pool_specs:
           description: The worker pool specs.
           parameterType: LIST
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'Serialized gcp_resources proto tracking the custom training
-
-            job.'
+          description: Serialized gcp_resources proto tracking the custom training
+            job.
           parameterType: STRING
 deploymentSpec:
   executors:
     exec-automl-tabular-finalizer:
       container:
         args:
         - --type
@@ -3024,15 +2835,15 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
@@ -3146,16 +2957,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -3164,15 +2975,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 30.0
     exec-generate-xgboost-trainer-worker-pool-specs:
       container:
         args:
         - --executor_input
@@ -3290,18 +3101,18 @@
           \ Parameter that controls the variance of the Tweedie\n      distribution.\n\
           \    huber_slope: A parameter used for Pseudo-Huber loss to define the delta\n\
           \      term.\n\n  Raises:\n    ValueError: If accelerator_count <= 0 and\
           \ accelerator_type is specified.\n\n  Returns:\n    Outputs containing the\
           \ worker pool specs.\n  \"\"\"\n  import copy\n  import collections\n  import\
           \ os\n  import re\n\n  def get_gcs_path(path):\n    return re.sub(r'/gcs/',\
           \ 'gs://', path)\n\n  formatted_job_dir = get_gcs_path(job_dir)\n  prediction_docker_uri\
-          \ = (\n      'us-docker.pkg.dev/vertex-ai/automl-tabular/xgboost-prediction-server:20231029_0125'\n\
+          \ = (\n      'us-docker.pkg.dev/vertex-ai/automl-tabular/xgboost-prediction-server:20240119_0125'\n\
           \  )\n  master_worker_pool_spec = {\n      'replica_count': 1,\n      'machine_spec':\
           \ {\n          'machine_type': machine_type,\n      },\n      'container_spec':\
-          \ {\n          'image_uri': 'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/xgboost-training:20231029_0125',\n\
+          \ {\n          'image_uri': 'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/xgboost-training:20240119_0125',\n\
           \          'args': [\n              f'--job_dir={formatted_job_dir}',\n\
           \              f'--target_column={target_column}',\n              f'--objective={objective}',\n\
           \              f'--training_data_path={get_gcs_path(materialized_train_split)}',\n\
           \              f'--validation_data_path={get_gcs_path(materialized_eval_split)}',\n\
           \              f'--transform_output_path={get_gcs_path(transform_output)}',\n\
           \              f'--training_schema_path={get_gcs_path(training_schema_uri)}',\n\
           \              f'--baseline_path={get_gcs_path(instance_baseline)}',\n \
@@ -3587,15 +3398,15 @@
           \    raise ValueError(\n        'One of vertex_dataset, data_source_csv_filenames,'\n\
           \        ' data_source_bigquery_table_path must be specified'\n    )\n\n\
           \  return collections.namedtuple(\n      'Outputs',\n      [\n         \
           \ 'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
           \          'model_display_name',\n      ],\n  )(\n      data_source_csv_filenames,\n\
           \      data_source_bigquery_table_path,\n      model_display_name,\n  )\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-split-materialized-data:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _split_materialized_data
@@ -3633,15 +3444,15 @@
           \      'file_patterns']\n  else:\n    raise ValueError(f'Unsupported training\
           \ data source: {materialized_data_json}')\n\n  # we map indices to file\
           \ patterns based on the ordering of insertion order\n  # in our transform_data\
           \ (see above in _generate_analyze_and_transform_data)\n  with tf.io.gfile.GFile(materialized_train_split,\
           \ 'w') as f:\n    f.write(file_patterns[0])\n\n  with tf.io.gfile.GFile(materialized_eval_split,\
           \ 'w') as f:\n    f.write(file_patterns[1])\n\n  with tf.io.gfile.GFile(materialized_test_split,\
           \ 'w') as f:\n    f.write(file_patterns[2])\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
     exec-training-configurator-and-validator:
       container:
         args:
         - training_configurator_and_validator
         - '{"Concat": ["--instance_schema_path=", "{{$.inputs.artifacts[''instance_schema''].uri}}"]}'
         - '{"Concat": ["--training_schema_path=", "{{$.inputs.artifacts[''training_schema''].uri}}"]}'
         - '{"Concat": ["--dataset_stats_path=", "{{$.inputs.artifacts[''dataset_stats''].uri}}"]}'
@@ -3678,15 +3489,15 @@
           "{{$.inputs.parameters[''group_columns'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
     exec-xgboost-trainer:
       container:
         args:
         - --type
         - CustomJob
         - --project
         - '{{$.inputs.parameters[''project'']}}'
```

## google_cloud_pipeline_components/preview/llm/rlhf/component.py

```diff
@@ -64,24 +64,30 @@
     reward_model_learning_rate_multiplier: Constant used to adjust the base learning rate used when training a reward model. Multiply by a number > 1 to increase the magnitude of updates applied at each training step or multiply by a number < 1 to decrease the magnitude of updates. Default value is 1.0.
     reinforcement_learning_rate_multiplier: Constant used to adjust the base learning rate used during reinforcement learning. Multiply by a number > 1 to increase the magnitude of updates applied at each training step or multiply by a number < 1 to decrease the magnitude of updates. Default value is 1.0.
     reward_model_train_steps: Number of steps to use when training a reward model. Default value is 1000.
     reinforcement_learning_train_steps: Number of reinforcement learning steps to perform when tuning a base model. Default value is 1000.
     kl_coeff: Coefficient for KL penalty. This regularizes the policy model and penalizes if it diverges from its initial distribution. If set to 0, the reference language model is not loaded into memory. Default value is 0.1.
     instruction: This field lets the model know what task it needs to perform. Base models have been trained over a large set of varied instructions. You can give a simple and intuitive description of the task and the model will follow it, e.g. "Classify this movie review as positive or negative" or "Translate this sentence to Danish". Do not specify this if your dataset already prepends the instruction to the inputs field.
     deploy_model: Whether to deploy the model to an endpoint in `us-central1`. Default is True.
-    eval_dataset: Optional Cloud storage path to an evaluation dataset. If provided, inference will be performed on this dataset after training. The dataset format is jsonl. Each example in the dataset must contain a field `input_text` that contains the prompt.
+    eval_dataset: Optional Cloud storage path to an evaluation dataset. Note, eval dataset can only be provided for third-party models. If provided, inference will be performed on this dataset after training. The dataset format is jsonl. Each example in the dataset must contain a field `input_text` that contains the prompt.
     project: Project used to run custom jobs. If not specified the project used to run the pipeline will be used.
     location: Location used to run custom jobs. If not specified the location used to run the pipeline will be used.
     tensorboard_resource_id: Optional tensorboard resource id in format `projects/{project_number}/locations/{location}/tensorboards/{tensorboard_id}`. If provided, tensorboard metrics will be uploaded to this location.
 
   Returns:
     model_resource_name: Path to the model uploaded to the Model Registry. This will be an empty string if the model was not deployed.
     endpoint_resource_name: Path the Online Prediction Endpoint. This will be an empty string if the model was not deployed.
   """
   # fmt: on
+
+  function_based.validate_rlhf_inputs(
+      large_model_reference=large_model_reference,
+      eval_dataset=eval_dataset,
+  ).set_display_name('Validate Inputs')
+
   reward_model_pipeline = (
       reward_model_graph.pipeline(
           preference_dataset=preference_dataset,
           large_model_reference=large_model_reference,
           prompt_sequence_length=prompt_sequence_length,
           target_sequence_length=target_sequence_length,
           instruction=instruction,
@@ -106,33 +112,41 @@
       kl_coeff=kl_coeff,
       instruction=instruction,
       project=project,
       location=location,
       tensorboard_resource_id=tensorboard_resource_id,
   ).set_display_name('Reinforcement Learning')
 
-  should_perform_inference = function_based.value_exists(
+  has_inference_dataset = function_based.value_exists(
       value=eval_dataset
   ).set_display_name('Resolve Inference Dataset')
   with kfp.dsl.Condition(
-      should_perform_inference.output == True, name='Perform Inference'  # pylint: disable=singleton-comparison
+      has_inference_dataset.output == True,  # pylint: disable=singleton-comparison
+      name='Perform Inference',
   ):
-    component.infer_pipeline(
-        project=project,
-        location=location,
-        large_model_reference=large_model_reference,
-        model_checkpoint=rl_model_pipeline.outputs['output_model_path'],
-        prompt_dataset=eval_dataset,
-        prompt_sequence_length=prompt_sequence_length,
-        target_sequence_length=target_sequence_length,
-        instruction=instruction,
-    )
+    has_model_checkpoint = function_based.value_exists(
+        value=rl_model_pipeline.outputs['output_model_path']
+    ).set_display_name('Resolve Model Checkpoint')
+    with kfp.dsl.Condition(
+        has_model_checkpoint.output == True,  # pylint: disable=singleton-comparison
+        name='Test Model Checkpoint Exists',
+    ):
+      component.infer_pipeline(
+          project=project,
+          location=location,
+          large_model_reference=large_model_reference,
+          model_checkpoint=rl_model_pipeline.outputs['output_model_path'],
+          prompt_dataset=eval_dataset,
+          prompt_sequence_length=prompt_sequence_length,
+          target_sequence_length=target_sequence_length,
+          instruction=instruction,
+      )
 
   llm_model_handler = deployment_graph.pipeline(
-      output_adapter_path=rl_model_pipeline.outputs['output_model_path'],
+      output_adapter_path=rl_model_pipeline.outputs['output_adapter_path'],
       large_model_reference=large_model_reference,
       model_display_name=model_display_name,
       deploy_model=deploy_model,
   ).set_display_name('Upload and Deploy Tuned Model')
 
   return PipelineOutput(
       model_resource_name=llm_model_handler.outputs['model_resource_name'],
```

## google_cloud_pipeline_components/preview/model_evaluation/evaluation_llm_classification_pipeline.py

```diff
@@ -31,15 +31,15 @@
 @dsl.pipeline(name=_PIPELINE_NAME)
 def evaluation_llm_classification_pipeline(  # pylint: disable=dangerous-default-value
     project: str,
     location: str,
     target_field_name: str,
     batch_predict_gcs_source_uris: List[str],
     batch_predict_gcs_destination_output_uri: str,
-    model_name: str = 'publishers/google/models/text-bison@001',
+    model_name: str = 'publishers/google/models/text-bison@002',
     evaluation_task: str = 'text-classification',
     evaluation_class_labels: List[str] = [],
     input_field_name: str = 'input_text',
     batch_predict_instances_format: str = 'jsonl',
     batch_predict_predictions_format: str = 'jsonl',
     batch_predict_model_parameters: Dict[str, str] = {},
     machine_type: str = 'e2-highmem-16',
@@ -58,19 +58,19 @@
     evaluation_metrics=ClassificationMetrics,
     evaluation_resource_name=str,
 ):
   # fmt: off
   """The LLM Text Classification Evaluation pipeline.
 
   Args:
-    project: The GCP project that runs the pipeline components.
-    location: The GCP region that runs the pipeline components.
-    target_field_name: The target field's name. Formatted to be able to find nested columns, delimited by `.`. Prefixed with 'instance.' on the component for Vertex Batch Prediction.
-    batch_predict_gcs_source_uris: Google Cloud Storage URI(-s) to your instances data to run batch prediction on. The instances data should also contain the ground truth (target) data, used for evaluation. May contain wildcards. For more information on wildcards, see https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames. For more details about this input config, see https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
-    batch_predict_gcs_destination_output_uri: The Google Cloud Storage location of the directory where the output is to be written to.
+    project: Required. The GCP project that runs the pipeline components.
+    location: Required. The GCP region that runs the pipeline components.
+    target_field_name: Required. The target field's name. Formatted to be able to find nested columns, delimited by `.`. Prefixed with 'instance.' on the component for Vertex Batch Prediction.
+    batch_predict_gcs_source_uris: Required. Google Cloud Storage URI(-s) to your instances data to run batch prediction on. The instances data should also contain the ground truth (target) data, used for evaluation. May contain wildcards. For more information on wildcards, see https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames. For more details about this input config, see https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_gcs_destination_output_uri: Required. The Google Cloud Storage location of the directory where the output is to be written to.
     model_name: The Model name used to run evaluation. Must be a publisher Model or a managed Model sharing the same ancestor location. Starting this job has no impact on any existing deployments of the Model and their resources.
     evaluation_task: The task that the large language model will be evaluated on. The evaluation component computes a set of metrics relevant to that specific task. Currently supported Classification tasks is: `text-classification`.
     evaluation_class_labels: The JSON array of class names for the target_field, in the same order they appear in the batch predictions input file.
     input_field_name: The field name of the input eval dataset instances that contains the input prompts to the LLM.
     batch_predict_instances_format: The format in which instances are given, must be one of the Model's supportedInputStorageFormats. For more details about this input config, see https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
     batch_predict_predictions_format: The format in which Vertex AI gives the predictions. Must be one of the Model's supportedOutputStorageFormats. For more details about this output config, see https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
     batch_predict_model_parameters: A map of parameters that govern the predictions. Some acceptable parameters include: maxOutputTokens, topK, topP, and temperature.
```

## google_cloud_pipeline_components/preview/model_evaluation/evaluation_llm_text_generation_pipeline.py

```diff
@@ -29,15 +29,15 @@
 
 @dsl.pipeline(name=_PIPELINE_NAME)
 def evaluation_llm_text_generation_pipeline(  # pylint: disable=dangerous-default-value
     project: str,
     location: str,
     batch_predict_gcs_source_uris: List[str],
     batch_predict_gcs_destination_output_uri: str,
-    model_name: str = 'publishers/google/models/text-bison@001',
+    model_name: str = 'publishers/google/models/text-bison@002',
     evaluation_task: str = 'text-generation',
     input_field_name: str = 'input_text',
     target_field_name: str = 'output_text',
     batch_predict_instances_format: str = 'jsonl',
     batch_predict_predictions_format: str = 'jsonl',
     batch_predict_model_parameters: Dict[str, str] = {},
     enable_row_based_metrics: bool = False,
@@ -52,18 +52,18 @@
   # fmt: off
   """LLM Text Generation Evaluation pipeline.
 
   This pipeline supports evaluating large language models, publisher or managed
   models, performing the following generative tasks: `summarization`, `question-answering`, and `text-generation`.
 
   Args:
-    project: The GCP project that runs the pipeline components.
-    location: The GCP region that runs the pipeline components.
-    batch_predict_gcs_source_uris: Google Cloud Storage URI(-s) to your eval dataset instances data to run batch prediction on. The instances data should also contain the ground truth (target) data, used for evaluation. May contain wildcards. For more information on wildcards, see https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames. For more details about this input config, see https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
-    batch_predict_gcs_destination_output_uri: The Google Cloud Storage location of the directory where the eval pipeline output is to be written to.
+    project: Required. The GCP project that runs the pipeline components.
+    location: Required. The GCP region that runs the pipeline components.
+    batch_predict_gcs_source_uris: Required. Google Cloud Storage URI(-s) to your eval dataset instances data to run batch prediction on. The instances data should also contain the ground truth (target) data, used for evaluation. May contain wildcards. For more information on wildcards, see https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames. For more details about this input config, see https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
+    batch_predict_gcs_destination_output_uri: Required. The Google Cloud Storage location of the directory where the eval pipeline output is to be written to.
     model_name: The Model name used to run evaluation. Must be a publisher Model or a managed Model sharing the same ancestor location. Starting this job has no impact on any existing deployments of the Model and their resources.
     evaluation_task: The task that the large language model will be evaluated on. The evaluation component computes a set of metrics relevant to that specific task. Currently supported tasks are: `summarization`, `question-answering`, `text-generation`.
     input_field_name: The field name of the input eval dataset instances that contains the input prompts to the LLM.
     target_field_name: The field name of the eval dataset instance that contains an example reference text response. Alternatively referred to as the ground truth (or ground_truth_column) field. If not set, defaulted to `output_text`.
     batch_predict_instances_format: The format in which instances are given, must be one of the Model's supportedInputStorageFormats. Only "jsonl" is currently supported. For more details about this input config, see https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig.
     batch_predict_predictions_format: The format in which Vertex AI gives the predictions. Must be one of the Model's supportedOutputStorageFormats. Only "jsonl" is currently supported. For more details about this output config, see https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#OutputConfig.
     batch_predict_model_parameters: A map of parameters that govern the predictions. Some acceptable parameters include: maxOutputTokens, topK, topP, and temperature.
```

## google_cloud_pipeline_components/preview/model_evaluation/model_based_llm_evaluation/autosxs/autosxs_pipeline.py

```diff
@@ -79,78 +79,63 @@
       prediction_uris_inference_provided.output,
   )
 
   # We can't directly output dsl.OneOf, so we need to use identity.
   return function_based.identity(x=prediction_uris).output
 
 
-# pylint: disable=dangerous-default-value,g-bare-generic
+# pylint: disable=dangerous-default-value,g-bare-generic,unused-argument
 @dsl.pipeline(
     name='autosxs-template',
     description='Determines the SxS winrate between two models.',
 )
 def autosxs_pipeline(
     evaluation_dataset: str,
     task: str,
     id_columns: List[str],
     model_a: str = '',
     model_b: str = '',
-    autorater_prompt_parameters: Dict[str, Dict[str, str]] = {},  # pylint: disable=unused-argument
+    autorater_prompt_parameters: Dict[str, Dict[str, str]] = {},
     model_a_prompt_parameters: Dict[str, Dict[str, str]] = {},
     model_b_prompt_parameters: Dict[str, Dict[str, str]] = {},
     response_column_a: str = '',
     response_column_b: str = '',
     model_a_parameters: Dict[str, str] = {},
     model_b_parameters: Dict[str, str] = {},
     human_preference_column: str = '',
     project: str = _placeholders.PROJECT_ID_PLACEHOLDER,
     location: str = _placeholders.LOCATION_PLACEHOLDER,
     judgments_format: str = 'jsonl',
     bigquery_destination_prefix: str = '',
     experimental_args: Dict[str, Any] = {},
 ):
+  # fmt: off
   """Evaluates two models side-by-side using an arbiter model.
 
   Args:
-    evaluation_dataset: A list of GCS paths to a JSONL dataset containing
-      evaluation examples.
-    task: Evaluation task in the form {task}@{version}. task can be one of
-      "summarization", "question_answer". Version is an integer with 3 digits or
-      "latest". Ex: summarization@001 or question_answer@latest.
+    evaluation_dataset: A BigQuery table or comma-separated list of GCS paths to a JSONL dataset containing evaluation examples.
+    task: Evaluation task in the form `{task}@{version}`. task can be one of `[summarization, question_answering]`. Version is an integer with 3 digits or "latest". Ex: `summarization@001` or `question_answering@latest`.
     id_columns: The columns which distinguish unique evaluation examples.
-    model_a: A fully-qualified model resource name. This parameter is optional
-      if Model A responses are specified.
-    model_b: A fully-qualified model resource name. This parameter is optional
-      if Model B responses are specified.
-    autorater_prompt_parameters: Map of autorater prompt parameters to columns
-      or templates. The expected parameters are: inference_instruction - Details
-      on how to perform a task. inference_context - Content to reference to
-      perform the task.
-    model_a_prompt_parameters: Map of Model A prompt template parameters to
-      columns or templates.
-    model_b_prompt_parameters: Map of Model B prompt template parameters to
-      columns or templates.
-    response_column_a: The column containing responses for model A. Required if
-      any response tables are provided for model A.
-    response_column_b: The column containing responses for model B. Required if
-      any response tables are provided for model B.
-    model_a_parameters: The parameters that govern the predictions from model A.
-    model_b_parameters: The parameters that govern the predictions from model B.
-    human_preference_column: The column containing ground truths. Only required
-      when users want to check the autorater alignment against human preference.
-    project: Project used to run custom jobs. Default is the same project used
-      to run the pipeline.
-    location: Location used to run custom jobs. Default is the same location
-      used to run the pipeline.
-    judgments_format: The format to write judgments to. Can be either 'json' or
-      'bigquery'.
-    bigquery_destination_prefix: BigQuery table to write judgments to if the
-      specified format is 'bigquery'.
+    model_a: A fully-qualified model resource name (`projects/{project}/locations/{location}/models/{model}@{version}`) or publisher model resource name (`publishers/{publisher}/models/{model}`).  This parameter is optional if Model A responses are specified.
+    model_b: A fully-qualified model resource name (`projects/{project}/locations/{location}/models/{model}@{version}`) or publisher model resource name (`publishers/{publisher}/models/{model}`).  This parameter is optional if Model B responses are specified.
+    autorater_prompt_parameters: Map of autorater prompt parameters to columns or templates. The expected parameters are: `inference_instruction` (details on how to perform a task) and `inference_context` (content to reference to perform the task). As an example, `{'inference_context': {'column': 'my_prompt'}}` uses the evaluation dataset's `my_prompt` column for the AutoRater's context.
+    model_a_prompt_parameters: Map of Model A prompt template parameters to columns or templates. This parameter is optional if Model A predictions are predefined. Example - `{'prompt': {'column': 'my_prompt'}}` uses the evaluation dataset's `my_prompt` column for the prompt parameter named `prompt`.
+    model_b_prompt_parameters: Map of Model B prompt template parameters to columns or templates. This parameter is optional if Model B predictions are predefined. Example - `{'prompt': {'column': 'my_prompt'}}` uses the evaluation dataset's `my_prompt` column for the prompt parameter named `prompt`.
+    response_column_a: Either the name of a column in the evaluation dataset containing predefined predictions, or the name of the column in the Model A output containing predictions. If no value is provided, the correct model output column name will attempt to be inferred.
+    response_column_b: Either the name of a column in the evaluation dataset containing predefined predictions, or the name of the column in the Model B output containing predictions. If no value is provided, the correct model output column name will attempt to be inferred.
+    model_a_parameters: The parameters that govern the predictions from model A, such as temperature or maximum output tokens.
+    model_b_parameters: The parameters that govern the predictions from model B, such as temperature or maximum output tokens.
+    human_preference_column: The column containing ground truth winners for each example. Providing this parameter adds additional metrics for checking the AutoRater alignment with human preferences.
+    project: Project used to run custom jobs. Default is the same project used to run the pipeline.
+    location: Location used to run custom jobs. Default is the same location used to run the pipeline.
+    judgments_format: The format to write judgments to. Can be either `[json, bigquery]`.
+    bigquery_destination_prefix: BigQuery table to write judgments to if the specified format is 'bigquery'.
     experimental_args: Experimentally released arguments. Subject to change.
   """
+  # fmt: on
   prediction_inputs_a = task_preprocess.task_preprocess(
       evaluation_dataset=evaluation_dataset,
       task=task,
       model_prompt_parameters=model_a_prompt_parameters,
       response_column=response_column_a,
       human_preference_column=human_preference_column,
       id_columns=id_columns,
```

## google_cloud_pipeline_components/proto/gcp_resources_pb2.py

```diff
@@ -1,236 +1,46 @@
 # -*- coding: utf-8 -*-
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: gcp_resources.proto
 """Generated protocol buffer code."""
 from google.protobuf import descriptor as _descriptor
+from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import message as _message
 from google.protobuf import reflection as _reflection
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 from google.rpc import status_pb2 as google_dot_rpc_dot_status__pb2
 
 
-DESCRIPTOR = _descriptor.FileDescriptor(
-    name='gcp_resources.proto',
-    package='gcp_launcher',
-    syntax='proto3',
-    serialized_options=None,
-    create_key=_descriptor._internal_create_key,
-    serialized_pb=(
-        b'\n\x13gcp_resources.proto\x12\x0cgcp_launcher\x1a\x17google/rpc/status.proto"\xe0\x01\n\x0cGcpResources\x12\x36\n\tresources\x18\x01'
-        b' \x03(\x0b\x32#.gcp_launcher.GcpResources.Resource\x1a\x97\x01\n\x08Resource\x12\x1a\n\rresource_type\x18\x01'
-        b' \x01(\tH\x00\x88\x01\x01\x12\x19\n\x0cresource_uri\x18\x02'
-        b' \x01(\tH\x01\x88\x01\x01\x12!\n\x05\x65rror\x18\x03'
-        b' \x01(\x0b\x32\x12.google.rpc.Status\x12\x0e\n\x06labels\x18\x04'
-        b' \x03(\tB\x10\n\x0e_resource_typeB\x0f\n\r_resource_urib\x06proto3'
-    ),
-    dependencies=[
-        google_dot_rpc_dot_status__pb2.DESCRIPTOR,
-    ],
-)
-
-
-_GCPRESOURCES_RESOURCE = _descriptor.Descriptor(
-    name='Resource',
-    full_name='gcp_launcher.GcpResources.Resource',
-    filename=None,
-    file=DESCRIPTOR,
-    containing_type=None,
-    create_key=_descriptor._internal_create_key,
-    fields=[
-        _descriptor.FieldDescriptor(
-            name='resource_type',
-            full_name='gcp_launcher.GcpResources.Resource.resource_type',
-            index=0,
-            number=1,
-            type=9,
-            cpp_type=9,
-            label=1,
-            has_default_value=False,
-            default_value=b''.decode('utf-8'),
-            message_type=None,
-            enum_type=None,
-            containing_type=None,
-            is_extension=False,
-            extension_scope=None,
-            serialized_options=None,
-            file=DESCRIPTOR,
-            create_key=_descriptor._internal_create_key,
-        ),
-        _descriptor.FieldDescriptor(
-            name='resource_uri',
-            full_name='gcp_launcher.GcpResources.Resource.resource_uri',
-            index=1,
-            number=2,
-            type=9,
-            cpp_type=9,
-            label=1,
-            has_default_value=False,
-            default_value=b''.decode('utf-8'),
-            message_type=None,
-            enum_type=None,
-            containing_type=None,
-            is_extension=False,
-            extension_scope=None,
-            serialized_options=None,
-            file=DESCRIPTOR,
-            create_key=_descriptor._internal_create_key,
-        ),
-        _descriptor.FieldDescriptor(
-            name='error',
-            full_name='gcp_launcher.GcpResources.Resource.error',
-            index=2,
-            number=3,
-            type=11,
-            cpp_type=10,
-            label=1,
-            has_default_value=False,
-            default_value=None,
-            message_type=None,
-            enum_type=None,
-            containing_type=None,
-            is_extension=False,
-            extension_scope=None,
-            serialized_options=None,
-            file=DESCRIPTOR,
-            create_key=_descriptor._internal_create_key,
-        ),
-        _descriptor.FieldDescriptor(
-            name='labels',
-            full_name='gcp_launcher.GcpResources.Resource.labels',
-            index=3,
-            number=4,
-            type=9,
-            cpp_type=9,
-            label=3,
-            has_default_value=False,
-            default_value=[],
-            message_type=None,
-            enum_type=None,
-            containing_type=None,
-            is_extension=False,
-            extension_scope=None,
-            serialized_options=None,
-            file=DESCRIPTOR,
-            create_key=_descriptor._internal_create_key,
-        ),
-    ],
-    extensions=[],
-    nested_types=[],
-    enum_types=[],
-    serialized_options=None,
-    is_extendable=False,
-    syntax='proto3',
-    extension_ranges=[],
-    oneofs=[
-        _descriptor.OneofDescriptor(
-            name='_resource_type',
-            full_name='gcp_launcher.GcpResources.Resource._resource_type',
-            index=0,
-            containing_type=None,
-            create_key=_descriptor._internal_create_key,
-            fields=[],
-        ),
-        _descriptor.OneofDescriptor(
-            name='_resource_uri',
-            full_name='gcp_launcher.GcpResources.Resource._resource_uri',
-            index=1,
-            containing_type=None,
-            create_key=_descriptor._internal_create_key,
-            fields=[],
-        ),
-    ],
-    serialized_start=136,
-    serialized_end=287,
-)
-
-_GCPRESOURCES = _descriptor.Descriptor(
-    name='GcpResources',
-    full_name='gcp_launcher.GcpResources',
-    filename=None,
-    file=DESCRIPTOR,
-    containing_type=None,
-    create_key=_descriptor._internal_create_key,
-    fields=[
-        _descriptor.FieldDescriptor(
-            name='resources',
-            full_name='gcp_launcher.GcpResources.resources',
-            index=0,
-            number=1,
-            type=11,
-            cpp_type=10,
-            label=3,
-            has_default_value=False,
-            default_value=[],
-            message_type=None,
-            enum_type=None,
-            containing_type=None,
-            is_extension=False,
-            extension_scope=None,
-            serialized_options=None,
-            file=DESCRIPTOR,
-            create_key=_descriptor._internal_create_key,
-        ),
-    ],
-    extensions=[],
-    nested_types=[
-        _GCPRESOURCES_RESOURCE,
-    ],
-    enum_types=[],
-    serialized_options=None,
-    is_extendable=False,
-    syntax='proto3',
-    extension_ranges=[],
-    oneofs=[],
-    serialized_start=63,
-    serialized_end=287,
-)
-
-# pytype: disable=module-attr
-_GCPRESOURCES_RESOURCE.fields_by_name['error'].message_type = (
-    google_dot_rpc_dot_status__pb2._STATUS
-)
-_GCPRESOURCES_RESOURCE.containing_type = _GCPRESOURCES
-_GCPRESOURCES_RESOURCE.oneofs_by_name['_resource_type'].fields.append(
-    _GCPRESOURCES_RESOURCE.fields_by_name['resource_type']
-)
-_GCPRESOURCES_RESOURCE.fields_by_name['resource_type'].containing_oneof = (
-    _GCPRESOURCES_RESOURCE.oneofs_by_name['_resource_type']
-)
-_GCPRESOURCES_RESOURCE.oneofs_by_name['_resource_uri'].fields.append(
-    _GCPRESOURCES_RESOURCE.fields_by_name['resource_uri']
-)
-_GCPRESOURCES_RESOURCE.fields_by_name['resource_uri'].containing_oneof = (
-    _GCPRESOURCES_RESOURCE.oneofs_by_name['_resource_uri']
-)
-_GCPRESOURCES.fields_by_name['resources'].message_type = _GCPRESOURCES_RESOURCE
-DESCRIPTOR.message_types_by_name['GcpResources'] = _GCPRESOURCES
-_sym_db.RegisterFileDescriptor(DESCRIPTOR)
-
-GcpResources = _reflection.GeneratedProtocolMessageType(
-    'GcpResources',
-    (_message.Message,),
-    {
-        'Resource': _reflection.GeneratedProtocolMessageType(
-            'Resource',
-            (_message.Message,),
-            {
-                'DESCRIPTOR': _GCPRESOURCES_RESOURCE,
-                '__module__': 'gcp_resources_pb2',
-                # @@protoc_insertion_point(class_scope:gcp_launcher.GcpResources.Resource)
-            },
-        ),
-        'DESCRIPTOR': _GCPRESOURCES,
-        '__module__': 'gcp_resources_pb2',
-        # @@protoc_insertion_point(class_scope:gcp_launcher.GcpResources)
-    },
-)
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x13gcp_resources.proto\x12\x0cgcp_launcher\x1a\x17google/rpc/status.proto\"\xe0\x01\n\x0cGcpResources\x12\x36\n\tresources\x18\x01 \x03(\x0b\x32#.gcp_launcher.GcpResources.Resource\x1a\x97\x01\n\x08Resource\x12\x1a\n\rresource_type\x18\x01 \x01(\tH\x00\x88\x01\x01\x12\x19\n\x0cresource_uri\x18\x02 \x01(\tH\x01\x88\x01\x01\x12!\n\x05\x65rror\x18\x03 \x01(\x0b\x32\x12.google.rpc.Status\x12\x0e\n\x06labels\x18\x04 \x03(\tB\x10\n\x0e_resource_typeB\x0f\n\r_resource_urib\x06proto3')
+
+
+
+_GCPRESOURCES = DESCRIPTOR.message_types_by_name['GcpResources']
+_GCPRESOURCES_RESOURCE = _GCPRESOURCES.nested_types_by_name['Resource']
+GcpResources = _reflection.GeneratedProtocolMessageType('GcpResources', (_message.Message,), {
+
+  'Resource' : _reflection.GeneratedProtocolMessageType('Resource', (_message.Message,), {
+    'DESCRIPTOR' : _GCPRESOURCES_RESOURCE,
+    '__module__' : 'gcp_resources_pb2'
+    # @@protoc_insertion_point(class_scope:gcp_launcher.GcpResources.Resource)
+    })
+  ,
+  'DESCRIPTOR' : _GCPRESOURCES,
+  '__module__' : 'gcp_resources_pb2'
+  # @@protoc_insertion_point(class_scope:gcp_launcher.GcpResources)
+  })
 _sym_db.RegisterMessage(GcpResources)
 _sym_db.RegisterMessage(GcpResources.Resource)
 
+if _descriptor._USE_C_DESCRIPTORS == False:
 
+  DESCRIPTOR._options = None
+  _GCPRESOURCES._serialized_start=63
+  _GCPRESOURCES._serialized_end=287
+  _GCPRESOURCES_RESOURCE._serialized_start=136
+  _GCPRESOURCES_RESOURCE._serialized_end=287
 # @@protoc_insertion_point(module_scope)
```

## google_cloud_pipeline_components/v1/automl/forecasting/__init__.py

```diff
@@ -11,11 +11,19 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """GA AutoML forecasting components."""
 
 from google_cloud_pipeline_components.v1.automl.forecasting.prophet_trainer import prophet_trainer as ProphetTrainerOp
+from google_cloud_pipeline_components.v1.automl.forecasting.utils import get_bqml_arima_predict_pipeline_and_parameters
+from google_cloud_pipeline_components.v1.automl.forecasting.utils import get_bqml_arima_train_pipeline_and_parameters
+from google_cloud_pipeline_components.v1.automl.forecasting.utils import get_prophet_prediction_pipeline_and_parameters
+from google_cloud_pipeline_components.v1.automl.forecasting.utils import get_prophet_train_pipeline_and_parameters
 
 __all__ = [
     'ProphetTrainerOp',
+    'get_bqml_arima_predict_pipeline_and_parameters',
+    'get_bqml_arima_train_pipeline_and_parameters',
+    'get_prophet_prediction_pipeline_and_parameters',
+    'get_prophet_train_pipeline_and_parameters',
 ]
```

## google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_predict_pipeline.yaml

```diff
@@ -654,15 +654,15 @@
           \ dataset.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  import collections\n\n  from google.cloud import bigquery\n  # pylint:\
           \ enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project, location=location)\n  ref\
           \ = client.create_dataset(dataset=dataset, exists_ok=exists_ok)\n  return\
           \ collections.namedtuple('Outputs', ['project_id', 'dataset_id'])(\n   \
           \   ref.project, ref.dataset_id)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-bigquery-create-dataset-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - bigquery_create_dataset
@@ -689,15 +689,15 @@
           \ dataset.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  import collections\n\n  from google.cloud import bigquery\n  # pylint:\
           \ enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project, location=location)\n  ref\
           \ = client.create_dataset(dataset=dataset, exists_ok=exists_ok)\n  return\
           \ collections.namedtuple('Outputs', ['project_id', 'dataset_id'])(\n   \
           \   ref.project, ref.dataset_id)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-bigquery-delete-dataset-with-prefix:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - bigquery_delete_dataset_with_prefix
@@ -723,15 +723,15 @@
           \  \"\"\"Deletes all BigQuery datasets matching the given prefix.\"\"\"\n\
           \  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project)\n  for dataset in client.list_datasets(project=project):\n\
           \    if dataset.dataset_id.startswith(dataset_prefix):\n      client.delete_dataset(\n\
           \          dataset=dataset.dataset_id,\n          delete_contents=delete_contents)\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-bigquery-query-job:
       container:
         args:
         - --type
         - BigqueryQueryJob
         - --project
         - '{{$.inputs.parameters[''project'']}}'
@@ -784,15 +784,15 @@
           \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
           \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
           \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
           \    config['destinationTable'] = {\n        'projectId': project_id,\n\
           \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
           \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
           \  return config\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-first-valid:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_first_valid
@@ -814,15 +814,15 @@
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef get_first_valid(values: str) -> str:\n  \"\"\"Returns the first\
           \ truthy value from the given serialized JSON list.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  import json\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  for value in json.loads(values):\n    if value:\n      return value\n\
           \  raise ValueError('No valid values.')\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-model-metadata:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_model_metadata
@@ -853,15 +853,15 @@
           \n  client = bigquery.Client(project=project, location=location)\n  options\
           \ = client.get_model(model).training_runs[0].training_options\n  return\
           \ collections.namedtuple(\n      'Outputs', [\n          'time_column',\n\
           \          'time_series_identifier_column',\n          'target_column',\n\
           \          'forecast_horizon',\n      ],\n  )(\n      options.time_series_timestamp_column,\n\
           \      options.time_series_id_column,\n      options.time_series_data_column,\n\
           \      options.horizon,\n  )\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-table-location:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_table_location
@@ -889,15 +889,15 @@
           \ Location to return if no table was given.\n\n  Returns:\n    A GCP region\
           \ or multi-region.\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  if not table:\n    return default_location\n\n  client = bigquery.Client(project=project)\n\
           \  if table.startswith('bq://'):\n    table = table[len('bq://'):]\n  elif\
           \ table.startswith('bigquery://'):\n    table = table[len('bigquery://'):]\n\
           \  return client.get_table(table).location\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-load-table-from-uri:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - load_table_from_uri
@@ -930,15 +930,15 @@
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  if not source_uris:\n    return ''\n\n  csv_list = [filename.strip()\
           \ for filename in source_uris.split(',')]\n  client = bigquery.Client(project=project,\
           \ location=location)\n  job_config = bigquery.LoadJobConfig(\n      autodetect=True,\
           \ source_format=source_format)\n  client.load_table_from_uri(\n      source_uris=csv_list,\n\
           \      destination=destination,\n      project=project,\n      location=location,\n\
           \      job_config=job_config).result()\n  return destination\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-maybe-replace-with-default:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - maybe_replace_with_default
@@ -958,15 +958,15 @@
           python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef maybe_replace_with_default(value: str, default: str = '') ->\
           \ str:\n  \"\"\"Replaces string with another value if it is a dash.\"\"\"\
           \n  return default if not value else value\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-validate-inputs:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - validate_inputs
@@ -1060,15 +1060,15 @@
           \    f'{valid_optimization_objectives}, got: {optimization_objective}.')\n\
           \n  # Validate data granularity unit.\n  valid_data_granularity_units =\
           \ [\n      'minute', 'hour', 'day', 'week', 'month', 'year']\n  if data_granularity_unit:\n\
           \    if data_granularity_unit not in valid_data_granularity_units:\n   \
           \   raise ValueError(\n          'Granularity unit should be one of the\
           \ following: '\n          f'{valid_data_granularity_units}, got: {data_granularity_unit}.')\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
 pipelineInfo:
   description: Forecasts using a BQML ARIMA_PLUS model.
   name: automl-tabular-bqml-arima-prediction
 root:
   dag:
     tasks:
       bigquery-delete-dataset-with-prefix:
```

## google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_train_pipeline.yaml

```diff
@@ -2021,194 +2021,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -2216,42 +2180,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -2293,38 +2249,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -2349,116 +2298,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -2474,272 +2396,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -2765,19 +2635,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -2790,44 +2658,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-for-loop-3:
     dag:
       tasks:
         build-job-configuration-query-2:
           cachingOptions:
             enableCache: true
@@ -3535,15 +3395,15 @@
           \ dataset.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  import collections\n\n  from google.cloud import bigquery\n  # pylint:\
           \ enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project, location=location)\n  ref\
           \ = client.create_dataset(dataset=dataset, exists_ok=exists_ok)\n  return\
           \ collections.namedtuple('Outputs', ['project_id', 'dataset_id'])(\n   \
           \   ref.project, ref.dataset_id)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-bigquery-create-dataset-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - bigquery_create_dataset
@@ -3570,15 +3430,15 @@
           \ dataset.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  import collections\n\n  from google.cloud import bigquery\n  # pylint:\
           \ enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project, location=location)\n  ref\
           \ = client.create_dataset(dataset=dataset, exists_ok=exists_ok)\n  return\
           \ collections.namedtuple('Outputs', ['project_id', 'dataset_id'])(\n   \
           \   ref.project, ref.dataset_id)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-bigquery-create-model-job:
       container:
         args:
         - --type
         - BigqueryCreateModelJob
         - --project
         - '{{$.inputs.parameters[''project'']}}'
@@ -3630,15 +3490,15 @@
           \  \"\"\"Deletes all BigQuery datasets matching the given prefix.\"\"\"\n\
           \  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project)\n  for dataset in client.list_datasets(project=project):\n\
           \    if dataset.dataset_id.startswith(dataset_prefix):\n      client.delete_dataset(\n\
           \          dataset=dataset.dataset_id,\n          delete_contents=delete_contents)\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-bigquery-list-rows:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - bigquery_list_rows
@@ -3668,15 +3528,15 @@
           \  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project, location=location)\n  metadata\
           \ = table.metadata\n  rows = client.list_rows('.'.join(\n      [metadata['projectId'],\
           \ metadata['datasetId'], metadata['tableId']]))\n  result = []\n  for row\
           \ in rows:\n    result.append({col: str(value) for col, value in dict(row).items()})\n\
           \  return result\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-bigquery-list-rows-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - bigquery_list_rows
@@ -3706,15 +3566,15 @@
           \  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project, location=location)\n  metadata\
           \ = table.metadata\n  rows = client.list_rows('.'.join(\n      [metadata['projectId'],\
           \ metadata['datasetId'], metadata['tableId']]))\n  result = []\n  for row\
           \ in rows:\n    result.append({col: str(value) for col, value in dict(row).items()})\n\
           \  return result\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-bigquery-query-job:
       container:
         args:
         - --type
         - BigqueryQueryJob
         - --project
         - '{{$.inputs.parameters[''project'']}}'
@@ -3875,15 +3735,15 @@
           \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
           \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
           \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
           \    config['destinationTable'] = {\n        'projectId': project_id,\n\
           \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
           \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
           \  return config\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-build-job-configuration-query-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - build_job_configuration_query
@@ -3909,15 +3769,15 @@
           \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
           \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
           \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
           \    config['destinationTable'] = {\n        'projectId': project_id,\n\
           \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
           \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
           \  return config\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-build-job-configuration-query-3:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - build_job_configuration_query
@@ -3943,15 +3803,15 @@
           \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
           \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
           \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
           \    config['destinationTable'] = {\n        'projectId': project_id,\n\
           \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
           \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
           \  return config\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-build-job-configuration-query-4:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - build_job_configuration_query
@@ -3977,15 +3837,15 @@
           \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
           \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
           \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
           \    config['destinationTable'] = {\n        'projectId': project_id,\n\
           \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
           \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
           \  return config\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-build-job-configuration-query-5:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - build_job_configuration_query
@@ -4011,15 +3871,15 @@
           \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
           \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
           \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
           \    config['destinationTable'] = {\n        'projectId': project_id,\n\
           \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
           \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
           \  return config\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-build-job-configuration-query-6:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - build_job_configuration_query
@@ -4045,15 +3905,15 @@
           \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
           \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
           \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
           \    config['destinationTable'] = {\n        'projectId': project_id,\n\
           \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
           \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
           \  return config\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-build-serialized-query-parameters:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - build_serialized_query_parameters
@@ -4122,15 +3982,15 @@
           \  'name': 'prediction_count',\n        'parameterType': {\n           \
           \ 'type': 'INTEGER'\n        },\n        'parameterValue': {\n         \
           \   'value': window['count']\n        },\n    })\n\n  start_time = window['start_time']\
           \ if window else str(datetime.datetime.max)\n  query_parameters.append({\n\
           \      'name': 'start_time',\n      'parameterType': {\n          'type':\
           \ 'TIMESTAMP'\n      },\n      'parameterValue': {\n          'value': start_time\n\
           \      },\n  })\n  return query_parameters\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-build-serialized-query-parameters-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - build_serialized_query_parameters
@@ -4199,15 +4059,15 @@
           \  'name': 'prediction_count',\n        'parameterType': {\n           \
           \ 'type': 'INTEGER'\n        },\n        'parameterValue': {\n         \
           \   'value': window['count']\n        },\n    })\n\n  start_time = window['start_time']\
           \ if window else str(datetime.datetime.max)\n  query_parameters.append({\n\
           \      'name': 'start_time',\n      'parameterType': {\n          'type':\
           \ 'TIMESTAMP'\n      },\n      'parameterValue': {\n          'value': start_time\n\
           \      },\n  })\n  return query_parameters\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-build-serialized-query-parameters-3:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - build_serialized_query_parameters
@@ -4276,15 +4136,15 @@
           \  'name': 'prediction_count',\n        'parameterType': {\n           \
           \ 'type': 'INTEGER'\n        },\n        'parameterValue': {\n         \
           \   'value': window['count']\n        },\n    })\n\n  start_time = window['start_time']\
           \ if window else str(datetime.datetime.max)\n  query_parameters.append({\n\
           \      'name': 'start_time',\n      'parameterType': {\n          'type':\
           \ 'TIMESTAMP'\n      },\n      'parameterValue': {\n          'value': start_time\n\
           \      },\n  })\n  return query_parameters\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-cond:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - cond
@@ -4304,15 +4164,15 @@
           python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef cond(predicate: bool, true_str: str, false_str: str) -> str:\n\
           \  \"\"\"Returns true_str if predicate is true, else false_str.\"\"\"\n\
           \  return true_str if predicate else false_str\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-create-metrics-artifact:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - create_metrics_artifact
@@ -4336,15 +4196,15 @@
           \ *\n\ndef create_metrics_artifact(\n    metrics_rows: List[Dict[str, str]],\n\
           \    evaluation_metrics: dsl.Output[dsl.Metrics],\n) -> None:\n  \"\"\"\
           Converts the rows of a metrics table into an Artifact.\"\"\"\n  metric_name_map\
           \ = {\n      'MAE': 'meanAbsoluteError',\n      'RMSE': 'rootMeanSquaredError',\n\
           \      'MAPE': 'meanAbsolutePercentageError',\n  }\n  metrics = {metric_name_map[k]:\
           \ v for k, v in dict(metrics_rows[0]).items()}\n  evaluation_metrics.metadata\
           \ = metrics\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-feature-transform-engine:
       container:
         args:
         - feature_transform_engine
         - '{"Concat": ["--project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--location=", "{{$.inputs.parameters[''location'']}}"]}'
         - '{"Concat": ["--dataset_level_custom_transformation_definitions=", "{{$.inputs.parameters[''dataset_level_custom_transformation_definitions'']}}"]}'
@@ -4421,16 +4281,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -4439,15 +4299,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
     exec-get-fte-suffix:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_fte_suffix
@@ -4473,15 +4333,15 @@
           \ the intermediate FTE table name.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project, location=location)\n  for\
           \ table in client.list_tables(bigquery_staging_full_dataset_id):\n    if\
           \ table.table_id.startswith(fte_table):\n      return table.table_id[len(fte_table)\
           \ + 1:]\n  raise ValueError(\n      f'No FTE output tables found in {bigquery_staging_full_dataset_id}.')\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-table-location:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_table_location
@@ -4509,15 +4369,15 @@
           \ Location to return if no table was given.\n\n  Returns:\n    A GCP region\
           \ or multi-region.\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  if not table:\n    return default_location\n\n  client = bigquery.Client(project=project)\n\
           \  if table.startswith('bq://'):\n    table = table[len('bq://'):]\n  elif\
           \ table.startswith('bigquery://'):\n    table = table[len('bigquery://'):]\n\
           \  return client.get_table(table).location\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-value:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_value
@@ -4536,15 +4396,15 @@
 
           python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef get_value(d: Dict[str, str], key: str) -> str:\n  return d[key]\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-window-query-priority:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_window_query_priority
@@ -4566,15 +4426,15 @@
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef get_window_query_priority(\n    window: Dict[str, str],\n   \
           \ max_interactive: int = 100,\n) -> str:\n  \"\"\"Returns a query priority\
           \ depending on the window number.\"\"\"\n  if int(window['window_number'])\
           \ <= max_interactive:\n    return 'INTERACTIVE'\n  else:\n    return 'BATCH'\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-maybe-replace-with-default:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - maybe_replace_with_default
@@ -4594,15 +4454,15 @@
           python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef maybe_replace_with_default(value: str, default: str = '') ->\
           \ str:\n  \"\"\"Replaces string with another value if it is a dash.\"\"\"\
           \n  return default if not value else value\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-query-with-retry:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - query_with_retry
@@ -4648,15 +4508,15 @@
           \    except (exceptions.BadRequest, exceptions.Forbidden) as e:\n      if\
           \ retry_count >= max_retry_count:\n        logging.info('Maximum retries\
           \ reached.')\n        raise\n      wait_time = (\n          retry_wait_seconds\
           \ * (2 ** retry_count) * random.uniform(1, 1.5))\n      logging.info(\n\
           \          'Query failed with %s. Retrying after %d seconds.', e, wait_time)\n\
           \      time.sleep(wait_time)\n      retry_count += 1\n  return destination_uri\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-query-with-retry-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - query_with_retry
@@ -4702,15 +4562,15 @@
           \    except (exceptions.BadRequest, exceptions.Forbidden) as e:\n      if\
           \ retry_count >= max_retry_count:\n        logging.info('Maximum retries\
           \ reached.')\n        raise\n      wait_time = (\n          retry_wait_seconds\
           \ * (2 ** retry_count) * random.uniform(1, 1.5))\n      logging.info(\n\
           \          'Query failed with %s. Retrying after %d seconds.', e, wait_time)\n\
           \      time.sleep(wait_time)\n      retry_count += 1\n  return destination_uri\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-query-with-retry-3:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - query_with_retry
@@ -4756,15 +4616,15 @@
           \    except (exceptions.BadRequest, exceptions.Forbidden) as e:\n      if\
           \ retry_count >= max_retry_count:\n        logging.info('Maximum retries\
           \ reached.')\n        raise\n      wait_time = (\n          retry_wait_seconds\
           \ * (2 ** retry_count) * random.uniform(1, 1.5))\n      logging.info(\n\
           \          'Query failed with %s. Retrying after %d seconds.', e, wait_time)\n\
           \      time.sleep(wait_time)\n      retry_count += 1\n  return destination_uri\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-table-to-uri:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - table_to_uri
@@ -4792,15 +4652,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-table-to-uri-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - table_to_uri
@@ -4828,15 +4688,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-validate-inputs:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - validate_inputs
@@ -4930,15 +4790,15 @@
           \    f'{valid_optimization_objectives}, got: {optimization_objective}.')\n\
           \n  # Validate data granularity unit.\n  valid_data_granularity_units =\
           \ [\n      'minute', 'hour', 'day', 'week', 'month', 'year']\n  if data_granularity_unit:\n\
           \    if data_granularity_unit not in valid_data_granularity_units:\n   \
           \   raise ValueError(\n          'Granularity unit should be one of the\
           \ following: '\n          f'{valid_data_granularity_units}, got: {data_granularity_unit}.')\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-wrapped-in-list:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - wrapped_in_list
@@ -4957,15 +4817,15 @@
 
           python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef wrapped_in_list(value: str) -> List[str]:\n  \"\"\"Wraps a string\
           \ in a list.\"\"\"\n  return [value]\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
 pipelineInfo:
   description: Trains a BQML ARIMA_PLUS model.
   name: automl-tabular-bqml-arima-train
 root:
   dag:
     outputs:
       artifacts:
```

## google_cloud_pipeline_components/v1/automl/forecasting/prophet_predict_pipeline.yaml

```diff
@@ -1457,15 +1457,15 @@
           \ dataset.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  import collections\n\n  from google.cloud import bigquery\n  # pylint:\
           \ enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project, location=location)\n  ref\
           \ = client.create_dataset(dataset=dataset, exists_ok=exists_ok)\n  return\
           \ collections.namedtuple('Outputs', ['project_id', 'dataset_id'])(\n   \
           \   ref.project, ref.dataset_id)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-bigquery-delete-dataset-with-prefix:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - bigquery_delete_dataset_with_prefix
@@ -1491,15 +1491,15 @@
           \  \"\"\"Deletes all BigQuery datasets matching the given prefix.\"\"\"\n\
           \  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project)\n  for dataset in client.list_datasets(project=project):\n\
           \    if dataset.dataset_id.startswith(dataset_prefix):\n      client.delete_dataset(\n\
           \          dataset=dataset.dataset_id,\n          delete_contents=delete_contents)\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-bigquery-query-job:
       container:
         args:
         - --type
         - BigqueryQueryJob
         - --project
         - '{{$.inputs.parameters[''project'']}}'
@@ -1579,15 +1579,15 @@
           \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
           \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
           \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
           \    config['destinationTable'] = {\n        'projectId': project_id,\n\
           \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
           \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
           \  return config\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-build-job-configuration-query-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - build_job_configuration_query
@@ -1613,15 +1613,15 @@
           \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
           \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
           \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
           \    config['destinationTable'] = {\n        'projectId': project_id,\n\
           \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
           \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
           \  return config\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-first-valid:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_first_valid
@@ -1643,15 +1643,15 @@
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef get_first_valid(values: str) -> str:\n  \"\"\"Returns the first\
           \ truthy value from the given serialized JSON list.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  import json\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  for value in json.loads(values):\n    if value:\n      return value\n\
           \  raise ValueError('No valid values.')\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-table-location:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_table_location
@@ -1679,15 +1679,15 @@
           \ Location to return if no table was given.\n\n  Returns:\n    A GCP region\
           \ or multi-region.\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  if not table:\n    return default_location\n\n  client = bigquery.Client(project=project)\n\
           \  if table.startswith('bq://'):\n    table = table[len('bq://'):]\n  elif\
           \ table.startswith('bigquery://'):\n    table = table[len('bigquery://'):]\n\
           \  return client.get_table(table).location\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-table-location-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_table_location
@@ -1715,15 +1715,15 @@
           \ Location to return if no table was given.\n\n  Returns:\n    A GCP region\
           \ or multi-region.\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  if not table:\n    return default_location\n\n  client = bigquery.Client(project=project)\n\
           \  if table.startswith('bq://'):\n    table = table[len('bq://'):]\n  elif\
           \ table.startswith('bigquery://'):\n    table = table[len('bigquery://'):]\n\
           \  return client.get_table(table).location\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-load-table-from-uri:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - load_table_from_uri
@@ -1756,15 +1756,15 @@
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  if not source_uris:\n    return ''\n\n  csv_list = [filename.strip()\
           \ for filename in source_uris.split(',')]\n  client = bigquery.Client(project=project,\
           \ location=location)\n  job_config = bigquery.LoadJobConfig(\n      autodetect=True,\
           \ source_format=source_format)\n  client.load_table_from_uri(\n      source_uris=csv_list,\n\
           \      destination=destination,\n      project=project,\n      location=location,\n\
           \      job_config=job_config).result()\n  return destination\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-make-vertex-model-artifact:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - make_vertex_model_artifact
@@ -1786,15 +1786,15 @@
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef make_vertex_model_artifact(\n    location: str,\n    model_resource_name:\
           \ str,\n    vertex_model: dsl.Output[dsl.Artifact],\n) -> None:\n  \"\"\"\
           Creates a google.VertexModel artifact.\"\"\"\n  vertex_model.metadata =\
           \ {'resourceName': model_resource_name}\n  vertex_model.uri = (f'https://{location}-aiplatform.googleapis.com'\n\
           \                      f'/v1/{model_resource_name}')\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-maybe-replace-with-default:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - maybe_replace_with_default
@@ -1814,15 +1814,15 @@
           python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef maybe_replace_with_default(value: str, default: str = '') ->\
           \ str:\n  \"\"\"Replaces string with another value if it is a dash.\"\"\"\
           \n  return default if not value else value\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-model-batch-predict:
       container:
         args:
         - --type
         - BatchPredictionJob
         - --payload
         - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''job_display_name'']}}",
@@ -1899,15 +1899,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-table-to-uri-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - table_to_uri
@@ -1935,15 +1935,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-validate-inputs:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - validate_inputs
@@ -2037,15 +2037,15 @@
           \    f'{valid_optimization_objectives}, got: {optimization_objective}.')\n\
           \n  # Validate data granularity unit.\n  valid_data_granularity_units =\
           \ [\n      'minute', 'hour', 'day', 'week', 'month', 'year']\n  if data_granularity_unit:\n\
           \    if data_granularity_unit not in valid_data_granularity_units:\n   \
           \   raise ValueError(\n          'Granularity unit should be one of the\
           \ following: '\n          f'{valid_data_granularity_units}, got: {data_granularity_unit}.')\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
 pipelineInfo:
   description: Creates a batch prediction using a Prophet model.
   name: prophet-predict
 root:
   dag:
     tasks:
       bigquery-delete-dataset-with-prefix:
```

## google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer.py

```diff
@@ -104,25 +104,25 @@
                   '"encryption_spec": {"kms_key_name":"',
                   encryption_spec_key_name,
                   '"}, ',
                   '"job_spec": {"worker_pool_specs": [{"replica_count":"1", ',
                   '"machine_spec": {"machine_type": "n1-standard-4"}, ',
                   (
                       '"container_spec":'
-                      ' {"image_uri":"us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", '
+                      ' {"image_uri":"us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", '
                   ),
                   '"args": ["prophet_trainer", "',
                   (
                       f'--job_name=dataflow-{dsl.PIPELINE_JOB_NAME_PLACEHOLDER}", "'
                   ),
                   (
-                      '--dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125", "'
+                      '--dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125", "'
                   ),
                   (
-                      '--prediction_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/fte-prediction-server:20231029_0125", "'
+                      '--prediction_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/fte-prediction-server:20240119_0125", "'
                   ),
                   '--artifacts_dir=',
                   root_dir,
                   f'/{dsl.PIPELINE_JOB_ID_PLACEHOLDER}/model/", "',
                   '--evaluated_examples_dir=',
                   root_dir,
                   f'/{dsl.PIPELINE_JOB_ID_PLACEHOLDER}/eval/", "',
```

## google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer_pipeline.yaml

```diff
@@ -779,194 +779,158 @@
           description: 'If True, infers the column types
 
             when importing CSVs into BigQuery.'
           isOptional: true
           parameterType: BOOLEAN
         bigquery_staging_full_dataset_id:
           defaultValue: ''
-          description: 'Dataset in
-
-            "projectId.datasetId" format for storing intermediate-FTE BigQuery
-
-            tables.  If the specified dataset does not exist in BigQuery, FTE will
-
-            create the dataset. If no bigquery_staging_full_dataset_id is specified,
-
-            all intermediate tables will be stored in a dataset created under the
-
-            provided project in the input data source''s location during FTE
-
-            execution called
-
-            "vertex_feature_transform_engine_staging_{location.replace(''-'', ''_'')}".
-
-            All tables generated by FTE will have a 30 day TTL.'
+          description: Dataset in "projectId.datasetId" format for storing intermediate-FTE
+            BigQuery tables.  If the specified dataset does not exist in BigQuery,
+            FTE will create the dataset. If no bigquery_staging_full_dataset_id is
+            specified, all intermediate tables will be stored in a dataset created
+            under the provided project in the input data source's location during
+            FTE execution called "vertex_feature_transform_engine_staging_{location.replace('-',
+            '_')}". All tables generated by FTE will have a 30 day TTL.
           isOptional: true
           parameterType: STRING
         data_source_bigquery_table_path:
           defaultValue: ''
-          description: 'BigQuery input data
-
-            source to run feature transform on.'
+          description: BigQuery input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
-          description: 'CSV input data source to run
-
-            feature transform on.'
+          description: CSV input data source to run feature transform on.
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            Dataflow jobs.'
+          description: Custom service account to run Dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         dataset_level_custom_transformation_definitions:
           defaultValue: []
-          description: "List of dataset-level custom transformation definitions. \
-            \ Custom,\nbring-your-own dataset-level transform functions, where users\
-            \ can define\nand import their own transform function and use it with\
-            \ FTE's built-in\ntransformations. Using custom transformations is an\
-            \ experimental feature\nand it is currently not supported during batch\
-            \ prediction.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"ConcatCols\",\n    \"module_path\": \"/path/to/custom_transform_fn_dlt.py\"\
-            ,\n    \"function_name\": \"concat_cols\" } ]  Using custom transform\
-            \ function\n    together with FTE's built-in transformations:  .. code-block::\n\
-            \    python  [ { \"transformation\": \"Join\", \"right_table_uri\":\n\
-            \    \"bq://test-project.dataset_test.table\", \"join_keys\":\n    [[\"\
-            join_key_col\", \"join_key_col\"]] },{ \"transformation\":\n    \"ConcatCols\"\
-            , \"cols\": [\"feature_1\", \"feature_2\"], \"output_col\":\n    \"feature_1_2\"\
-            \ } ]"
+          description: 'List of dataset-level custom transformation definitions.  Custom,
+            bring-your-own dataset-level transform functions, where users can define
+            and import their own transform function and use it with FTE''s built-in
+            transformations. Using custom transformations is an experimental feature
+            and it is currently not supported during batch prediction.
+
+            [ { "transformation": "ConcatCols", "module_path": "/path/to/custom_transform_fn_dlt.py",
+            "function_name": "concat_cols" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "Join", "right_table_uri": "bq://test-project.dataset_test.table", "join_keys":
+            [["join_key_col", "join_key_col"]] },{ "transformation": "ConcatCols",
+            "cols": ["feature_1", "feature_2"], "output_col": "feature_1_2" } ]'
           isOptional: true
           parameterType: LIST
         dataset_level_transformations:
           defaultValue: []
-          description: "List of dataset-level\ntransformations.\nExample:  .. code-block::\
-            \ python  [ { \"transformation\": \"Join\",\n  \"right_table_uri\": \"\
-            bq://test-project.dataset_test.table\",\n  \"join_keys\": [[\"join_key_col\"\
-            , \"join_key_col\"]] }, ... ]  Additional\n  information about FTE's currently\
-            \ supported built-in\n  transformations:\n    Join: Joins features from\
-            \ right_table_uri. For each join key, the\n      left table keys will\
-            \ be included and the right table keys will\n      be dropped.\n     \
-            \   Example:  .. code-block:: python  { \"transformation\": \"Join\",\n\
-            \          \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
-            ,\n          \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }\n\
-            \        Arguments:\n            right_table_uri: Right table BigQuery\
-            \ uri to join\n              with input_full_table_id.\n            join_keys:\
-            \ Features to join on. For each\n              nested list, the first\
-            \ element is a left table column\n              and the second is its\
-            \ corresponding right table column.\n    TimeAggregate: Creates a new\
-            \ feature composed of values of an\n      existing feature from a fixed\
-            \ time period ago or in the future.\n      Ex: A feature for sales by\
-            \ store 1 year ago.\n        Example:  .. code-block:: python  { \"transformation\"\
-            :\n          \"TimeAggregate\", \"time_difference\": 40,\n          \"\
-            time_difference_units\": \"DAY\",\n          \"time_series_identifier_columns\"\
-            : [\"store_id\"],\n          \"time_column\": \"time_col\", \"time_difference_target_column\"\
-            :\n          \"target_col\", \"output_column\": \"output_col\" }\n   \
-            \     Arguments:\n            time_difference: Number of time_difference_units\
-            \ to\n              look back or into the future on our\n            \
-            \  time_difference_target_column.\n            time_difference_units:\
-            \ Units of time_difference to\n              look back or into the future\
-            \ on our\n              time_difference_target_column. Must be one of\
-            \ * 'DAY' *\n              'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER'\
-            \ *\n              'YEAR'\n            time_series_identifier_columns:\
-            \ Names of the\n              time series identifier columns.\n      \
-            \      time_column: Name of the time column.\n            time_difference_target_column:\
-            \ Column we wish to get\n              the value of time_difference time_difference_units\
-            \ in\n              the past or future.\n            output_column: Name\
-            \ of our new time aggregate\n              feature.\n            is_future:\
-            \ Whether we wish to look\n              forward in time. Defaults to\
-            \ False.\n              PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\n\
-            \              Performs a partition by reduce operation (one of max,\n\
-            \              min, avg, or sum) with a fixed historic time period. Ex:\n\
-            \              Getting avg sales (the reduce column) for each store\n\
-            \              (partition_by_column) over the previous 5 days\n      \
-            \        (time_column, time_ago_units, and time_ago).\n        Example:\
-            \  .. code-block:: python  { \"transformation\":\n          \"PartitionByMax\"\
-            , \"reduce_column\": \"sell_price\",\n          \"partition_by_columns\"\
-            : [\"store_id\", \"state_id\"],\n          \"time_column\": \"date\",\
-            \ \"time_ago\": 1, \"time_ago_units\":\n          \"WEEK\", \"output_column\"\
-            : \"partition_by_reduce_max_output\" }\n        Arguments:\n         \
-            \   reduce_column: Column to apply the reduce operation\n            \
-            \  on. Reduce operations include the\n                following: Max,\
-            \ Min, Avg, Sum.\n            partition_by_columns: List of columns to\n\
-            \              partition by.\n            time_column: Time column for\
-            \ the partition by\n              operation's window function.\n     \
-            \       time_ago: Number of time_ago_units to look back on\n         \
-            \     our target_column, starting from time_column\n              (inclusive).\n\
-            \            time_ago_units: Units of time_ago to look back on\n     \
-            \         our target_column. Must be one of * 'DAY' * 'WEEK'\n       \
-            \     output_column: Name of our output feature."
+          description: "List of dataset-level transformations.\n[ { \"transformation\"\
+            : \"Join\", \"right_table_uri\": \"bq://test-project.dataset_test.table\"\
+            , \"join_keys\": [[\"join_key_col\", \"join_key_col\"]] }, ... ]  Additional\
+            \ information about FTE's currently supported built-in\n    transformations:\n\
+            \    Join: Joins features from right_table_uri. For each join key, the\
+            \ left table keys will be included and the right table keys will be dropped.\n\
+            \        Example:  .. code-block:: python  { \"transformation\": \"Join\"\
+            , \"right_table_uri\": \"bq://test-project.dataset_test.table\", \"join_keys\"\
+            : [[\"join_key_col\", \"join_key_col\"]] }\n        Arguments:\n     \
+            \       right_table_uri: Right table BigQuery uri to join with input_full_table_id.\n\
+            \            join_keys: Features to join on. For each nested list, the\
+            \ first element is a left table column and the second is its corresponding\
+            \ right table column.\n    TimeAggregate: Creates a new feature composed\
+            \ of values of an existing feature from a fixed time period ago or in\
+            \ the future.\n      Ex: A feature for sales by store 1 year ago.\n  \
+            \      Example:  .. code-block:: python  { \"transformation\": \"TimeAggregate\"\
+            , \"time_difference\": 40, \"time_difference_units\": \"DAY\", \"time_series_identifier_columns\"\
+            : [\"store_id\"], \"time_column\": \"time_col\", \"time_difference_target_column\"\
+            : \"target_col\", \"output_column\": \"output_col\" }\n        Arguments:\n\
+            \            time_difference: Number of time_difference_units to look\
+            \ back or into the future on our time_difference_target_column.\n    \
+            \        time_difference_units: Units of time_difference to look back\
+            \ or into the future on our time_difference_target_column. Must be one\
+            \ of * 'DAY' * 'WEEK' (Equivalent to 7 DAYs) * 'MONTH' * 'QUARTER' * 'YEAR'\n\
+            \            time_series_identifier_columns: Names of the time series\
+            \ identifier columns.\n            time_column: Name of the time column.\n\
+            \            time_difference_target_column: Column we wish to get the\
+            \ value of time_difference time_difference_units in the past or future.\n\
+            \            output_column: Name of our new time aggregate feature.\n\
+            \            is_future: Whether we wish to look forward in time. Defaults\
+            \ to False. PartitionByMax/PartitionByMin/PartitionByAvg/PartitionBySum:\
+            \ Performs a partition by reduce operation (one of max, min, avg, or sum)\
+            \ with a fixed historic time period. Ex: Getting avg sales (the reduce\
+            \ column) for each store (partition_by_column) over the previous 5 days\
+            \ (time_column, time_ago_units, and time_ago).\n        Example:  .. code-block::\
+            \ python  { \"transformation\": \"PartitionByMax\", \"reduce_column\"\
+            : \"sell_price\", \"partition_by_columns\": [\"store_id\", \"state_id\"\
+            ], \"time_column\": \"date\", \"time_ago\": 1, \"time_ago_units\": \"\
+            WEEK\", \"output_column\": \"partition_by_reduce_max_output\" }\n    \
+            \    Arguments:\n            reduce_column: Column to apply the reduce\
+            \ operation on. Reduce operations include the\n                following:\
+            \ Max, Min, Avg, Sum.\n            partition_by_columns: List of columns\
+            \ to partition by.\n            time_column: Time column for the partition\
+            \ by operation's window function.\n            time_ago: Number of time_ago_units\
+            \ to look back on our target_column, starting from time_column (inclusive).\n\
+            \            time_ago_units: Units of time_ago to look back on our target_column.\
+            \ Must be one of * 'DAY' * 'WEEK'\n            output_column: Name of\
+            \ our output feature."
           isOptional: true
           parameterType: LIST
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         feature_selection_algorithm:
           defaultValue: AMI
-          description: "The algorithm of feature\nselection. One of \"AMI\", \"CMIM\"\
-            , \"JMIM\", \"MRMR\", default to be \"AMI\".\nThe algorithms available\
-            \ are: AMI(Adjusted Mutual Information):\n   Reference:\n     https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\n\
-            \       Arrays are not yet supported in this algorithm.  CMIM(Conditional\n\
-            \       Mutual Information Maximization): Reference paper: Mohamed\n \
-            \      Bennasar, Yulia Hicks, Rossitza Setchi, \u201CFeature selection\
-            \ using\n       Joint Mutual Information Maximisation,\u201D Expert Systems\
-            \ with\n       Applications, vol. 42, issue 22, 1 December 2015, Pages\n\
-            \       8520-8532. JMIM(Joint Mutual Information Maximization): Reference\n\
-            \       paper: Mohamed Bennasar, Yulia Hicks, Rossitza Setchi, \u201C\
-            Feature\n         selection using Joint Mutual Information Maximisation,\u201D\
-            \ Expert\n         Systems with Applications, vol. 42, issue 22, 1 December\
-            \ 2015,\n         Pages 8520-8532. MRMR(MIQ Minimum-redundancy\n     \
-            \    Maximum-relevance): Reference paper: Hanchuan Peng, Fuhui Long,\n\
-            \         and Chris Ding. \"Feature selection based on mutual information\n\
-            \         criteria of max-dependency, max-relevance, and min-redundancy.\"\
-            \n         IEEE Transactions on pattern analysis and machine intelligence\n\
-            \         27, no.\n       8: 1226-1238."
+          description: "The algorithm of feature selection. One of \"AMI\", \"CMIM\"\
+            , \"JMIM\", \"MRMR\", default to be \"AMI\". The algorithms available\
+            \ are: AMI(Adjusted Mutual Information):\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html\
+            \ Arrays are not yet supported in this algorithm.  CMIM(Conditional Mutual\
+            \ Information Maximization): Reference paper: Mohamed Bennasar, Yulia\
+            \ Hicks, Rossitza Setchi, \u201CFeature selection using Joint Mutual Information\
+            \ Maximisation,\u201D Expert Systems with Applications, vol. 42, issue\
+            \ 22, 1 December 2015, Pages 8520-8532. JMIM(Joint Mutual Information\
+            \ Maximization\nReference:\n   paper: Mohamed Bennasar, Yulia Hicks, Rossitza\
+            \ Setchi, \u201CFeature selection using Joint Mutual Information Maximisation,\u201D\
+            \ Expert Systems with Applications, vol. 42, issue 22, 1 December 2015,\
+            \ Pages 8520-8532. MRMR(MIQ Minimum-redundancy Maximum-relevance): Reference\
+            \ paper: Hanchuan Peng, Fuhui Long, and Chris Ding. \"Feature selection\
+            \ based on mutual information criteria of max-dependency, max-relevance,\
+            \ and min-redundancy.\" IEEE Transactions on pattern analysis and machine\
+            \ intelligence 27, no.\n   8: 1226-1238."
           isOptional: true
           parameterType: STRING
         feature_selection_execution_engine:
           defaultValue: dataflow
           description: Execution engine to run feature selection, value can be dataflow,
             bigquery.
           isOptional: true
@@ -974,42 +938,34 @@
         forecasting_apply_windowing:
           defaultValue: true
           description: Whether to apply window strategy.
           isOptional: true
           parameterType: BOOLEAN
         forecasting_available_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            available at forecast columns.'
+          description: Forecasting available at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_context_window:
           defaultValue: -1.0
           description: Forecasting context window.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_forecast_horizon:
           defaultValue: -1.0
           description: Forecasting horizon.
           isOptional: true
           parameterType: NUMBER_INTEGER
         forecasting_holiday_regions:
           defaultValue: []
-          description: 'The geographical region based on which the
-
-            holiday effect is applied in modeling by adding holiday categorical
-
-            array feature that include all holidays matching the date. This option
-
-            only allowed when data granularity is day. By default, holiday effect
-
-            modeling is disabled. To turn it on, specify the holiday region using
-
-            this option.
+          description: 'The geographical region based on which the holiday effect
+            is applied in modeling by adding holiday categorical array feature that
+            include all holidays matching the date. This option only allowed when
+            data granularity is day. By default, holiday effect modeling is disabled.
+            To turn it on, specify the holiday region using this option.
 
             Top level: * ''GLOBAL''
 
             Second level: continental regions: * ''NA'': North America
 
             * ''JAPAC'': Japan and Asia Pacific
 
@@ -1051,38 +1007,31 @@
         forecasting_time_column:
           defaultValue: ''
           description: Forecasting time column.
           isOptional: true
           parameterType: STRING
         forecasting_time_series_attribute_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            time series attribute columns.'
+          description: Forecasting time series attribute columns.
           isOptional: true
           parameterType: LIST
         forecasting_time_series_identifier_column:
           description: '[Deprecated] A forecasting time series identifier column.
-            Raises an
-
-            exception if used - use the "time_series_identifier_column" field
-
-            instead.'
+            Raises an exception if used - use the "time_series_identifier_column"
+            field instead.'
           isOptional: true
           parameterType: STRING
         forecasting_time_series_identifier_columns:
           defaultValue: []
           description: The list of forecasting time series identifier columns.
           isOptional: true
           parameterType: LIST
         forecasting_unavailable_at_forecast_columns:
           defaultValue: []
-          description: 'Forecasting
-
-            unavailable at forecast columns.'
+          description: Forecasting unavailable at forecast columns.
           isOptional: true
           parameterType: LIST
         forecasting_window_max_count:
           defaultValue: -1.0
           description: Forecasting window max count.
           isOptional: true
           parameterType: NUMBER_INTEGER
@@ -1107,116 +1056,89 @@
           isOptional: true
           parameterType: STRING
         location:
           description: Location for the created GCP services.
           parameterType: STRING
         materialized_examples_format:
           defaultValue: tfrecords_gzip
-          description: 'The format to use for the
-
-            materialized examples. Should be either ''tfrecords_gzip'' (default) or
-
-            ''parquet''.'
+          description: The format to use for the materialized examples. Should be
+            either 'tfrecords_gzip' (default) or 'parquet'.
           isOptional: true
           parameterType: STRING
         max_selected_features:
           defaultValue: 1000.0
-          description: 'Maximum number of features to
-
-            select.  If specified, the transform config will be purged by only using
-
-            the selected features that ranked top in the feature ranking, which has
-
-            the ranking value for all supported features. If the number of input
-
-            features is smaller than max_selected_features specified, we will still
-
-            run the feature selection process and generate the feature ranking, no
-
-            features will be excluded.  The value will be set to 1000 by default if
-
-            run_feature_selection is enabled.'
+          description: Maximum number of features to select.  If specified, the transform
+            config will be purged by only using the selected features that ranked
+            top in the feature ranking, which has the ranking value for all supported
+            features. If the number of input features is smaller than max_selected_features
+            specified, we will still run the feature selection process and generate
+            the feature ranking, no features will be excluded.  The value will be
+            set to 1000 by default if run_feature_selection is enabled.
           isOptional: true
           parameterType: NUMBER_INTEGER
         model_type:
-          description: 'Model type, which we wish to engineer features
-
-            for. Can be one of: neural_network, boosted_trees, l2l, seq2seq, tft,
-            or
-
-            tide. Defaults to the empty value, `None`.'
+          description: 'Model type, which we wish to engineer features for. Can be
+            one of: neural_network, boosted_trees, l2l, seq2seq, tft, or tide. Defaults
+            to the empty value, `None`.'
           isOptional: true
           parameterType: STRING
         multimodal_image_columns:
           defaultValue: []
-          description: 'List of multimodal image
-
-            columns. Defaults to an empty list.'
+          description: List of multimodal image columns. Defaults to an empty list.
           isOptional: true
           parameterType: LIST
         multimodal_tabular_columns:
           defaultValue: []
-          description: 'List of multimodal tabular
-
-            columns. Defaults to an empty list'
+          description: List of multimodal tabular columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_text_columns:
           defaultValue: []
-          description: 'List of multimodal text
-
-            columns. Defaults to an empty list'
+          description: List of multimodal text columns. Defaults to an empty list
           isOptional: true
           parameterType: LIST
         multimodal_timeseries_columns:
           defaultValue: []
-          description: 'List of multimodal timeseries
-
-            columns. Defaults to an empty list'
+          description: List of multimodal timeseries columns. Defaults to an empty
+            list
           isOptional: true
           parameterType: LIST
         predefined_split_key:
           defaultValue: ''
           description: Predefined split key.
           isOptional: true
           parameterType: STRING
         prediction_type:
           defaultValue: ''
-          description: 'Model prediction type. One of
-
-            "classification", "regression", "time_series".'
+          description: Model prediction type. One of "classification", "regression",
+            "time_series".
           isOptional: true
           parameterType: STRING
         project:
           description: Project to run feature transform engine.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distill:
           defaultValue: false
-          description: '(deprecated) Whether the distillation should be applied
-
-            to the training.'
+          description: (deprecated) Whether the distillation should be applied to
+            the training.
           isOptional: true
           parameterType: BOOLEAN
         run_feature_selection:
           defaultValue: false
-          description: 'Whether the feature selection
-
-            should be applied to the dataset.'
+          description: Whether the feature selection should be applied to the dataset.
           isOptional: true
           parameterType: BOOLEAN
         stats_gen_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            statistics generation. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental.'
+          description: 'Execution engine to perform statistics generation. Can be
+            one of: "dataflow" (by default) or "bigquery". Using "bigquery" as the
+            execution engine is experimental.'
           isOptional: true
           parameterType: STRING
         stratified_split_key:
           defaultValue: ''
           description: Stratified split key.
           isOptional: true
           parameterType: STRING
@@ -1232,272 +1154,220 @@
         test_fraction:
           defaultValue: -1.0
           description: Fraction of input data for testing.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         tf_auto_transform_features:
           defaultValue: {}
-          description: "Dict mapping auto and/or type-resolutions to\nTF transform\
-            \ features. FTE will automatically configure a set of\nbuilt-in transformations\
-            \ for each feature based on its data statistics.\nIf users do not want\
-            \ auto type resolution, but want the set of\ntransformations for a given\
-            \ type to be automatically generated, they\nmay specify pre-resolved transformations\
-            \ types. The following type hint\ndict keys are supported: * 'auto' *\
-            \ 'categorical' * 'numeric' * 'text'\n* 'timestamp'\n  Example:  .. code-block::\
-            \ python { \"auto\": [\"feature1\"],\n    \"categorical\": [\"feature2\"\
-            , \"feature3\"], }  Note that the target and\n    weight column may not\
-            \ be included as an auto transformation unless\n    users are running\
-            \ forecasting."
+          description: 'Dict mapping auto and/or type-resolutions to TF transform
+            features. FTE will automatically configure a set of built-in transformations
+            for each feature based on its data statistics. If users do not want auto
+            type resolution, but want the set of transformations for a given type
+            to be automatically generated, they may specify pre-resolved transformations
+            types. The following type hint dict keys are supported: * ''auto'' * ''categorical''
+            * ''numeric'' * ''text'' * ''timestamp'' Example: `{ "auto": ["feature1"],
+            "categorical": ["feature2", "feature3"], }`. Note that the target and
+            weight column may not be included as an auto transformation unless users
+            are running forecasting.'
           isOptional: true
           parameterType: STRUCT
         tf_custom_transformation_definitions:
           defaultValue: []
-          description: "List of\nTensorFlow-based custom transformation definitions.\
-            \  Custom,\nbring-your-own transform functions, where users can define\
-            \ and import\ntheir own transform function and use it with FTE's built-in\n\
-            transformations.\n  Example:  .. code-block:: python  [ { \"transformation\"\
-            : \"PlusOne\",\n    \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"plus_one_transform\" }, { \"transformation\"\
-            :\n    \"MultiplyTwo\", \"module_path\": \"gs://bucket/custom_transform_fn.py\"\
-            ,\n    \"function_name\": \"multiply_two_transform\" } ]  Using custom\n\
-            \    transform function together with FTE's built-in transformations:\
-            \  ..\n    code-block:: python  [ { \"transformation\": \"CastToFloat\"\
-            ,\n    \"input_columns\": [\"feature_1\"], \"output_columns\": [\"feature_1\"\
-            ] },{\n    \"transformation\": \"PlusOne\", \"input_columns\": [\"feature_1\"\
-            ]\n    \"output_columns\": [\"feature_1_plused_one\"] },{ \"transformation\"\
-            :\n    \"MultiplyTwo\", \"input_columns\": [\"feature_1\"] \"output_columns\"\
-            :\n    [\"feature_1_multiplied_two\"] } ]"
+          description: 'List of TensorFlow-based custom transformation definitions.  Custom,
+            bring-your-own transform functions, where users can define and import
+            their own transform function and use it with FTE''s built-in transformations.
+            `[ { "transformation": "PlusOne", "module_path": "gs://bucket/custom_transform_fn.py",
+            "function_name": "plus_one_transform" }, { "transformation": "MultiplyTwo",
+            "module_path": "gs://bucket/custom_transform_fn.py", "function_name":
+            "multiply_two_transform" } ]  Using custom transform function together
+            with FTE''s built-in transformations:  .. code-block:: python  [ { "transformation":
+            "CastToFloat", "input_columns": ["feature_1"], "output_columns": ["feature_1"]
+            },{ "transformation": "PlusOne", "input_columns": ["feature_1"] "output_columns":
+            ["feature_1_plused_one"] },{ "transformation": "MultiplyTwo", "input_columns":
+            ["feature_1"] "output_columns": ["feature_1_multiplied_two"] } ]'
           isOptional: true
           parameterType: LIST
         tf_transform_execution_engine:
           defaultValue: dataflow
-          description: 'Execution engine to perform
-
-            row-level TF transformations. Can be one of: "dataflow" (by default) or
-
-            "bigquery". Using "bigquery" as the execution engine is experimental and
-
-            is for allowlisted customers only. In addition, executing on "bigquery"
-
-            only supports auto transformations (i.e., specified by
-
-            tf_auto_transform_features) and will raise an error when
-
-            tf_custom_transformation_definitions or tf_transformations_path is set.'
+          description: 'Execution engine to perform row-level TF transformations.
+            Can be one of: "dataflow" (by default) or "bigquery". Using "bigquery"
+            as the execution engine is experimental and is for allowlisted customers
+            only. In addition, executing on "bigquery" only supports auto transformations
+            (i.e., specified by tf_auto_transform_features) and will raise an error
+            when tf_custom_transformation_definitions or tf_transformations_path is
+            set.'
           isOptional: true
           parameterType: STRING
         tf_transformations_path:
           defaultValue: ''
-          description: "Path to TensorFlow-based\ntransformation configuration.  Path\
-            \ to a JSON file used to specified\nFTE's TF transformation configurations.\
-            \  In the following, we provide\nsome sample transform configurations\
-            \ to demonstrate FTE's capabilities.\nAll transformations on input columns\
-            \ are explicitly specified with FTE's\nbuilt-in transformations. Chaining\
-            \ of multiple transformations on a\nsingle column is also supported. For\
-            \ example:  .. code-block:: python  [\n{ \"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }, {\n\"transformation\": \"ZScale\"\
-            , \"input_columns\": [\"feature_2\"] } ]\nAdditional information about\
-            \ FTE's currently supported built-in\ntransformations:\n      Datetime:\
-            \ Extracts datetime featues from a column containing\n        timestamp\
-            \ strings.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"Datetime\", \"input_columns\": [\"feature_1\"], \"time_format\"\
-            :\n            \"%Y-%m-%d\" }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the datetime\
-            \ transformation on.\n              output_columns: Names of output\n\
-            \                columns, one for each datetime_features element.\n  \
-            \            time_format: Datetime format string. Time format is\n   \
-            \             a combination of Date + Time Delimiter (optional) + Time\n\
-            \                (optional) directives. Valid date directives are as\n\
-            \                follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  #\n\
-            \                2018/11/30 * '%y-%m-%d'  # 18-11-30 * '%y/%m/%d'  #\n\
-            \                18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'  #\n\
-            \                11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  #\n\
-            \                11/30/18 * '%d-%m-%Y'  # 30-11-2018 * '%d/%m/%Y'  #\n\
-            \                30/11/2018 * '%d-%B-%Y'  # 30-November-2018 * '%d-%m-%y'\n\
-            \                # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  #\n\
-            \                30-November-18 * '%d%m%Y'    # 30112018 * '%m%d%Y'  \
-            \  #\n                11302018 * '%Y%m%d'    # 20181130 Valid time delimiters\n\
-            \                are as follows * 'T' * ' ' Valid time directives are\
-            \ as\n                follows * '%H:%M'          # 23:59 * '%H:%M:%S'\
-            \       #\n                23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456]\
-            \ *\n                  '%H:%M:%S.%f%z'  # 23:59:58[.123456]+0000 *\n \
-            \                 '%H:%M:%S%z',    # 23:59:58+0000\n              datetime_features:\
-            \ List of datetime\n                features to be extract. Each entry\
-            \ must be one of *\n                'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK'\
-            \ * 'DAY_OF_YEAR'\n                * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR'\
-            \ * 'MINUTE' *\n                'SECOND' Defaults to ['YEAR', 'MONTH',\
-            \ 'DAY',\n                'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
-            \      Log: Performs the natural log on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Log\",\n          \
-            \  \"input_columns\": [\"feature_1\"] }\n          Arguments:\n      \
-            \        input_columns: A list with a single column to\n             \
-            \   perform the log transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      ZScale:\
-            \ Performs Z-scale normalization on a numeric column.\n          Example:\
-            \  .. code-block:: python  { \"transformation\":\n            \"ZScale\"\
-            , \"input_columns\": [\"feature_1\"] }\n          Arguments:\n       \
-            \       input_columns: A list with a single column to\n              \
-            \  perform the z-scale transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n      Vocabulary:\
-            \ Converts strings to integers, where each unique string\n        gets\
-            \ a unique integer representation.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"Vocabulary\", \"input_columns\"\
-            : [\"feature_1\"] }\n          Arguments:\n              input_columns:\
-            \ A list with a single column to\n                perform the vocabulary\
-            \ transformation on.\n              output_columns: A list with a single\n\
-            \                output column name, corresponding to the output of our\n\
-            \                transformation.\n              top_k: Number of the most\
-            \ frequent words\n                in the vocabulary to use for generating\
-            \ dictionary\n                lookup indices. If not specified, all words\
-            \ in the\n                vocabulary will be used. Defaults to None.\n\
-            \              frequency_threshold: Limit the vocabulary\n           \
-            \     only to words whose number of occurrences in the input\n       \
-            \         exceeds frequency_threshold. If not specified, all words\n \
-            \               in the vocabulary will be included. If both top_k and\n\
-            \                frequency_threshold are specified, a word must satisfy\n\
-            \                both conditions to be included. Defaults to None.\n \
-            \     Categorical: Transforms categorical columns to integer columns.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Categorical\", \"input_columns\": [\"feature_1\"], \"top_k\"\
-            : 10 }\n          Arguments:\n              input_columns: A list with\
-            \ a single column to\n                perform the categorical transformation\
-            \ on.\n              output_columns: A list with a single\n          \
-            \      output column name, corresponding to the output of our\n      \
-            \          transformation.\n              top_k: Number of the most frequent\
-            \ words\n                in the vocabulary to use for generating dictionary\n\
-            \                lookup indices. If not specified, all words in the\n\
-            \                vocabulary will be used.\n              frequency_threshold:\
-            \ Limit the vocabulary\n                only to words whose number of\
-            \ occurrences in the input\n                exceeds frequency_threshold.\
-            \ If not specified, all words\n                in the vocabulary will\
-            \ be included. If both top_k and\n                frequency_threshold\
-            \ are specified, a word must satisfy\n                both conditions\
-            \ to be included.\n      Reduce: Given a column where each entry is a\
-            \ numeric array,\n        reduces arrays according to our reduce_mode.\n\
-            \          Example:  .. code-block:: python  { \"transformation\":\n \
-            \           \"Reduce\", \"input_columns\": [\"feature_1\"], \"reduce_mode\"\
-            :\n            \"MEAN\", \"output_columns\": [\"feature_1_mean\"] }\n\
-            \          Arguments:\n              input_columns: A list with a single\
-            \ column to\n                perform the reduce transformation on.\n \
-            \             output_columns: A list with a single\n                output\
-            \ column name, corresponding to the output of our\n                transformation.\n\
-            \              reduce_mode: One of * 'MAX' * 'MIN' *\n               \
-            \ 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n              last_k: The number\
-            \ of last k elements when\n                'LAST_K' reduce mode is used.\
-            \ Defaults to 1.\n      SplitString: Given a column of strings, splits\
-            \ strings into token\n        arrays.\n          Example:  .. code-block::\
-            \ python  { \"transformation\":\n            \"SplitString\", \"input_columns\"\
-            : [\"feature_1\"], \"separator\":\n            \"$\" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the split string transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  separator: Separator to split input string\n                into tokens.\
-            \ Defaults to ' '.\n              missing_token: Missing token to use\
-            \ when\n                no string is included. Defaults to ' _MISSING_\
-            \ '.\n      NGram: Given a column of strings, splits strings into token\
-            \ arrays\n        where each token is an integer.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"NGram\",\n        \
-            \    \"input_columns\": [\"feature_1\"], \"min_ngram_size\": 1,\n    \
-            \        \"max_ngram_size\": 2, \"separator\": \" \" }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_ngram_size: Minimum n-gram size. Must\n                be a positive\
-            \ number and <= max_ngram_size. Defaults to\n                1.\n    \
-            \          max_ngram_size: Maximum n-gram size. Must\n               \
-            \ be a positive number and >= min_ngram_size. Defaults to\n          \
-            \      2.\n              top_k: Number of the most frequent words\n  \
-            \              in the vocabulary to use for generating dictionary\n  \
-            \              lookup indices. If not specified, all words in the\n  \
-            \              vocabulary will be used. Defaults to None.\n          \
-            \    frequency_threshold: Limit the\n                dictionary's vocabulary\
-            \ only to words whose number of\n                occurrences in the input\
-            \ exceeds frequency_threshold. If\n                not specified, all\
-            \ words in the vocabulary will be\n                included. If both top_k\
-            \ and frequency_threshold are\n                specified, a word must\
-            \ satisfy both conditions to be\n                included. Defaults to\
-            \ None.\n              separator: Separator to split input string\n  \
-            \              into tokens. Defaults to ' '.\n              missing_token:\
-            \ Missing token to use when\n                no string is included. Defaults\
-            \ to ' _MISSING_ '.\n      Clip: Given a numeric column, clips elements\
-            \ such that elements <\n        min_value are assigned min_value, and\
-            \ elements > max_value are\n        assigned max_value.\n          Example:\
-            \  .. code-block:: python  { \"transformation\": \"Clip\",\n         \
-            \   \"input_columns\": [\"col1\"], \"output_columns\":\n            [\"\
-            col1_clipped\"], \"min_value\": 1., \"max_value\": 10., }\n          Arguments:\n\
-            \              input_columns: A list with a single column to\n       \
-            \         perform the n-gram transformation on.\n              output_columns:\
-            \ A list with a single\n                output column name, corresponding\
-            \ to the output of our\n                transformation.\n            \
-            \  min_value: Number where all values below\n                min_value\
-            \ are set to min_value. If no min_value is\n                provided,\
-            \ min clipping will not occur. Defaults to None.\n              max_value:\
-            \ Number where all values above\n                max_value are set to\
-            \ max_value If no max_value is\n                provided, max clipping\
-            \ will not occur. Defaults to None.\n      MultiHotEncoding: Performs\
-            \ multi-hot encoding on a categorical\n        array column.\n       \
-            \   Example:  .. code-block:: python  { \"transformation\":\n        \
-            \    \"MultiHotEncoding\", \"input_columns\": [\"col1\"], }  The number\n\
-            \            of classes is determened by the largest number included in\n\
-            \            the input if it is numeric or the total number of unique\n\
-            \            values of the input if it is type str.  If the input is has\n\
-            \            type str and an element contians separator tokens, the input\n\
-            \            will be split at separator indices, and the each element\
-            \ of\n            the split list will be considered a seperate class.\
-            \ For\n            example,\n          Input:  .. code-block:: python\
-            \  [ [\"foo bar\"],      # Example\n            0 [\"foo\", \"bar\"],\
-            \   # Example 1 [\"foo\"],          # Example\n            2 [\"bar\"\
-            ],          # Example 3 ]\n          Output (with default separator=\"\
-            \ \"):  .. code-block:: python [\n            [1, 1],          # Example\
-            \ 0 [1, 1],          # Example 1\n            [1, 0],          # Example\
-            \ 2 [0, 1],          # Example 3 ]\n          Arguments:\n           \
-            \   input_columns: A list with a single column to\n                perform\
-            \ the multi-hot-encoding on.\n              output_columns: A list with\
-            \ a single\n                output column name, corresponding to the output\
-            \ of our\n                transformation.\n              top_k: Number\
-            \ of the most frequent words\n                in the vocabulary to use\
-            \ for generating dictionary\n                lookup indices. If not specified,\
-            \ all words in the\n                vocabulary will be used. Defaults\
-            \ to None.\n              frequency_threshold: Limit the\n           \
-            \     dictionary's vocabulary only to words whose number of\n        \
-            \        occurrences in the input exceeds frequency_threshold. If\n  \
-            \              not specified, all words in the vocabulary will be\n  \
-            \              included. If both top_k and frequency_threshold are\n \
-            \               specified, a word must satisfy both conditions to be\n\
-            \                included. Defaults to None.\n              separator:\
-            \ Separator to split input string\n                into tokens. Defaults\
-            \ to ' '.\n      MaxAbsScale: Performs maximum absolute scaling on a numeric\n\
-            \        column.\n          Example:  .. code-block:: python  { \"transformation\"\
-            :\n            \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\"\
-            :\n            [\"col1_max_abs_scaled\"] }\n          Arguments:\n   \
-            \           input_columns: A list with a single column to\n          \
-            \      perform max-abs-scale on.\n              output_columns: A list\
-            \ with a single\n                output column name, corresponding to\
-            \ the output of our\n                transformation.\n      Custom: Transformations\
-            \ defined in\n        tf_custom_transformation_definitions are included\
-            \ here in the\n        TensorFlow-based transformation configuration.\
-            \  For example,\n        given the following tf_custom_transformation_definitions:\
-            \  ..\n        code-block:: python  [ { \"transformation\": \"PlusX\"\
-            ,\n        \"module_path\": \"gs://bucket/custom_transform_fn.py\",\n\
-            \        \"function_name\": \"plus_one_transform\" } ]  We can include\
-            \ the\n        following transformation:  .. code-block:: python  {\n\
-            \        \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"],\n\
-            \        \"output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note\
-            \ that\n        input_columns must still be included in our arguments\
-            \ and\n        output_columns is optional. All other arguments are those\n\
-            \        defined in custom_transform_fn.py, which includes `\"x\"` in\
-            \ this\n        case. See tf_custom_transformation_definitions above.\n\
-            \        legacy_transformations_path (Optional[str]) Deprecated. Prefer\n\
-            \        tf_auto_transform_features.  Path to a GCS file containing JSON\n\
-            \        string for legacy style transformations. Note that\n        legacy_transformations_path\
-            \ and tf_auto_transform_features\n        cannot both be specified."
+          description: "Path to TensorFlow-based transformation configuration.  Path\
+            \ to a JSON file used to specified FTE's TF transformation configurations.\
+            \  In the following, we provide some sample transform configurations to\
+            \ demonstrate FTE's capabilities. All transformations on input columns\
+            \ are explicitly specified with FTE's built-in transformations. Chaining\
+            \ of multiple transformations on a single column is also supported. For\
+            \ example:  .. code-block:: python  [ { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_1\"] }, { \"transformation\": \"ZScale\"\
+            , \"input_columns\": [\"feature_2\"] } ]`. Additional information about\
+            \ FTE's currently supported built-in\ntransformations:\nDatetime: Extracts\
+            \ datetime featues from a column containing timestamp strings.\n    Example:\
+            \  .. code-block:: python  { \"transformation\": \"Datetime\", \"input_columns\"\
+            : [\"feature_1\"], \"time_format\": \"%Y-%m-%d\" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the datetime\
+            \ transformation on.\n        output_columns: Names of output columns,\
+            \ one for each datetime_features element.\n        time_format: Datetime\
+            \ format string. Time format is a combination of Date + Time Delimiter\
+            \ (optional) + Time (optional) directives. Valid date directives are as\
+            \ follows * '%Y-%m-%d'  # 2018-11-30 * '%Y/%m/%d'  # 2018/11/30 * '%y-%m-%d'\
+            \  # 18-11-30 * '%y/%m/%d'  # 18/11/30 * '%m-%d-%Y'  # 11-30-2018 * '%m/%d/%Y'\
+            \  # 11/30/2018 * '%m-%d-%y'  # 11-30-18 * '%m/%d/%y'  # 11/30/18 * '%d-%m-%Y'\
+            \  # 30-11-2018 * '%d/%m/%Y'  # 30/11/2018 * '%d-%B-%Y'  # 30-November-2018\
+            \ * '%d-%m-%y' # 30-11-18 * '%d/%m/%y'  # 30/11/18 * '%d-%B-%y'  # 30-November-18\
+            \ * '%d%m%Y'    # 30112018 * '%m%d%Y'    # 11302018 * '%Y%m%d'    # 20181130\
+            \ Valid time delimiters are as follows * 'T' * ' ' Valid time directives\
+            \ are as follows * '%H:%M'          # 23:59 * '%H:%M:%S'       #\n   \
+            \       23:59:58 * '%H:%M:%S.%f'    # 23:59:58[.123456] * '%H:%M:%S.%f%z'\
+            \  # 23:59:58[.123456]+0000 * '%H:%M:%S%z',    # 23:59:58+0000\n     \
+            \   datetime_features: List of datetime features to be extract. Each entry\
+            \ must be one of * 'YEAR' * 'MONTH' * 'DAY' * 'DAY_OF_WEEK' * 'DAY_OF_YEAR'\
+            \ * 'WEEK_OF_YEAR' * 'QUARTER' * 'HOUR' * 'MINUTE' * 'SECOND' Defaults\
+            \ to ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'WEEK_OF_YEAR']\n\
+            Log: Performs the natural log on a numeric column.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Log\", \"input_columns\": [\"feature_1\"\
+            ] }\n    Arguments:\n        input_columns: A list with a single column\
+            \ to perform the log transformation on.\n        output_columns: A list\
+            \ with a single output column name, corresponding to the output of our\
+            \ transformation.\nZScale: Performs Z-scale normalization on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"ZScale\", \"input_columns\": [\"feature_1\"] }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the z-scale\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\nVocabulary:\
+            \ Converts strings to integers, where each unique string gets a unique\
+            \ integer representation.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"Vocabulary\", \"input_columns\": [\"feature_1\"] }\n\
+            \    Arguments:\n        input_columns: A list with a single column to\
+            \ perform the vocabulary transformation on.\n        output_columns: A\
+            \ list with a single output column name, corresponding to the output of\
+            \ our transformation.\n        top_k: Number of the most frequent words\
+            \ in the vocabulary to use for generating dictionary lookup indices. If\
+            \ not specified, all words in the vocabulary will be used. Defaults to\
+            \ None.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included. Defaults to None.\nCategorical: Transforms\
+            \ categorical columns to integer columns.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Categorical\", \"input_columns\": [\"\
+            feature_1\"], \"top_k\": 10 }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform the categorical transformation\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used.\n        frequency_threshold: Limit the vocabulary only to words\
+            \ whose number of occurrences in the input exceeds frequency_threshold.\
+            \ If not specified, all words in the vocabulary will be included. If both\
+            \ top_k and frequency_threshold are specified, a word must satisfy both\
+            \ conditions to be included.\nReduce: Given a column where each entry\
+            \ is a numeric array, reduces arrays according to our reduce_mode.\n \
+            \   Example:  .. code-block:: python  { \"transformation\": \"Reduce\"\
+            , \"input_columns\": [\"feature_1\"], \"reduce_mode\": \"MEAN\", \"output_columns\"\
+            : [\"feature_1_mean\"] }\n    Arguments:\n        input_columns: A list\
+            \ with a single column to perform the reduce transformation on.\n    \
+            \    output_columns: A list with a single output column name, corresponding\
+            \ to the output of our transformation.\n        reduce_mode: One of *\
+            \ 'MAX' * 'MIN' * 'MEAN' * 'LAST_K' Defaults to 'MEAN'.\n        last_k:\
+            \ The number of last k elements when 'LAST_K' reduce mode is used. Defaults\
+            \ to 1.\nSplitString: Given a column of strings, splits strings into token\
+            \ arrays.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"SplitString\", \"input_columns\": [\"feature_1\"], \"separator\":\
+            \ \"$\" }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the split string transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        separator: Separator to split input\
+            \ string into tokens. Defaults to ' '.\n        missing_token: Missing\
+            \ token to use when no string is included. Defaults to ' _MISSING_ '.\n\
+            NGram: Given a column of strings, splits strings into token arrays where\
+            \ each token is an integer.\n    Example:  .. code-block:: python  { \"\
+            transformation\": \"NGram\", \"input_columns\": [\"feature_1\"], \"min_ngram_size\"\
+            : 1, \"max_ngram_size\": 2, \"separator\": \" \" }\n    Arguments:\n \
+            \       input_columns: A list with a single column to perform the n-gram\
+            \ transformation on.\n        output_columns: A list with a single output\
+            \ column name, corresponding to the output of our transformation.\n  \
+            \      min_ngram_size: Minimum n-gram size. Must be a positive number\
+            \ and <= max_ngram_size. Defaults to 1.\n        max_ngram_size: Maximum\
+            \ n-gram size. Must be a positive number and >= min_ngram_size. Defaults\
+            \ to 2.\n        top_k: Number of the most frequent words in the vocabulary\
+            \ to use for generating dictionary lookup indices. If not specified, all\
+            \ words in the vocabulary will be used. Defaults to None.\n        frequency_threshold:\
+            \ Limit the dictionary's vocabulary only to words whose number of occurrences\
+            \ in the input exceeds frequency_threshold. If not specified, all words\
+            \ in the vocabulary will be included. If both top_k and frequency_threshold\
+            \ are specified, a word must satisfy both conditions to be included. Defaults\
+            \ to None.\n        separator: Separator to split input string into tokens.\
+            \ Defaults to ' '.\n        missing_token: Missing token to use when no\
+            \ string is included. Defaults to ' _MISSING_ '.\nClip: Given a numeric\
+            \ column, clips elements such that elements < min_value are assigned min_value,\
+            \ and elements > max_value are assigned max_value.\n    Example:  .. code-block::\
+            \ python  { \"transformation\": \"Clip\", \"input_columns\": [\"col1\"\
+            ], \"output_columns\": [\"col1_clipped\"], \"min_value\": 1., \"max_value\"\
+            : 10., }\n    Arguments:\n        input_columns: A list with a single\
+            \ column to perform the n-gram transformation on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\n        min_value: Number where all values below\
+            \ min_value are set to min_value. If no min_value is provided, min clipping\
+            \ will not occur. Defaults to None.\n        max_value: Number where all\
+            \ values above max_value are set to max_value If no max_value is provided,\
+            \ max clipping will not occur. Defaults to None.\nMultiHotEncoding: Performs\
+            \ multi-hot encoding on a categorical array column.\n    Example:  ..\
+            \ code-block:: python  { \"transformation\": \"MultiHotEncoding\", \"\
+            input_columns\": [\"col1\"], }  The number of classes is determened by\
+            \ the largest number included in the input if it is numeric or the total\
+            \ number of unique values of the input if it is type str.  If the input\
+            \ is has type str and an element contians separator tokens, the input\
+            \ will be split at separator indices, and the each element of the split\
+            \ list will be considered a seperate class. For example,\n    Input: \
+            \ .. code-block:: python  [ [\"foo bar\"],      # Example 0 [\"foo\",\
+            \ \"bar\"],   # Example 1 [\"foo\"],          # Example 2 [\"bar\"], \
+            \         # Example 3 ] Output (with default separator=\" \"):  .. code-block::\
+            \ python [ [1, 1],          # Example 0 [1, 1],          # Example 1 [1,\
+            \ 0],          # Example 2 [0, 1],          # Example 3 ]\n    Arguments:\n\
+            \        input_columns: A list with a single column to perform the multi-hot-encoding\
+            \ on.\n        output_columns: A list with a single output column name,\
+            \ corresponding to the output of our transformation.\n        top_k: Number\
+            \ of the most frequent words in the vocabulary to use for generating dictionary\
+            \ lookup indices. If not specified, all words in the vocabulary will be\
+            \ used. Defaults to None.\n        frequency_threshold: Limit the dictionary's\
+            \ vocabulary only to words whose number of occurrences in the input exceeds\
+            \ frequency_threshold. If not specified, all words in the vocabulary will\
+            \ be included. If both top_k and frequency_threshold are specified, a\
+            \ word must satisfy both conditions to be included. Defaults to None.\n\
+            \        separator: Separator to split input string into tokens. Defaults\
+            \ to ' '.\nMaxAbsScale: Performs maximum absolute scaling on a numeric\
+            \ column.\n    Example:  .. code-block:: python  { \"transformation\"\
+            : \"MaxAbsScale\", \"input_columns\": [\"col1\"], \"output_columns\":\
+            \ [\"col1_max_abs_scaled\"] }\n    Arguments:\n        input_columns:\
+            \ A list with a single column to perform max-abs-scale on.\n        output_columns:\
+            \ A list with a single output column name, corresponding to the output\
+            \ of our transformation.\nCustom: Transformations defined in tf_custom_transformation_definitions\
+            \ are included here in the TensorFlow-based transformation configuration.\
+            \  For example, given the following tf_custom_transformation_definitions:\
+            \  .. code-block:: python  [ { \"transformation\": \"PlusX\", \"module_path\"\
+            : \"gs://bucket/custom_transform_fn.py\", \"function_name\": \"plus_one_transform\"\
+            \ } ]  We can include the following transformation:  .. code-block:: python\
+            \  { \"transformation\": \"PlusX\", \"input_columns\": [\"col1\"], \"\
+            output_columns\": [\"col1_max_abs_scaled\"] \"x\": 5 }  Note that input_columns\
+            \ must still be included in our arguments and output_columns is optional.\
+            \ All other arguments are those defined in custom_transform_fn.py, which\
+            \ includes `\"x\"` in this case. See tf_custom_transformation_definitions\
+            \ above. legacy_transformations_path (Optional[str]) Deprecated. Prefer\
+            \ tf_auto_transform_features.  Path to a GCS file containing JSON string\
+            \ for legacy style transformations. Note that legacy_transformations_path\
+            \ and tf_auto_transform_features cannot both be specified."
           isOptional: true
           parameterType: STRING
         timestamp_split_key:
           defaultValue: ''
           description: Timestamp split key.
           isOptional: true
           parameterType: STRING
@@ -1523,19 +1393,17 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The stats of the dataset.
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The ranking of features, all features supported in the
-
-            dataset will be included. For "AMI" algorithm, array features won''t be
-
-            available in the ranking as arrays are not supported yet.'
+          description: The ranking of features, all features supported in the dataset
+            will be included. For "AMI" algorithm, array features won't be available
+            in the ranking as arrays are not supported yet.
         instance_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         materialized_data:
           artifactType:
             schemaTitle: system.Dataset
@@ -1548,44 +1416,36 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         bigquery_downsampled_test_split_uri:
-          description: 'BigQuery URI for the downsampled test
-
-            split to pass to the batch prediction component during batch explain.'
+          description: BigQuery URI for the downsampled test split to pass to the
+            batch prediction component during batch explain.
           parameterType: STRING
         bigquery_test_split_uri:
-          description: 'BigQuery URI for the test split to pass to the
-
-            batch prediction component during evaluation.'
+          description: BigQuery URI for the test split to pass to the batch prediction
+            component during evaluation.
           parameterType: STRING
         bigquery_train_split_uri:
-          description: 'BigQuery URI for the train split to pass to the
-
-            batch prediction component during distillation.'
+          description: BigQuery URI for the train split to pass to the batch prediction
+            component during distillation.
           parameterType: STRING
         bigquery_validation_split_uri:
-          description: 'BigQuery URI for the validation split to
-
-            pass to the batch prediction component during distillation.'
+          description: BigQuery URI for the validation split to pass to the batch
+            prediction component during distillation.
           parameterType: STRING
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         split_example_counts:
-          description: 'JSON string of data split example counts for train,
-
-            validate, and test splits.'
+          description: JSON string of data split example counts for train, validate,
+            and test splits.
           parameterType: STRING
   comp-get-fte-suffix:
     executorLabel: exec-get-fte-suffix
     inputDefinitions:
       parameters:
         bigquery_staging_full_dataset_id:
           parameterType: STRING
@@ -1930,149 +1790,116 @@
             which tracks the upload Model's long-running operation.
           parameterType: STRING
   comp-prophet-trainer:
     executorLabel: exec-prophet-trainer
     inputDefinitions:
       parameters:
         data_granularity_unit:
-          description: 'String representing the units of time for the
-
-            time column.'
+          description: String representing the units of time for the time column.
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'Dataflow worker''s disk size in GB
-
-            during training.'
+          description: Dataflow worker's disk size in GB during training.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-1
-          description: 'The dataflow machine type used for
-
-            training.'
+          description: The dataflow machine type used for training.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 10.0
-          description: 'The max number of Dataflow
-
-            workers used for training.'
+          description: The max number of Dataflow workers used for training.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            dataflow jobs.'
+          description: Custom service account to run dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used.'
+          description: Dataflow's fully qualified subnetwork name, when empty the
+            default subnetwork will be used.
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         forecast_horizon:
-          description: 'The number of time periods into the future for
-
-            which forecasts will be created. Future periods start after the latest
-
-            timestamp for each time series.'
+          description: The number of time periods into the future for which forecasts
+            will be created. Future periods start after the latest timestamp for each
+            time series.
           parameterType: NUMBER_INTEGER
         location:
           description: The GCP region for Vertex AI.
           parameterType: STRING
         max_num_trials:
           defaultValue: 6.0
-          description: 'Maximum number of tuning trials to perform
-
-            per time series. There are up to 100 possible combinations to explore
-
-            for each time series. Recommended values to try are 3, 6, and 24.'
+          description: Maximum number of tuning trials to perform per time series.
+            There are up to 100 possible combinations to explore for each time series.
+            Recommended values to try are 3, 6, and 24.
           isOptional: true
           parameterType: NUMBER_INTEGER
         optimization_objective:
           defaultValue: rmse
-          description: 'Optimization objective for tuning. Supported
-
-            metrics come from Prophet''s performance_metrics function. These are mse,
-
-            rmse, mae, mape, mdape, smape, and coverage.'
+          description: Optimization objective for tuning. Supported metrics come from
+            Prophet's performance_metrics function. These are mse, rmse, mae, mape,
+            mdape, smape, and coverage.
           isOptional: true
           parameterType: STRING
         predefined_split_column:
-          description: 'The predefined_split column name. A string
-
-            that represents a list of comma separated CSV filenames.'
+          description: The predefined_split column name. A string that represents
+            a list of comma separated CSV filenames.
           parameterType: STRING
         project:
           description: The GCP project that runs the pipeline components.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         source_bigquery_uri:
-          description: 'The BigQuery table path of format
-
-            bq (str)://bq_project.bq_dataset.bq_table'
+          description: The BigQuery table path of format bq (str)://bq_project.bq_dataset.bq_table
           parameterType: STRING
         target_column:
-          description: 'Name of the column that the model is to predict
-
-            values for.'
+          description: Name of the column that the model is to predict values for.
           parameterType: STRING
         time_column:
-          description: 'Name of the column that identifies time order in the
-
-            time series.'
+          description: Name of the column that identifies time order in the time series.
           parameterType: STRING
         time_series_identifier_column:
-          description: 'Name of the column that identifies
-
-            the time series.'
+          description: Name of the column that identifies the time series.
           parameterType: STRING
         window_column:
-          description: 'Name of the column that should be used to filter
-
-            input rows.  The column should contain either booleans or string
-
-            booleans; if the value of the row is True, generate a sliding window
-
-            from that row.'
+          description: Name of the column that should be used to filter input rows.  The
+            column should contain either booleans or string booleans; if the value
+            of the row is True, generate a sliding window from that row.
           parameterType: STRING
     outputDefinitions:
       artifacts:
         evaluated_examples_directory:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
           description: The UnmanagedContainerModel artifact.
       parameters:
         gcp_resources:
-          description: 'Serialized gcp_resources proto tracking the custom training
-
-            job.'
+          description: Serialized gcp_resources proto tracking the custom training
+            job.
           parameterType: STRING
   comp-table-to-uri:
     executorLabel: exec-table-to-uri
     inputDefinitions:
       artifacts:
         table:
           artifactType:
@@ -2190,15 +2017,15 @@
           \ dataset.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  import collections\n\n  from google.cloud import bigquery\n  # pylint:\
           \ enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project, location=location)\n  ref\
           \ = client.create_dataset(dataset=dataset, exists_ok=exists_ok)\n  return\
           \ collections.namedtuple('Outputs', ['project_id', 'dataset_id'])(\n   \
           \   ref.project, ref.dataset_id)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-bigquery-delete-dataset-with-prefix:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - bigquery_delete_dataset_with_prefix
@@ -2224,15 +2051,15 @@
           \  \"\"\"Deletes all BigQuery datasets matching the given prefix.\"\"\"\n\
           \  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project)\n  for dataset in client.list_datasets(project=project):\n\
           \    if dataset.dataset_id.startswith(dataset_prefix):\n      client.delete_dataset(\n\
           \          dataset=dataset.dataset_id,\n          delete_contents=delete_contents)\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-bigquery-query-job:
       container:
         args:
         - --type
         - BigqueryQueryJob
         - --project
         - '{{$.inputs.parameters[''project'']}}'
@@ -2285,15 +2112,15 @@
           \ str = '',\n    priority: str = 'INTERACTIVE',\n) -> dict:  # pylint: disable=g-bare-generic\n\
           \  \"\"\"Creates a JobConfigurationQuery object.\"\"\"\n  config = {\n \
           \     'priority': priority,\n  }\n  if all([project_id, dataset_id, table_id]):\n\
           \    config['destinationTable'] = {\n        'projectId': project_id,\n\
           \        'datasetId': dataset_id,\n        'tableId': table_id,\n    }\n\
           \  if write_disposition:\n    config['write_disposition'] = write_disposition\n\
           \  return config\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-feature-transform-engine:
       container:
         args:
         - feature_transform_engine
         - '{"Concat": ["--project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--location=", "{{$.inputs.parameters[''location'']}}"]}'
         - '{"Concat": ["--dataset_level_custom_transformation_definitions=", "{{$.inputs.parameters[''dataset_level_custom_transformation_definitions'']}}"]}'
@@ -2370,16 +2197,16 @@
         - '{"Concat": ["--dataflow_project=", "{{$.inputs.parameters[''project'']}}"]}'
         - '{"Concat": ["--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging"]}'
         - '{"Concat": ["--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp"]}'
         - '{"Concat": ["--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}"]}'
         - '{"Concat": ["--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}"]}'
-        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125
-        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        - --dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125
+        - --feature_transform_engine_docker_uri=us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
         - '{"Concat": ["--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}"]}'
         - '{"Concat": ["--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}"]}'
         - '{"Concat": ["--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}"]}'
         - '{"Concat": ["--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}"]}'
         - '{"Concat": ["--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
         - '{"Concat": ["--autodetect_csv_schema=", "{{$.inputs.parameters[''autodetect_csv_schema'']}}"]}'
         - '{"Concat": ["--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}"]}'
@@ -2388,15 +2215,15 @@
         - '{"IfPresent": {"InputName": "group_total_weight", "Then": {"Concat": ["--group_total_weight=",
           "{{$.inputs.parameters[''group_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "temporal_total_weight", "Then": {"Concat":
           ["--temporal_total_weight=", "{{$.inputs.parameters[''temporal_total_weight'']}}"]}}}'
         - '{"IfPresent": {"InputName": "group_temporal_total_weight", "Then": {"Concat":
           ["--group_temporal_total_weight=", "{{$.inputs.parameters[''group_temporal_total_weight'']}}"]}}}'
         - '{"Concat": ["--encryption_spec_key_name=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}"]}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125
     exec-get-fte-suffix:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_fte_suffix
@@ -2422,15 +2249,15 @@
           \ the intermediate FTE table name.\"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  client = bigquery.Client(project=project, location=location)\n  for\
           \ table in client.list_tables(bigquery_staging_full_dataset_id):\n    if\
           \ table.table_id.startswith(fte_table):\n      return table.table_id[len(fte_table)\
           \ + 1:]\n  raise ValueError(\n      f'No FTE output tables found in {bigquery_staging_full_dataset_id}.')\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-get-table-location:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - get_table_location
@@ -2458,15 +2285,15 @@
           \ Location to return if no table was given.\n\n  Returns:\n    A GCP region\
           \ or multi-region.\n  \"\"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \  from google.cloud import bigquery\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel,redefined-outer-name,reimported\n\
           \n  if not table:\n    return default_location\n\n  client = bigquery.Client(project=project)\n\
           \  if table.startswith('bq://'):\n    table = table[len('bq://'):]\n  elif\
           \ table.startswith('bigquery://'):\n    table = table[len('bigquery://'):]\n\
           \  return client.get_table(table).location\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-model-evaluation-regression:
       container:
         args:
         - --setup_file
         - /setup.py
         - --json_mode
         - 'true'
@@ -2569,18 +2396,18 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"prophet-trainer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           ", "\"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, ", "\"job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\"1\",
           ", "\"machine_spec\": {\"machine_type\": \"n1-standard-4\"}, ", "\"container_spec\":
-          {\"image_uri\":\"us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125\",
+          {\"image_uri\":\"us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125\",
           ", "\"args\": [\"prophet_trainer\", \"", "--job_name=dataflow-{{$.pipeline_job_name}}\",
-          \"", "--dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125\",
-          \"", "--prediction_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/fte-prediction-server:20231029_0125\",
+          \"", "--dataflow_worker_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125\",
+          \"", "--prediction_container_image=us-docker.pkg.dev/vertex-ai/automl-tabular/fte-prediction-server:20240119_0125\",
           \"", "--artifacts_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/model/\",
           \"", "--evaluated_examples_dir=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/eval/\", \"", "--region=", "{{$.inputs.parameters[''location'']}}",
           "\", \"", "--source_bigquery_uri=", "{{$.inputs.parameters[''source_bigquery_uri'']}}",
           "\", \"", "--target_column=", "{{$.inputs.parameters[''target_column'']}}",
           "\", \"", "--time_column=", "{{$.inputs.parameters[''time_column'']}}",
           "\", \"", "--time_series_identifier_column=", "{{$.inputs.parameters[''time_series_identifier_column'']}}",
@@ -2636,15 +2463,15 @@
           \"\"\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n\
           \  import collections\n  # pylint: enable=g-import-not-at-top,import-outside-toplevel\n\
           \n  outputs = [\n      table.metadata['projectId'],\n      table.metadata['datasetId'],\n\
           \      table.metadata['tableId'],\n  ]\n  bq_uri = '.'.join(outputs)\n \
           \ if use_bq_prefix:\n    bq_uri = 'bq://' + bq_uri\n  outputs.append(bq_uri)\n\
           \  return collections.namedtuple(\n      'Outputs',\n      ['project_id',\
           \ 'dataset_id', 'table_id', 'uri'],\n  )(*outputs)\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-validate-inputs:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - validate_inputs
@@ -2738,15 +2565,15 @@
           \    f'{valid_optimization_objectives}, got: {optimization_objective}.')\n\
           \n  # Validate data granularity unit.\n  valid_data_granularity_units =\
           \ [\n      'minute', 'hour', 'day', 'week', 'month', 'year']\n  if data_granularity_unit:\n\
           \    if data_granularity_unit not in valid_data_granularity_units:\n   \
           \   raise ValueError(\n          'Granularity unit should be one of the\
           \ following: '\n          f'{valid_data_granularity_units}, got: {data_granularity_unit}.')\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-wrapped-in-list:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - wrapped_in_list
@@ -2765,15 +2592,15 @@
 
           python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
 
           '
         - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
           \ *\n\ndef wrapped_in_list(value: str) -> List[str]:\n  \"\"\"Wraps a string\
           \ in a list.\"\"\"\n  return [value]\n\n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
 pipelineInfo:
   description: Trains one Prophet model per time series.
   name: prophet-train
 root:
   dag:
     tasks:
       bigquery-delete-dataset-with-prefix:
```

## google_cloud_pipeline_components/v1/automl/forecasting/utils.py

```diff
@@ -27,62 +27,45 @@
     window_stride_length: int = -1,
     window_max_count: int = -1,
     bigquery_destination_uri: str = '',
     override_destination: bool = False,
     max_order: int = 5,
     run_evaluation: bool = True,
 ) -> Tuple[str, Dict[str, Any]]:
+  # fmt: off
   """Get the BQML ARIMA_PLUS training pipeline.
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region for Vertex AI.
     root_dir: The Cloud Storage location to store the output.
-    time_column: Name of the column that identifies time order in the time
-      series.
-    time_series_identifier_column: Name of the column that identifies the time
-      series.
+    time_column: Name of the column that identifies time order in the time series.
+    time_series_identifier_column: Name of the column that identifies the time series.
     target_column: Name of the column that the model is to predict values for.
-    forecast_horizon: The number of time periods into the future for which
-      forecasts will be created. Future periods start after the latest timestamp
-      for each time series.
-    data_granularity_unit: The data granularity unit. Accepted values are:
-      minute, hour, day, week, month, year.
+    forecast_horizon: The number of time periods into the future for which forecasts will be created. Future periods start after the latest timestamp for each time series.
+    data_granularity_unit: The data granularity unit. Accepted values are: minute, hour, day, week, month, year.
     predefined_split_key: The predefined_split column name.
     timestamp_split_key: The timestamp_split column name.
     training_fraction: The training fraction.
     validation_fraction: The validation fraction.
     test_fraction: float = The test fraction.
-    data_source_csv_filenames: A string that represents a list of comma
-      separated CSV filenames.
-    data_source_bigquery_table_path: The BigQuery table path of format
-      bq://bq_project.bq_dataset.bq_table
-    window_column: Name of the column that should be used to filter input rows.
-      The column should contain either booleans or string booleans; if the value
-      of the row is True, generate a sliding window from that row.
-    window_stride_length: Step length used to generate input examples. Every
-      window_stride_length rows will be used to generate a sliding window.
-    window_max_count: Number of rows that should be used to generate input
-      examples. If the total row count is larger than this number, the input
-      data will be randomly sampled to hit the count.
-    bigquery_destination_uri: URI of the desired destination dataset. If not
-      specified, resources will be created under a new dataset in the project.
-      Unlike in Vertex Forecasting, all resources will be given hardcoded names
-      under this dataset, and the model artifact will also be exported here.
-    override_destination: Whether to overwrite the metrics and evaluated
-      examples tables if they already exist. If this is False and the tables
-      exist, this pipeline will fail.
-    max_order: Integer between 1 and 5 representing the size of the parameter
-      search space for ARIMA_PLUS. 5 would result in the highest accuracy model,
-      but also the longest training runtime.
+    data_source_csv_filenames: A string that represents a list of comma separated CSV filenames.
+    data_source_bigquery_table_path: The BigQuery table path of format: `bq://bq_project.bq_dataset.bq_table`.
+    window_column: Name of the column that should be used to filter input rows.  The column should contain either booleans or string booleans; if the value of the row is True, generate a sliding window from that row.
+    window_stride_length: Step length used to generate input examples. Every window_stride_length rows will be used to generate a sliding window.
+    window_max_count: Number of rows that should be used to generate input examples. If the total row count is larger than this number, the input data will be randomly sampled to hit the count.
+    bigquery_destination_uri: URI of the desired destination dataset. If not specified, resources will be created under a new dataset in the project.  Unlike in Vertex Forecasting, all resources will be given hardcoded names under this dataset, and the model artifact will also be exported here.
+    override_destination: Whether to overwrite the metrics and evaluated examples tables if they already exist. If this is False and the tables exist, this pipeline will fail.
+    max_order: Integer between 1 and 5 representing the size of the parameter search space for ARIMA_PLUS. 5 would result in the highest accuracy model, but also the longest training runtime.
     run_evaluation: Whether to run evaluation steps during training.
 
   Returns:
     Tuple of pipeline_definition_path and parameter_values.
   """
+  # fmt: on
   parameter_values = {
       'project': project,
       'location': location,
       'root_dir': root_dir,
       'time_column': time_column,
       'time_series_identifier_column': time_series_identifier_column,
       'target_column': target_column,
@@ -114,33 +97,30 @@
     location: str,
     model_name: str,
     data_source_csv_filenames: str = '',
     data_source_bigquery_table_path: str = '',
     bigquery_destination_uri: str = '',
     generate_explanation: bool = False,
 ) -> Tuple[str, Dict[str, Any]]:
+  # fmt: off
   """Get the BQML ARIMA_PLUS prediction pipeline.
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region for Vertex AI.
     model_name: ARIMA_PLUS BQML model URI.
-    data_source_csv_filenames: A string that represents a list of comma
-      separated CSV filenames.
-    data_source_bigquery_table_path: The BigQuery table path of format
-      bq://bq_project.bq_dataset.bq_table
-    bigquery_destination_uri: URI of the desired destination dataset. If not
-      specified, a resource will be created under a new dataset in the project.
-    generate_explanation: Generate explanation along with the batch prediction
-      results. This will cause the batch prediction output to include
-      explanations.
+    data_source_csv_filenames: A string that represents a list of comma separated CSV filenames.
+    data_source_bigquery_table_path: The BigQuery table path of format: `bq://bq_project.bq_dataset.bq_table`.
+    bigquery_destination_uri: URI of the desired destination dataset. If not specified, a resource will be created under a new dataset in the project.
+    generate_explanation: Generate explanation along with the batch prediction results. This will cause the batch prediction output to include explanations.
 
   Returns:
     Tuple of pipeline_definition_path and parameter_values.
   """
+  # fmt: on
   parameter_values = {
       'project': project,
       'location': location,
       'model_name': model_name,
       'data_source_csv_filenames': data_source_csv_filenames,
       'data_source_bigquery_table_path': data_source_bigquery_table_path,
       'bigquery_destination_uri': bigquery_destination_uri,
@@ -180,70 +160,53 @@
     evaluation_dataflow_max_num_workers: int = 10,
     evaluation_dataflow_disk_size_gb: int = 40,
     dataflow_service_account: str = '',
     dataflow_subnetwork: str = '',
     dataflow_use_public_ips: bool = True,
     run_evaluation: bool = True,
 ) -> Tuple[str, Dict[str, Any]]:
+  # fmt: off
   """Returns Prophet train pipeline and formatted parameters.
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region for Vertex AI.
     root_dir: The Cloud Storage location to store the output.
-    time_column: Name of the column that identifies time order in the time
-      series.
-    time_series_identifier_column: Name of the column that identifies the time
-      series.
+    time_column: Name of the column that identifies time order in the time series.
+    time_series_identifier_column: Name of the column that identifies the time series.
     target_column: Name of the column that the model is to predict values for.
-    forecast_horizon: The number of time periods into the future for which
-      forecasts will be created. Future periods start after the latest timestamp
-      for each time series.
+    forecast_horizon: The number of time periods into the future for which forecasts will be created. Future periods start after the latest timestamp for each time series.
     optimization_objective: Optimization objective for the model.
-    data_granularity_unit: String representing the units of time for the time
-      column.
+    data_granularity_unit: String representing the units of time for the time column.
     predefined_split_key: The predefined_split column name.
     timestamp_split_key: The timestamp_split column name.
     training_fraction: The training fraction.
     validation_fraction: The validation fraction.
     test_fraction: float = The test fraction.
-    data_source_csv_filenames: A string that represents a list of comma
-      separated CSV filenames.
-    data_source_bigquery_table_path: The BigQuery table path of format
-      bq://bq_project.bq_dataset.bq_table
-    window_column: Name of the column that should be used to filter input rows.
-      The column should contain either booleans or string booleans; if the value
-      of the row is True, generate a sliding window from that row.
-    window_stride_length: Step length used to generate input examples. Every
-      window_stride_length rows will be used to generate a sliding window.
-    window_max_count: Number of rows that should be used to generate input
-      examples. If the total row count is larger than this number, the input
-      data will be randomly sampled to hit the count.
+    data_source_csv_filenames: A string that represents a list of comma separated CSV filenames.
+    data_source_bigquery_table_path: The BigQuery table path of format: `bq://bq_project.bq_dataset.bq_table`.
+    window_column: Name of the column that should be used to filter input rows.  The column should contain either booleans or string booleans; if the value of the row is True, generate a sliding window from that row.
+    window_stride_length: Step length used to generate input examples. Every window_stride_length rows will be used to generate a sliding window.
+    window_max_count: Number of rows that should be used to generate input examples. If the total row count is larger than this number, the input data will be randomly sampled to hit the count.
     max_num_trials: Maximum number of tuning trials to perform per time series.
     trainer_dataflow_machine_type: The dataflow machine type used for training.
-    trainer_dataflow_max_num_workers: The max number of Dataflow workers used
-      for training.
-    trainer_dataflow_disk_size_gb: Dataflow worker's disk size in GB during
-      training.
-    evaluation_dataflow_machine_type: The dataflow machine type used for
-      evaluation.
-    evaluation_dataflow_max_num_workers: The max number of Dataflow workers used
-      for evaluation.
-    evaluation_dataflow_disk_size_gb: Dataflow worker's disk size in GB during
-      evaluation.
+    trainer_dataflow_max_num_workers: The max number of Dataflow workers used for training.
+    trainer_dataflow_disk_size_gb: Dataflow worker's disk size in GB during training.
+    evaluation_dataflow_machine_type: The dataflow machine type used for evaluation.
+    evaluation_dataflow_max_num_workers: The max number of Dataflow workers used for evaluation.
+    evaluation_dataflow_disk_size_gb: Dataflow worker's disk size in GB during evaluation.
     dataflow_service_account: Custom service account to run dataflow jobs.
-    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty
-      the default subnetwork will be used.
-    dataflow_use_public_ips: Specifies whether Dataflow workers use public IP
-      addresses.
+    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used.
+    dataflow_use_public_ips: Specifies whether Dataflow workers use public IP addresses.
     run_evaluation: Whether to run evaluation steps during training.
 
   Returns:
     Tuple of pipeline_definition_path and parameter_values.
   """
+  # fmt: on
   parameter_values = {
       'project': project,
       'location': location,
       'root_dir': root_dir,
       'time_column': time_column,
       'time_series_identifier_column': time_series_identifier_column,
       'target_column': target_column,
@@ -289,43 +252,39 @@
     target_column: str,
     data_source_csv_filenames: str = '',
     data_source_bigquery_table_path: str = '',
     bigquery_destination_uri: str = '',
     machine_type: str = 'n1-standard-2',
     max_num_workers: int = 10,
 ) -> Tuple[str, Dict[str, Any]]:
+  # fmt: off
   """Returns Prophet prediction pipeline and formatted parameters.
 
   Unlike the prediction server for Vertex Forecasting, the Prophet prediction
   server returns predictions batched by time series id. This pipeline shows how
   these predictions can be disaggregated to get results similar to what Vertex
   Forecasting provides.
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region for Vertex AI.
-    model_name: The name of the Model resource, in a form of
-      projects/{project}/locations/{location}/models/{model}.
-    time_column: Name of the column that identifies time order in the time
-      series.
-    time_series_identifier_column: Name of the column that identifies the time
-      series.
+    model_name: The name of the Model resource, in a form of `projects/{project}/locations/{location}/models/{model}`.
+    time_column: Name of the column that identifies time order in the time series.
+    time_series_identifier_column: Name of the column that identifies the time series.
     target_column: Name of the column that the model is to predict values for.
-    data_source_csv_filenames: A string that represents a list of comma
-      separated CSV filenames.
-    data_source_bigquery_table_path: The BigQuery table path of format
-      bq://bq_project.bq_dataset.bq_table
-    bigquery_destination_uri: URI of the desired destination dataset. If not
-      specified, resources will be created under a new dataset in the project.
+    data_source_csv_filenames: A string that represents a list of comma separated CSV filenames.
+    data_source_bigquery_table_path: The BigQuery table path of format: `bq://bq_project.bq_dataset.bq_table`.
+    bigquery_destination_uri: URI of the desired destination dataset. If not specified, resources will be created under a new dataset in the project.
     machine_type: The machine type used for batch prediction.
     max_num_workers: The max number of workers used for batch prediction.
 
   Returns:
     Tuple of pipeline_definition_path and parameter_values.
   """
+  # fmt: on
   parameter_values = {
       'project': project,
       'location': location,
       'model_name': model_name,
       'time_column': time_column,
       'time_series_identifier_column': time_series_identifier_column,
       'target_column': target_column,
```

## google_cloud_pipeline_components/v1/automl/tabular/__init__.py

```diff
@@ -21,26 +21,28 @@
 from google_cloud_pipeline_components.v1.automl.tabular.finalizer import automl_tabular_finalizer as FinalizerOp
 from google_cloud_pipeline_components.v1.automl.tabular.infra_validator import automl_tabular_infra_validator as InfraValidatorOp
 from google_cloud_pipeline_components.v1.automl.tabular.split_materialized_data import split_materialized_data as SplitMaterializedDataOp
 from google_cloud_pipeline_components.v1.automl.tabular.stage_1_tuner import automl_tabular_stage_1_tuner as Stage1TunerOp
 from google_cloud_pipeline_components.v1.automl.tabular.stats_and_example_gen import tabular_stats_and_example_gen as StatsAndExampleGenOp
 from google_cloud_pipeline_components.v1.automl.tabular.training_configurator_and_validator import training_configurator_and_validator as TrainingConfiguratorAndValidatorOp
 from google_cloud_pipeline_components.v1.automl.tabular.transform import automl_tabular_transform as TransformOp
+from google_cloud_pipeline_components.v1.automl.tabular.utils import get_automl_tabular_pipeline_and_parameters
 from kfp import components
 
 __all__ = [
     'CvTrainerOp',
-    'InfraValidatorOp',
-    'Stage1TunerOp',
     'EnsembleOp',
-    'StatsAndExampleGenOp',
-    'TransformOp',
     'FinalizerOp',
+    'InfraValidatorOp',
     'SplitMaterializedDataOp',
+    'Stage1TunerOp',
+    'StatsAndExampleGenOp',
     'TrainingConfiguratorAndValidatorOp',
+    'TransformOp',
+    'get_automl_tabular_pipeline_and_parameters',
 ]
 
 automl_tabular_pipeline = components.load_component_from_file(
     # Note, please don't name it as `component.yaml` which will conflict with
     # the generated file.
     os.path.join(os.path.dirname(__file__), 'automl_tabular_pipeline.yaml')
 )
```

## google_cloud_pipeline_components/v1/automl/tabular/automl_tabular_pipeline.yaml

```diff
@@ -109,60 +109,51 @@
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_features:
           defaultValue: 0.0
-          description: 'Number of selected features. The number of
-
-            features to learn in the NN models.'
+          description: Number of selected features. The number of features to learn
+            in the NN models.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run Cross-validation trainer.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         execution_metrics:
           description: Core metrics in dictionary of component execution.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-cv-trainer-2:
     executorLabel: exec-automl-tabular-cv-trainer-2
     inputDefinitions:
       artifacts:
         materialized_cv_splits:
           artifactType:
@@ -197,117 +188,99 @@
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_features:
           defaultValue: 0.0
-          description: 'Number of selected features. The number of
-
-            features to learn in the NN models.'
+          description: Number of selected features. The number of features to learn
+            in the NN models.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run Cross-validation trainer.
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         execution_metrics:
           description: Core metrics in dictionary of component execution.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-ensemble:
     executorLabel: exec-automl-tabular-ensemble
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The schema of the dataset.
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
         warmup_data:
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
-          description: 'The warm up data. Ensemble component will save the
-
-            warm up data together with the model artifact, used to warm up the model
-
-            when prediction server starts.'
+          description: The warm up data. Ensemble component will save the warm up
+            data together with the model artifact, used to warm up the model when
+            prediction server starts.
           isOptional: true
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         export_additional_model_without_custom_ops:
           defaultValue: false
-          description: 'True if export
-
-            an additional model without custom TF operators to the
-
-            `model_without_custom_ops` output.'
+          description: True if export an additional model without custom TF operators
+            to the `model_without_custom_ops` output.
           isOptional: true
           parameterType: BOOLEAN
         location:
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         project:
           description: Project to run Cross-validation trainer.
@@ -345,75 +318,64 @@
         explanation_metadata:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         explanation_parameters:
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-ensemble-2:
     executorLabel: exec-automl-tabular-ensemble-2
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The schema of the dataset.
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
         warmup_data:
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
-          description: 'The warm up data. Ensemble component will save the
-
-            warm up data together with the model artifact, used to warm up the model
-
-            when prediction server starts.'
+          description: The warm up data. Ensemble component will save the warm up
+            data together with the model artifact, used to warm up the model when
+            prediction server starts.
           isOptional: true
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         export_additional_model_without_custom_ops:
           defaultValue: false
-          description: 'True if export
-
-            an additional model without custom TF operators to the
-
-            `model_without_custom_ops` output.'
+          description: True if export an additional model without custom TF operators
+            to the `model_without_custom_ops` output.
           isOptional: true
           parameterType: BOOLEAN
         location:
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         project:
           description: Project to run Cross-validation trainer.
@@ -451,75 +413,64 @@
         explanation_metadata:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         explanation_parameters:
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-ensemble-3:
     executorLabel: exec-automl-tabular-ensemble-3
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The schema of the dataset.
         instance_baseline:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The instance baseline
-
-            used to calculate explanations.'
+          description: The instance baseline used to calculate explanations.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
         tuning_result_input:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'AutoML Tabular tuning
-
-            result.'
+          description: AutoML Tabular tuning result.
         warmup_data:
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
-          description: 'The warm up data. Ensemble component will save the
-
-            warm up data together with the model artifact, used to warm up the model
-
-            when prediction server starts.'
+          description: The warm up data. Ensemble component will save the warm up
+            data together with the model artifact, used to warm up the model when
+            prediction server starts.
           isOptional: true
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         export_additional_model_without_custom_ops:
           defaultValue: false
-          description: 'True if export
-
-            an additional model without custom TF operators to the
-
-            `model_without_custom_ops` output.'
+          description: True if export an additional model without custom TF operators
+            to the `model_without_custom_ops` output.
           isOptional: true
           parameterType: BOOLEAN
         location:
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         project:
           description: Project to run Cross-validation trainer.
@@ -557,18 +508,16 @@
         explanation_metadata:
           description: The explanation parameters used by Vertex online and batch
             explanations.
           parameterType: STRUCT
         explanation_parameters:
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-finalizer:
     executorLabel: exec-automl-tabular-finalizer
     inputDefinitions:
       parameters:
         encryption_spec_key_name:
           defaultValue: ''
@@ -583,52 +532,44 @@
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
     outputDefinitions:
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-infra-validator:
     executorLabel: exec-automl-tabular-infra-validator
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-automl-tabular-infra-validator-2:
     executorLabel: exec-automl-tabular-infra-validator-2
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-automl-tabular-infra-validator-3:
     executorLabel: exec-automl-tabular-infra-validator-3
     inputDefinitions:
       artifacts:
         unmanaged_container_model:
           artifactType:
             schemaTitle: google.UnmanagedContainerModel
             schemaVersion: 0.0.1
-          description: 'google.UnmanagedContainerModel for model
-
-            to be validated.'
+          description: google.UnmanagedContainerModel for model to be validated.
   comp-automl-tabular-stage-1-tuner:
     executorLabel: exec-automl-tabular-stage-1-tuner
     inputDefinitions:
       artifacts:
         feature_ranking:
           artifactType:
             schemaTitle: system.Artifact
@@ -639,38 +580,32 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         deadline_hours:
-          description: 'Number of hours the cross-validation trainer
-
-            should run.'
+          description: Number of hours the cross-validation trainer should run.
           parameterType: NUMBER_DOUBLE
         disable_early_stopping:
           defaultValue: false
-          description: 'True if disable early stopping. Default
-
-            value is false.'
+          description: True if disable early stopping. Default value is false.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
@@ -678,87 +613,72 @@
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_features:
           defaultValue: 0.0
-          description: 'Number of selected features. The number of
-
-            features to learn in the NN models.'
+          description: Number of selected features. The number of features to learn
+            in the NN models.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run Cross-validation trainer.
           parameterType: STRING
         reduce_search_space_mode:
           defaultValue: regular
-          description: 'The reduce search space mode. Possible
-
-            values: "regular" (default), "minimal", "full".'
+          description: 'The reduce search space mode. Possible values: "regular" (default),
+            "minimal", "full".'
           isOptional: true
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distillation:
           defaultValue: false
-          description: 'True if in distillation mode. The default value
-
-            is false.'
+          description: True if in distillation mode. The default value is false.
           isOptional: true
           parameterType: BOOLEAN
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         study_spec_parameters_override:
           defaultValue: []
-          description: 'JSON study spec. E.g.,
-
-            [{"parameter_id": "model_type","categorical_value_spec": {"values":
-
-            ["nn"]}}]'
+          description: 'JSON study spec. E.g., [{"parameter_id": "model_type","categorical_value_spec":
+            {"values": ["nn"]}}]'
           isOptional: true
           parameterType: LIST
         tune_feature_selection_rate:
           defaultValue: false
           isOptional: true
           parameterType: BOOLEAN
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         execution_metrics:
           description: Core metrics in dictionary of component execution.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-stage-1-tuner-2:
     executorLabel: exec-automl-tabular-stage-1-tuner-2
     inputDefinitions:
       artifacts:
         feature_ranking:
           artifactType:
@@ -770,38 +690,32 @@
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The materialized eval split.
         materialized_train_split:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
-          description: 'The materialized train
-
-            split.'
+          description: The materialized train split.
         metadata:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The tabular example gen metadata.
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         deadline_hours:
-          description: 'Number of hours the cross-validation trainer
-
-            should run.'
+          description: Number of hours the cross-validation trainer should run.
           parameterType: NUMBER_DOUBLE
         disable_early_stopping:
           defaultValue: false
-          description: 'True if disable early stopping. Default
-
-            value is false.'
+          description: True if disable early stopping. Default value is false.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
@@ -809,87 +723,72 @@
           description: Location for running the Cross-validation trainer.
           parameterType: STRING
         num_parallel_trials:
           description: Number of parallel training trials.
           parameterType: NUMBER_INTEGER
         num_selected_features:
           defaultValue: 0.0
-          description: 'Number of selected features. The number of
-
-            features to learn in the NN models.'
+          description: Number of selected features. The number of features to learn
+            in the NN models.
           isOptional: true
           parameterType: NUMBER_INTEGER
         num_selected_trials:
-          description: 'Number of selected trials. The number of weak
-
-            learners in the final model is 5 * num_selected_trials.'
+          description: Number of selected trials. The number of weak learners in the
+            final model is 5 * num_selected_trials.
           parameterType: NUMBER_INTEGER
         project:
           description: Project to run Cross-validation trainer.
           parameterType: STRING
         reduce_search_space_mode:
           defaultValue: regular
-          description: 'The reduce search space mode. Possible
-
-            values: "regular" (default), "minimal", "full".'
+          description: 'The reduce search space mode. Possible values: "regular" (default),
+            "minimal", "full".'
           isOptional: true
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distillation:
           defaultValue: false
-          description: 'True if in distillation mode. The default value
-
-            is false.'
+          description: True if in distillation mode. The default value is false.
           isOptional: true
           parameterType: BOOLEAN
         single_run_max_secs:
           description: Max number of seconds each training trial runs.
           parameterType: NUMBER_INTEGER
         study_spec_parameters_override:
           defaultValue: []
-          description: 'JSON study spec. E.g.,
-
-            [{"parameter_id": "model_type","categorical_value_spec": {"values":
-
-            ["nn"]}}]'
+          description: 'JSON study spec. E.g., [{"parameter_id": "model_type","categorical_value_spec":
+            {"values": ["nn"]}}]'
           isOptional: true
           parameterType: LIST
         tune_feature_selection_rate:
           defaultValue: false
           isOptional: true
           parameterType: BOOLEAN
         worker_pool_specs_override_json:
           defaultValue: []
-          description: 'JSON worker pool specs. E.g.,
-
-            [{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type":
-
-            "n1-standard-16"}}]'
+          description: 'JSON worker pool specs. E.g., [{"machine_spec": {"machine_type":
+            "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]'
           isOptional: true
           parameterType: LIST
     outputDefinitions:
       artifacts:
         tuning_result_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The trained model and architectures.
       parameters:
         execution_metrics:
           description: Core metrics in dictionary of component execution.
           parameterType: STRUCT
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-transform:
     executorLabel: exec-automl-tabular-transform
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
@@ -915,54 +814,44 @@
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
           description: The train split.
       parameters:
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            dataflow jobs.'
+          description: Custom service account to run dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More
-
-            details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
@@ -999,18 +888,16 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-automl-tabular-transform-2:
     executorLabel: exec-automl-tabular-transform-2
     inputDefinitions:
       artifacts:
         dataset_schema:
           artifactType:
@@ -1036,54 +923,44 @@
           artifactType:
             schemaTitle: system.Dataset
             schemaVersion: 0.0.1
           description: The train split.
       parameters:
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            dataflow jobs.'
+          description: Custom service account to run dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More
-
-            details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
@@ -1120,18 +997,16 @@
         transform_output:
           artifactType:
             schemaTitle: system.Artifact
             schemaVersion: 0.0.1
           description: The transform output artifact.
       parameters:
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
   comp-bool-identity:
     executorLabel: exec-bool-identity
     inputDefinitions:
       parameters:
         value:
           description: Boolean value to return
@@ -8301,138 +8176,112 @@
           parameterType: STRING
         data_source_csv_filenames:
           defaultValue: ''
           isOptional: true
           parameterType: STRING
         dataflow_disk_size_gb:
           defaultValue: 40.0
-          description: 'The disk size, in gigabytes, to use
-
-            on each Dataflow worker instance. If not set, default to 40.'
+          description: The disk size, in gigabytes, to use on each Dataflow worker
+            instance. If not set, default to 40.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_machine_type:
           defaultValue: n1-standard-16
-          description: 'The machine type used for dataflow
-
-            jobs. If not set, default to n1-standard-16.'
+          description: The machine type used for dataflow jobs. If not set, default
+            to n1-standard-16.
           isOptional: true
           parameterType: STRING
         dataflow_max_num_workers:
           defaultValue: 25.0
-          description: 'The number of workers to run the
-
-            dataflow job. If not set, default to 25.'
+          description: The number of workers to run the dataflow job. If not set,
+            default to 25.
           isOptional: true
           parameterType: NUMBER_INTEGER
         dataflow_service_account:
           defaultValue: ''
-          description: 'Custom service account to run
-
-            dataflow jobs.'
+          description: Custom service account to run dataflow jobs.
           isOptional: true
           parameterType: STRING
         dataflow_subnetwork:
           defaultValue: ''
-          description: 'Dataflow''s fully qualified subnetwork
-
-            name, when empty the default subnetwork will be used. More
-
-            details:
-
-            https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
+          description: 'Dataflow''s fully qualified subnetwork name, when empty the
+            default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications'
           isOptional: true
           parameterType: STRING
         dataflow_use_public_ips:
           defaultValue: true
-          description: 'Specifies whether Dataflow
-
-            workers use public IP addresses.'
+          description: Specifies whether Dataflow workers use public IP addresses.
           isOptional: true
           parameterType: BOOLEAN
         enable_probabilistic_inference:
           defaultValue: false
           isOptional: true
           parameterType: BOOLEAN
         encryption_spec_key_name:
           defaultValue: ''
           description: Customer-managed encryption key.
           isOptional: true
           parameterType: STRING
         location:
-          description: 'Location for running dataset statistics and example
-
-            generation.'
+          description: Location for running dataset statistics and example generation.
           parameterType: STRING
         optimization_objective:
           defaultValue: ''
-          description: "Objective function the model is optimizing\ntowards. The training\
-            \ process creates a model that maximizes/minimizes\nthe value of the objective\
-            \ function over the validation set. The\nsupported optimization objectives\
-            \ depend on the prediction type. If the\nfield is not set, a default objective\
-            \ function is used.\n  classification: \"maximize-au-roc\" (default) -\
-            \ Maximize the\n    area under the receiver operating characteristic (ROC)\
-            \ curve.\n    \"minimize-log-loss\" - Minimize log loss. \"maximize-au-prc\"\
-            \ -\n    Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\"\
-            \ - Maximize precision for a specified\n    recall value. \"maximize-recall-at-precision\"\
-            \ - Maximize recall for a\n    specified precision value.\n  classification\
-            \ (multi-class): \"minimize-log-loss\" (default) - Minimize\n    log loss.\n\
-            \  regression: \"minimize-rmse\" (default) - Minimize root-mean-squared\n\
-            \    error (RMSE). \"minimize-mae\" - Minimize mean-absolute error (MAE).\n\
-            \    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE)."
+          description: 'Objective function the model is optimizing towards. The training
+            process creates a model that maximizes/minimizes the value of the objective
+            function over the validation set. The supported optimization objectives
+            depend on the prediction type. If the field is not set, a default objective
+            function is used.  classification: "maximize-au-roc" (default) - Maximize
+            the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss"
+            - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall
+            curve.  "maximize-precision-at-recall" - Maximize precision for a specified
+            recall value. "maximize-recall-at-precision" - Maximize recall for a specified
+            precision value.  classification (multi-class): "minimize-log-loss" (default)
+            - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize
+            root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute
+            error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error
+            (RMSLE).'
           isOptional: true
           parameterType: STRING
         optimization_objective_precision_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-recall-at-precision". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-recall-at-precision".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         optimization_objective_recall_value:
           defaultValue: -1.0
-          description: 'Required when
-
-            optimization_objective is "maximize-precision-at-recall". Must be
-
-            between 0 and 1, inclusive.'
+          description: Required when optimization_objective is "maximize-precision-at-recall".
+            Must be between 0 and 1, inclusive.
           isOptional: true
           parameterType: NUMBER_DOUBLE
         predefined_split_key:
           defaultValue: ''
           isOptional: true
           parameterType: STRING
         prediction_type:
-          description: 'The prediction type. Supported values:
-
-            "classification", "regression".'
+          description: 'The prediction type. Supported values: "classification", "regression".'
           parameterType: STRING
         project:
-          description: 'Project to run dataset statistics and example
-
-            generation.'
+          description: Project to run dataset statistics and example generation.
           parameterType: STRING
         quantiles:
           defaultValue: []
           isOptional: true
           parameterType: LIST
         request_type:
           defaultValue: COLUMN_STATS_ONLY
           isOptional: true
           parameterType: STRING
         root_dir:
           description: The Cloud Storage location to store the output.
           parameterType: STRING
         run_distillation:
           defaultValue: false
-          description: 'True if in distillation mode. The default value
-
-            is false.'
+          description: True if in distillation mode. The default value is false.
           isOptional: true
           parameterType: BOOLEAN
         stratified_split_key:
           defaultValue: ''
           isOptional: true
           parameterType: STRING
         target_column_name:
@@ -8447,29 +8296,22 @@
           isOptional: true
           parameterType: STRING
         training_fraction:
           defaultValue: -1.0
           isOptional: true
           parameterType: NUMBER_DOUBLE
         transformations:
-          description: 'Quote escaped JSON string for transformations. Each
-
-            transformation will apply transform function to given input column. And
-
-            the result will be used for training. When creating transformation for
-
-            BigQuery Struct column, the column should be flattened using "." as the
-
-            delimiter.'
+          description: Quote escaped JSON string for transformations. Each transformation
+            will apply transform function to given input column. And the result will
+            be used for training. When creating transformation for BigQuery Struct
+            column, the column should be flattened using "." as the delimiter.
           parameterType: STRING
         transformations_path:
           defaultValue: ''
-          description: 'Path to a GCS file containing JSON
-
-            string for transformations.'
+          description: Path to a GCS file containing JSON string for transformations.
           isOptional: true
           parameterType: STRING
         validation_fraction:
           defaultValue: -1.0
           isOptional: true
           parameterType: NUMBER_DOUBLE
         weight_column_name:
@@ -8515,18 +8357,16 @@
             schemaVersion: 0.0.1
           description: The train split.
       parameters:
         downsampled_test_split_json:
           description: The downsampled test split JSON object.
           parameterType: LIST
         gcp_resources:
-          description: 'GCP resources created by this component. For more details,
-            see
-
-            https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.'
+          description: GCP resources created by this component. For more details,
+            see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.
           parameterType: STRING
         test_split_json:
           description: The test split JSON object.
           parameterType: LIST
   comp-write-bp-result-path:
     executorLabel: exec-write-bp-result-path
     inputDefinitions:
@@ -8571,17 +8411,17 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-cv-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"l2l_cv_tuner\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
-          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125",
+          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125",
           "\", \"--component_id={{$.pipeline_task_uuid}}\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--valid_trials_completed_threshold=0.7\", \"--num_selected_trials=",
           "{{$.inputs.parameters[''num_selected_trials'']}}", "\", \"--num_selected_features=",
@@ -8614,17 +8454,17 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-cv-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"l2l_cv_tuner\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
-          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125",
+          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125",
           "\", \"--component_id={{$.pipeline_task_uuid}}\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
           "\", \"--single_run_max_secs=", "{{$.inputs.parameters[''single_run_max_secs'']}}",
           "\", \"--deadline_hours=", "{{$.inputs.parameters[''deadline_hours'']}}",
           "\", \"--valid_trials_completed_threshold=0.7\", \"--num_selected_trials=",
           "{{$.inputs.parameters[''num_selected_trials'']}}", "\", \"--num_selected_features=",
@@ -8657,27 +8497,27 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-highmem-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"ensemble\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--model_output_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model\",
           \"--custom_model_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/custom_model\", \"--error_file_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--export_custom_model=", "{{$.inputs.parameters[''export_additional_model_without_custom_ops'']}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--dataset_schema_path=", "{{$.inputs.artifacts[''dataset_schema''].uri}}",
           "\", \"--tuning_result_input_path=", "{{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "\", \"--instance_baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--warmup_data=", "{{$.inputs.artifacts[''warmup_data''].uri}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--model_path=", "{{$.outputs.artifacts[''model''].uri}}", "\", \"--custom_model_path=",
           "{{$.outputs.artifacts[''model_without_custom_ops''].uri}}", "\", \"--explanation_metadata_path=",
           "{{$.outputs.parameters[''explanation_metadata''].output_file}}", ",", "{{$.outputs.artifacts[''explanation_metadata_artifact''].uri}}",
           "\", \"--explanation_parameters_path=", "{{$.outputs.parameters[''explanation_parameters''].output_file}}",
           "\", \"--model_architecture_path=", "{{$.outputs.artifacts[''model_architecture''].uri}}",
           "\", \"--use_json=true\", \"--executor_input={{$.json_escape[1]}}\"]}}]}}"]}'
         command:
@@ -8698,27 +8538,27 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-highmem-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"ensemble\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--model_output_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model\",
           \"--custom_model_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/custom_model\", \"--error_file_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--export_custom_model=", "{{$.inputs.parameters[''export_additional_model_without_custom_ops'']}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--dataset_schema_path=", "{{$.inputs.artifacts[''dataset_schema''].uri}}",
           "\", \"--tuning_result_input_path=", "{{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "\", \"--instance_baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--warmup_data=", "{{$.inputs.artifacts[''warmup_data''].uri}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--model_path=", "{{$.outputs.artifacts[''model''].uri}}", "\", \"--custom_model_path=",
           "{{$.outputs.artifacts[''model_without_custom_ops''].uri}}", "\", \"--explanation_metadata_path=",
           "{{$.outputs.parameters[''explanation_metadata''].output_file}}", ",", "{{$.outputs.artifacts[''explanation_metadata_artifact''].uri}}",
           "\", \"--explanation_parameters_path=", "{{$.outputs.parameters[''explanation_parameters''].output_file}}",
           "\", \"--model_architecture_path=", "{{$.outputs.artifacts[''model_architecture''].uri}}",
           "\", \"--use_json=true\", \"--executor_input={{$.json_escape[1]}}\"]}}]}}"]}'
         command:
@@ -8739,27 +8579,27 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-ensemble-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-highmem-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"ensemble\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
           "\", \"--model_output_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/model\",
           \"--custom_model_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/custom_model\", \"--error_file_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--export_custom_model=", "{{$.inputs.parameters[''export_additional_model_without_custom_ops'']}}",
           "\", \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\",
           \"--dataset_schema_path=", "{{$.inputs.artifacts[''dataset_schema''].uri}}",
           "\", \"--tuning_result_input_path=", "{{$.inputs.artifacts[''tuning_result_input''].uri}}",
           "\", \"--instance_baseline_path=", "{{$.inputs.artifacts[''instance_baseline''].uri}}",
           "\", \"--warmup_data=", "{{$.inputs.artifacts[''warmup_data''].uri}}", "\",
-          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125",
+          \"--prediction_docker_uri=", "us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125",
           "\", \"--model_path=", "{{$.outputs.artifacts[''model''].uri}}", "\", \"--custom_model_path=",
           "{{$.outputs.artifacts[''model_without_custom_ops''].uri}}", "\", \"--explanation_metadata_path=",
           "{{$.outputs.parameters[''explanation_metadata''].output_file}}", ",", "{{$.outputs.artifacts[''explanation_metadata_artifact''].uri}}",
           "\", \"--explanation_parameters_path=", "{{$.outputs.parameters[''explanation_parameters''].output_file}}",
           "\", \"--model_architecture_path=", "{{$.outputs.artifacts[''model_architecture''].uri}}",
           "\", \"--use_json=true\", \"--executor_input={{$.json_escape[1]}}\"]}}]}}"]}'
         command:
@@ -8780,48 +8620,48 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-finalizer-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"cancel_l2l_tuner\", \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\", \"--cleanup_lro_job_infos=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\"]}}]}}"]}'
         command:
         - python3
         - -u
         - -m
         - google_cloud_pipeline_components.container.v1.custom_job.launcher
         image: gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.44
     exec-automl-tabular-infra-validator:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-automl-tabular-infra-validator-2:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-automl-tabular-infra-validator-3:
       container:
         args:
         - --executor_input
         - '{{$}}'
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125
         resources:
           cpuLimit: 8.0
           memoryLimit: 52.0
     exec-automl-tabular-stage-1-tuner:
       container:
         args:
         - --type
@@ -8833,17 +8673,17 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-stage-1-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"l2l_stage_1_tuner\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
-          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125",
+          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125",
           "\", \"--feature_selection_result_path=", "{{$.inputs.artifacts[''feature_ranking''].uri}}",
           "\", \"--disable_early_stopping=", "{{$.inputs.parameters[''disable_early_stopping'']}}",
           "\", \"--tune_feature_selection_rate=", "{{$.inputs.parameters[''tune_feature_selection_rate'']}}",
           "\", \"--reduce_search_space_mode=", "{{$.inputs.parameters[''reduce_search_space_mode'']}}",
           "\", \"--component_id={{$.pipeline_task_uuid}}\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
@@ -8880,17 +8720,17 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-stage-1-tuner-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"l2l_stage_1_tuner\", \"--transform_output_path=", "{{$.inputs.artifacts[''transform_output''].uri}}",
-          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125",
+          "\", \"--training_docker_uri=", "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125",
           "\", \"--feature_selection_result_path=", "{{$.inputs.artifacts[''feature_ranking''].uri}}",
           "\", \"--disable_early_stopping=", "{{$.inputs.parameters[''disable_early_stopping'']}}",
           "\", \"--tune_feature_selection_rate=", "{{$.inputs.parameters[''tune_feature_selection_rate'']}}",
           "\", \"--reduce_search_space_mode=", "{{$.inputs.parameters[''reduce_search_space_mode'']}}",
           "\", \"--component_id={{$.pipeline_task_uuid}}\", \"--training_base_dir=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/train\",
           \"--num_parallel_trial=", "{{$.inputs.parameters[''num_parallel_trials'']}}",
@@ -8927,15 +8767,15 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-transform-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"transform\", \"--is_mp=true\", \"--transform_output_artifact_path=",
           "{{$.outputs.artifacts[''transform_output''].uri}}", "\", \"--transform_output_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/transform\",
           \"--materialized_splits_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/transform_materialized\",
           \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\", \"--dataset_schema_path=",
           "{{$.inputs.artifacts[''dataset_schema''].uri}}", "\", \"--train_split=",
@@ -8948,15 +8788,15 @@
           "\", \"--job_name=automl-tabular-transform-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "\", \"--dataflow_project=", "{{$.inputs.parameters[''project'']}}", "\",
           \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging\",
           \"--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp\",
           \"--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}",
           "\", \"--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}",
-          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125",
+          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125",
           "\", \"--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}",
           "\", \"--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}",
           "\", \"--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}",
           "\", \"--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\", \"--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}",
           "\", \"--lro_job_info=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\",
           \"--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}",
@@ -8979,15 +8819,15 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"automl-tabular-transform-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"transform\", \"--is_mp=true\", \"--transform_output_artifact_path=",
           "{{$.outputs.artifacts[''transform_output''].uri}}", "\", \"--transform_output_path=",
           "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/transform\",
           \"--materialized_splits_output_path=", "{{$.inputs.parameters[''root_dir'']}}",
           "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/transform_materialized\",
           \"--metadata_path=", "{{$.inputs.artifacts[''metadata''].uri}}", "\", \"--dataset_schema_path=",
           "{{$.inputs.artifacts[''dataset_schema''].uri}}", "\", \"--train_split=",
@@ -9000,15 +8840,15 @@
           "\", \"--job_name=automl-tabular-transform-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "\", \"--dataflow_project=", "{{$.inputs.parameters[''project'']}}", "\",
           \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging\",
           \"--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp\",
           \"--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}",
           "\", \"--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}",
-          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125",
+          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125",
           "\", \"--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}",
           "\", \"--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}",
           "\", \"--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}",
           "\", \"--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\", \"--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}",
           "\", \"--lro_job_info=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/lro\",
           \"--gcp_resources_path=", "{{$.outputs.parameters[''gcp_resources''].output_file}}",
@@ -10232,15 +10072,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-model-upload-2:
       container:
         args:
         - --type
         - UploadModel
         - --payload
         - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
@@ -10261,15 +10101,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-model-upload-3:
       container:
         args:
         - --type
         - UploadModel
         - --payload
         - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
@@ -10290,15 +10130,15 @@
         - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
           "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
         command:
         - python3
         - -u
         - -m
         - launcher
-        image: gcr.io/ml-pipeline/automl-tables-private:1.0.15
+        image: gcr.io/ml-pipeline/automl-tables-private:1.0.17
     exec-read-input-uri:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _read_input_uri
@@ -10415,15 +10255,15 @@
           \    raise ValueError(\n        'One of vertex_dataset, data_source_csv_filenames,'\n\
           \        ' data_source_bigquery_table_path must be specified'\n    )\n\n\
           \  return collections.namedtuple(\n      'Outputs',\n      [\n         \
           \ 'data_source_csv_filenames',\n          'data_source_bigquery_table_path',\n\
           \          'model_display_name',\n      ],\n  )(\n      data_source_csv_filenames,\n\
           \      data_source_bigquery_table_path,\n      model_display_name,\n  )\n\
           \n"
-        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20231029_0125
+        image: us-docker.pkg.dev/vertex-ai/automl-tabular/kfp-v2-base:20240119_0125
     exec-string-not-empty:
       container:
         args:
         - --executor_input
         - '{{$}}'
         - --function_to_execute
         - _string_not_empty
@@ -10462,15 +10302,15 @@
         - --gcp_resources
         - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
         - --payload
         - '{"Concat": ["{\"display_name\": \"tabular-stats-and-example-gen-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}\",
           \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\"}, \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\": 1, \"machine_spec\":
           {\"machine_type\": \"n1-standard-8\"}, \"container_spec\": {\"image_uri\":\"",
-          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125", "\",
+          "us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125", "\",
           \"args\": [\"stats_generator\",", "\"--train_spec={\\\"prediction_type\\\":
           \\\"", "{{$.inputs.parameters[''prediction_type'']}}", "\\\", \\\"target_column\\\":
           \\\"", "{{$.inputs.parameters[''target_column_name'']}}", "\\\", \\\"optimization_objective\\\":
           \\\"", "{{$.inputs.parameters[''optimization_objective'']}}", "\\\", \\\"weight_column_name\\\":
           \\\"", "{{$.inputs.parameters[''weight_column_name'']}}", "\\\", \\\"transformations\\\":
           ", "{{$.inputs.parameters[''transformations'']}}", ", \\\"quantiles\\\":
           ", "{{$.inputs.parameters[''quantiles'']}}", ", \\\"enable_probabilistic_inference\\\":
@@ -10495,15 +10335,15 @@
           "\", \"--dataset_schema_path=", "{{$.outputs.artifacts[''dataset_schema''].uri}}",
           "\", \"--job_name=tabular-stats-and-example-gen-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}",
           "\", \"--dataflow_project=", "{{$.inputs.parameters[''project'']}}", "\",
           \"--error_file_path=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/error.pb\",
           \"--dataflow_staging_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_staging\",
           \"--dataflow_tmp_dir=", "{{$.inputs.parameters[''root_dir'']}}", "/{{$.pipeline_job_uuid}}/{{$.pipeline_task_uuid}}/dataflow_tmp\",
           \"--dataflow_max_num_workers=", "{{$.inputs.parameters[''dataflow_max_num_workers'']}}",
-          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125",
+          "\", \"--dataflow_worker_container_image=", "us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125",
           "\", \"--dataflow_machine_type=", "{{$.inputs.parameters[''dataflow_machine_type'']}}",
           "\", \"--dataflow_disk_size_gb=", "{{$.inputs.parameters[''dataflow_disk_size_gb'']}}",
           "\", \"--dataflow_kms_key=", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
           "\", \"--dataflow_subnetwork_fully_qualified=", "{{$.inputs.parameters[''dataflow_subnetwork'']}}",
           "\", \"--dataflow_use_public_ips=", "{{$.inputs.parameters[''dataflow_use_public_ips'']}}",
           "\", \"--dataflow_service_account=", "{{$.inputs.parameters[''dataflow_service_account'']}}",
           "\", \"--is_distill=", "{{$.inputs.parameters[''run_distillation'']}}",
```

## google_cloud_pipeline_components/v1/automl/tabular/cv_trainer.py

```diff
@@ -95,19 +95,19 @@
                   ),
                   encryption_spec_key_name,
                   (
                       '"}, "job_spec": {"worker_pool_specs": [{"replica_count":'
                       ' 1, "machine_spec": {"machine_type": "n1-standard-8"},'
                       ' "container_spec": {"image_uri":"'
                   ),
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125',
                   '", "args": ["l2l_cv_tuner", "--transform_output_path=',
                   transform_output.uri,
                   '", "--training_docker_uri=',
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125',
                   (
                       f'", "--component_id={dsl.PIPELINE_TASK_ID_PLACEHOLDER}",'
                       ' "--training_base_dir='
                   ),
                   root_dir,
                   (
                       f'/{dsl.PIPELINE_JOB_ID_PLACEHOLDER}/{dsl.PIPELINE_TASK_ID_PLACEHOLDER}/train",'
```

## google_cloud_pipeline_components/v1/automl/tabular/ensemble.py

```diff
@@ -102,15 +102,15 @@
                   ),
                   encryption_spec_key_name,
                   (
                       '"}, "job_spec": {"worker_pool_specs": [{"replica_count":'
                       ' 1, "machine_spec": {"machine_type": "n1-highmem-8"},'
                       ' "container_spec": {"image_uri":"'
                   ),
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125',
                   '", "args": ["ensemble", "--transform_output_path=',
                   transform_output.uri,
                   '", "--model_output_path=',
                   root_dir,
                   (
                       f'/{dsl.PIPELINE_JOB_ID_PLACEHOLDER}/{dsl.PIPELINE_TASK_ID_PLACEHOLDER}/model",'
                       ' "--custom_model_output_path='
@@ -133,15 +133,15 @@
                   '", "--tuning_result_input_path=',
                   tuning_result_input.uri,
                   '", "--instance_baseline_path=',
                   instance_baseline.uri,
                   '", "--warmup_data=',
                   warmup_data.uri,
                   '", "--prediction_docker_uri=',
-                  'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125',
                   '", "--model_path=',
                   model.uri,
                   '", "--custom_model_path=',
                   model_without_custom_ops.uri,
                   '", "--explanation_metadata_path=',
                   explanation_metadata,
                   ',',
```

## google_cloud_pipeline_components/v1/automl/tabular/finalizer.py

```diff
@@ -68,15 +68,15 @@
                   ),
                   encryption_spec_key_name,
                   (
                       '"}, "job_spec": {"worker_pool_specs": [{"replica_count":'
                       ' 1, "machine_spec": {"machine_type": "n1-standard-8"},'
                       ' "container_spec": {"image_uri":"'
                   ),
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125',
                   '", "args": ["cancel_l2l_tuner", "--error_file_path=',
                   root_dir,
                   (
                       f'/{dsl.PIPELINE_JOB_ID_PLACEHOLDER}/{dsl.PIPELINE_TASK_ID_PLACEHOLDER}/error.pb",'
                       ' "--cleanup_lro_job_infos='
                   ),
                   root_dir,
```

## google_cloud_pipeline_components/v1/automl/tabular/infra_validator.py

```diff
@@ -28,11 +28,11 @@
 
   Args:
       unmanaged_container_model: google.UnmanagedContainerModel for model to be validated.
   """
   # fmt: on
 
   return dsl.ContainerSpec(
-      image='us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20231029_0125',
+      image='us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:20240119_0125',
       command=[],
       args=['--executor_input', '{{$}}'],
   )
```

## google_cloud_pipeline_components/v1/automl/tabular/split_materialized_data.py

```diff
@@ -48,15 +48,15 @@
       materialized_train_split: Path patern to materialized train split.
       materialized_eval_split: Path patern to materialized eval split.
       materialized_test_split: Path patern to materialized test split.
   """
   # fmt: on
 
   return dsl.ContainerSpec(
-      image='us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125',
+      image='us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125',
       command=[
           'sh',
           '-ec',
           (
               'program_path=$(mktemp -d)\nprintf "%s" "$0" >'
               ' "$program_path/ephemeral_component.py"\npython3 -m'
               ' kfp.components.executor_main                        '
```

## google_cloud_pipeline_components/v1/automl/tabular/stage_1_tuner.py

```diff
@@ -53,16 +53,15 @@
 
   Args:
       project: Project to run Cross-validation trainer.
       location: Location for running the Cross-validation trainer.
       root_dir: The Cloud Storage location to store the output.
       study_spec_parameters_override: JSON study spec. E.g., [{"parameter_id": "model_type","categorical_value_spec": {"values": ["nn"]}}]
       worker_pool_specs_override_json: JSON worker pool specs. E.g., [{"machine_spec": {"machine_type": "n1-standard-16"}},{},{},{"machine_spec": {"machine_type": "n1-standard-16"}}]
-      reduce_search_space_mode: The reduce search space mode. Possible
-        values: "regular" (default), "minimal", "full".
+      reduce_search_space_mode: The reduce search space mode. Possible values: "regular" (default), "minimal", "full".
       num_selected_trials: Number of selected trials. The number of weak learners in the final model is 5 * num_selected_trials.
       num_selected_features: Number of selected features. The number of features to learn in the NN models.
       deadline_hours: Number of hours the cross-validation trainer should run.
       disable_early_stopping: True if disable early stopping. Default value is false.
       num_parallel_trials: Number of parallel training trials.
       single_run_max_secs: Max number of seconds each training trial runs.
       metadata: The tabular example gen metadata.
@@ -106,19 +105,19 @@
                   ),
                   encryption_spec_key_name,
                   (
                       '"}, "job_spec": {"worker_pool_specs": [{"replica_count":'
                       ' 1, "machine_spec": {"machine_type": "n1-standard-8"},'
                       ' "container_spec": {"image_uri":"'
                   ),
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125',
                   '", "args": ["l2l_stage_1_tuner", "--transform_output_path=',
                   transform_output.uri,
                   '", "--training_docker_uri=',
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125',
                   '", "--feature_selection_result_path=',
                   feature_ranking.uri,
                   '", "--disable_early_stopping=',
                   disable_early_stopping,
                   '", "--tune_feature_selection_rate=',
                   tune_feature_selection_rate,
                   '", "--reduce_search_space_mode=',
```

## google_cloud_pipeline_components/v1/automl/tabular/stats_and_example_gen.py

```diff
@@ -73,26 +73,23 @@
   Args:
       project: Project to run dataset statistics and example generation.
       location: Location for running dataset statistics and example generation.
       root_dir: The Cloud Storage location to store the output.
       target_column_name: The target column name.
       weight_column_name: The weight column name.
       prediction_type: The prediction type. Supported values: "classification", "regression".
-      optimization_objective: Objective function the model is optimizing towards. The training process creates a model that maximizes/minimizes the value of the objective function over the validation set. The supported optimization objectives depend on the prediction type. If the field is not set, a default objective function is used.
-          classification: "maximize-au-roc" (default) - Maximize the area under the receiver operating characteristic (ROC) curve. "minimize-log-loss" - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall curve. "maximize-precision-at-recall" - Maximize precision for a specified recall value. "maximize-recall-at-precision" - Maximize recall for a specified precision value. classification (multi-class): "minimize-log-loss" (default) - Minimize log loss.
-          regression: "minimize-rmse" (default) - Minimize root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute error (MAE). "minimize-rmsle" - Minimize root-mean-squared log error (RMSLE).
+      optimization_objective: Objective function the model is optimizing towards. The training process creates a model that maximizes/minimizes the value of the objective function over the validation set. The supported optimization objectives depend on the prediction type. If the field is not set, a default objective function is used.  classification: "maximize-au-roc" (default) - Maximize the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss" - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall curve.  "maximize-precision-at-recall" - Maximize precision for a specified recall value. "maximize-recall-at-precision" - Maximize recall for a specified precision value.  classification (multi-class): "minimize-log-loss" (default) - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error (RMSLE).
       optimization_objective_recall_value: Required when optimization_objective is "maximize-precision-at-recall". Must be between 0 and 1, inclusive.
       optimization_objective_precision_value: Required when optimization_objective is "maximize-recall-at-precision". Must be between 0 and 1, inclusive.
       transformations: Quote escaped JSON string for transformations. Each transformation will apply transform function to given input column. And the result will be used for training. When creating transformation for BigQuery Struct column, the column should be flattened using "." as the delimiter.
       transformations_path: Path to a GCS file containing JSON string for transformations.
       dataflow_machine_type: The machine type used for dataflow jobs. If not set, default to n1-standard-16.
       dataflow_max_num_workers: The number of workers to run the dataflow job. If not set, default to 25.
       dataflow_disk_size_gb: The disk size, in gigabytes, to use on each Dataflow worker instance. If not set, default to 40.
-      dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used. More
-        details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
+      dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
       dataflow_use_public_ips: Specifies whether Dataflow workers use public IP addresses.
       dataflow_service_account: Custom service account to run dataflow jobs.
       encryption_spec_key_name: Customer-managed encryption key.
       run_distillation: True if in distillation mode. The default value is false.
 
   Returns:
       dataset_schema: The schema of the dataset.
@@ -135,15 +132,15 @@
                   ),
                   encryption_spec_key_name,
                   (
                       '"}, "job_spec": {"worker_pool_specs": [{"replica_count":'
                       ' 1, "machine_spec": {"machine_type": "n1-standard-8"},'
                       ' "container_spec": {"image_uri":"'
                   ),
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125',
                   '", "args": ["stats_generator",',
                   '"--train_spec={\\"prediction_type\\": \\"',
                   prediction_type,
                   '\\", \\"target_column\\": \\"',
                   target_column_name,
                   '\\", \\"optimization_objective\\": \\"',
                   optimization_objective,
@@ -214,15 +211,15 @@
                   root_dir,
                   (
                       f'/{dsl.PIPELINE_JOB_ID_PLACEHOLDER}/{dsl.PIPELINE_TASK_ID_PLACEHOLDER}/dataflow_tmp",'
                       ' "--dataflow_max_num_workers='
                   ),
                   dataflow_max_num_workers,
                   '", "--dataflow_worker_container_image=',
-                  'us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125',
                   '", "--dataflow_machine_type=',
                   dataflow_machine_type,
                   '", "--dataflow_disk_size_gb=',
                   dataflow_disk_size_gb,
                   '", "--dataflow_kms_key=',
                   encryption_spec_key_name,
                   '", "--dataflow_subnetwork_fully_qualified=',
```

## google_cloud_pipeline_components/v1/automl/tabular/training_configurator_and_validator.py

```diff
@@ -61,24 +61,22 @@
       dataset_stats: Dataset stats generated by feature transform engine.
       split_example_counts: JSON string of data split example counts for train, validate, and test splits.
       training_schema_path: Schema of input data to the tf_model at training time.
       instance_schema: Schema of input data to the tf_model at serving time.
       target_column: Target column of input data.
       weight_column: Weight column of input data.
       prediction_type: Model prediction type. One of "classification", "regression", "time_series".
-      optimization_objective: Objective function the model is optimizing towards. The training process creates a model that maximizes/minimizes the value of the objective function over the validation set. The supported optimization objectives depend on the prediction type. If the field is not set, a default objective function is used.
-          classification: "maximize-au-roc" (default) - Maximize the area under the receiver operating characteristic (ROC) curve. "minimize-log-loss" - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall curve. "maximize-precision-at-recall" - Maximize precision for a specified recall value. "maximize-recall-at-precision" - Maximize recall for a specified precision value. classification (multi-class): "minimize-log-loss" (default) - Minimize log loss.
-          regression: "minimize-rmse" (default) - Minimize root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute error (MAE). "minimize-rmsle" - Minimize root-mean-squared log error (RMSLE).
+      optimization_objective: Objective function the model is optimizing towards. The training process creates a model that maximizes/minimizes the value of the objective function over the validation set. The supported optimization objectives depend on the prediction type. If the field is not set, a default objective function is used.  classification: "maximize-au-roc" (default) - Maximize the area under the receiver operating characteristic (ROC) curve.  "minimize-log-loss" - Minimize log loss. "maximize-au-prc" - Maximize the area under the precision-recall curve.  "maximize-precision-at-recall" - Maximize precision for a specified recall value. "maximize-recall-at-precision" - Maximize recall for a specified precision value.  classification (multi-class): "minimize-log-loss" (default) - Minimize log loss.  regression: "minimize-rmse" (default) - Minimize root-mean-squared error (RMSE). "minimize-mae" - Minimize mean-absolute error (MAE).  "minimize-rmsle" - Minimize root-mean-squared log error (RMSLE).
       optimization_objective_recall_value: Required when optimization_objective is "maximize-precision-at-recall". Must be between 0 and 1, inclusive.
       optimization_objective_precision_value: Required when optimization_objective is "maximize-recall-at-precision". Must be between 0 and 1, inclusive.
       run_evaluation: Whether we are running evaluation in the training pipeline.
       run_distill: Whether the distillation should be applied to the training.
-      enable_probabilistic_inference: If probabilistic inference is enabled, the model will fit a distribution that captures the uncertainty of a prediction. At inference time, the predictive distribution is used to make a point prediction that minimizes the optimization objective. For example, the mean of a predictive distribution is the point prediction that minimizes RMSE loss. If quantiles are specified, then the quantiles of the distribution are also returned.
+      enable_probabilistic_inference: If probabilistic inference is enabled, the model will fit a distribution that captures the uncertainty of a prediction. At inference time, the predictive distribution is used to make a point prediction that minimizes the optimization objective.  For example, the mean of a predictive distribution is the point prediction that minimizes RMSE loss. If quantiles are specified, then the quantiles of the distribution are also returned.
       time_series_identifier_column: [Deprecated] The time series identifier column. Used by forecasting only. Raises exception if used - use the "time_series_identifier_column" field instead.
-      time_series_identifier_columns: The list of time series identifier columns. Used by forecasting only.
+      time_series_identifier_columns: The list of time series identifier columns.  Used by forecasting only.
       time_column: The column that indicates the time. Used by forecasting only.
       time_series_attribute_columns: The column names of the time series attributes.
       available_at_forecast_columns: The names of the columns that are available at forecast time.
       unavailable_at_forecast_columns: The names of the columns that are not available at forecast time.
       quantiles: All quantiles that the model need to predict.
       context_window: The length of the context window.
       forecast_horizon: The length of the forecast horizon.
@@ -93,15 +91,15 @@
 
   Returns:
       metadata: The tabular example gen metadata.
   """
   # fmt: on
 
   return dsl.ContainerSpec(
-      image='us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20231029_0125',
+      image='us-docker.pkg.dev/vertex-ai/automl-tabular/feature-transform-engine:20240119_0125',
       command=[],
       args=[
           'training_configurator_and_validator',
           dsl.ConcatPlaceholder(
               items=['--instance_schema_path=', instance_schema.uri]
           ),
           dsl.ConcatPlaceholder(
```

## google_cloud_pipeline_components/v1/automl/tabular/transform.py

```diff
@@ -58,16 +58,15 @@
       dataset_schema: The schema of the dataset.
       train_split: The train split.
       eval_split: The eval split.
       test_split: The test split.
       dataflow_machine_type: The machine type used for dataflow jobs. If not set, default to n1-standard-16.
       dataflow_max_num_workers: The number of workers to run the dataflow job. If not set, default to 25.
       dataflow_disk_size_gb: The disk size, in gigabytes, to use on each Dataflow worker instance. If not set, default to 40.
-      dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used. More
-        details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
+      dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used. More details: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
       dataflow_use_public_ips: Specifies whether Dataflow workers use public IP addresses.
       dataflow_service_account: Custom service account to run dataflow jobs.
       encryption_spec_key_name: Customer-managed encryption key.
 
   Returns:
       materialized_train_split: The materialized train split.
       materialized_eval_split: The materialized eval split.
@@ -105,15 +104,15 @@
                   ),
                   encryption_spec_key_name,
                   (
                       '"}, "job_spec": {"worker_pool_specs": [{"replica_count":'
                       ' 1, "machine_spec": {"machine_type": "n1-standard-8"},'
                       ' "container_spec": {"image_uri":"'
                   ),
-                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai-restricted/automl-tabular/training:20240119_0125',
                   (
                       '", "args": ["transform", "--is_mp=true",'
                       ' "--transform_output_artifact_path='
                   ),
                   transform_output.uri,
                   '", "--transform_output_path=',
                   root_dir,
@@ -164,15 +163,15 @@
                       f'/{dsl.PIPELINE_JOB_ID_PLACEHOLDER}/{dsl.PIPELINE_TASK_ID_PLACEHOLDER}/dataflow_tmp",'
                       ' "--dataflow_max_num_workers='
                   ),
                   dataflow_max_num_workers,
                   '", "--dataflow_machine_type=',
                   dataflow_machine_type,
                   '", "--dataflow_worker_container_image=',
-                  'us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20231029_0125',
+                  'us-docker.pkg.dev/vertex-ai/automl-tabular/dataflow-worker:20240119_0125',
                   '", "--dataflow_disk_size_gb=',
                   dataflow_disk_size_gb,
                   '", "--dataflow_subnetwork_fully_qualified=',
                   dataflow_subnetwork,
                   '", "--dataflow_use_public_ips=',
                   dataflow_use_public_ips,
                   '", "--dataflow_kms_key=',
```

## google_cloud_pipeline_components/v1/automl/tabular/utils.py

```diff
@@ -460,128 +460,81 @@
     stage_1_tuning_result_artifact_uri: Optional[str] = None,
     quantiles: Optional[List[float]] = None,
     enable_probabilistic_inference: bool = False,
     num_selected_features: Optional[int] = None,
     model_display_name: str = '',
     model_description: str = '',
 ) -> Tuple[str, Dict[str, Any]]:
+  # fmt: off
   """Get the AutoML Tabular v1 default training pipeline.
 
   Args:
     project: The GCP project that runs the pipeline components.
     location: The GCP region that runs the pipeline components.
     root_dir: The root GCS directory for the pipeline components.
     target_column: The target column name.
-    prediction_type: The type of prediction the model is to produce.
-      "classification" or "regression".
-    optimization_objective: For binary classification, "maximize-au-roc",
-      "minimize-log-loss", "maximize-au-prc", "maximize-precision-at-recall", or
-      "maximize-recall-at-precision". For multi class classification,
-      "minimize-log-loss". For regression, "minimize-rmse", "minimize-mae", or
-      "minimize-rmsle".
-    transformations: The path to a GCS file containing the transformations to
-      apply.
-    train_budget_milli_node_hours: The train budget of creating this model,
-      expressed in milli node hours i.e. 1,000 value in this field means 1 node
-      hour.
+    prediction_type: The type of prediction the model is to produce.  "classification" or "regression".
+    optimization_objective: For binary classification, "maximize-au-roc", "minimize-log-loss", "maximize-au-prc", "maximize-precision-at-recall", or "maximize-recall-at-precision". For multi class classification, "minimize-log-loss". For regression, "minimize-rmse", "minimize-mae", or "minimize-rmsle".
+    transformations: The path to a GCS file containing the transformations to apply.
+    train_budget_milli_node_hours: The train budget of creating this model, expressed in milli node hours i.e. 1,000 value in this field means 1 node hour.
     stage_1_num_parallel_trials: Number of parallel trails for stage 1.
     stage_2_num_parallel_trials: Number of parallel trails for stage 2.
     stage_2_num_selected_trials: Number of selected trials for stage 2.
     data_source_csv_filenames: The CSV data source.
     data_source_bigquery_table_path: The BigQuery data source.
     predefined_split_key: The predefined_split column name.
     timestamp_split_key: The timestamp_split column name.
     stratified_split_key: The stratified_split column name.
     training_fraction: The training fraction.
     validation_fraction: The validation fraction.
     test_fraction: float = The test fraction.
     weight_column: The weight column name.
-    study_spec_parameters_override: The list for overriding study spec. The list
-      should be of format
-      https://github.com/googleapis/googleapis/blob/4e836c7c257e3e20b1de14d470993a2b1f4736a8/google/cloud/aiplatform/v1beta1/study.proto#L181.
-    optimization_objective_recall_value: Required when optimization_objective is
-      "maximize-precision-at-recall". Must be between 0 and 1, inclusive.
-    optimization_objective_precision_value: Required when optimization_objective
-      is "maximize-recall-at-precision". Must be between 0 and 1, inclusive.
-    stage_1_tuner_worker_pool_specs_override: The dictionary for overriding.
-      stage 1 tuner worker pool spec. The dictionary should be of format
-        https://github.com/googleapis/googleapis/blob/4e836c7c257e3e20b1de14d470993a2b1f4736a8/google/cloud/aiplatform/v1beta1/custom_job.proto#L172.
-    cv_trainer_worker_pool_specs_override: The dictionary for overriding stage
-      cv trainer worker pool spec. The dictionary should be of format
-        https://github.com/googleapis/googleapis/blob/4e836c7c257e3e20b1de14d470993a2b1f4736a8/google/cloud/aiplatform/v1beta1/custom_job.proto#L172.
-    export_additional_model_without_custom_ops: Whether to export additional
-      model without custom TensorFlow operators.
-    stats_and_example_gen_dataflow_machine_type: The dataflow machine type for
-      stats_and_example_gen component.
-    stats_and_example_gen_dataflow_max_num_workers: The max number of Dataflow
-      workers for stats_and_example_gen component.
-    stats_and_example_gen_dataflow_disk_size_gb: Dataflow worker's disk size in
-      GB for stats_and_example_gen component.
-    transform_dataflow_machine_type: The dataflow machine type for transform
-      component.
-    transform_dataflow_max_num_workers: The max number of Dataflow workers for
-      transform component.
-    transform_dataflow_disk_size_gb: Dataflow worker's disk size in GB for
-      transform component.
-    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty
-      the default subnetwork will be used. Example:
-        https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
-    dataflow_use_public_ips: Specifies whether Dataflow workers use public IP
-      addresses.
+    study_spec_parameters_override: The list for overriding study spec. The list should be of format: https://github.com/googleapis/googleapis/blob/4e836c7c257e3e20b1de14d470993a2b1f4736a8/google/cloud/aiplatform/v1beta1/study.proto#L181.
+    optimization_objective_recall_value: Required when optimization_objective is "maximize-precision-at-recall". Must be between 0 and 1, inclusive.
+    optimization_objective_precision_value: Required when optimization_objective is "maximize-recall-at-precision". Must be between 0 and 1, inclusive.
+    stage_1_tuner_worker_pool_specs_override: The dictionary for overriding.  stage 1 tuner worker pool spec. The dictionary should be of format: https://github.com/googleapis/googleapis/blob/4e836c7c257e3e20b1de14d470993a2b1f4736a8/google/cloud/aiplatform/v1beta1/custom_job.proto#L172.
+    cv_trainer_worker_pool_specs_override: The dictionary for overriding stage cv trainer worker pool spec. The dictionary should be of format: https://github.com/googleapis/googleapis/blob/4e836c7c257e3e20b1de14d470993a2b1f4736a8/google/cloud/aiplatform/v1beta1/custom_job.proto#L172.
+    export_additional_model_without_custom_ops: Whether to export additional model without custom TensorFlow operators.
+    stats_and_example_gen_dataflow_machine_type: The dataflow machine type for stats_and_example_gen component.
+    stats_and_example_gen_dataflow_max_num_workers: The max number of Dataflow workers for stats_and_example_gen component.
+    stats_and_example_gen_dataflow_disk_size_gb: Dataflow worker's disk size in GB for stats_and_example_gen component.
+    transform_dataflow_machine_type: The dataflow machine type for transform component.
+    transform_dataflow_max_num_workers: The max number of Dataflow workers for transform component.
+    transform_dataflow_disk_size_gb: Dataflow worker's disk size in GB for transform component.
+    dataflow_subnetwork: Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used. Example: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications
+    dataflow_use_public_ips: Specifies whether Dataflow workers use public IP addresses.
     encryption_spec_key_name: The KMS key name.
     additional_experiments: Use this field to config private preview features.
     dataflow_service_account: Custom service account to run dataflow jobs.
     run_evaluation: Whether to run evaluation in the training pipeline.
-    evaluation_batch_predict_machine_type: The prediction server machine type
-      for batch predict components during evaluation.
-    evaluation_batch_predict_starting_replica_count: The initial number of
-      prediction server for batch predict components during evaluation.
-    evaluation_batch_predict_max_replica_count: The max number of prediction
-      server for batch predict components during evaluation.
-    evaluation_batch_explain_machine_type: The prediction server machine type
-      for batch explain components during evaluation.
-    evaluation_batch_explain_starting_replica_count: The initial number of
-      prediction server for batch explain components during evaluation.
-    evaluation_batch_explain_max_replica_count: The max number of prediction
-      server for batch explain components during evaluation.
-    evaluation_dataflow_machine_type: The dataflow machine type for evaluation
-      components.
-    evaluation_dataflow_starting_num_workers: The initial number of Dataflow
-      workers for evaluation components.
-    evaluation_dataflow_max_num_workers: The max number of Dataflow workers for
-      evaluation components.
-    evaluation_dataflow_disk_size_gb: Dataflow worker's disk size in GB for
-      evaluation components.
+    evaluation_batch_predict_machine_type: The prediction server machine type for batch predict components during evaluation.
+    evaluation_batch_predict_starting_replica_count: The initial number of prediction server for batch predict components during evaluation.
+    evaluation_batch_predict_max_replica_count: The max number of prediction server for batch predict components during evaluation.
+    evaluation_batch_explain_machine_type: The prediction server machine type for batch explain components during evaluation.
+    evaluation_batch_explain_starting_replica_count: The initial number of prediction server for batch explain components during evaluation.
+    evaluation_batch_explain_max_replica_count: The max number of prediction server for batch explain components during evaluation.
+    evaluation_dataflow_machine_type: The dataflow machine type for evaluation components.
+    evaluation_dataflow_starting_num_workers: The initial number of Dataflow workers for evaluation components.
+    evaluation_dataflow_max_num_workers: The max number of Dataflow workers for evaluation components.
+    evaluation_dataflow_disk_size_gb: Dataflow worker's disk size in GB for evaluation components.
     run_distillation: Whether to run distill in the training pipeline.
-    distill_batch_predict_machine_type: The prediction server machine type for
-      batch predict component in the model distillation.
-    distill_batch_predict_starting_replica_count: The initial number of
-      prediction server for batch predict component in the model distillation.
-    distill_batch_predict_max_replica_count: The max number of prediction server
-      for batch predict component in the model distillation.
-    stage_1_tuning_result_artifact_uri: The stage 1 tuning result artifact GCS
-      URI.
-    quantiles: Quantiles to use for probabilistic inference. Up to 5 quantiles
-      are allowed of values between 0 and 1, exclusive. Represents the quantiles
-      to use for that objective. Quantiles must be unique.
-    enable_probabilistic_inference: If probabilistic inference is enabled, the
-      model will fit a distribution that captures the uncertainty of a
-      prediction. At inference time, the predictive distribution is used to make
-      a point prediction that minimizes the optimization objective. For example,
-      the mean of a predictive distribution is the point prediction that
-      minimizes RMSE loss. If quantiles are specified, then the quantiles of the
-      distribution are also returned.
-    num_selected_features: Number of selected features for feature selection,
-      defaults to None, in which case all features are used.
+    distill_batch_predict_machine_type: The prediction server machine type for batch predict component in the model distillation.
+    distill_batch_predict_starting_replica_count: The initial number of prediction server for batch predict component in the model distillation.
+    distill_batch_predict_max_replica_count: The max number of prediction server for batch predict component in the model distillation.
+    stage_1_tuning_result_artifact_uri: The stage 1 tuning result artifact GCS URI.
+    quantiles: Quantiles to use for probabilistic inference. Up to 5 quantiles are allowed of values between 0 and 1, exclusive. Represents the quantiles to use for that objective. Quantiles must be unique.
+    enable_probabilistic_inference: If probabilistic inference is enabled, the model will fit a distribution that captures the uncertainty of a prediction. At inference time, the predictive distribution is used to make a point prediction that minimizes the optimization objective. For example, the mean of a predictive distribution is the point prediction that minimizes RMSE loss. If quantiles are specified, then the quantiles of the distribution are also returned.
+    num_selected_features: Number of selected features for feature selection, defaults to None, in which case all features are used.
     model_display_name: The display name of the uploaded Vertex model.
     model_description: The description for the uploaded model.
 
   Returns:
     Tuple of pipeline_definition_path and parameter_values.
   """
+  # fmt: on
   parameter_values = _get_default_pipeline_params(
       project=project,
       location=location,
       root_dir=root_dir,
       target_column=target_column,
       prediction_type=prediction_type,
       optimization_objective=optimization_objective,
```

## google_cloud_pipeline_components/v1/bigquery/query_job/component.py

```diff
@@ -35,14 +35,16 @@
     labels: Dict[str, str] = {},
     encryption_spec_key_name: str = '',
     project: str = _placeholders.PROJECT_ID_PLACEHOLDER,
 ):
   # fmt: off
   """Launch a BigQuery query job and waits for it to finish.
 
+  Note: The total input commands/args to the component can be at most 50KB. This means the BigQuery query must be less than 50KB, since the input commands/args contain other non-query characters, including all parameter names, parameter values, and various JSON characters.
+
   Args:
       location: Location for creating the BigQuery job. If not set, default to `US` multi-region.  For more details, see https://cloud.google.com/bigquery/docs/locations#specifying_your_location
       query: SQL query text to execute. Only standard SQL is supported.  If query are both specified in here and in job_configuration_query, the value in here will override the other one.
       query_parameters: jobs.query parameters for standard SQL queries.  If query_parameters are both specified in here and in job_configuration_query, the value in here will override the other one.
       job_configuration_query: A json formatted string describing the rest of the job configuration.  For more details, see https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationQuery
       labels: The labels associated with this job. You can use these to organize and group your jobs. Label keys and values can be no longer than 63 characters, can only containlowercase letters, numeric characters, underscores and dashes. International characters are allowed. Label values are optional. Label keys must start with a letter and each label in the list must have a different key.
         Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
```

## google_cloud_pipeline_components/v1/model/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 The Kubeflow Authors. All Rights Reserved.
+# Copyright 2024 The Kubeflow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,14 +13,16 @@
 # limitations under the License.
 # fmt: off
 """Manage models via [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)."""
 # fmt: on
 
 from google_cloud_pipeline_components.v1.model.delete_model.component import model_delete as ModelDeleteOp
 from google_cloud_pipeline_components.v1.model.export_model.component import model_export as ModelExportOp
+from google_cloud_pipeline_components.v1.model.get_model.component import model_get as ModelGetOp
 from google_cloud_pipeline_components.v1.model.upload_model.component import model_upload as ModelUploadOp
 
 __all__ = [
     'ModelExportOp',
     'ModelUploadOp',
     'ModelDeleteOp',
+    'ModelGetOp',
 ]
```

## google_cloud_pipeline_components/v1/model/upload_model/component.py

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict
+from typing import Dict, List
 
 from google_cloud_pipeline_components import _image
 from google_cloud_pipeline_components import _placeholders
 from google_cloud_pipeline_components.types.artifact_types import UnmanagedContainerModel
 from google_cloud_pipeline_components.types.artifact_types import VertexModel
 from kfp import dsl
 from kfp.dsl import ConcatPlaceholder
@@ -35,14 +35,15 @@
     model: Output[VertexModel],
     location: str = 'us-central1',
     description: str = '',
     parent_model: Input[VertexModel] = None,
     unmanaged_container_model: Input[UnmanagedContainerModel] = None,
     explanation_metadata: Dict[str, str] = {},
     explanation_parameters: Dict[str, str] = {},
+    version_aliases: List[str] = [],
     labels: Dict[str, str] = {},
     encryption_spec_key_name: str = '',
     project: str = _placeholders.PROJECT_ID_PLACEHOLDER,
 ):
   # fmt: off
   """[Uploads](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) a Google Cloud Vertex [Model](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models) and returns a Model artifact representing the uploaded Model resource, with a tag for the particular version. See [Model upload](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) method for more information.
 
@@ -56,14 +57,15 @@
         from kfp import dsl
         from google_cloud_pipeline_components.types import artifact_types
 
         importer_spec = dsl.importer( artifact_uri='gs://managed-pipeline-gcpc-e2e-test/automl-tabular/model', artifact_class=artifact_types.UnmanagedContainerModel, metadata={ 'containerSpec': { 'imageUri': 'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:prod' } })
 
       explanation_metadata: Metadata describing the Model's input and output for explanation. Both `explanation_metadata` and `explanation_parameters` must be passed together when used. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata)
       explanation_parameters: Parameters to configure explaining for Model's predictions.  [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#ExplanationParameters)
+      version_aliases: User provided version aliases so that a model version can be referenced via alias (i.e. `projects/{project}/locations/{location}/models/{modelId}@{version_alias}` instead of auto-generated version id (i.e. `projects/{project}/locations/{location}/models/{modelId}@{versionId}`). The format is [a-z][a-zA-Z0-9-]{0,126}[a-z0-9] to distinguish from versionId. A default version alias will be created for the first version of the model, and there must be exactly one default version alias for a model.
       encryption_spec_key_name: Customer-managed encryption key spec for a Model. If set, this Model and all sub-resources of this Model will be secured by this key.  Has the form: `projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
       labels: The labels with user-defined metadata to organize your model.  Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed.  See https://goo.gl/xmQnxf for more information and examples of labels.
       project: Project to upload this Model to. Defaults to the project in which the PipelineJob is run.
 
   Returns:
       model: Artifact tracking the created Model version.
       gcp_resources: Serialized JSON of `gcp_resources` [proto](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components/proto) which tracks the upload Model's long-running operation.
@@ -94,14 +96,16 @@
               explanation_parameters,
               ', "metadata": ',
               explanation_metadata,
               '}',
               ', "encryption_spec": {"kms_key_name":"',
               encryption_spec_key_name,
               '"}',
+              ', "version_aliases": ',
+              version_aliases,
               ', "labels": ',
               labels,
               ', "pipeline_job": "',
               f'projects/{project}/locations/{location}/pipelineJobs/{dsl.PIPELINE_JOB_ID_PLACEHOLDER}',
               '"',
               '}',
           ]),
```

## Comparing `google_cloud_pipeline_components-2.8.0.dist-info/LICENSE` & `google_cloud_pipeline_components-2.9.0.dist-info/LICENSE`

 * *Files 8% similar despite different names*

```diff
@@ -195,8 +195,44 @@
 
        http://www.apache.org/licenses/LICENSE-2.0
 
    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
-   limitations under the License.
+   limitations under the License.
+
+
+   ------------------
+
+   Copyright 2008 Google Inc.  All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+
+    * Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above
+copyright notice, this list of conditions and the following disclaimer
+in the documentation and/or other materials provided with the
+distribution.
+    * Neither the name of Google Inc. nor the names of its
+contributors may be used to endorse or promote products derived from
+this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+Code generated by the Protocol Buffer compiler is owned by the owner
+of the input file used when generating it.  This code is not
+standalone and requires a support library to be linked with it.  This
+support library is itself covered by the above license.
```

## Comparing `google_cloud_pipeline_components-2.8.0.dist-info/METADATA` & `google_cloud_pipeline_components-2.9.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: google-cloud-pipeline-components
-Version: 2.8.0
+Version: 2.9.0
 Summary: This SDK enables a set of First Party (Google owned) pipeline components that allow users to take their experience from Vertex AI SDK and other Google Cloud services and create a corresponding pipeline using KFP or Managed Pipelines.
 Home-page: https://github.com/kubeflow/pipelines/tree/master/components/google-cloud
 Author: The Google Cloud Pipeline Components authors
 Author-email: google-cloud-pipeline-components@google.com
 License: Apache License 2.0
 Project-URL: User Documentation, https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction
 Project-URL: Reference Documentation, https://google-cloud-pipeline-components.readthedocs.io/
@@ -28,26 +28,26 @@
 Classifier: Topic :: Software Development
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Requires-Python: >=3.7.0,<3.12.0
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: google-api-core !=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5
-Requires-Dist: kfp <=2.4.0,>=2.0.0b10
+Requires-Dist: kfp <=2.6.0,>=2.6.0
 Requires-Dist: google-cloud-aiplatform <2,>=1.14.0
 Requires-Dist: Jinja2 ==3.1.2
 Provides-Extra: docs
-Requires-Dist: protobuf <4.0.0dev,>=3.19.0 ; extra == 'docs'
+Requires-Dist: protobuf <5,>=4.21.1 ; extra == 'docs'
 Requires-Dist: grpcio-status <=1.47.0 ; extra == 'docs'
 Requires-Dist: commonmark ==0.9.1 ; extra == 'docs'
 Requires-Dist: autodocsumm ==0.2.9 ; extra == 'docs'
 Requires-Dist: sphinx ==5.0.2 ; extra == 'docs'
 Requires-Dist: sphinx-immaterial ==0.9.0 ; extra == 'docs'
 Requires-Dist: sphinx-rtd-theme ==1.0.0 ; extra == 'docs'
-Requires-Dist: m2r2 ==0.3.2 ; extra == 'docs'
+Requires-Dist: m2r2 ==0.3.3 ; extra == 'docs'
 Requires-Dist: sphinx-notfound-page ==0.8.3 ; extra == 'docs'
 Provides-Extra: tests
 Requires-Dist: mock >=4.0.0 ; extra == 'tests'
 Requires-Dist: flake8 >=3.0.0 ; extra == 'tests'
 Requires-Dist: pytest >=6.0.0 ; extra == 'tests'
 
 # Google Cloud Pipeline Components
```

## Comparing `google_cloud_pipeline_components-2.8.0.dist-info/RECORD` & `google_cloud_pipeline_components-2.9.0.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,49 +1,50 @@
 google_cloud_pipeline_components/__init__.py,sha256=3Mr8_YbBkTzArlgPDkUKoMzoKHZx1PM8KVgBXEtshj4,1189
 google_cloud_pipeline_components/_image.py,sha256=lANDYNk1WSuGZSoTTRcWdjsUvCkkA-PmwouTM9Et7fY,828
-google_cloud_pipeline_components/_placeholders.py,sha256=MoglmqYGMu8Jwf1kAX5Kas5gOfFOBTkrKGwn_ZqHmm4,859
+google_cloud_pipeline_components/_placeholders.py,sha256=517N_NQthPEBFJtsy8NE3WkBJm_dmwmlXdYNtk5gH-c,1233
 google_cloud_pipeline_components/utils.py,sha256=9FG7umyEXhyUvtNeC46NuQ04olDMR3o-Wp78V1xs8GY,11045
-google_cloud_pipeline_components/version.py,sha256=ntQhWa8g7nBANZSkX5N4Bj7Q-qAFzFu1TLkKuTkHUok,677
+google_cloud_pipeline_components/version.py,sha256=idWy7fIxzlJszTfBrT5bf-3tVudME5Znm7JhfTJPLuc,677
 google_cloud_pipeline_components/_implementation/__init__.py,sha256=sb6SfJl6rt3AKjiWxd-KO9DSiZ3PzGZRcsqKuc1A2Cg,606
 google_cloud_pipeline_components/_implementation/llm/__init__.py,sha256=sb6SfJl6rt3AKjiWxd-KO9DSiZ3PzGZRcsqKuc1A2Cg,606
 google_cloud_pipeline_components/_implementation/llm/arbiter_preprocess.py,sha256=An0zwfwzVSEWa2Ec9BlLV81HWRDqWAccWfv4P3VigJk,6252
-google_cloud_pipeline_components/_implementation/llm/autosxs_arbiter.py,sha256=5fdM2BHgP5w4M9NzMeYQD17RVs6nbr8AIntRho_qZe0,4432
+google_cloud_pipeline_components/_implementation/llm/autosxs_arbiter.py,sha256=HUmZXYBlf2SINkcbqecn_LwwMlfG71LVwUgJD-DC9TI,4638
 google_cloud_pipeline_components/_implementation/llm/autosxs_metrics_computer.py,sha256=fASbnHAFgwedtNmQ4-Xt_nQ5ABt16qkKAaOIAn4gYh4,2591
 google_cloud_pipeline_components/_implementation/llm/bulk_inferrer.py,sha256=u5vQCgTCaRs5SWl9AFBm-TUJuMsYI1rfQRYz9D38kgE,3598
 google_cloud_pipeline_components/_implementation/llm/deploy_llm_model.py,sha256=URIl8iX-SM7vM-5rFQzAyFx9cWjEoiMsItU0oOijD8k,5045
-google_cloud_pipeline_components/_implementation/llm/deployment_graph.py,sha256=DDCffBKLQoebz1FmJ1_fKOS6TyUNeC1457_t9nBivVA,4601
-google_cloud_pipeline_components/_implementation/llm/env.py,sha256=9R95KqEMcQj3Rb1JIZU5hAa8uJ7lDvikitetXeAA3Vg,1521
-google_cloud_pipeline_components/_implementation/llm/function_based.py,sha256=HOISDC2aDyW1sRDHN3F7CPyxY9nhu0DpXBfpiMG859c,20955
+google_cloud_pipeline_components/_implementation/llm/deployment_graph.py,sha256=Z8EzlJSvE0Idjxz0hW6PiEULa5E8y1PDJEkH4EgOvx0,4671
+google_cloud_pipeline_components/_implementation/llm/env.py,sha256=qNAC1Sq21epTmgmUhQUaoRVL2KrWpqX6pcptv7exsh0,1796
+google_cloud_pipeline_components/_implementation/llm/function_based.py,sha256=EyTJwcVzqTaAp2equ_n0KfldWz6EWPyn9rdwT44bTdE,21780
 google_cloud_pipeline_components/_implementation/llm/preprocess_chat_dataset.py,sha256=dMf2uXW4znn9W8xv9ZRNSPI6nZvp64FTq5GAqjXFRDk,11746
 google_cloud_pipeline_components/_implementation/llm/private_text_comparison_importer.py,sha256=PkDNcSVQAEaBzJT5r-n2x775iUaqQjy5NljcfywqD1E,3702
 google_cloud_pipeline_components/_implementation/llm/private_text_importer.py,sha256=9g91Zl1KXnpp2RaiCTvThB5cU-ls1QcIOXUyoe1pvtA,4060
 google_cloud_pipeline_components/_implementation/llm/reinforcement_learning_graph.py,sha256=AIzRVG9uIC-O8d1Amz7KTmwWVeBHmllqPhB1g93bDZk,10007
 google_cloud_pipeline_components/_implementation/llm/reinforcer.py,sha256=unR8F7NovxGwIeuCpng3A9OuwPESm3Cy1jdR66PMwys,5634
 google_cloud_pipeline_components/_implementation/llm/reward_model_graph.py,sha256=RMD6t8wChiET8cxaWZql7Yjnh9qDTHKJfE4pLUa0ZyU,9764
 google_cloud_pipeline_components/_implementation/llm/reward_model_trainer.py,sha256=fjQ1jdIyaBW6GzDN5U_mzvIR_MAk7HbDk-tcHE5hqHQ,4853
 google_cloud_pipeline_components/_implementation/llm/supervised_fine_tuner.py,sha256=vHYhNpoOrJhxhP0arj60oyqE3GLGF9YEcgbnqR_2mdo,4931
 google_cloud_pipeline_components/_implementation/llm/task_preprocess.py,sha256=MN8T58h6u9Zoo33eCN47rsMqh_TlQBToSoSAh1DHhnc,4175
 google_cloud_pipeline_components/_implementation/llm/upload_llm_model.py,sha256=YGNEMSGRcJHzbl2a3OGeRJd1op2ZI4-N3icojIc5x0U,4734
 google_cloud_pipeline_components/_implementation/llm/upload_tensorboard_metrics.py,sha256=BN-0TQFl49TcE54ltBRt4iZYTjO7718eCLwHKR58ips,4010
-google_cloud_pipeline_components/_implementation/llm/utils.py,sha256=jq5jYRQvu83iDOZvbe7cJhzJNZ6xkAESdbgPATXuIeQ,4218
-google_cloud_pipeline_components/_implementation/llm/utils_test.py,sha256=H1AAqjEzq4ueOuIffIZ0S5WclAnguJe43yiTFAVk-Yg,1881
+google_cloud_pipeline_components/_implementation/llm/utils.py,sha256=0OXBfo6TTmJ4sCgH3C527xtKrwXFPtq7ZIflZ98XVQE,4680
+google_cloud_pipeline_components/_implementation/llm/utils_test.py,sha256=co8gWyrowY5CpkFNsaLGQlD_gpIykkVI7czxIizp5cM,2864
 google_cloud_pipeline_components/_implementation/model/__init__.py,sha256=KmOW74re0WZ93DWM1lqqQYbv6w1aIW66BMV3gaAdg3s,811
 google_cloud_pipeline_components/_implementation/model/get_model/__init__.py,sha256=cXMkDUZHVSbXeXSa3qsI6Ef8Tad9nmusw5NUZaYORdE,662
 google_cloud_pipeline_components/_implementation/model/get_model/component.py,sha256=H2sbMTWCw8nMDMT-Ni9-pdzVXEFmHYjtP3z1LcI5m5w,2307
 google_cloud_pipeline_components/_implementation/model_evaluation/__init__.py,sha256=Ubgd_Fi7p7O0y_dzmYFRdbR36W9kKvTuxJOFSMiUrMk,5645
 google_cloud_pipeline_components/_implementation/model_evaluation/utils.py,sha256=9V34RtPZSRNeBwcsImaZM6YC3T7CafT_E00Iby4KHxw,3540
 google_cloud_pipeline_components/_implementation/model_evaluation/version.py,sha256=Qfa2_V-1QNxFKQFVZnOhl6BOBZUIBmHzuhyIf19CqYI,963
 google_cloud_pipeline_components/_implementation/model_evaluation/chunking/__init__.py,sha256=PRHVsIq1dFb0mweuU0kfUUP90FbX--kxdBGCpwfLTgA,665
-google_cloud_pipeline_components/_implementation/model_evaluation/chunking/component.py,sha256=nN7S2MrqnjXTwYILwVyFO6VnJBjQwsxFF871AKMrYnM,4259
+google_cloud_pipeline_components/_implementation/model_evaluation/chunking/component.py,sha256=lv0eNdIBtnCUs9wi9CGUWE_fd4zodGN7M8fTBiCg0g0,4436
+google_cloud_pipeline_components/_implementation/model_evaluation/chunking/feature_store_grounding_pipeline.py,sha256=zGCMJhBCWBV-Dg5gz-wpheLQhD-GuEW_fgcb0cWa78U,4167
 google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/__init__.py,sha256=wBCzT0CQxnhyR2lJQ6220nZEP9iAn_OqjYT_-bJTFSc,669
 google_cloud_pipeline_components/_implementation/model_evaluation/data_sampler/component.py,sha256=y3CzEm94k3o76SKqKlD2yYhObBb5SWw0zThD2sx_n74,5452
 google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/__init__.py,sha256=s7-HxTa9wuPsOcxs9iN1gKrjIlAhoPLje96aFSqM1Vs,677
 google_cloud_pipeline_components/_implementation/model_evaluation/dataset_preprocessor/component.py,sha256=bBJfDO5RNgRnyIpbSyTEERDnzf5F5blvuZz0UycKsr0,7039
 google_cloud_pipeline_components/_implementation/model_evaluation/endpoint_batch_predict/__init__.py,sha256=t-qAYEJd27dzl0Wrf21b8J8TON6rTJsoEmK5p5o83wE,679
-google_cloud_pipeline_components/_implementation/model_evaluation/endpoint_batch_predict/component.py,sha256=9G2ebQ_OfNVPzwDc7Qon_53vJf2QpKFadaVuYz1Qvp4,11837
+google_cloud_pipeline_components/_implementation/model_evaluation/endpoint_batch_predict/component.py,sha256=YdUOuJfmnc4A7Mg4x_3qF5AgCNO7ysL0OUy0UAMeI3o,12001
 google_cloud_pipeline_components/_implementation/model_evaluation/error_analysis_annotation/__init__.py,sha256=LSc-h1veE7xA7pVsP0LlamhQ4cTeN6X-ghAKyazdN7Q,671
 google_cloud_pipeline_components/_implementation/model_evaluation/error_analysis_annotation/component.py,sha256=xyJ89zWKj2yrQjTlbTgYd2DzJ6rqXe2MQGK-EETFP9E,4092
 google_cloud_pipeline_components/_implementation/model_evaluation/evaluated_annotation/__init__.py,sha256=UBMB8ThxUAXaLxUe4RP_0LpiQRdBuFpKBVC3ldaP7CM,677
 google_cloud_pipeline_components/_implementation/model_evaluation/evaluated_annotation/component.py,sha256=bmEEHGe_QtOshJBIktRq9F0TCRd9GJvLBEGiz2sjbhs,5842
 google_cloud_pipeline_components/_implementation/model_evaluation/feature_attribution/__init__.py,sha256=pIXVwg8rbGCJASN_DIapQ5czufmFP5SEcd5UtS-ffqc,674
 google_cloud_pipeline_components/_implementation/model_evaluation/feature_attribution/feature_attribution_component.py,sha256=Ph7AsKKco2xDeH8bQBfVmOplnr5ru09A-OF2cFYn6M8,7664
 google_cloud_pipeline_components/_implementation/model_evaluation/feature_attribution/feature_attribution_graph_component.py,sha256=SuFc42FYZZsJG4zuN93P7HpWCOklT_ICTvAcT3BRvpI,14290
@@ -56,26 +57,26 @@
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_classification_postprocessor/__init__.py,sha256=kEQ4aaKnV-KulHqbhb6eJIZzk4O7uSBRPzm_e3q_hcA,697
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_classification_postprocessor/component.py,sha256=H0g7nMK3JVdQonLe5jpKvvzm9N2YzFGQUWJwwfxQ33s,11227
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_embedding/__init__.py,sha256=CUSeP0l2KFuo2wbw00DE5Zur0jpgHpZ1aThg7yqWuGY,680
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_embedding/evaluation_llm_embedding_pipeline.py,sha256=7Rah6V-8jSVoy6QNBI8c93RgkAW89z5VwN0FRtRPmJ8,14207
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_embedding_retrieval/__init__.py,sha256=Q9YimgEKkKP8QW8fV50nNRjWXdt0_90Qr_gfQ0A9fao,691
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_embedding_retrieval/component.py,sha256=er26AxV0ydpKFbhrLs2IIia9hbLxXhe78Q-fQvvK_20,7265
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/__init__.py,sha256=gDyltb_vTZRncaVZbMUkXYBHZsEg_CuaPAQVWUOGy7c,671
-google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/component.py,sha256=4z5X3QMK4Y8U3Mx8tvqZF_fUXX2SqbNnfiKDG2bB_Fs,7485
+google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation/component.py,sha256=3eo6QFoHj9Sujvfq3Z8vT3mhOLbSe4cA6hLww_wuJ1M,7478
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation_preprocessor/__init__.py,sha256=LLvQQ9Mv_md1POK_XErR7NUr-XAZX28w5KD0EQiT32w,684
-google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation_preprocessor/component.py,sha256=cA3PiwAjzeF7cbS0ZWeC6svvOuuZpi7uxfL-rlK0qq8,7913
+google_cloud_pipeline_components/_implementation/model_evaluation/llm_evaluation_preprocessor/component.py,sha256=n-d-Re3qBHZh4YZutF891yWtIahV0TE5193TT40T3N0,7952
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_information_retrieval_preprocessor/__init__.py,sha256=UcAfohvqwJ7X8rlV6I2RsZ5ohYWbafXI5yYRAAwvspE,695
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_information_retrieval_preprocessor/component.py,sha256=hpwra0rXUP_LG6WNdnB9RD0cy1Gn3InrzAUJIk-oRc4,8142
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_retrieval_metrics/__init__.py,sha256=oVVVTCE230KYGcuz_rXYEuypZmm6OKg7K2BsFtpcDw0,701
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_retrieval_metrics/component.py,sha256=VHIGY-D-6tyvb-LIlEAXXvg6TbhFfgHa_M1_J5KH5TA,6483
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_safety_bias/__init__.py,sha256=wYzl962BwoNXiMvfq-l12u9tM_4sqT54-QF6vt7OHXc,673
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_safety_bias/component.py,sha256=3kplf9bqYYk5gLIP2ZQsOkcNqEt9-guuj62eu5gufMk,4932
 google_cloud_pipeline_components/_implementation/model_evaluation/llm_safety_bias/evaluation_llm_safety_bias_pipeline.py,sha256=n5I3_RPbDR3XZh8E2z_3hX2H3oVZB5nFzmrXOM3lB-A,7005
 google_cloud_pipeline_components/_implementation/model_evaluation/model_inference/__init__.py,sha256=71Kfj1wk1UuUch15F1u2Nv47v2lAjeL28uZ-8Fw61_c,672
-google_cloud_pipeline_components/_implementation/model_evaluation/model_inference/component.py,sha256=EgH3Lld3AuGZrI73ySVaESzjiuV_guioeoszg0udxQ0,14383
+google_cloud_pipeline_components/_implementation/model_evaluation/model_inference/component.py,sha256=0A9-gUDLg7lb3KBXd1ipOb1hnY0z48ey_M2NDIXFkqU,16211
 google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/__init__.py,sha256=B4VuH-pN_qGbJjaVNWB5b2vfdPP5yqqTphRNLukMY6o,682
 google_cloud_pipeline_components/_implementation/model_evaluation/target_field_data_remover/component.py,sha256=OeMON9Oms1xso5Emm4W5q6oUgcix9XWLlKoTmN-OUfI,5738
 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql/__init__.py,sha256=xGhjYMo_kirteCvrJqoF2jiSLexdkSRY0C-2sNkNlbk,664
 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql/evaluation_llm_text2sql_pipeline.py,sha256=2DdUpl5I9ATg4tkGoo8jEg1LcGXGvGrEGc-u1PoJzcQ,8386
 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_evaluation/__init__.py,sha256=ee1WkPGUIqRdVlChUxbOv0g-yvzg6JgRdu28H25FDSM,680
 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_evaluation/component.py,sha256=4XkezgUsyc9U69GfL4YSdicUAYFBevRC4PFi0pij3IQ,5646
 google_cloud_pipeline_components/_implementation/model_evaluation/text2sql_preprocess/__init__.py,sha256=mUn9EpSX_euTYq6uqpcLdnp4FESBFk4Jkuf3Md6gghE,691
@@ -251,54 +252,57 @@
 google_cloud_pipeline_components/container/v1/model/__init__.py,sha256=_zQJ06QF2BTxnmWO8WXv-AglK2C6voA7N19q9ar6MOM,652
 google_cloud_pipeline_components/container/v1/model/delete_model/__init__.py,sha256=jxf8BAcS5eqlwqyEWLhQ0sA1MxhEFpCpO7ncQPeP3Cs,688
 google_cloud_pipeline_components/container/v1/model/delete_model/launcher.py,sha256=KW_URGz1eyH8bFG-NnqE5wMdftntXlKK1KYSKR479rk,1923
 google_cloud_pipeline_components/container/v1/model/delete_model/remote_runner.py,sha256=HrlaO2QvuGxnYiB4PHgdZr9UJ2nzjEY_Pf8mGoF8ugw,2464
 google_cloud_pipeline_components/container/v1/model/export_model/__init__.py,sha256=X-GJYtcyiTQhUZR9LK3iQfMg4uQWKC0QJJHxuuDEWSM,688
 google_cloud_pipeline_components/container/v1/model/export_model/launcher.py,sha256=vlK1KjGyse10PR8r0ieLnajdF2GcFqjO5ELz6AVnjnw,2201
 google_cloud_pipeline_components/container/v1/model/export_model/remote_runner.py,sha256=YJwkjq5UhJtpX43pjqGs09dNPWpjJqct6cRAHDFBmZk,2479
+google_cloud_pipeline_components/container/v1/model/get_model/__init__.py,sha256=PgJlyiNYIex-iqZFd3B2H3SA8y0Kpajg8aFBEq36pwU,685
+google_cloud_pipeline_components/container/v1/model/get_model/launcher.py,sha256=VTDS59v35zz25UqA4mmTwhdmqluuKk6uinadn4KtgQ4,1909
+google_cloud_pipeline_components/container/v1/model/get_model/remote_runner.py,sha256=pUed_Pq5u6kl_CzSWwGvEeyA4PfRgREiVkScTRdIILs,2062
 google_cloud_pipeline_components/container/v1/model/upload_model/__init__.py,sha256=NZJ1pbjANZ1XDV-RvjtJcKX7Bdhw39_Eh_DRMq6vOko,688
 google_cloud_pipeline_components/container/v1/model/upload_model/launcher.py,sha256=QwnM9K43By8MNM9_iiIJH7LexeI6WLA5-V8iRS2S-0Y,2331
 google_cloud_pipeline_components/container/v1/model/upload_model/remote_runner.py,sha256=elCve0-6IrfQyI4qWVt6d5yI_DwwxOXVUE5aKF1U-mk,4480
 google_cloud_pipeline_components/container/v1/vertex_notification_email/__init__.py,sha256=p3LsPxd6b8FpOTqoFdnGDE9pH7FbVntFT1G0vWqQM_o,662
 google_cloud_pipeline_components/container/v1/vertex_notification_email/executor.py,sha256=NWlooFYXMgsBFw_XEiBYJN8pEOwjPMBTseXnUXJBYSE,1074
 google_cloud_pipeline_components/container/v1/wait_gcp_resources/__init__.py,sha256=AEioMAXo4wGRE2vvdduUOIraafLiS5GngRk_0_HZN88,694
 google_cloud_pipeline_components/container/v1/wait_gcp_resources/launcher.py,sha256=Emm-dcZM5VhADG4ygWMQeWvf4kvZiettecr58lN9EBE,1928
 google_cloud_pipeline_components/container/v1/wait_gcp_resources/remote_runner.py,sha256=T9hsFy-Uih360V_Rv3OQILFONSOrFEcz2VzEEgi-ua0,5968
 google_cloud_pipeline_components/preview/__init__.py,sha256=sb6SfJl6rt3AKjiWxd-KO9DSiZ3PzGZRcsqKuc1A2Cg,606
 google_cloud_pipeline_components/preview/automl/__init__.py,sha256=sb6SfJl6rt3AKjiWxd-KO9DSiZ3PzGZRcsqKuc1A2Cg,606
-google_cloud_pipeline_components/preview/automl/forecasting/__init__.py,sha256=2wTIYPQZ9nVEmEyqBa6WrOel3Qnmw-deRo5SSG61iZg,2691
-google_cloud_pipeline_components/preview/automl/forecasting/forecasting_ensemble.py,sha256=E_0ttj2SD6bAuoucXgvnANQv--6iUXglO7r70k80H3Y,5655
-google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_1_tuner.py,sha256=KtYR62EYkxNOHK_0UI5IcguFHDJPd3wPRYD1UxUogBM,6564
-google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_2_tuner.py,sha256=5lGa6qUgWhzCxils5qBH0XB3Cz9adYtHTD_ouGnumv4,6376
-google_cloud_pipeline_components/preview/automl/forecasting/learn_to_learn_forecasting_pipeline.yaml,sha256=zSaddMJ_rWjXn7JPzj5NxufMfqRP5IIOW614JCGa1IE,367100
-google_cloud_pipeline_components/preview/automl/forecasting/sequence_to_sequence_forecasting_pipeline.yaml,sha256=a-_5-S2DmaC5jXYWrAMC7NxT2-lE5muKNNfyduufEH0,365279
-google_cloud_pipeline_components/preview/automl/forecasting/temporal_fusion_transformer_forecasting_pipeline.yaml,sha256=YfSVm5SPrSt92u25mcqD5Q6hADeG4XSKgiRwZXb_5TI,364385
-google_cloud_pipeline_components/preview/automl/forecasting/time_series_dense_encoder_forecasting_pipeline.yaml,sha256=7lE6d7Dn-khjdWGJISz56rsywa3je3wo0IjBsYESpFk,367175
-google_cloud_pipeline_components/preview/automl/forecasting/utils.py,sha256=f-m7MBJc83P2XWXoldTGtoDoG2G8cC5xHGoJx7INiOw,53844
-google_cloud_pipeline_components/preview/automl/tabular/__init__.py,sha256=mmUIOV75zkD-e6YoRz5XC3oSUmysAY2b0uYcsC-EzHY,3105
-google_cloud_pipeline_components/preview/automl/tabular/auto_feature_engineering.py,sha256=mLinpxOU_nL51A9A8c5r_8Bw_XwukWDf-A-x48WD7wI,3643
-google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_feature_selection_pipeline.yaml,sha256=Fzi-G7q1v_YroV6F2uHCMUWp2lb-Z9ixzINbZ8Zv29g,516309
-google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_v2_pipeline.yaml,sha256=1aM9XzVVpbAXipYpz3P4p6rLsL2CoO2KSszqbvFys0c,589607
-google_cloud_pipeline_components/preview/automl/tabular/distillation_stage_feature_transform_engine.py,sha256=AqKMOfHsPvPnI6zFxHxvS99rJPoNt0jNqbKdYamvyVM,9602
-google_cloud_pipeline_components/preview/automl/tabular/feature_selection.py,sha256=vZQqMRsDr_BwskvqgLXJlOQR-47SHayTRnR0M_akpFY,7546
-google_cloud_pipeline_components/preview/automl/tabular/feature_selection_pipeline.yaml,sha256=7C3LT7g27g8gdxzlXD4Dxh3_8oILQ_f-K66xcwSEk2k,80892
-google_cloud_pipeline_components/preview/automl/tabular/feature_transform_engine.py,sha256=ZvYVCoDXC6pgkdOXucH33pEdWoqAHlqlfAi5XsH4b3Y,47372
-google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job.py,sha256=bE1196J5CJCiIGG42lfsmhcqunMRvnGU5yocqQfl8qg,9980
-google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job_pipeline.yaml,sha256=oD_9ONSDbjWQxPaNTEupYw7LFFdigPIomawNCt2w6Dw,248549
-google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer.py,sha256=XWcET3rfWx_A2fyp8SR6777XQnSKMhSGgSJa191Fhho,12748
-google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer_pipeline.yaml,sha256=b400Ihfyyo74tByD1cNPe9diGOfC1ReFqbvsoBlGHp8,207374
-google_cloud_pipeline_components/preview/automl/tabular/utils.py,sha256=q1j0_QxYAJfnmtvD6ZjtcjhUwNpMHm1IQLLKFa-WxWY,169668
-google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job.py,sha256=hF8EdQa3yT3GdiSLFolZiGcr9Xdd8kyDAAECO9BmBW0,9975
-google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job_pipeline.yaml,sha256=-XwT1WPj-I5ZgK9uG_Qz9SRY-X5idlKJwChyM0MFnkM,200559
-google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer.py,sha256=esdZHd1jHGeOvvrrvOA9zwBqvFybRQ2i9dV8dK0QSo4,11922
-google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer_pipeline.yaml,sha256=KOVsH72PUUSjBryKFbGDgDff7qAVy1cJWumlYDtmolM,196045
+google_cloud_pipeline_components/preview/automl/forecasting/__init__.py,sha256=Y-PNjGWILh5wMGQLmvaIukfr9KC9oD2GQLhfJDofrAQ,3532
+google_cloud_pipeline_components/preview/automl/forecasting/forecasting_ensemble.py,sha256=8WMPNLtXu37KPvtgzg7I5lCWVFIsX6EzogT0EtShnCQ,5655
+google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_1_tuner.py,sha256=7U5AhN4JT6j0wAw_MD0af6gLFJ8NVchAwExAMEyIywE,6564
+google_cloud_pipeline_components/preview/automl/forecasting/forecasting_stage_2_tuner.py,sha256=qnEQvVmjqQVJC32mnQa7DcajTouIYa5EfDSLWPvV01k,6376
+google_cloud_pipeline_components/preview/automl/forecasting/learn_to_learn_forecasting_pipeline.yaml,sha256=RSVRC0ETfd0B_L473iQ165V0nIVEaMA8H56eTUWNtno,361458
+google_cloud_pipeline_components/preview/automl/forecasting/sequence_to_sequence_forecasting_pipeline.yaml,sha256=p_HPnnB6lycc8S1bIW5ZBHt6Mo21IHfNg9uMnK3KVn0,359637
+google_cloud_pipeline_components/preview/automl/forecasting/temporal_fusion_transformer_forecasting_pipeline.yaml,sha256=AyL--NDR3yIpG31cOOI0ofSXs-uqDQ61o_C6g0vNoa8,358743
+google_cloud_pipeline_components/preview/automl/forecasting/time_series_dense_encoder_forecasting_pipeline.yaml,sha256=RGGcDZaMh9KNMwH5bZr92rfYryGV71bpIlCVjO7_j2I,361533
+google_cloud_pipeline_components/preview/automl/forecasting/utils.py,sha256=WpN7O7y95kVry-E-6o9uvR1_DDNXZ-xAA4tQdY6werU,53462
+google_cloud_pipeline_components/preview/automl/tabular/__init__.py,sha256=jzu9udS0cH8lSSvgn3XerrKbs69GpMzI2NGSpNQmbcI,4247
+google_cloud_pipeline_components/preview/automl/tabular/auto_feature_engineering.py,sha256=ytrWe-uAGpFdGrGZX_eEWDJXU04DsHraKZ2b4kocUos,3643
+google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_feature_selection_pipeline.yaml,sha256=AB1FUNrMyasVj4hL1f5Zh7FL5ECJJyPBPKUWlPJbXns,515159
+google_cloud_pipeline_components/preview/automl/tabular/automl_tabular_v2_pipeline.yaml,sha256=kNI7gZ8YmsLuMwWQW3bCZELKisZbIrSk855Ezt8CXHE,582988
+google_cloud_pipeline_components/preview/automl/tabular/distillation_stage_feature_transform_engine.py,sha256=vNI_wHxq-nu22-oELs3WCP-KIt8C2oTuXUB5AQ1o0cM,9629
+google_cloud_pipeline_components/preview/automl/tabular/feature_selection.py,sha256=s2GUu04mpx0s5Gdf7tZzc02WCqPnEbEPRrO0nPWXfG8,7546
+google_cloud_pipeline_components/preview/automl/tabular/feature_selection_pipeline.yaml,sha256=yre2iN7Ob5vu10cp9UjcUUJ7Zl89a8YT3QzO0mK5zLY,75354
+google_cloud_pipeline_components/preview/automl/tabular/feature_transform_engine.py,sha256=emrnm4NyY5G39IIEZfHSC58rdxbISHOOc09dRPkejw8,47372
+google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job.py,sha256=XYJedTxMnMT8EPb4wfaDkW5MVO91rhkDgji1KvlZ-ME,9980
+google_cloud_pipeline_components/preview/automl/tabular/tabnet_hyperparameter_tuning_job_pipeline.yaml,sha256=1kLTqrb24ylNUw3qAl87Ztq7ZU8evIZPKTEWG1pfF7U,242808
+google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer.py,sha256=EfqOsvEEaoAoYSKFl-6og0i92QlYU7zwPCBqLjZ5Vk4,12748
+google_cloud_pipeline_components/preview/automl/tabular/tabnet_trainer_pipeline.yaml,sha256=8IZeQbE0ZCV6xkdH0s2oBusoar5EjAkXzXEYTSy2Qt4,201551
+google_cloud_pipeline_components/preview/automl/tabular/utils.py,sha256=R5Vzq9dlFh619Rbw03xuCMpgFxSX2rDNf5i5gGE93yM,168283
+google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job.py,sha256=efSgFFkY3Y1Rm_4WwiKQ_oDDnl48dZD4cv8KxpY--Rk,9975
+google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_hyperparameter_tuning_job_pipeline.yaml,sha256=ZOhSOpOrM2UJeElAFsd8h3uiOTAx7hhSGNcnxin11tU,194816
+google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer.py,sha256=N4WXlo4lSqP5jMNQcccvvZabocc_QUfJqRCmKK9D6GY,11922
+google_cloud_pipeline_components/preview/automl/tabular/wide_and_deep_trainer_pipeline.yaml,sha256=_rN_zlBUs3bjOAiqcafNxqZjsjq7Bmue88AB0edh2kk,190208
 google_cloud_pipeline_components/preview/automl/tabular/xgboost_hyperparameter_tuning_job.py,sha256=BYgs8CyzSObP6NeHbIXQaCV2Cgn2f0uJ47sxWHM6rT0,4787
-google_cloud_pipeline_components/preview/automl/tabular/xgboost_hyperparameter_tuning_job_pipeline.yaml,sha256=Fn7WqlNscoDH4D3SqjGTL52qNiipannrZimDDXxNJsg,229291
+google_cloud_pipeline_components/preview/automl/tabular/xgboost_hyperparameter_tuning_job_pipeline.yaml,sha256=agxb0G_QoLsBRPQ0A2ZnvL41XCxoCJ7s6zjiJfGAUQU,223628
 google_cloud_pipeline_components/preview/automl/tabular/xgboost_trainer.py,sha256=fpc36b4aiggwj7suMa3oCxcJbQoFQZ5H15eFeustDWc,2333
-google_cloud_pipeline_components/preview/automl/tabular/xgboost_trainer_pipeline.yaml,sha256=NeIboOSDPZ_V0elazZl4gO79sMz0YX30m1o7ArojZKA,213303
+google_cloud_pipeline_components/preview/automl/tabular/xgboost_trainer_pipeline.yaml,sha256=u4QjIp5LXs1oh4ivwQro_CDR5vVsLcMsvOxw9kG0BtU,207747
 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_large_search_space.json,sha256=PCUg32sSgMbrLrZu2QVV6IyaUyNokUHq3CjS0_0y3uo,3292
 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_medium_search_space.json,sha256=ARIW4gkMZfCgjIJVTSUfzJVH4qc6dWmxdJFWSSXAFmA,3261
 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_large_data_small_search_space.json,sha256=IXoaCZ0heGqduiQKkR-sizbwR1151y9ZeDKqTYVlvPQ,2975
 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_large_search_space.json,sha256=06xYcZU5j5WRXXg3fwOlFrCA3776vB6uMY0voIWL1R8,3287
 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_medium_search_space.json,sha256=fTSgM-8cHE4LDICSVNzRWgoWzhI_NxNWCJSazaxAIgg,3266
 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_medium_data_small_search_space.json,sha256=GDbRYvwwO0OZElSZYsXCRDpRFDIAIuN8plUgu11_w9U,2974
 google_cloud_pipeline_components/preview/automl/tabular/configs/tabnet_params_small_data_large_search_space.json,sha256=HlGIfAEVbes_ZiwUOEDgobsoqXN9V2G8Rta6Sl56MOo,3269
@@ -313,51 +317,51 @@
 google_cloud_pipeline_components/preview/custom_job/component.py,sha256=dnSrcilFi3CbEjh3YxeYYuE8v_SRCbVsWogAksaLEAg,7098
 google_cloud_pipeline_components/preview/custom_job/utils.py,sha256=knm5G8cI1_zdRilHQWujUJ3PNZG9RMo7diQ22FQQXs0,16062
 google_cloud_pipeline_components/preview/dataflow/__init__.py,sha256=74-o9aye0R356KRmf5sBeXZ3OkBWEn62pywrclsEIW4,773
 google_cloud_pipeline_components/preview/llm/__init__.py,sha256=Jt4x_9CWqHYoP5vE_QxPHnEWJuGZGWtAVHSMiQbY-eU,886
 google_cloud_pipeline_components/preview/llm/infer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 google_cloud_pipeline_components/preview/llm/infer/component.py,sha256=Izx-PkCwQFEZBrE9BjYt5xSlt3jWMvH0ko2Qe7jFEg0,7384
 google_cloud_pipeline_components/preview/llm/rlhf/__init__.py,sha256=sb6SfJl6rt3AKjiWxd-KO9DSiZ3PzGZRcsqKuc1A2Cg,606
-google_cloud_pipeline_components/preview/llm/rlhf/component.py,sha256=eXY25PYDDmKXBfql_EU_FRBQHnZ7bJROkuFSeLu4eSA,10044
+google_cloud_pipeline_components/preview/llm/rlhf/component.py,sha256=nTEyRVEa1pWe9hpKBeYhe4WJ75qX00sPI32VWCMWiWk,10629
 google_cloud_pipeline_components/preview/model_evaluation/__init__.py,sha256=k5dgMt_ijQR94SN_xr9pWgynxWM0z3d71Jnw7JZgg18,1949
 google_cloud_pipeline_components/preview/model_evaluation/data_bias_component.py,sha256=YiwkWfbGymX_lDIg_x7AP6nYMm3MQp_NgV8xuSZxCpU,5791
-google_cloud_pipeline_components/preview/model_evaluation/evaluation_llm_classification_pipeline.py,sha256=5aFCFIuqZg7A3bImKjLGOvw_UEXtt16MnQ9KXYKDNJI,11212
-google_cloud_pipeline_components/preview/model_evaluation/evaluation_llm_text_generation_pipeline.py,sha256=Mp8HiahgHtbjyUmbkvOuLij0mS2ZB9PsFj9VHSFTFoY,9560
+google_cloud_pipeline_components/preview/model_evaluation/evaluation_llm_classification_pipeline.py,sha256=IjCIakZFh7KB6kmku7ztaH-qFm9LoEctwwMtxmDOg5g,11262
+google_cloud_pipeline_components/preview/model_evaluation/evaluation_llm_text_generation_pipeline.py,sha256=mcWYU8WwoRWooGWbXKpAEbWs0jo80TWUMhi8we7QRFA,9600
 google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_component.py,sha256=XWrI1inQ9hKixFrp2LUdgu7hONYUvbsxv2GXZ-UTkCY,7450
 google_cloud_pipeline_components/preview/model_evaluation/feature_attribution_graph_component.py,sha256=jesgBUKbIB_qQoYb5-Bv_LBbFHl0tPyMlVFx-o1eE6k,13624
 google_cloud_pipeline_components/preview/model_evaluation/model_bias_component.py,sha256=R8WhT8jf_OOpMuABRh2BYTDEcfiGAf6VA-vFgiTymYY,6674
 google_cloud_pipeline_components/preview/model_evaluation/utils.py,sha256=oRlEvA3zMSTzgxJklZD0A-BzFDx0-PsBHBXZ4kmaREY,7539
 google_cloud_pipeline_components/preview/model_evaluation/model_based_llm_evaluation/__init__.py,sha256=ee_EyGhwqXIjR3Rx9t-o2gV9TssU-VErMU7LtDA7s9k,838
 google_cloud_pipeline_components/preview/model_evaluation/model_based_llm_evaluation/autosxs/__init__.py,sha256=sb6SfJl6rt3AKjiWxd-KO9DSiZ3PzGZRcsqKuc1A2Cg,606
-google_cloud_pipeline_components/preview/model_evaluation/model_based_llm_evaluation/autosxs/autosxs_pipeline.py,sha256=-uh6w6Pyechi8WiuXlOvAZxT2NiCfuJY66DWWIexdEI,9975
+google_cloud_pipeline_components/preview/model_evaluation/model_based_llm_evaluation/autosxs/autosxs_pipeline.py,sha256=2XzasTFNurY0oAEtzd_BsiMtJ4DaCV97Rea3JQBnbcA,11205
 google_cloud_pipeline_components/proto/__init__.py,sha256=aiPUc6gpQwG9cRTYfw3ChFCJfDr3vAIsm2eMYUDJjJQ,661
-google_cloud_pipeline_components/proto/gcp_resources_pb2.py,sha256=EYn1KnHwuH6-Q7eHurgqYx_zA-tjBCqN2yncg07SAAY,7687
+google_cloud_pipeline_components/proto/gcp_resources_pb2.py,sha256=ssNNm4zjiWbuBUS7IH6kyrvvfmcC_Z5F7hOAuQe_YLk,2134
 google_cloud_pipeline_components/types/__init__.py,sha256=1WFkL49QEy-gNb6ywQOE4yZkD7DoULAeiL1tLdb3S28,606
 google_cloud_pipeline_components/types/artifact_types.py,sha256=zvwvzRuFb_s1VS1mtKkltOOACATJk-kG7dVFOUasfw4,23725
 google_cloud_pipeline_components/v1/__init__.py,sha256=E1Fie3Gq3KKLHEBGUUBeuao-Eo1uwuBfAWWhx2siwaE,765
 google_cloud_pipeline_components/v1/automl/__init__.py,sha256=l8whL8MMhZ-KMyacLpGzO-5cNxxD2wUW6PtMVx0C_mI,631
-google_cloud_pipeline_components/v1/automl/forecasting/__init__.py,sha256=cGZ52eyY83aDKd-fTIkQCDsAX6n7feK9KcKkMg3SHeE,806
-google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_predict_pipeline.yaml,sha256=jyNRbkApyvBHkDu-BplVVsrbIHEVzKcpkzL5yiAWcj0,52484
-google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_train_pipeline.yaml,sha256=1W0mRIC59DoqJ9avpjf8F0zn5NxToMEix5L1eAsJNXw,252190
-google_cloud_pipeline_components/v1/automl/forecasting/prophet_predict_pipeline.yaml,sha256=7yAu-Yxr9mLYpsB7lzysFDz4i2apk2o1l79bZJl1jJ4,95718
-google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer.py,sha256=7-9NRcT-gdbkDxw3RRMbrB_mFpu5P4CjBzo3jWvXESw,8363
-google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer_pipeline.yaml,sha256=otEguZA1aBpIr4dPRpY29bxx7gFkpZyP0rQUX8JWdYw,149452
-google_cloud_pipeline_components/v1/automl/forecasting/utils.py,sha256=xf3nJR2riWDDxep8mFvaqOeTztRGrf7QLFEIxLkqSBU,15028
-google_cloud_pipeline_components/v1/automl/tabular/__init__.py,sha256=dmJqevxJZdEge69BeFrO1T7l4-kFejMNbbAQWvtFofw,2318
-google_cloud_pipeline_components/v1/automl/tabular/automl_tabular_pipeline.yaml,sha256=h2F89wIJWlcuEwrxtDr0S4YdmTomTHuxK_mGcHVIAC4,503740
-google_cloud_pipeline_components/v1/automl/tabular/cv_trainer.py,sha256=dhR7BlbjaEkMIlG2Q6msImaohRtoP_jziw_3h54F25E,6708
-google_cloud_pipeline_components/v1/automl/tabular/ensemble.py,sha256=hMNZhCYj-YNZm5BSMAUB1zpT04-JhhkaUs1ZAeFvlC0,6935
-google_cloud_pipeline_components/v1/automl/tabular/finalizer.py,sha256=mXf0ZeBk3kW7h5D2MzQHrHhmNIczkYvEnmUtVrtb6jI,3108
-google_cloud_pipeline_components/v1/automl/tabular/infra_validator.py,sha256=eSY5OnclPiLDvHcNXH3erxEqnv2Ke6vdrflSWMv2iSk,1333
-google_cloud_pipeline_components/v1/automl/tabular/split_materialized_data.py,sha256=4XaDppXRaDDfJ9S9QyJDbA6aJuW6NjMpDwbiw89gdVY,5571
-google_cloud_pipeline_components/v1/automl/tabular/stage_1_tuner.py,sha256=SyOQ_xD5_IZJtV0GeU176UUbS5mZ1YosXytUzA86dck,7769
-google_cloud_pipeline_components/v1/automl/tabular/stats_and_example_gen.py,sha256=7nb452G085wpVAZuCHVEgzD6qaTi-kzjo39jJCAfbpY,13389
-google_cloud_pipeline_components/v1/automl/tabular/training_configurator_and_validator.py,sha256=ue6HNSiYVsI4goaM7ZUJT1CON7FZHR-gkgCi-5LPP_Q,12214
-google_cloud_pipeline_components/v1/automl/tabular/transform.py,sha256=m3MlUOCrBjLAPBxUsPlpXHCR848UrIXHWXRwkHwslx0,8249
-google_cloud_pipeline_components/v1/automl/tabular/utils.py,sha256=BLFnCCviq-isjxVaCXF9OoARRJD_H85-fb6EH8D2RwI,74895
+google_cloud_pipeline_components/v1/automl/forecasting/__init__.py,sha256=FVS4qHJEuAxXZuW9ffOD16_-T0xwaUUBh2j1d4AF8HQ,1488
+google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_predict_pipeline.yaml,sha256=5GmCc48s-R_qS162dVN44ZpZsaekmrYWDp4C9_av6DA,52484
+google_cloud_pipeline_components/v1/automl/forecasting/bqml_arima_train_pipeline.yaml,sha256=L7pWON8GjUMK7Ls6esi4rAB32JFaP8mCQOj9fQABxtc,247001
+google_cloud_pipeline_components/v1/automl/forecasting/prophet_predict_pipeline.yaml,sha256=LUE4m14qAN4rupIIKqBlNL0WBvnedcf0hIhlsnc18qM,95718
+google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer.py,sha256=oIBrEPV1MvykdLfsCbAmsRkBc7n-Ud3GfM3TlC7Iep4,8363
+google_cloud_pipeline_components/v1/automl/forecasting/prophet_trainer_pipeline.yaml,sha256=YiP31ItMcuVLOzoUqVdBQOqbmuyu3qKFMYAWTnk8P_w,144072
+google_cloud_pipeline_components/v1/automl/forecasting/utils.py,sha256=Q0SxHlj2jhlpX-buOkxZqDL0wN8Si2-f5iMo08w54ms,14855
+google_cloud_pipeline_components/v1/automl/tabular/__init__.py,sha256=Clqtp6KThsxtvgu6Egj-dbkftci-VWDSKXXo_T6bTv8,2480
+google_cloud_pipeline_components/v1/automl/tabular/automl_tabular_pipeline.yaml,sha256=zGuV5AmbMSziPn0NZKimpA7llcJ_N6jhDw0NnWH62H0,502694
+google_cloud_pipeline_components/v1/automl/tabular/cv_trainer.py,sha256=Xb1iNkwUF4gm7ka_0fWciKj5OaUZmJqU3UikZFTHibQ,6708
+google_cloud_pipeline_components/v1/automl/tabular/ensemble.py,sha256=nIwpBXj1Hntq0IBjOMs6-MutrGjrG45L4j17X1tUHvc,6935
+google_cloud_pipeline_components/v1/automl/tabular/finalizer.py,sha256=W-xQY2cFbTFhqrjUKl4Ih2UNzmjbA53aCIepiXgt4Tc,3108
+google_cloud_pipeline_components/v1/automl/tabular/infra_validator.py,sha256=OgaDAqPDlZGZP7G7k-3knG5XAAojx9m-KINqaExMEJ8,1333
+google_cloud_pipeline_components/v1/automl/tabular/split_materialized_data.py,sha256=zP141V-p3HDXAoaah3TjkzSmguUzoDC9981UjnYP3QE,5571
+google_cloud_pipeline_components/v1/automl/tabular/stage_1_tuner.py,sha256=0RJfiWPOfl1BQzX2Ftth911IVP4yzWpRFo9OiO5XHLA,7761
+google_cloud_pipeline_components/v1/automl/tabular/stats_and_example_gen.py,sha256=d8IC0d7sHzX6jI3tWhv2la_VanCIf4ZlN81mrQzXbYA,13367
+google_cloud_pipeline_components/v1/automl/tabular/training_configurator_and_validator.py,sha256=DSlK5TmuXMo0zdGeyrDf9kzJuYUO6Ud4-zAGKL2lsC0,12202
+google_cloud_pipeline_components/v1/automl/tabular/transform.py,sha256=xItkZW_Yjdhid3FLpf9ah9_EqiMHGsDgo5iLLliVeCY,8241
+google_cloud_pipeline_components/v1/automl/tabular/utils.py,sha256=2iBuwuUXfDKhZA1nfhfvl_IcnI4j0uesR4TbM9W13uU,74625
 google_cloud_pipeline_components/v1/automl/tabular/deprecated/default_pipeline.json,sha256=r9w-yavDpzGQX94geZ4q58aJdQdn2VvCm14IsNFIZyQ,321170
 google_cloud_pipeline_components/v1/automl/training_job/__init__.py,sha256=oCltxPx9ycdB2VJYLTNx3_o5t1zBXJFjNUOFFpoADwE,1803
 google_cloud_pipeline_components/v1/automl/training_job/automl_forecasting_training_job/__init__.py,sha256=frKfHDfi0BBzLErUs-xaQlj_MpE_Yr28scoHceyvXLE,655
 google_cloud_pipeline_components/v1/automl/training_job/automl_forecasting_training_job/component.py,sha256=I7tKCMfHqgauQkxy_zucGBTWEEk77mJmllz9BpDpDQw,27062
 google_cloud_pipeline_components/v1/automl/training_job/automl_image_training_job/__init__.py,sha256=izgZx63tUVNrC7AxoJ4_mEvW5LOLYGu4IvEIfUkCocs,649
 google_cloud_pipeline_components/v1/automl/training_job/automl_image_training_job/component.py,sha256=9KPaRpOS7Xuc2l1dWZtssJUoff6FOsredAPMXumWMX4,18382
 google_cloud_pipeline_components/v1/automl/training_job/automl_tabular_training_job/__init__.py,sha256=zXcb1Dv4oYh5Bv_f550bZG6dxL7mJX5WZpr3qhKPhGw,651
@@ -416,15 +420,15 @@
 google_cloud_pipeline_components/v1/bigquery/ml_trial_info/__init__.py,sha256=UcVE91B8ahaEaC07KuB2ihR5kYaaqPu-t2ELriSLDyM,671
 google_cloud_pipeline_components/v1/bigquery/ml_trial_info/component.py,sha256=x8mfLr6cn4-TnQg1ZDGv50kr2GdYdiStpe5lGIIAW5o,5083
 google_cloud_pipeline_components/v1/bigquery/ml_weights/__init__.py,sha256=yL2Xpphod8RlvxUaX7Vt5SjmX3HXJ1uFWZUv5E9Zoz8,668
 google_cloud_pipeline_components/v1/bigquery/ml_weights/component.py,sha256=o9SKxC3DtmFyjtAT1pM-clvJNKTii68wPYKVc-mNs4s,4502
 google_cloud_pipeline_components/v1/bigquery/predict_model/__init__.py,sha256=qvPMXRAmG93aCuOvfHoFU9XDhGpyYF9ICgYl1yOj60Y,671
 google_cloud_pipeline_components/v1/bigquery/predict_model/component.py,sha256=18IoQFsddxCXNwkjm2CxWCUyE3ZBBfyzG6E9emFjIlc,6279
 google_cloud_pipeline_components/v1/bigquery/query_job/__init__.py,sha256=Ot_1BluGAMGP2l6lok-ksqUrcFegEuuvBnEFHcGjydI,667
-google_cloud_pipeline_components/v1/bigquery/query_job/component.py,sha256=8Tlaj9WdP6KHllhCsXZaswfI6ouwIHCLgi2dFyA4HbM,4912
+google_cloud_pipeline_components/v1/bigquery/query_job/component.py,sha256=100KwAgJZ8fQrSCfy8oMwjghsO9HICvQRHOQjDwqh_I,5187
 google_cloud_pipeline_components/v1/custom_job/__init__.py,sha256=qxHupfqqpAQ3sK3Pp7pKNGfPKsS4qb1E9xM067U5MMw,1278
 google_cloud_pipeline_components/v1/custom_job/component.py,sha256=vluHmVr_7jqJgx4t9n8HE2dHhxjZRtgT05sTa-95Pvw,6070
 google_cloud_pipeline_components/v1/custom_job/utils.py,sha256=M_AN2mh5fk_6A4M1_G8rT8JkA6eGjGLmcrcNKYjMT1Q,15509
 google_cloud_pipeline_components/v1/dataflow/__init__.py,sha256=ZPJGssKq2P5iwFY_I68gZPoXSPHVNYQ59nVlA8mOtOo,1063
 google_cloud_pipeline_components/v1/dataflow/flex_template/__init__.py,sha256=uG42x7_0zehtVV7f_fHvPHBJ48aqi3jJwLY6tplH8jk,669
 google_cloud_pipeline_components/v1/dataflow/flex_template/component.py,sha256=C9oHSlOwh0fTUPN9b2u7vNvM2NbkWT-FX82kPOKkzLc,11701
 google_cloud_pipeline_components/v1/dataflow/python_job/__init__.py,sha256=05M05rCfm7pVr4rGkPpVDfczWdUzbdcZtoi_kLJUwrE,664
@@ -482,33 +486,35 @@
 google_cloud_pipeline_components/v1/forecasting/preprocess/__init__.py,sha256=G9Eq_6xCFACo4S-wN3J9VZDzZ5yROT40F4dQzm-0wUw,646
 google_cloud_pipeline_components/v1/forecasting/preprocess/component.py,sha256=0o1baDbV8bZ20PyAOzyBVwA0CLlSXxfhunhDcgAa-lY,2241
 google_cloud_pipeline_components/v1/forecasting/validate/__init__.py,sha256=ALUsZKDjhydG-suCblD6-jF4Hc3qank5tMYuKGuO8EI,644
 google_cloud_pipeline_components/v1/forecasting/validate/component.py,sha256=B5LRMMebkZOrSoxe8KpboGBZjPanzL8XKlwpIXpH1Zw,1891
 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/__init__.py,sha256=HKGYoOErTL6i4-NwcQXeUH5zbYPHJOYFY2EZUXIPv5U,1300
 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/component.py,sha256=uQ8Y5Svl1XegXzxIxx8TKEiHQ5TtRhzMoKvf8_XYIuY,9479
 google_cloud_pipeline_components/v1/hyperparameter_tuning_job/utils.py,sha256=6Ngxh1nfPrMM-fIUhjI19HLp_WTzL0vqURQ0XYZh9jk,3118
-google_cloud_pipeline_components/v1/model/__init__.py,sha256=poWsOUoJ46NBFuK4FrxtICk1MUT2xYkHq8bsW_bMdaI,1148
+google_cloud_pipeline_components/v1/model/__init__.py,sha256=_HMSMNAvUp4W4EOOsEu5_5GacNybVRgn1C19A1uEGos,1264
 google_cloud_pipeline_components/v1/model/delete_model/__init__.py,sha256=vH1ffcOuoYGAkK3Jtks2QCQqBswzyDS-zkHIrT9MpyA,638
 google_cloud_pipeline_components/v1/model/delete_model/component.py,sha256=uKA9soKJTKMVJrZEZH1nrpk90KGoA4ngNJODrMhswfw,2490
 google_cloud_pipeline_components/v1/model/export_model/__init__.py,sha256=c4fG2x5dxyMDY4m9YrqJK67qqSVa3W7HA0E0jCO2OYQ,661
 google_cloud_pipeline_components/v1/model/export_model/component.py,sha256=7Pb4GI9U7ITAqpCNBMvM43r4qCQqMsjgZJRNWAJ51lI,4908
+google_cloud_pipeline_components/v1/model/get_model/__init__.py,sha256=oAWKl9PXSXXsMY-M9EizQbIVvHNY-Qw84xfBGimvYLQ,662
+google_cloud_pipeline_components/v1/model/get_model/component.py,sha256=zLd4zHVdCIkrf6jN04r1sLw7k5zbg4CMbRnJfPoawZM,2117
 google_cloud_pipeline_components/v1/model/upload_model/__init__.py,sha256=6uwVQw6h3TXxei5imUE4JaS97XXzDRPQyNnTE-qFjck,661
-google_cloud_pipeline_components/v1/model/upload_model/component.py,sha256=GeOz2XZHGXInzeUvlWSL4AxRA7Y6HmrTChUCTLODvy0,6646
+google_cloud_pipeline_components/v1/model/upload_model/component.py,sha256=6zy9G2AK2twiyT-B2X15qovvi6qHu0koRzzzelgN8CQ,7280
 google_cloud_pipeline_components/v1/model_evaluation/__init__.py,sha256=8hxWO8ijYDmg48H3IEy0G3ihjORkVctdTkur4ECoyR0,2358
 google_cloud_pipeline_components/v1/model_evaluation/classification_component.py,sha256=x0pUY4OwFIkmS11Q7rDLI6fspaDUBo6wU5BBP2jAKC0,12122
 google_cloud_pipeline_components/v1/model_evaluation/error_analysis_pipeline.py,sha256=l972cEWDViVV41oCy0jTsX96Pau49D3KdJA3yAjKEY0,20122
 google_cloud_pipeline_components/v1/model_evaluation/evaluated_annotation_pipeline.py,sha256=JskLsIHvLNNvNaMD8gTa0NWlB5gKiSSyqeC78Fn5OW8,12142
 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_feature_attribution_pipeline.py,sha256=sEoMoR5ITp0fWGnbUvGO6ZApz5Z04URseMONyMytSWI,46187
 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_tabular_pipeline.py,sha256=p-GH_tVqffHwck5Sll0BHsnvVAHQk48WNAUohZxATcs,37108
 google_cloud_pipeline_components/v1/model_evaluation/evaluation_automl_unstructure_data_pipeline.py,sha256=aiZOK5BE5mdqJL3s4pU1Y_ynHvWBE9JIxl9UrJuNsco,42404
 google_cloud_pipeline_components/v1/model_evaluation/evaluation_feature_attribution_pipeline.py,sha256=ChDwHvPCn0prrK1FLvEhAbaTmA153M9NG3Wj3QIlNHs,51173
 google_cloud_pipeline_components/v1/model_evaluation/forecasting_component.py,sha256=gOnvKAJWa3velczeuVBCzW6b_tcc2v_lNFqHXGhjD44,10017
 google_cloud_pipeline_components/v1/model_evaluation/regression_component.py,sha256=eFrjrKQot3-SlRCoKoTOEsyp2Xj0GfDtrjpxTDKAHYY,9117
 google_cloud_pipeline_components/v1/vertex_notification_email/__init__.py,sha256=YIRljNy_oHY_vRda-kfhm5QiulNd_SIIPbmpzOiYJ0k,863
 google_cloud_pipeline_components/v1/vertex_notification_email/component.py,sha256=Dau8ZI0mzLBnLOUBQm6EtK8gbtX1u57t76Ud5qlg9xc,2163
 google_cloud_pipeline_components/v1/wait_gcp_resources/__init__.py,sha256=w6dfz-rYsYnxFapRH1Dix3GVz0mhPW0m1IVpE6z8jbg,878
 google_cloud_pipeline_components/v1/wait_gcp_resources/component.py,sha256=Nsfj5c3eeZq83fHLvv2IlpK4jrjxLxRksFYOl5W6JnA,2468
-google_cloud_pipeline_components-2.8.0.dist-info/LICENSE,sha256=WNHhf_5RCaeuKWyq_K39vmp9F28LxKsB4SpomwSZ2L0,11357
-google_cloud_pipeline_components-2.8.0.dist-info/METADATA,sha256=8Wcj2Hpu9_zO-fbe0KXuqFCsAb8Fmg5OcISbfdAfSTw,5853
-google_cloud_pipeline_components-2.8.0.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
-google_cloud_pipeline_components-2.8.0.dist-info/top_level.txt,sha256=E8T4T8KGMGLXbHvt2goa98oezRpxryPC6QhWBZ27Hhc,33
-google_cloud_pipeline_components-2.8.0.dist-info/RECORD,,
+google_cloud_pipeline_components-2.9.0.dist-info/LICENSE,sha256=VAc1R5OxOELKsX5L5Ldp5THfNtxtt1cMIZBaC0Jdj5Q,13118
+google_cloud_pipeline_components-2.9.0.dist-info/METADATA,sha256=iBkPb2XZavjU3FY-q979x5akpAo9g10Mf6Qq7Kqxef0,5843
+google_cloud_pipeline_components-2.9.0.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+google_cloud_pipeline_components-2.9.0.dist-info/top_level.txt,sha256=E8T4T8KGMGLXbHvt2goa98oezRpxryPC6QhWBZ27Hhc,33
+google_cloud_pipeline_components-2.9.0.dist-info/RECORD,,
```

