# Comparing `tmp/vsrife-4.2.0-py3-none-any.whl.zip` & `tmp/vsrife-5.0.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,33 +1,52 @@
-Zip file size: 25234 bytes, number of entries: 31
--rw-r--r--  2.0 fat     4157 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_0.py
--rw-r--r--  2.0 fat     4157 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_1.py
--rw-r--r--  2.0 fat     4371 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_10.py
--rw-r--r--  2.0 fat     4371 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_11.py
--rw-r--r--  2.0 fat     3599 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_2.py
--rw-r--r--  2.0 fat     3599 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_3.py
--rw-r--r--  2.0 fat     3599 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_4.py
--rw-r--r--  2.0 fat     4615 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_5.py
--rw-r--r--  2.0 fat     4615 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_6.py
--rw-r--r--  2.0 fat     4171 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_7.py
--rw-r--r--  2.0 fat     4171 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_8.py
--rw-r--r--  2.0 fat     4171 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_9.py
--rw-r--r--  2.0 fat    12151 b- defN 20-Feb-02 00:00 vsrife/__init__.py
--rw-r--r--  2.0 fat     1148 b- defN 20-Feb-02 00:00 vsrife/__main__.py
--rw-r--r--  2.0 fat     1145 b- defN 20-Feb-02 00:00 vsrife/warplayer.py
+Zip file size: 41434 bytes, number of entries: 50
+-rw-r--r--  2.0 fat     4152 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_0.py
+-rw-r--r--  2.0 fat     4152 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_1.py
+-rw-r--r--  2.0 fat     4581 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_10.py
+-rw-r--r--  2.0 fat     4581 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_11.py
+-rw-r--r--  2.0 fat     4581 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_12.py
+-rw-r--r--  2.0 fat     4576 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_12_lite.py
+-rw-r--r--  2.0 fat     4915 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_13.py
+-rw-r--r--  2.0 fat     4576 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_13_lite.py
+-rw-r--r--  2.0 fat     4915 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_14.py
+-rw-r--r--  2.0 fat     4915 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_14_lite.py
+-rw-r--r--  2.0 fat     4915 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_15.py
+-rw-r--r--  2.0 fat     4910 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_15_lite.py
+-rw-r--r--  2.0 fat     4910 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_16_lite.py
+-rw-r--r--  2.0 fat     3570 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_2.py
+-rw-r--r--  2.0 fat     3570 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_3.py
+-rw-r--r--  2.0 fat     3570 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_4.py
+-rw-r--r--  2.0 fat     4043 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_5.py
+-rw-r--r--  2.0 fat     4043 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_6.py
+-rw-r--r--  2.0 fat     4381 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_7.py
+-rw-r--r--  2.0 fat     4381 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_8.py
+-rw-r--r--  2.0 fat     4381 b- defN 20-Feb-02 00:00 vsrife/IFNet_HDv3_v4_9.py
+-rw-r--r--  2.0 fat    13432 b- defN 20-Feb-02 00:00 vsrife/__init__.py
+-rw-r--r--  2.0 fat     1407 b- defN 20-Feb-02 00:00 vsrife/__main__.py
+-rw-r--r--  2.0 fat    17133 b- defN 20-Feb-02 00:00 vsrife/interpolate.py
+-rw-r--r--  2.0 fat     1033 b- defN 20-Feb-02 00:00 vsrife/warplayer.py
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.0.pkl
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.1.pkl
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.10.pkl
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.11.pkl
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.12.lite.pkl
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.12.pkl
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.13.lite.pkl
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.13.pkl
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.14.lite.pkl
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.14.pkl
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.15.lite.pkl
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.15.pkl
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.16.lite.pkl
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.2.pkl
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.3.pkl
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.4.pkl
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.5.pkl
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.6.pkl
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.7.pkl
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.8.pkl
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsrife/models/flownet_v4.9.pkl
-?rw-r--r--  2.0 fat     3212 b- defN 20-Feb-02 00:00 vsrife-4.2.0.dist-info/METADATA
-?rw-r--r--  2.0 fat       87 b- defN 20-Feb-02 00:00 vsrife-4.2.0.dist-info/WHEEL
-?rw-r--r--  2.0 fat     1084 b- defN 20-Feb-02 00:00 vsrife-4.2.0.dist-info/licenses/LICENSE
-?rw-r--r--  2.0 fat     2522 b- defN 20-Feb-02 00:00 vsrife-4.2.0.dist-info/RECORD
-31 files, 70945 bytes uncompressed, 21164 bytes compressed:  70.2%
+?rw-r--r--  2.0 fat     1922 b- defN 20-Feb-02 00:00 vsrife-5.0.0.dist-info/METADATA
+?rw-r--r--  2.0 fat       87 b- defN 20-Feb-02 00:00 vsrife-5.0.0.dist-info/WHEEL
+?rw-r--r--  2.0 fat     1084 b- defN 20-Feb-02 00:00 vsrife-5.0.0.dist-info/licenses/LICENSE
+?rw-r--r--  2.0 fat     4163 b- defN 20-Feb-02 00:00 vsrife-5.0.0.dist-info/RECORD
+50 files, 132879 bytes uncompressed, 34752 bytes compressed:  73.8%
```

## zipnote {}

```diff
@@ -6,14 +6,41 @@
 
 Filename: vsrife/IFNet_HDv3_v4_10.py
 Comment: 
 
 Filename: vsrife/IFNet_HDv3_v4_11.py
 Comment: 
 
+Filename: vsrife/IFNet_HDv3_v4_12.py
+Comment: 
+
+Filename: vsrife/IFNet_HDv3_v4_12_lite.py
+Comment: 
+
+Filename: vsrife/IFNet_HDv3_v4_13.py
+Comment: 
+
+Filename: vsrife/IFNet_HDv3_v4_13_lite.py
+Comment: 
+
+Filename: vsrife/IFNet_HDv3_v4_14.py
+Comment: 
+
+Filename: vsrife/IFNet_HDv3_v4_14_lite.py
+Comment: 
+
+Filename: vsrife/IFNet_HDv3_v4_15.py
+Comment: 
+
+Filename: vsrife/IFNet_HDv3_v4_15_lite.py
+Comment: 
+
+Filename: vsrife/IFNet_HDv3_v4_16_lite.py
+Comment: 
+
 Filename: vsrife/IFNet_HDv3_v4_2.py
 Comment: 
 
 Filename: vsrife/IFNet_HDv3_v4_3.py
 Comment: 
 
 Filename: vsrife/IFNet_HDv3_v4_4.py
@@ -36,14 +63,17 @@
 
 Filename: vsrife/__init__.py
 Comment: 
 
 Filename: vsrife/__main__.py
 Comment: 
 
+Filename: vsrife/interpolate.py
+Comment: 
+
 Filename: vsrife/warplayer.py
 Comment: 
 
 Filename: vsrife/models/flownet_v4.0.pkl
 Comment: 
 
 Filename: vsrife/models/flownet_v4.1.pkl
@@ -51,14 +81,41 @@
 
 Filename: vsrife/models/flownet_v4.10.pkl
 Comment: 
 
 Filename: vsrife/models/flownet_v4.11.pkl
 Comment: 
 
+Filename: vsrife/models/flownet_v4.12.lite.pkl
+Comment: 
+
+Filename: vsrife/models/flownet_v4.12.pkl
+Comment: 
+
+Filename: vsrife/models/flownet_v4.13.lite.pkl
+Comment: 
+
+Filename: vsrife/models/flownet_v4.13.pkl
+Comment: 
+
+Filename: vsrife/models/flownet_v4.14.lite.pkl
+Comment: 
+
+Filename: vsrife/models/flownet_v4.14.pkl
+Comment: 
+
+Filename: vsrife/models/flownet_v4.15.lite.pkl
+Comment: 
+
+Filename: vsrife/models/flownet_v4.15.pkl
+Comment: 
+
+Filename: vsrife/models/flownet_v4.16.lite.pkl
+Comment: 
+
 Filename: vsrife/models/flownet_v4.2.pkl
 Comment: 
 
 Filename: vsrife/models/flownet_v4.3.pkl
 Comment: 
 
 Filename: vsrife/models/flownet_v4.4.pkl
@@ -75,20 +132,20 @@
 
 Filename: vsrife/models/flownet_v4.8.pkl
 Comment: 
 
 Filename: vsrife/models/flownet_v4.9.pkl
 Comment: 
 
-Filename: vsrife-4.2.0.dist-info/METADATA
+Filename: vsrife-5.0.0.dist-info/METADATA
 Comment: 
 
-Filename: vsrife-4.2.0.dist-info/WHEEL
+Filename: vsrife-5.0.0.dist-info/WHEEL
 Comment: 
 
-Filename: vsrife-4.2.0.dist-info/licenses/LICENSE
+Filename: vsrife-5.0.0.dist-info/licenses/LICENSE
 Comment: 
 
-Filename: vsrife-4.2.0.dist-info/RECORD
+Filename: vsrife-5.0.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## vsrife/IFNet_HDv3_v4_0.py

```diff
@@ -30,15 +30,15 @@
             conv(c, c),
         )
         self.lastconv = nn.ConvTranspose2d(c, 5, 4, 2, 1)
 
     def forward(self, x, flow=None, scale=1):
         x = F.interpolate(x, scale_factor= 1. / scale, mode="bilinear")
         if flow is not None:
-            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") * 1. / scale
+            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") / scale
             x = torch.cat((x, flow), 1)
         feat = self.conv0(x)
         feat = self.convblock(feat) + feat
         tmp = self.lastconv(feat)
         tmp = F.interpolate(tmp, scale_factor=scale*2, mode="bilinear")
         flow = tmp[:, :4] * scale * 2
         mask = tmp[:, 4:5]
```

## vsrife/IFNet_HDv3_v4_1.py

```diff
@@ -30,15 +30,15 @@
             conv(c, c),
         )
         self.lastconv = nn.ConvTranspose2d(c, 5, 4, 2, 1)
 
     def forward(self, x, flow=None, scale=1):
         x = F.interpolate(x, scale_factor= 1. / scale, mode="bilinear")
         if flow is not None:
-            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") * 1. / scale
+            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") / scale
             x = torch.cat((x, flow), 1)
         feat = self.conv0(x)
         feat = self.convblock(feat) + feat
         tmp = self.lastconv(feat)
         tmp = F.interpolate(tmp, scale_factor=scale*2, mode="bilinear")
         flow = tmp[:, :4] * scale * 2
         mask = tmp[:, 4:5]
```

## vsrife/IFNet_HDv3_v4_10.py

```diff
@@ -1,35 +1,21 @@
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
+from .interpolate import interpolate
 from .warplayer import warp
 
-torch.fx.wrap('warp')
 
 def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):
     return nn.Sequential(
         nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
                   padding=padding, dilation=dilation, bias=True),
         nn.LeakyReLU(0.2, True)
     )
 
-class MyPixelShuffle(nn.Module):
-    def __init__(self, upscale_factor):
-        super(MyPixelShuffle, self).__init__()
-        self.upscale_factor = upscale_factor
-
-    def forward(self, input):
-        b, c, hh, hw = input.size()
-        out_channel = c // (self.upscale_factor**2)
-        h = hh * self.upscale_factor
-        w = hw * self.upscale_factor
-        x_view = input.view(b, out_channel, self.upscale_factor, self.upscale_factor, hh, hw)
-        return x_view.permute(0, 1, 4, 2, 5, 3).reshape(b, out_channel, h, w)
-
 class ResConv(nn.Module):
     def __init__(self, c, dilation=1):
         super(ResConv, self).__init__()
         self.conv = nn.Conv2d(c, c, 3, 1, dilation, dilation=dilation, groups=1\
 )
         self.beta = nn.Parameter(torch.ones((1, c, 1, 1)), requires_grad=True)
         self.relu = nn.LeakyReLU(0.2, True)
@@ -52,26 +38,26 @@
             ResConv(c),
             ResConv(c),
             ResConv(c),
             ResConv(c),
         )
         self.lastconv = nn.Sequential(
             nn.ConvTranspose2d(c, 4*6, 4, 2, 1),
-            MyPixelShuffle(2)
+            nn.PixelShuffle(2)
         )
 
     def forward(self, x, flow=None, scale=1):
-        x = F.interpolate(x, scale_factor= 1. / scale, mode="bilinear")
+        x = interpolate(x, scale_factor= 1. / scale, mode="bilinear")
         if flow is not None:
-            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") * 1. / scale
+            flow = interpolate(flow, scale_factor= 1. / scale, mode="bilinear") / scale
             x = torch.cat((x, flow), 1)
         feat = self.conv0(x)
         feat = self.convblock(feat)
         tmp = self.lastconv(feat)
-        tmp = F.interpolate(tmp, scale_factor=scale, mode="bilinear")
+        tmp = interpolate(tmp, scale_factor=scale, mode="bilinear")
         flow = tmp[:, :4] * scale
         mask = tmp[:, 4:5]
         return flow, mask
 
 class IFNet(nn.Module):
     def __init__(self, scale=1, ensemble=False):
         super(IFNet, self).__init__()
@@ -85,14 +71,15 @@
             nn.Conv2d(32, 32, 3, 1, 1),
             nn.LeakyReLU(0.2, True),
             nn.Conv2d(32, 32, 3, 1, 1),
             nn.LeakyReLU(0.2, True),
             nn.ConvTranspose2d(32, 8, 4, 2, 1)
         )
         self.scale_list = [8/scale, 4/scale, 2/scale, 1/scale]
+        self.ensemble = ensemble
 
     def forward(self, img0, img1, timestep):
         f0 = self.encode(img0[:, :3])
         f1 = self.encode(img1[:, :3])
         flow_list = []
         merged = []
         mask_list = []
@@ -100,16 +87,28 @@
         warped_img1 = img1
         flow = None
         mask = None
         block = [self.block0, self.block1, self.block2, self.block3]
         for i in range(4):
             if flow is None:
                 flow, mask = block[i](torch.cat((img0[:, :3], img1[:, :3], f0, f1, timestep), 1), None, scale=self.scale_list[i])
+                if self.ensemble:
+                    f_, m_ = block[i](torch.cat((img1[:, :3], img0[:, :3], f1, f0, 1-timestep), 1), None, scale=self.scale_list[i])
+                    flow = (flow + torch.cat((f_[:, 2:4], f_[:, :2]), 1)) / 2
+                    mask = (mask + (-m_)) / 2
             else:
-                fd, mask = block[i](torch.cat((warped_img0[:, :3], warped_img1[:, :3], warp(f0, flow[:, :2]), warp(f1, flow[:, 2:4]), timestep, mask), 1), flow, scale=self.scale_list[i])
+                wf0 = warp(f0, flow[:, :2])
+                wf1 = warp(f1, flow[:, 2:4])
+                fd, m0 = block[i](torch.cat((warped_img0[:, :3], warped_img1[:, :3], wf0, wf1, timestep, mask), 1), flow, scale=self.scale_list[i])
+                if self.ensemble:
+                    f_, m_ = block[i](torch.cat((warped_img1[:, :3], warped_img0[:, :3], wf1, wf0, 1-timestep, -mask), 1), torch.cat((flow[:, 2:4], flow[:, :2]), 1), scale=self.scale_list[i])
+                    fd = (fd + torch.cat((f_[:, 2:4], f_[:, :2]), 1)) / 2
+                    mask = (m0 + (-m_)) / 2
+                else:
+                    mask = m0
                 flow = flow + fd
             mask_list.append(mask)
             flow_list.append(flow)
             warped_img0 = warp(img0, flow[:, :2])
             warped_img1 = warp(img1, flow[:, 2:4])
             merged.append((warped_img0, warped_img1))
         mask = torch.sigmoid(mask)
```

## vsrife/IFNet_HDv3_v4_11.py

```diff
@@ -1,35 +1,21 @@
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
+from .interpolate import interpolate
 from .warplayer import warp
 
-torch.fx.wrap('warp')
 
 def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):
     return nn.Sequential(
         nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
                   padding=padding, dilation=dilation, bias=True),
         nn.LeakyReLU(0.2, True)
     )
 
-class MyPixelShuffle(nn.Module):
-    def __init__(self, upscale_factor):
-        super(MyPixelShuffle, self).__init__()
-        self.upscale_factor = upscale_factor
-
-    def forward(self, input):
-        b, c, hh, hw = input.size()
-        out_channel = c // (self.upscale_factor**2)
-        h = hh * self.upscale_factor
-        w = hw * self.upscale_factor
-        x_view = input.view(b, out_channel, self.upscale_factor, self.upscale_factor, hh, hw)
-        return x_view.permute(0, 1, 4, 2, 5, 3).reshape(b, out_channel, h, w)
-
 class ResConv(nn.Module):
     def __init__(self, c, dilation=1):
         super(ResConv, self).__init__()
         self.conv = nn.Conv2d(c, c, 3, 1, dilation, dilation=dilation, groups=1\
 )
         self.beta = nn.Parameter(torch.ones((1, c, 1, 1)), requires_grad=True)
         self.relu = nn.LeakyReLU(0.2, True)
@@ -52,26 +38,26 @@
             ResConv(c),
             ResConv(c),
             ResConv(c),
             ResConv(c),
         )
         self.lastconv = nn.Sequential(
             nn.ConvTranspose2d(c, 4*6, 4, 2, 1),
-            MyPixelShuffle(2)
+            nn.PixelShuffle(2)
         )
 
     def forward(self, x, flow=None, scale=1):
-        x = F.interpolate(x, scale_factor= 1. / scale, mode="bilinear")
+        x = interpolate(x, scale_factor= 1. / scale, mode="bilinear")
         if flow is not None:
-            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") * 1. / scale
+            flow = interpolate(flow, scale_factor= 1. / scale, mode="bilinear") / scale
             x = torch.cat((x, flow), 1)
         feat = self.conv0(x)
         feat = self.convblock(feat)
         tmp = self.lastconv(feat)
-        tmp = F.interpolate(tmp, scale_factor=scale, mode="bilinear")
+        tmp = interpolate(tmp, scale_factor=scale, mode="bilinear")
         flow = tmp[:, :4] * scale
         mask = tmp[:, 4:5]
         return flow, mask
 
 class IFNet(nn.Module):
     def __init__(self, scale=1, ensemble=False):
         super(IFNet, self).__init__()
@@ -85,14 +71,15 @@
             nn.Conv2d(32, 32, 3, 1, 1),
             nn.LeakyReLU(0.2, True),
             nn.Conv2d(32, 32, 3, 1, 1),
             nn.LeakyReLU(0.2, True),
             nn.ConvTranspose2d(32, 8, 4, 2, 1)
         )
         self.scale_list = [8/scale, 4/scale, 2/scale, 1/scale]
+        self.ensemble = ensemble
 
     def forward(self, img0, img1, timestep):
         f0 = self.encode(img0[:, :3])
         f1 = self.encode(img1[:, :3])
         flow_list = []
         merged = []
         mask_list = []
@@ -100,16 +87,28 @@
         warped_img1 = img1
         flow = None
         mask = None
         block = [self.block0, self.block1, self.block2, self.block3]
         for i in range(4):
             if flow is None:
                 flow, mask = block[i](torch.cat((img0[:, :3], img1[:, :3], f0, f1, timestep), 1), None, scale=self.scale_list[i])
+                if self.ensemble:
+                    f_, m_ = block[i](torch.cat((img1[:, :3], img0[:, :3], f1, f0, 1-timestep), 1), None, scale=self.scale_list[i])
+                    flow = (flow + torch.cat((f_[:, 2:4], f_[:, :2]), 1)) / 2
+                    mask = (mask + (-m_)) / 2
             else:
-                fd, mask = block[i](torch.cat((warped_img0[:, :3], warped_img1[:, :3], warp(f0, flow[:, :2]), warp(f1, flow[:, 2:4]), timestep, mask), 1), flow, scale=self.scale_list[i])
+                wf0 = warp(f0, flow[:, :2])
+                wf1 = warp(f1, flow[:, 2:4])
+                fd, m0 = block[i](torch.cat((warped_img0[:, :3], warped_img1[:, :3], wf0, wf1, timestep, mask), 1), flow, scale=self.scale_list[i])
+                if self.ensemble:
+                    f_, m_ = block[i](torch.cat((warped_img1[:, :3], warped_img0[:, :3], wf1, wf0, 1-timestep, -mask), 1), torch.cat((flow[:, 2:4], flow[:, :2]), 1), scale=self.scale_list[i])
+                    fd = (fd + torch.cat((f_[:, 2:4], f_[:, :2]), 1)) / 2
+                    mask = (m0 + (-m_)) / 2
+                else:
+                    mask = m0
                 flow = flow + fd
             mask_list.append(mask)
             flow_list.append(flow)
             warped_img0 = warp(img0, flow[:, :2])
             warped_img1 = warp(img1, flow[:, 2:4])
             merged.append((warped_img0, warped_img1))
         mask = torch.sigmoid(mask)
```

## vsrife/IFNet_HDv3_v4_2.py

```diff
@@ -1,14 +1,13 @@
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
+from .interpolate import interpolate
 from .warplayer import warp
 
-torch.fx.wrap('warp')
 
 def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):
     return nn.Sequential(
         nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
                   padding=padding, dilation=dilation, bias=True),
         nn.LeakyReLU(0.2, True)
     )
@@ -29,22 +28,22 @@
             conv(c, c),
             conv(c, c),
             conv(c, c),
         )
         self.lastconv = nn.ConvTranspose2d(c, 5, 4, 2, 1)
 
     def forward(self, x, flow=None, scale=1):
-        x = F.interpolate(x, scale_factor= 1. / scale, mode="bilinear")
+        x = interpolate(x, scale_factor= 1. / scale, mode="bilinear")
         if flow is not None:
-            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") * 1. / scale
+            flow = interpolate(flow, scale_factor= 1. / scale, mode="bilinear") / scale
             x = torch.cat((x, flow), 1)
         feat = self.conv0(x)
         feat = self.convblock(feat)
         tmp = self.lastconv(feat)
-        tmp = F.interpolate(tmp, scale_factor=scale*2, mode="bilinear")
+        tmp = interpolate(tmp, scale_factor=scale*2, mode="bilinear")
         flow = tmp[:, :4] * scale * 2
         mask = tmp[:, 4:5]
         return flow, mask
 
 class IFNet(nn.Module):
     def __init__(self, scale=1, ensemble=False):
         super(IFNet, self).__init__()
```

## vsrife/IFNet_HDv3_v4_3.py

```diff
@@ -1,14 +1,13 @@
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
+from .interpolate import interpolate
 from .warplayer import warp
 
-torch.fx.wrap('warp')
 
 def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):
     return nn.Sequential(
         nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
                   padding=padding, dilation=dilation, bias=True),
         nn.LeakyReLU(0.2, True)
     )
@@ -29,22 +28,22 @@
             conv(c, c),
             conv(c, c),
             conv(c, c),
         )
         self.lastconv = nn.ConvTranspose2d(c, 5, 4, 2, 1)
 
     def forward(self, x, flow=None, scale=1):
-        x = F.interpolate(x, scale_factor= 1. / scale, mode="bilinear")
+        x = interpolate(x, scale_factor= 1. / scale, mode="bilinear")
         if flow is not None:
-            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") * 1. / scale
+            flow = interpolate(flow, scale_factor= 1. / scale, mode="bilinear") / scale
             x = torch.cat((x, flow), 1)
         feat = self.conv0(x)
         feat = self.convblock(feat)
         tmp = self.lastconv(feat)
-        tmp = F.interpolate(tmp, scale_factor=scale*2, mode="bilinear")
+        tmp = interpolate(tmp, scale_factor=scale*2, mode="bilinear")
         flow = tmp[:, :4] * scale * 2
         mask = tmp[:, 4:5]
         return flow, mask
 
 class IFNet(nn.Module):
     def __init__(self, scale=1, ensemble=False):
         super(IFNet, self).__init__()
```

## vsrife/IFNet_HDv3_v4_4.py

```diff
@@ -1,14 +1,13 @@
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
+from .interpolate import interpolate
 from .warplayer import warp
 
-torch.fx.wrap('warp')
 
 def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):
     return nn.Sequential(
         nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
                   padding=padding, dilation=dilation, bias=True),
         nn.LeakyReLU(0.2, True)
     )
@@ -29,22 +28,22 @@
             conv(c, c),
             conv(c, c),
             conv(c, c),
         )
         self.lastconv = nn.ConvTranspose2d(c, 5, 4, 2, 1)
 
     def forward(self, x, flow=None, scale=1):
-        x = F.interpolate(x, scale_factor= 1. / scale, mode="bilinear")
+        x = interpolate(x, scale_factor= 1. / scale, mode="bilinear")
         if flow is not None:
-            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") * 1. / scale
+            flow = interpolate(flow, scale_factor= 1. / scale, mode="bilinear") / scale
             x = torch.cat((x, flow), 1)
         feat = self.conv0(x)
         feat = self.convblock(feat)
         tmp = self.lastconv(feat)
-        tmp = F.interpolate(tmp, scale_factor=scale*2, mode="bilinear")
+        tmp = interpolate(tmp, scale_factor=scale*2, mode="bilinear")
         flow = tmp[:, :4] * scale * 2
         mask = tmp[:, 4:5]
         return flow, mask
 
 class IFNet(nn.Module):
     def __init__(self, scale=1, ensemble=False):
         super(IFNet, self).__init__()
```

## vsrife/IFNet_HDv3_v4_5.py

```diff
@@ -1,35 +1,21 @@
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
+from .interpolate import interpolate
 from .warplayer import warp
 
-torch.fx.wrap('warp')
 
 def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):
     return nn.Sequential(
         nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
                   padding=padding, dilation=dilation, bias=True),
         nn.LeakyReLU(0.2, True)
     )
 
-class MyPixelShuffle(nn.Module):
-    def __init__(self, upscale_factor):
-        super(MyPixelShuffle, self).__init__()
-        self.upscale_factor = upscale_factor
-
-    def forward(self, input):
-        b, c, hh, hw = input.size()
-        out_channel = c // (self.upscale_factor**2)
-        h = hh * self.upscale_factor
-        w = hw * self.upscale_factor
-        x_view = input.view(b, out_channel, self.upscale_factor, self.upscale_factor, hh, hw)
-        return x_view.permute(0, 1, 4, 2, 5, 3).reshape(b, out_channel, h, w)
-
 class ResConv(nn.Module):
     def __init__(self, c, dilation=1):
         super(ResConv, self).__init__()
         self.conv = nn.Conv2d(c, c, 3, 1, dilation, dilation=dilation, groups=1\
 )
         self.beta = nn.Parameter(torch.ones((1, c, 1, 1)), requires_grad=True)
         self.relu = nn.LeakyReLU(0.2, True)
@@ -52,26 +38,26 @@
             ResConv(c),
             ResConv(c),
             ResConv(c),
             ResConv(c),
         )
         self.lastconv = nn.Sequential(
             nn.ConvTranspose2d(c, 4*5, 4, 2, 1),
-            MyPixelShuffle(2)
+            nn.PixelShuffle(2)
         )
 
     def forward(self, x, flow=None, scale=1):
-        x = F.interpolate(x, scale_factor= 1. / scale, mode="bilinear")
+        x = interpolate(x, scale_factor= 1. / scale, mode="bilinear")
         if flow is not None:
-            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") * 1. / scale
+            flow = interpolate(flow, scale_factor= 1. / scale, mode="bilinear") / scale
             x = torch.cat((x, flow), 1)
         feat = self.conv0(x)
         feat = self.convblock(feat)
         tmp = self.lastconv(feat)
-        tmp = F.interpolate(tmp, scale_factor=scale, mode="bilinear")
+        tmp = interpolate(tmp, scale_factor=scale, mode="bilinear")
         flow = tmp[:, :4] * scale
         mask = tmp[:, 4:5]
         return flow, mask
 
 class IFNet(nn.Module):
     def __init__(self, scale=1, ensemble=False):
         super(IFNet, self).__init__()
```

## vsrife/IFNet_HDv3_v4_6.py

```diff
@@ -1,35 +1,21 @@
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
+from .interpolate import interpolate
 from .warplayer import warp
 
-torch.fx.wrap('warp')
 
 def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):
     return nn.Sequential(
         nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
                   padding=padding, dilation=dilation, bias=True),
         nn.LeakyReLU(0.2, True)
     )
 
-class MyPixelShuffle(nn.Module):
-    def __init__(self, upscale_factor):
-        super(MyPixelShuffle, self).__init__()
-        self.upscale_factor = upscale_factor
-
-    def forward(self, input):
-        b, c, hh, hw = input.size()
-        out_channel = c // (self.upscale_factor**2)
-        h = hh * self.upscale_factor
-        w = hw * self.upscale_factor
-        x_view = input.view(b, out_channel, self.upscale_factor, self.upscale_factor, hh, hw)
-        return x_view.permute(0, 1, 4, 2, 5, 3).reshape(b, out_channel, h, w)
-
 class ResConv(nn.Module):
     def __init__(self, c, dilation=1):
         super(ResConv, self).__init__()
         self.conv = nn.Conv2d(c, c, 3, 1, dilation, dilation=dilation, groups=1\
 )
         self.beta = nn.Parameter(torch.ones((1, c, 1, 1)), requires_grad=True)
         self.relu = nn.LeakyReLU(0.2, True)
@@ -52,26 +38,26 @@
             ResConv(c),
             ResConv(c),
             ResConv(c),
             ResConv(c),
         )
         self.lastconv = nn.Sequential(
             nn.ConvTranspose2d(c, 4*6, 4, 2, 1),
-            MyPixelShuffle(2)
+            nn.PixelShuffle(2)
         )
 
     def forward(self, x, flow=None, scale=1):
-        x = F.interpolate(x, scale_factor= 1. / scale, mode="bilinear")
+        x = interpolate(x, scale_factor= 1. / scale, mode="bilinear")
         if flow is not None:
-            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") * 1. / scale
+            flow = interpolate(flow, scale_factor= 1. / scale, mode="bilinear") / scale
             x = torch.cat((x, flow), 1)
         feat = self.conv0(x)
         feat = self.convblock(feat)
         tmp = self.lastconv(feat)
-        tmp = F.interpolate(tmp, scale_factor=scale, mode="bilinear")
+        tmp = interpolate(tmp, scale_factor=scale, mode="bilinear")
         flow = tmp[:, :4] * scale
         mask = tmp[:, 4:5]
         return flow, mask
 
 class IFNet(nn.Module):
     def __init__(self, scale=1, ensemble=False):
         super(IFNet, self).__init__()
```

## vsrife/IFNet_HDv3_v4_7.py

```diff
@@ -1,35 +1,21 @@
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
+from .interpolate import interpolate
 from .warplayer import warp
 
-torch.fx.wrap('warp')
 
 def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):
     return nn.Sequential(
         nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
                   padding=padding, dilation=dilation, bias=True),
         nn.LeakyReLU(0.2, True)
     )
 
-class MyPixelShuffle(nn.Module):
-    def __init__(self, upscale_factor):
-        super(MyPixelShuffle, self).__init__()
-        self.upscale_factor = upscale_factor
-
-    def forward(self, input):
-        b, c, hh, hw = input.size()
-        out_channel = c // (self.upscale_factor**2)
-        h = hh * self.upscale_factor
-        w = hw * self.upscale_factor
-        x_view = input.view(b, out_channel, self.upscale_factor, self.upscale_factor, hh, hw)
-        return x_view.permute(0, 1, 4, 2, 5, 3).reshape(b, out_channel, h, w)
-
 class ResConv(nn.Module):
     def __init__(self, c, dilation=1):
         super(ResConv, self).__init__()
         self.conv = nn.Conv2d(c, c, 3, 1, dilation, dilation=dilation, groups=1\
 )
         self.beta = nn.Parameter(torch.ones((1, c, 1, 1)), requires_grad=True)
         self.relu = nn.LeakyReLU(0.2, True)
@@ -52,26 +38,26 @@
             ResConv(c),
             ResConv(c),
             ResConv(c),
             ResConv(c),
         )
         self.lastconv = nn.Sequential(
             nn.ConvTranspose2d(c, 4*6, 4, 2, 1),
-            MyPixelShuffle(2)
+            nn.PixelShuffle(2)
         )
 
     def forward(self, x, flow=None, scale=1):
-        x = F.interpolate(x, scale_factor= 1. / scale, mode="bilinear")
+        x = interpolate(x, scale_factor= 1. / scale, mode="bilinear")
         if flow is not None:
-            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") * 1. / scale
+            flow = interpolate(flow, scale_factor= 1. / scale, mode="bilinear") / scale
             x = torch.cat((x, flow), 1)
         feat = self.conv0(x)
         feat = self.convblock(feat)
         tmp = self.lastconv(feat)
-        tmp = F.interpolate(tmp, scale_factor=scale, mode="bilinear")
+        tmp = interpolate(tmp, scale_factor=scale, mode="bilinear")
         flow = tmp[:, :4] * scale
         mask = tmp[:, 4:5]
         return flow, mask
 
 class IFNet(nn.Module):
     def __init__(self, scale=1, ensemble=False):
         super(IFNet, self).__init__()
@@ -80,14 +66,15 @@
         self.block2 = IFBlock(8+4+8, c=96)
         self.block3 = IFBlock(8+4+8, c=64)
         self.encode = nn.Sequential(
             nn.Conv2d(3, 16, 3, 2, 1),
             nn.ConvTranspose2d(16, 4, 4, 2, 1)
         )
         self.scale_list = [8/scale, 4/scale, 2/scale, 1/scale]
+        self.ensemble = ensemble
 
     def forward(self, img0, img1, timestep):
         f0 = self.encode(img0[:, :3])
         f1 = self.encode(img1[:, :3])
         flow_list = []
         merged = []
         mask_list = []
@@ -95,16 +82,28 @@
         warped_img1 = img1
         flow = None
         mask = None
         block = [self.block0, self.block1, self.block2, self.block3]
         for i in range(4):
             if flow is None:
                 flow, mask = block[i](torch.cat((img0[:, :3], img1[:, :3], f0, f1, timestep), 1), None, scale=self.scale_list[i])
+                if self.ensemble:
+                    f_, m_ = block[i](torch.cat((img1[:, :3], img0[:, :3], f1, f0, 1-timestep), 1), None, scale=self.scale_list[i])
+                    flow = (flow + torch.cat((f_[:, 2:4], f_[:, :2]), 1)) / 2
+                    mask = (mask + (-m_)) / 2
             else:
-                fd, mask = block[i](torch.cat((warped_img0[:, :3], warped_img1[:, :3], warp(f0, flow[:, :2]), warp(f1, flow[:, 2:4]), timestep, mask), 1), flow, scale=self.scale_list[i])
+                wf0 = warp(f0, flow[:, :2])
+                wf1 = warp(f1, flow[:, 2:4])
+                fd, m0 = block[i](torch.cat((warped_img0[:, :3], warped_img1[:, :3], wf0, wf1, timestep, mask), 1), flow, scale=self.scale_list[i])
+                if self.ensemble:
+                    f_, m_ = block[i](torch.cat((warped_img1[:, :3], warped_img0[:, :3], wf1, wf0, 1-timestep, -mask), 1), torch.cat((flow[:, 2:4], flow[:, :2]), 1), scale=self.scale_list[i])
+                    fd = (fd + torch.cat((f_[:, 2:4], f_[:, :2]), 1)) / 2
+                    mask = (m0 + (-m_)) / 2
+                else:
+                    mask = m0
                 flow = flow + fd
             mask_list.append(mask)
             flow_list.append(flow)
             warped_img0 = warp(img0, flow[:, :2])
             warped_img1 = warp(img1, flow[:, 2:4])
             merged.append((warped_img0, warped_img1))
         mask = torch.sigmoid(mask)
```

## vsrife/IFNet_HDv3_v4_8.py

```diff
@@ -1,35 +1,21 @@
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
+from .interpolate import interpolate
 from .warplayer import warp
 
-torch.fx.wrap('warp')
 
 def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):
     return nn.Sequential(
         nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
                   padding=padding, dilation=dilation, bias=True),
         nn.LeakyReLU(0.2, True)
     )
 
-class MyPixelShuffle(nn.Module):
-    def __init__(self, upscale_factor):
-        super(MyPixelShuffle, self).__init__()
-        self.upscale_factor = upscale_factor
-
-    def forward(self, input):
-        b, c, hh, hw = input.size()
-        out_channel = c // (self.upscale_factor**2)
-        h = hh * self.upscale_factor
-        w = hw * self.upscale_factor
-        x_view = input.view(b, out_channel, self.upscale_factor, self.upscale_factor, hh, hw)
-        return x_view.permute(0, 1, 4, 2, 5, 3).reshape(b, out_channel, h, w)
-
 class ResConv(nn.Module):
     def __init__(self, c, dilation=1):
         super(ResConv, self).__init__()
         self.conv = nn.Conv2d(c, c, 3, 1, dilation, dilation=dilation, groups=1\
 )
         self.beta = nn.Parameter(torch.ones((1, c, 1, 1)), requires_grad=True)
         self.relu = nn.LeakyReLU(0.2, True)
@@ -52,26 +38,26 @@
             ResConv(c),
             ResConv(c),
             ResConv(c),
             ResConv(c),
         )
         self.lastconv = nn.Sequential(
             nn.ConvTranspose2d(c, 4*6, 4, 2, 1),
-            MyPixelShuffle(2)
+            nn.PixelShuffle(2)
         )
 
     def forward(self, x, flow=None, scale=1):
-        x = F.interpolate(x, scale_factor= 1. / scale, mode="bilinear")
+        x = interpolate(x, scale_factor= 1. / scale, mode="bilinear")
         if flow is not None:
-            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") * 1. / scale
+            flow = interpolate(flow, scale_factor= 1. / scale, mode="bilinear") / scale
             x = torch.cat((x, flow), 1)
         feat = self.conv0(x)
         feat = self.convblock(feat)
         tmp = self.lastconv(feat)
-        tmp = F.interpolate(tmp, scale_factor=scale, mode="bilinear")
+        tmp = interpolate(tmp, scale_factor=scale, mode="bilinear")
         flow = tmp[:, :4] * scale
         mask = tmp[:, 4:5]
         return flow, mask
 
 class IFNet(nn.Module):
     def __init__(self, scale=1, ensemble=False):
         super(IFNet, self).__init__()
@@ -80,14 +66,15 @@
         self.block2 = IFBlock(8+4+8, c=96)
         self.block3 = IFBlock(8+4+8, c=64)
         self.encode = nn.Sequential(
             nn.Conv2d(3, 16, 3, 2, 1),
             nn.ConvTranspose2d(16, 4, 4, 2, 1)
         )
         self.scale_list = [8/scale, 4/scale, 2/scale, 1/scale]
+        self.ensemble = ensemble
 
     def forward(self, img0, img1, timestep):
         f0 = self.encode(img0[:, :3])
         f1 = self.encode(img1[:, :3])
         flow_list = []
         merged = []
         mask_list = []
@@ -95,16 +82,28 @@
         warped_img1 = img1
         flow = None
         mask = None
         block = [self.block0, self.block1, self.block2, self.block3]
         for i in range(4):
             if flow is None:
                 flow, mask = block[i](torch.cat((img0[:, :3], img1[:, :3], f0, f1, timestep), 1), None, scale=self.scale_list[i])
+                if self.ensemble:
+                    f_, m_ = block[i](torch.cat((img1[:, :3], img0[:, :3], f1, f0, 1-timestep), 1), None, scale=self.scale_list[i])
+                    flow = (flow + torch.cat((f_[:, 2:4], f_[:, :2]), 1)) / 2
+                    mask = (mask + (-m_)) / 2
             else:
-                fd, mask = block[i](torch.cat((warped_img0[:, :3], warped_img1[:, :3], warp(f0, flow[:, :2]), warp(f1, flow[:, 2:4]), timestep, mask), 1), flow, scale=self.scale_list[i])
+                wf0 = warp(f0, flow[:, :2])
+                wf1 = warp(f1, flow[:, 2:4])
+                fd, m0 = block[i](torch.cat((warped_img0[:, :3], warped_img1[:, :3], wf0, wf1, timestep, mask), 1), flow, scale=self.scale_list[i])
+                if self.ensemble:
+                    f_, m_ = block[i](torch.cat((warped_img1[:, :3], warped_img0[:, :3], wf1, wf0, 1-timestep, -mask), 1), torch.cat((flow[:, 2:4], flow[:, :2]), 1), scale=self.scale_list[i])
+                    fd = (fd + torch.cat((f_[:, 2:4], f_[:, :2]), 1)) / 2
+                    mask = (m0 + (-m_)) / 2
+                else:
+                    mask = m0
                 flow = flow + fd
             mask_list.append(mask)
             flow_list.append(flow)
             warped_img0 = warp(img0, flow[:, :2])
             warped_img1 = warp(img1, flow[:, 2:4])
             merged.append((warped_img0, warped_img1))
         mask = torch.sigmoid(mask)
```

## vsrife/IFNet_HDv3_v4_9.py

```diff
@@ -1,35 +1,21 @@
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
+from .interpolate import interpolate
 from .warplayer import warp
 
-torch.fx.wrap('warp')
 
 def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):
     return nn.Sequential(
         nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
                   padding=padding, dilation=dilation, bias=True),
         nn.LeakyReLU(0.2, True)
     )
 
-class MyPixelShuffle(nn.Module):
-    def __init__(self, upscale_factor):
-        super(MyPixelShuffle, self).__init__()
-        self.upscale_factor = upscale_factor
-
-    def forward(self, input):
-        b, c, hh, hw = input.size()
-        out_channel = c // (self.upscale_factor**2)
-        h = hh * self.upscale_factor
-        w = hw * self.upscale_factor
-        x_view = input.view(b, out_channel, self.upscale_factor, self.upscale_factor, hh, hw)
-        return x_view.permute(0, 1, 4, 2, 5, 3).reshape(b, out_channel, h, w)
-
 class ResConv(nn.Module):
     def __init__(self, c, dilation=1):
         super(ResConv, self).__init__()
         self.conv = nn.Conv2d(c, c, 3, 1, dilation, dilation=dilation, groups=1\
 )
         self.beta = nn.Parameter(torch.ones((1, c, 1, 1)), requires_grad=True)
         self.relu = nn.LeakyReLU(0.2, True)
@@ -52,26 +38,26 @@
             ResConv(c),
             ResConv(c),
             ResConv(c),
             ResConv(c),
         )
         self.lastconv = nn.Sequential(
             nn.ConvTranspose2d(c, 4*6, 4, 2, 1),
-            MyPixelShuffle(2)
+            nn.PixelShuffle(2)
         )
 
     def forward(self, x, flow=None, scale=1):
-        x = F.interpolate(x, scale_factor= 1. / scale, mode="bilinear")
+        x = interpolate(x, scale_factor= 1. / scale, mode="bilinear")
         if flow is not None:
-            flow = F.interpolate(flow, scale_factor= 1. / scale, mode="bilinear") * 1. / scale
+            flow = interpolate(flow, scale_factor= 1. / scale, mode="bilinear") / scale
             x = torch.cat((x, flow), 1)
         feat = self.conv0(x)
         feat = self.convblock(feat)
         tmp = self.lastconv(feat)
-        tmp = F.interpolate(tmp, scale_factor=scale, mode="bilinear")
+        tmp = interpolate(tmp, scale_factor=scale, mode="bilinear")
         flow = tmp[:, :4] * scale
         mask = tmp[:, 4:5]
         return flow, mask
 
 class IFNet(nn.Module):
     def __init__(self, scale=1, ensemble=False):
         super(IFNet, self).__init__()
@@ -80,14 +66,15 @@
         self.block2 = IFBlock(8+4+8, c=96)
         self.block3 = IFBlock(8+4+8, c=64)
         self.encode = nn.Sequential(
             nn.Conv2d(3, 16, 3, 2, 1),
             nn.ConvTranspose2d(16, 4, 4, 2, 1)
         )
         self.scale_list = [8/scale, 4/scale, 2/scale, 1/scale]
+        self.ensemble = ensemble
 
     def forward(self, img0, img1, timestep):
         f0 = self.encode(img0[:, :3])
         f1 = self.encode(img1[:, :3])
         flow_list = []
         merged = []
         mask_list = []
@@ -95,16 +82,28 @@
         warped_img1 = img1
         flow = None
         mask = None
         block = [self.block0, self.block1, self.block2, self.block3]
         for i in range(4):
             if flow is None:
                 flow, mask = block[i](torch.cat((img0[:, :3], img1[:, :3], f0, f1, timestep), 1), None, scale=self.scale_list[i])
+                if self.ensemble:
+                    f_, m_ = block[i](torch.cat((img1[:, :3], img0[:, :3], f1, f0, 1-timestep), 1), None, scale=self.scale_list[i])
+                    flow = (flow + torch.cat((f_[:, 2:4], f_[:, :2]), 1)) / 2
+                    mask = (mask + (-m_)) / 2
             else:
-                fd, mask = block[i](torch.cat((warped_img0[:, :3], warped_img1[:, :3], warp(f0, flow[:, :2]), warp(f1, flow[:, 2:4]), timestep, mask), 1), flow, scale=self.scale_list[i])
+                wf0 = warp(f0, flow[:, :2])
+                wf1 = warp(f1, flow[:, 2:4])
+                fd, m0 = block[i](torch.cat((warped_img0[:, :3], warped_img1[:, :3], wf0, wf1, timestep, mask), 1), flow, scale=self.scale_list[i])
+                if self.ensemble:
+                    f_, m_ = block[i](torch.cat((warped_img1[:, :3], warped_img0[:, :3], wf1, wf0, 1-timestep, -mask), 1), torch.cat((flow[:, 2:4], flow[:, :2]), 1), scale=self.scale_list[i])
+                    fd = (fd + torch.cat((f_[:, 2:4], f_[:, :2]), 1)) / 2
+                    mask = (m0 + (-m_)) / 2
+                else:
+                    mask = m0
                 flow = flow + fd
             mask_list.append(mask)
             flow_list.append(flow)
             warped_img0 = warp(img0, flow[:, :2])
             warped_img1 = warp(img1, flow[:, 2:4])
             merged.append((warped_img0, warped_img1))
         mask = torch.sigmoid(mask)
```

## vsrife/__init__.py

```diff
@@ -1,73 +1,115 @@
 from __future__ import annotations
 
+import math
 import os
+import warnings
 from fractions import Fraction
 from threading import Lock
 
 import numpy as np
-import tensorrt
 import torch
 import torch.nn.functional as F
 import vapoursynth as vs
-from torch_tensorrt.fx import LowerSetting
-from torch_tensorrt.fx.lower import Lowerer
-from torch_tensorrt.fx.utils import LowerPrecision
 
-__version__ = "4.2.0"
+__version__ = "5.0.0"
 
 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
 
+warnings.filterwarnings("ignore", "At pre-dispatch tracing")
+warnings.filterwarnings("ignore", "Attempted to insert a get_attr Node with no underlying reference")
+warnings.filterwarnings("ignore", "Node _run_on_acc_0_engine target _run_on_acc_0_engine _run_on_acc_0_engine of")
+warnings.filterwarnings("ignore", "The given NumPy array is not writable")
+
 model_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "models")
 
+models = [
+    "4.0",
+    "4.1",
+    "4.2",
+    "4.3",
+    "4.4",
+    "4.5",
+    "4.6",
+    "4.7",
+    "4.8",
+    "4.9",
+    "4.10",
+    "4.11",
+    "4.12",
+    "4.12.lite",
+    "4.13",
+    "4.13.lite",
+    "4.14",
+    "4.14.lite",
+    "4.15",
+    "4.15.lite",
+    "4.16.lite",
+]
+
+models_str = ""
+for model in models:
+    models_str += "'" + model + "', "
+models_str = models_str[:-2]
+
 
 @torch.inference_mode()
 def rife(
     clip: vs.VideoNode,
-    device_index: int | None = None,
+    device_index: int = 0,
     num_streams: int = 2,
-    trt: bool = False,
-    trt_max_workspace_size: int = 1 << 30,
-    trt_cache_path: str = model_dir,
-    model: str = "4.6",
+    model: str = "4.15.lite",
     factor_num: int = 2,
     factor_den: int = 1,
     fps_num: int | None = None,
     fps_den: int | None = None,
     scale: float = 1.0,
     ensemble: bool = False,
     sc: bool = True,
     sc_threshold: float | None = None,
+    trt: bool = False,
+    trt_debug: bool = False,
+    trt_workspace_size: int = 0,
+    trt_max_aux_streams: int | None = None,
+    trt_optimization_level: int | None = None,
+    trt_cache_dir: str = model_dir,
 ) -> vs.VideoNode:
     """Real-Time Intermediate Flow Estimation for Video Frame Interpolation
 
     :param clip:                    Clip to process. Only RGBH and RGBS formats are supported.
                                     RGBH performs inference in FP16 mode while RGBS performs inference in FP32 mode.
     :param device_index:            Device ordinal of the GPU.
     :param num_streams:             Number of CUDA streams to enqueue the kernels.
-    :param trt:                     Use TensorRT for high-performance inference.
-                                    Not supported for '4.0' and '4.1' models.
-    :param trt_max_workspace_size:  Maximum workspace size for TensorRT engine.
-    :param trt_cache_path:          Path for TensorRT engine file. Engine will be cached when it's built for the first
-                                    time. Note each engine is created for specific settings such as model path/name,
-                                    precision, workspace etc, and specific GPUs and it's not portable.
-    :param model:                   Model version to use. Must be '4.0', '4.1', '4.2', '4.3', '4.4', '4.5', '4.6',
-                                    '4.7', '4.8', '4.9', '4.10', or '4.11'.
+    :param model:                   Model to use.
     :param factor_num:              Numerator of factor for target frame rate.
-                                    For example `factor_num=5, factor_den=2` will multiply the frame rate by 2.5.
     :param factor_den:              Denominator of factor for target frame rate.
-    :param fps_num:                 Numerator of target frame rate. Override `factor_num` and `factor_den` if specified.
+                                    For example `factor_num=5, factor_den=2` will multiply the frame rate by 2.5.
+    :param fps_num:                 Numerator of target frame rate.
     :param fps_den:                 Denominator of target frame rate.
+                                    Override `factor_num` and `factor_den` if specified.
     :param scale:                   Control the process resolution for optical flow model. Try scale=0.5 for 4K video.
                                     Must be 0.25, 0.5, 1.0, 2.0, or 4.0.
     :param ensemble:                Smooth predictions in areas where the estimation is uncertain.
-                                    Not supported for models after '4.6'.
     :param sc:                      Avoid interpolating frames over scene changes.
     :param sc_threshold:            Threshold for scene change detection. Must be between 0.0 and 1.0.
-                                    Leave it None if the clip already has _SceneChangeNext properly set.
+                                    Leave the argument as None if the frames already have _SceneChangeNext property set.
+    :param trt:                     Use TensorRT for high-performance inference.
+                                    Not supported for '4.0' and '4.1' models.
+    :param trt_debug:               Print out verbose debugging information.
+    :param trt_workspace_size:      Size constraints of workspace memory pool.
+    :param trt_max_aux_streams:     Maximum number of auxiliary streams per inference stream that TRT is allowed to use
+                                    to run kernels in parallel if the network contains ops that can run in parallel,
+                                    with the cost of more memory usage. Set this to 0 for optimal memory usage.
+                                    (default = using heuristics)
+    :param trt_optimization_level:  Builder optimization level. Higher level allows TensorRT to spend more building time
+                                    for more optimization options. Valid values include integers from 0 to the maximum
+                                    optimization level, which is currently 5. (default is 3)
+    :param trt_cache_dir:           Directory for TensorRT engine file. Engine will be cached when it's built for the
+                                    first time. Note each engine is created for specific settings such as model
+                                    path/name, precision, workspace etc, and specific GPUs and it's not portable.
     """
     if not isinstance(clip, vs.VideoNode):
         raise vs.Error("rife: this is not a clip")
 
     if clip.format.id not in [vs.RGBH, vs.RGBS]:
         raise vs.Error("rife: only RGBH and RGBS formats are supported")
 
@@ -76,19 +118,16 @@
 
     if not torch.cuda.is_available():
         raise vs.Error("rife: CUDA is not available")
 
     if num_streams < 1:
         raise vs.Error("rife: num_streams must be at least 1")
 
-    if model not in ["4.0", "4.1", "4.2", "4.3", "4.4", "4.5", "4.6", "4.7", "4.8", "4.9", "4.10", "4.11"]:
-        raise vs.Error(
-            "rife: model must be '4.0', '4.1', '4.2', '4.3', '4.4', '4.5', '4.6', '4.7', '4.8', '4.9', '4.10', "
-            + "or '4.11'"
-        )
+    if model not in models:
+        raise vs.Error(f"rife: model must be {models_str}")
 
     if factor_num < 1:
         raise vs.Error("rife: factor_num must be at least 1")
 
     if factor_den < 1:
         raise vs.Error("rife: factor_den must be at least 1")
 
@@ -106,16 +145,15 @@
 
     if os.path.getsize(os.path.join(model_dir, "flownet_v4.0.pkl")) == 0:
         raise vs.Error("rife: model files have not been downloaded. run 'python -m vsrife' first")
 
     torch.set_float32_matmul_precision("high")
 
     fp16 = clip.format.bits_per_sample == 16
-    if fp16:
-        torch.set_default_dtype(torch.half)
+    dtype = torch.half if fp16 else torch.float
 
     device = torch.device("cuda", device_index)
 
     stream = [torch.cuda.Stream(device=device) for _ in range(num_streams)]
     stream_lock = [Lock() for _ in range(num_streams)]
 
     match model:
@@ -131,104 +169,110 @@
             from .IFNet_HDv3_v4_4 import IFNet
         case "4.5":
             from .IFNet_HDv3_v4_5 import IFNet
         case "4.6":
             from .IFNet_HDv3_v4_6 import IFNet
         case "4.7":
             from .IFNet_HDv3_v4_7 import IFNet
-
-            if ensemble:
-                raise vs.Error("rife: ensemble not supported")
         case "4.8":
             from .IFNet_HDv3_v4_8 import IFNet
-
-            if ensemble:
-                raise vs.Error("rife: ensemble not supported")
         case "4.9":
             from .IFNet_HDv3_v4_9 import IFNet
-
-            if ensemble:
-                raise vs.Error("rife: ensemble not supported")
         case "4.10":
             from .IFNet_HDv3_v4_10 import IFNet
-
-            if ensemble:
-                raise vs.Error("rife: ensemble not supported")
         case "4.11":
             from .IFNet_HDv3_v4_11 import IFNet
-
-            if ensemble:
-                raise vs.Error("rife: ensemble not supported")
+        case "4.12":
+            from .IFNet_HDv3_v4_12 import IFNet
+        case "4.12.lite":
+            from .IFNet_HDv3_v4_12_lite import IFNet
+        case "4.13":
+            from .IFNet_HDv3_v4_13 import IFNet
+        case "4.13.lite":
+            from .IFNet_HDv3_v4_13_lite import IFNet
+        case "4.14":
+            from .IFNet_HDv3_v4_14 import IFNet
+        case "4.14.lite":
+            from .IFNet_HDv3_v4_14_lite import IFNet
+        case "4.15":
+            from .IFNet_HDv3_v4_15 import IFNet
+        case "4.15.lite":
+            from .IFNet_HDv3_v4_15_lite import IFNet
+        case "4.16.lite":
+            from .IFNet_HDv3_v4_16_lite import IFNet
 
     model_name = f"flownet_v{model}.pkl"
 
-    state_dict = torch.load(os.path.join(model_dir, model_name), map_location="cpu")
+    state_dict = torch.load(os.path.join(model_dir, model_name), map_location=device)
     state_dict = {k.replace("module.", ""): v for k, v in state_dict.items() if "module." in k}
 
     flownet = IFNet(scale, ensemble)
     flownet.load_state_dict(state_dict, strict=False)
-    flownet.eval().to(device, memory_format=torch.channels_last)
+    flownet.eval().to(device)
+    if fp16:
+        flownet.half()
+
+    if fps_num is not None and fps_den is not None:
+        factor = Fraction(fps_num, fps_den) / clip.fps
+        factor_num, factor_den = factor.as_integer_ratio()
 
     w = clip.width
     h = clip.height
     tmp = max(128, int(128 / scale))
-    pw = ((w - 1) // tmp + 1) * tmp
-    ph = ((h - 1) // tmp + 1) * tmp
+    pw = math.ceil(w / tmp) * tmp
+    ph = math.ceil(h / tmp) * tmp
     padding = (0, pw - w, 0, ph - h)
 
+    if sc_threshold is not None:
+        clip = sc_detect(clip, sc_threshold)
+
     if trt:
-        device_name = torch.cuda.get_device_name(device)
-        trt_version = tensorrt.__version__
-        dimensions = f"{pw}x{ph}"
-        precision = "fp16" if fp16 else "fp32"
+        import tensorrt
+        import torch_tensorrt
+
         trt_engine_path = os.path.join(
-            os.path.realpath(trt_cache_path),
+            os.path.realpath(trt_cache_dir),
             (
                 f"{model_name}"
-                + f"_{device_name}"
-                + f"_trt-{trt_version}"
-                + f"_{dimensions}"
-                + f"_{precision}"
-                + f"_workspace-{trt_max_workspace_size}"
+                + f"_{pw}x{ph}"
+                + f"_{"fp16" if fp16 else "fp32"}"
                 + f"_scale-{scale}"
                 + f"_ensemble-{ensemble}"
-                + ".pt"
+                + f"_{torch.cuda.get_device_name(device)}"
+                + f"_trt-{tensorrt.__version__}"
+                + (f"_workspace-{trt_workspace_size}" if trt_workspace_size > 0 else "")
+                + (f"_aux-{trt_max_aux_streams}" if trt_max_aux_streams is not None else "")
+                + (f"_level-{trt_optimization_level}" if trt_optimization_level is not None else "")
+                + ".ep"
             ),
         )
 
         if not os.path.isfile(trt_engine_path):
-            lower_setting = LowerSetting(
-                lower_precision=LowerPrecision.FP16 if fp16 else LowerPrecision.FP32,
-                min_acc_module_size=1,
-                max_workspace_size=trt_max_workspace_size,
-                dynamic_batch=False,
-                tactic_sources=1 << int(tensorrt.TacticSource.EDGE_MASK_CONVOLUTIONS)
-                | 1 << int(tensorrt.TacticSource.JIT_CONVOLUTIONS),
-            )
-            lowerer = Lowerer.create(lower_setting=lower_setting)
-            flownet = lowerer(
+            inputs = [
+                torch.zeros((1, 3, ph, pw), dtype=dtype, device=device),
+                torch.zeros((1, 3, ph, pw), dtype=dtype, device=device),
+                torch.zeros((1, 1, ph, pw), dtype=dtype, device=device),
+            ]
+
+            flownet = torch_tensorrt.compile(
                 flownet,
-                [
-                    torch.zeros((1, 3, ph, pw), device=device).to(memory_format=torch.channels_last),
-                    torch.zeros((1, 3, ph, pw), device=device).to(memory_format=torch.channels_last),
-                    torch.zeros((1, 1, ph, pw), device=device).to(memory_format=torch.channels_last),
-                ],
+                ir="dynamo",
+                inputs=inputs,
+                enabled_precisions={dtype},
+                debug=trt_debug,
+                workspace_size=trt_workspace_size,
+                min_block_size=1,
+                max_aux_streams=trt_max_aux_streams,
+                optimization_level=trt_optimization_level,
+                device=device,
             )
-            torch.save(flownet, trt_engine_path)
 
-        del flownet
-        torch.cuda.empty_cache()
-        flownet = [torch.load(trt_engine_path) for _ in range(num_streams)]
-
-    if fps_num is not None and fps_den is not None:
-        factor = Fraction(fps_num, fps_den) / clip.fps
-        factor_num, factor_den = factor.as_integer_ratio()
+            torch_tensorrt.save(flownet, trt_engine_path, inputs=inputs)
 
-    if sc_threshold is not None:
-        clip = sc_detect(clip, sc_threshold)
+        flownet = [torch.export.load(trt_engine_path).module() for _ in range(num_streams)]
 
     index = -1
     index_lock = Lock()
 
     @torch.inference_mode()
     def inference(n: int, f: list[vs.VideoFrame]) -> vs.VideoFrame:
         remainder = n * factor_den % factor_num
@@ -243,16 +287,15 @@
 
         with stream_lock[local_index], torch.cuda.stream(stream[local_index]):
             img0 = frame_to_tensor(f[0], device)
             img1 = frame_to_tensor(f[1], device)
             img0 = F.pad(img0, padding)
             img1 = F.pad(img1, padding)
 
-            timestep = torch.full((1, 1, img0.shape[2], img0.shape[3]), remainder / factor_num, device=device)
-            timestep = timestep.to(memory_format=torch.channels_last)
+            timestep = torch.full((1, 1, ph, pw), remainder / factor_num, dtype=dtype, device=device)
 
             if trt:
                 output = flownet[local_index](img0, img1, timestep)
             else:
                 output = flownet(img0, img1, timestep)
 
             return tensor_to_frame(output[:, :, :h, :w], f[0].copy())
@@ -277,16 +320,19 @@
         return fout
 
     sc_clip = clip.resize.Bicubic(format=vs.GRAY8, matrix_s="709").misc.SCDetect(threshold)
     return clip.std.FrameEval(lambda n: clip.std.ModifyFrame([clip, sc_clip], copy_property), clip_src=[clip, sc_clip])
 
 
 def frame_to_tensor(frame: vs.VideoFrame, device: torch.device) -> torch.Tensor:
-    array = np.stack([np.asarray(frame[plane]) for plane in range(frame.format.num_planes)])
-    return torch.from_numpy(array).unsqueeze(0).to(device, memory_format=torch.channels_last).clamp(0.0, 1.0)
+    return (
+        torch.stack([torch.from_numpy(np.asarray(frame[plane])).to(device) for plane in range(frame.format.num_planes)])
+        .unsqueeze(0)
+        .clamp(0.0, 1.0)
+    )
 
 
 def tensor_to_frame(tensor: torch.Tensor, frame: vs.VideoFrame) -> vs.VideoFrame:
     array = tensor.squeeze(0).detach().cpu().numpy()
     for plane in range(frame.format.num_planes):
-        np.copyto(np.asarray(frame[plane]), array[plane, :, :])
+        np.copyto(np.asarray(frame[plane]), array[plane])
     return frame
```

## vsrife/__main__.py

```diff
@@ -32,10 +32,19 @@
         "flownet_v4.5",
         "flownet_v4.6",
         "flownet_v4.7",
         "flownet_v4.8",
         "flownet_v4.9",
         "flownet_v4.10",
         "flownet_v4.11",
+        "flownet_v4.12",
+        "flownet_v4.12.lite",
+        "flownet_v4.13",
+        "flownet_v4.13.lite",
+        "flownet_v4.14",
+        "flownet_v4.14.lite",
+        "flownet_v4.15",
+        "flownet_v4.15.lite",
+        "flownet_v4.16.lite",
     ]
     for model in models:
         download_model(url + model + ".pkl")
```

## vsrife/warplayer.py

```diff
@@ -1,24 +1,20 @@
 import torch
 
 backwarp_tenGrid = {}
 
 
 def warp(tenInput, tenFlow):
-    orig_dtype = tenInput.dtype
-    tenInput = tenInput.float()
-    tenFlow = tenFlow.float()
-
     k = (str(tenFlow.device), str(tenFlow.size()))
     if k not in backwarp_tenGrid:
-        tenHorizontal = torch.linspace(-1.0, 1.0, tenFlow.shape[3], dtype=torch.float, device=tenInput.device).view(
+        tenHorizontal = torch.linspace(-1.0, 1.0, tenFlow.shape[3], dtype=tenFlow.dtype, device=tenFlow.device).view(
             1, 1, 1, tenFlow.shape[3]).expand(tenFlow.shape[0], -1, tenFlow.shape[2], -1)
-        tenVertical = torch.linspace(-1.0, 1.0, tenFlow.shape[2], dtype=torch.float, device=tenInput.device).view(
+        tenVertical = torch.linspace(-1.0, 1.0, tenFlow.shape[2], dtype=tenFlow.dtype, device=tenFlow.device).view(
             1, 1, tenFlow.shape[2], 1).expand(tenFlow.shape[0], -1, -1, tenFlow.shape[3])
         backwarp_tenGrid[k] = torch.cat(
             [tenHorizontal, tenVertical], 1)
 
     tenFlow = torch.cat([tenFlow[:, 0:1, :, :] / ((tenInput.shape[3] - 1.0) / 2.0),
                          tenFlow[:, 1:2, :, :] / ((tenInput.shape[2] - 1.0) / 2.0)], 1)
 
     g = (backwarp_tenGrid[k] + tenFlow).permute(0, 2, 3, 1)
-    return torch.nn.functional.grid_sample(input=tenInput, grid=g, mode='bilinear', padding_mode='border', align_corners=True).to(orig_dtype)
+    return torch.nn.functional.grid_sample(input=tenInput, grid=g, mode='bilinear', padding_mode='border', align_corners=True)
```

## Comparing `vsrife-4.2.0.dist-info/licenses/LICENSE` & `vsrife-5.0.0.dist-info/licenses/LICENSE`

 * *Files identical despite different names*

